<mediawiki><siteinfo>
    <sitename>Wikipedia</sitename>
    <dbname>enwiki</dbname>
    <base>https://en.wikipedia.org/wiki/Main_Page</base>
    <generator>MediaWiki 1.29.0-wmf.9</generator>
    <case>first-letter</case>
    <namespaces>
      <namespace case="first-letter" key="-2">Media</namespace>
      <namespace case="first-letter" key="-1">Special</namespace>
      <namespace case="first-letter" key="0" />
      <namespace case="first-letter" key="1">Talk</namespace>
      <namespace case="first-letter" key="2">User</namespace>
      <namespace case="first-letter" key="3">User talk</namespace>
      <namespace case="first-letter" key="4">Wikipedia</namespace>
      <namespace case="first-letter" key="5">Wikipedia talk</namespace>
      <namespace case="first-letter" key="6">File</namespace>
      <namespace case="first-letter" key="7">File talk</namespace>
      <namespace case="first-letter" key="8">MediaWiki</namespace>
      <namespace case="first-letter" key="9">MediaWiki talk</namespace>
      <namespace case="first-letter" key="10">Template</namespace>
      <namespace case="first-letter" key="11">Template talk</namespace>
      <namespace case="first-letter" key="12">Help</namespace>
      <namespace case="first-letter" key="13">Help talk</namespace>
      <namespace case="first-letter" key="14">Category</namespace>
      <namespace case="first-letter" key="15">Category talk</namespace>
      <namespace case="first-letter" key="100">Portal</namespace>
      <namespace case="first-letter" key="101">Portal talk</namespace>
      <namespace case="first-letter" key="108">Book</namespace>
      <namespace case="first-letter" key="109">Book talk</namespace>
      <namespace case="first-letter" key="118">Draft</namespace>
      <namespace case="first-letter" key="119">Draft talk</namespace>
      <namespace case="first-letter" key="446">Education Program</namespace>
      <namespace case="first-letter" key="447">Education Program talk</namespace>
      <namespace case="first-letter" key="710">TimedText</namespace>
      <namespace case="first-letter" key="711">TimedText talk</namespace>
      <namespace case="first-letter" key="828">Module</namespace>
      <namespace case="first-letter" key="829">Module talk</namespace>
      <namespace case="first-letter" key="2300">Gadget</namespace>
      <namespace case="first-letter" key="2301">Gadget talk</namespace>
      <namespace case="case-sensitive" key="2302">Gadget definition</namespace>
      <namespace case="case-sensitive" key="2303">Gadget definition talk</namespace>
    </namespaces>
  </siteinfo>
  <page>
    <title>Navigational database</title>
    <ns>0</ns>
    <id>622805</id>
    <revision>
      <id>744575422</id>
      <parentid>696272052</parentid>
      <timestamp>2016-10-16T03:52:47Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* Description */http&amp;rarr;https for [[Google Books]] and [[Google News]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6615" xml:space="preserve">{{Refimprove|date=July 2007}}
A '''navigational database''' is a type of [[database]] in which [[Record (computer science)|records]] or [[Object (computer science)|objects]] are found primarily by following references from other objects. They were a common type of database in the era when data was stored on [[magnetic tape]]; the navigational references told the computer where the next record on the tape was stored, allowing fast-forwarding (and in some cases, reversing) through the records without having to read every record along the way to see if it matched a given criterion.

The introduction of low-cost [[hard drive]]s that provided semi-random access to data led to new models of database storage better suited to these devices. Among these, the [[relational database]] and especially [[SQL]] became the canonical solution from the 1980s through to about 2010. At that time a reappraisal of the entire database market began, the various [[NoSQL]] concepts, which has led to the navigational model being reexamined. Offshoots of the concept, especially the [[graph database]], are finding new uses in modern [[transaction processing]] workloads.

==Description==
Navigational interfaces are usually procedural, though some modern systems like [[XPath]] can be considered to be simultaneously navigational and declarative. 

Navigational access is traditionally associated with the [[network model]] and [[hierarchical model]] of [[database]] interfaces, and some have even acquired set-oriented features.&lt;ref&gt;{{cite book | author = B&#322;a&#380;ewicz, Jacek |author2=Kr&#243;likowski, Zbyszko |author3=Morzy, Tadeusz | title = Handbook on Data Management in Information Systems  | publisher = Springer  | year = 2003  | location =  | page = 18  | url = https://books.google.com/books?id=AvLziHKyuLcC&amp;pg=PA18&amp;dq=%22Navigational+database%22+-wikipedia+network+model+and+hierarchical+model&amp;ie=ISO-8859-1| doi =  | isbn = 3-540-43893-9 }}&lt;/ref&gt; Navigational techniques use "pointers" and "paths" to navigate among data records (also known as "nodes"). This is in contrast to the [[relational model]] (implemented in [[relational database]]s), which strives to use "declarative" or [[logic programming]] techniques that ask the system for ''what'' to fetch instead of ''how'' to navigate to it.  

For example, to give directions to a house, the navigational approach would resemble something like "Get on highway 25 for 8 miles, turn onto Horse Road, left at the red barn, then stop at the 3rd house down the road", whereas the declarative approach would resemble "Visit the green house(s) within the following coordinates...."

Hierarchical models are also considered navigational because one "goes" up (to parent), down (to leaves), and there are "paths", such as the familiar file/folder paths in hierarchical file systems. In general, navigational systems will use combinations of paths and prepositions such as "next", "previous", "first", "last", "up", "down", "owner", etc.

"Paths" are often formed by concatenation of [[Node (computer science)|node]] names or node addresses. Example:

[[File:6n-graf.svg|thumb|250px|Sample database nodes: A labeled graph on 6 vertices and 7 edges. (Numbers are used for illustration purposes only. In practice more meaningful names are often used. Other potential attributes are not shown.)]]

  Node6.Node4.Node5.Node1

Or

  Node6/Node4/Node5/Node1

If there is no link between given nodes, then an error condition is usually triggered with a message such as "Invalid Path".  The path "Node6.Node2.Node1" would be invalid in most systems because there is no direct link between Node 6 and Node 2.

The usage of the term "navigational" allegedly is derived from a statement by [[Charles Bachman]] in which he describes the "programmer as navigator" while accessing his favored type of database.&lt;ref&gt;{{cite web|url=http://portal.acm.org/citation.cfm?id=362534&amp;coll=portal&amp;dl=ACM |title=The programmer as navigator |doi=10.1145/355611.362534 |publisher=Portal.acm.org |accessdate=2012-10-01}}&lt;/ref&gt;

Except for hierarchical file systems (which some consider a form of database), navigational techniques fell out of favor by the 1980s. However, [[object oriented programming]] and [[XML]] have kindled a renewed, but controversial interest in navigational techniques.

Critics of navigational techniques view them as "unstructured spaghetti messes", and liken them to the "[[Goto (command)|goto]]" of pre-[[structured programming]]. In other words, they are allegedly to data organization what goto's were to behavior flow. In this view, relational techniques provide improved discipline and consistency to data organization and usage because of its roots in [[set theory]] and [[predicate calculus]]. 

Some also suggest that navigational database engines are easier to build and take up less memory (RAM) than relational equivalents. However, the existence of relational or relational-based products of the late 1980s that possessed small engines (by today's standards) because they didn't use SQL suggest this is not necessarily the case. Whatever the reason, navigational techniques are still the preferred way to handle smaller-scale structures.

A current example of navigational structuring can be found in the [[Document Object Model]] (DOM) often used in web browsers and closely associated with [[JavaScript]]. The DOM "engine" is essentially a light-weight navigational database. The [[World Wide Web]] itself and Wikipedia could potentially be considered forms of navigational databases, though they focus on human-readable text rather than data (on a large scale, the Web is a network model and on smaller or local scales, such as domain and URL partitioning, it uses hierarchies).  In contrast, the [[Linked Data]] facet of the [[Semantic Web]] is specifically concerned with network-scale [[machine-readable data]], and follows precisely the 'follow your nose' paradigm implied by the navigational idea.

A new kind of navigational databases{{fact|date=August 2015}} has recently{{when|date=August 2015}} emerged, the [[graph databases]]. This category of databases is often included as one of the four family of the [[NoSQL]] databases.

==Examples==
* [[IBM Information Management System]]
* [[IDMS]]

==See also==
* [[CODASYL]]
* [[Graph database]]
* [[Network database]]
* [[Object database]]
* [[Relational database]]

==References==
{{Reflist}}

==External links==
* [http://db-engines.com/en/ranking/navigational+dbms DB-Engines Ranking of Navigational DBMS] by popularity, updated by month


[[Category:Data management]]
[[Category:Types of databases]]</text>
      <sha1>dwbtu66puzi6m18phdehjdncaioni0a</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Concurrency control</title>
    <ns>14</ns>
    <id>1462863</id>
    <revision>
      <id>546502164</id>
      <parentid>481028162</parentid>
      <timestamp>2013-03-23T09:06:21Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor />
      <comment>[[User:Addbot|Bot:]] Migrating 8 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q7285247]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="220" xml:space="preserve">{{Cat main|Concurrency control}}

[[Category:Data management]]
[[Category:Synchronization]]
[[Category:Operating system technology]]
[[Category:Concurrency (computer science)]]
[[Category:Distributed computing problems]]</text>
      <sha1>qs9409ni8e4xzljoornfpvxcg39n7bd</sha1>
    </revision>
  </page>
  <page>
    <title>Match report</title>
    <ns>0</ns>
    <id>2575602</id>
    <revision>
      <id>722501202</id>
      <parentid>720628645</parentid>
      <timestamp>2016-05-28T15:10:35Z</timestamp>
      <contributor>
        <username>Pegship</username>
        <id>355698</id>
      </contributor>
      <minor />
      <comment>stub sort</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1020" xml:space="preserve">{{Unreferenced|date=May 2009}}
In [[metadata]], a '''match report''' is a report that compares two distinct [[data dictionary|data dictionaries]] and creates a list of the [[data element]]s that have been identified as [[Semantic equivalence|semantically equivalent]].

== Use of match reports ==

Match reports are critical for systems that wish to automatically exchange data such as intelligent software agents.  If one computer system is requesting a report from a remote system that uses a distinct data dictionary and all of the data elements on the report manifest are included in the '''match report''' the report request can be executed.

Match reports are useful if data dictionaries use a metadata tagging system such as the [[UDEF]].

==See also==
*[[Data dictionary]]
*[[Data warehouse]]
*[[Metadata]]
*[[Semantic equivalence]]
*[[Universal Data Element Framework]]

[[Category:Knowledge representation]]
[[Category:Data management]]
[[Category:Technical communication]]
[[Category:Metadata]]
{{compu-stub}}</text>
      <sha1>tn7u4sg8ao1oc3xqzn4htei7sg6eh2p</sha1>
    </revision>
  </page>
  <page>
    <title>VMDS</title>
    <ns>0</ns>
    <id>3047078</id>
    <revision>
      <id>729653219</id>
      <parentid>692677501</parentid>
      <timestamp>2016-07-13T17:41:05Z</timestamp>
      <contributor>
        <username>HitroMilanese</username>
        <id>1305296</id>
      </contributor>
      <comment>Added {{[[Template:COI|COI]]}} and {{[[Template:unreferenced|unreferenced]]}} tags to article ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6574" xml:space="preserve">{{COI|date=July 2016}}
{{unreferenced|date=July 2016}}
'''VMDS''' abbreviates the relational database technology called '''Version Managed Data Store''' provided by [[GE Energy]] as part of its [[Smallworld]] technology platform and was designed from the outset to store and analyse the highly complex spatial and topological networks typically used by enterprise utilities such as power distribution and telecommunications.

VMDS was originally introduced in 1990 as has been improved and updated over the years. Its current version is 6.0.

VMDS has been designed as a [[spatial database]]. This gives VMDS a number of distinctive characteristics when compared to conventional attribute only relational databases.

==Distributed server processing==
VMDS is composed of two parts: a simple, highly scalable data block server called '''SWMFS''' (Smallworld Master File Server) and an intelligent client [[Application programming interface|API]] written in [[C (programming language)|C]] and [[Magik programming language|Magik]]. Spatial and attribute data are stored in data blocks that reside in special files called data store files on the server. When the client application requests data it has sufficient intelligence to work out the optimum set of data blocks that are required. This request is then made to SWMFS which returns the data to the client via the network for processing.

This approach is particularly efficient and scalable when dealing with spatial and topological data which tends to flow in larger volumes and require more processing then plain attribute data (for example during a map redraw operation). This approach makes VMDS well suited to enterprise deployment that might involve hundreds or even thousands of concurrent clients.

==Support for long transactions==
Relational databases support [[Database transaction|short transactions]] in which changes to data are relatively small and are brief in terms in duration (the maximum period between the start and the end of a transaction is typically a few seconds or less).

VMDS supports long transactions in which the volume of data involved in the transaction can be substantial and the duration of the transaction can be significant (days, weeks or even months). These types of transaction are common in advanced network applications used by, for example, power distribution utilities.

Due to the time span of a long transaction in this context the amount of change can be significant (not only within the scope of the transaction, but also within the context of the database as a whole). Accordingly, it is likely that the same record might be changed more than once. To cope with this scenario VMDS has inbuilt support for automatically managing such conflicts and allows applications to review changes and accept only those edits that are correct.

==Spatial and topological capabilities==
As well as conventional relational database features such as attribute querying, join fields, triggers and calculated fields, VMDS has numerous spatial and topological capabilities. This allows spatial data such as points, texts, polylines, polygons and raster data to be stored and analysed.

Spatial functions include: find all features within a polygon, calculate the [[Voronoi polygon]]s of a set of sites and perform a [[cluster analysis]] on a set of points.

Vector spatial data such as points, polylines and polygons can be given topological attributes that allow complex networks to be modelled. Network analysis engines are provided to answer questions such as find the shortest path between two nodes or how to optimize a delivery route (the [[travelling salesman problem]]). A topology engine can be configured with a set of rules that define how topological entities interact with each other when new data is added or existing data edited.

==Data abstraction==
In VMDS all data is presented to the application as objects. This is different from many relational databases that present the data as rows from a table or query result using say [[JDBC]]. VMDS provides a data modelling tool and underlying infrastructure as part of the [[Smallworld]] technology platform that allows administrators to associate a table in the database with a Magik exemplar (or class). Magik get and set methods for the Magik exemplar can be automatically generated that expose a table's field (or column). Each VMDS ''row'' manifests itself to the application as an instance of a [[Magik programming language|Magik]] object and is known as an '''RWO''' (or real world object). Tables are known as collections in Smallworld parlance.

  # all_rwos hold all the rwos in the database and is heterogeneous
  all_rwos &lt;&lt; my_application.rwo_set()
 
  # valve_collection holds the valve collection
  valves &lt;&lt; all_rwos.select(:collection, {:valve})
  number_of_valves &lt;&lt; valves.size

Queries are built up using predicate objects:

  # find 'open' valves.
  open_valves &lt;&lt; valves.select(predicate.eq(:operating_status, "open"))
  number_of_open_valves &lt;&lt; open_valves.size

  _for valve _over open_valves.elements()
  _loop
    write(valve.id)
  _endloop

Joins are implemented as methods on the parent RWO. For example a manager might have several employees who report to him:

  # get the employee collection.
  employees &lt;&lt; my_application.database.collection(:gis, :employees)

  # find a manager called 'Steve' and get the first matching element
  steve &lt;&lt; employees.select(predicate.eq(:name, "Steve").and(predicate.eq(:role, "manager")).an_element()

  # display the names of his direct reports. name is a field (or column)
  # on the employee collection (or table)
  _for employee _over steve.direct_reports.elements()
  _loop
     write(employee.name)
  _endloop

Performing a transaction:

  # each key in the hash table corresponds to the name of the field (or column) in
  # the collection (or table)
  valve_data &lt;&lt; hash_table.new_with(
    :asset_id, 57648576,
    :material, "Iron")

  # get the valve collection directly
  valve_collection &lt;&lt; my_application.database.collection(:gis, :valve)

  # create an insert transaction to insert a new valve record into the collection a
  # comment can be provide that describes the transaction
  transaction &lt;&lt; record_transaction.new_insert(valve_collection, valve_data, "Inserted a new valve")
  transaction.run()

==See also==
* [[List of relational database management systems]]
* [[List of object-relational database management systems]]
* [[Spatial database]]
* [[Multiversion concurrency control]]

[[Category:Data management]]
[[Category:GIS software]]</text>
      <sha1>1h1tqqyc5os5dy4jeeqld9rv078calq</sha1>
    </revision>
  </page>
  <page>
    <title>Control break</title>
    <ns>0</ns>
    <id>3584856</id>
    <revision>
      <id>736655742</id>
      <parentid>544248641</parentid>
      <timestamp>2016-08-29T00:10:41Z</timestamp>
      <contributor>
        <username>Peter Flass</username>
        <id>7557079</id>
      </contributor>
      <comment>fix wording, add unreferenced tag</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1160" xml:space="preserve">{{unreferenced|date=August 2016}}
{{dablink|Not to be confused with 'Control-Break' displayed in MS-DOS when cancelling an ongoing task by pressing [[break key|Ctrl+Break]] key combination.}}
In [[computer programming]] a '''control break''' is a change in the value of one of the [[Key field|key]]s on which a file is sorted which requires some extra processing.  For example, with an input file sorted by post code, the number of items found in each postal district might need to be printed on a report, and a heading shown for the next district.  Quite often there is a hierarchy of nested control breaks in a program, e.g. streets within districts within areas, with the need for a grand total at the end. [[Structured programming]] techniques have been developed to ensure correct processing of control breaks in languages such as [[COBOL]] and to ensure that conditions such as empty input files and [[sequence error]]s are handled properly.

With [[fourth generation language]]s such as [[SQL]], the programming language should handle most of the details of control breaks automatically.

[[Category:Conditional constructs]]
[[Category:Data management]]</text>
      <sha1>dgwy3ct9b3hmkf821ncyul5s5qbd0ih</sha1>
    </revision>
  </page>
  <page>
    <title>Distributed database</title>
    <ns>0</ns>
    <id>41054</id>
    <revision>
      <id>762561201</id>
      <parentid>750229994</parentid>
      <timestamp>2017-01-29T15:16:40Z</timestamp>
      <contributor>
        <ip>197.210.29.112</ip>
      </contributor>
      <comment>/* Homogeneous Distributed Databases Management System */Fixed punctuation</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="16643" xml:space="preserve">{{multiple issues|
{{refimprove|date=August 2010}}
{{Cleanup|date=June 2009}}
}}

A '''distributed database''' is a [[database]]  in which [[computer data storage|storage devices]] are not all attached to a common [[Processor (computing)|processor]].&lt;ref&gt;http://www.its.bldrdoc.gov/fs-1037/dir-012/_1750.htm&lt;/ref&gt; It may be stored in multiple [[computers]], located in the same physical location; or may be dispersed over a [[computer network|network]] of interconnected computers. Unlike [[Parallel computing|parallel systems]], in which the processors are tightly coupled and constitute a single database system, a distributed database system consists of loosely coupled sites that share no physical components.

System administrators can distribute collections of data (e.g. in a database) across multiple physical locations. A distributed database can reside on organized [[network servers]] or [[blockchain (database)|decentralized independent computers]] on the [[Internet]], on corporate [[intranets]] or [[extranets]], or on other organization [[Computer network|networks]]. Because they store data across multiple computers, distributed databases may improve performance at [[end-user]] worksites by allowing transactions to be processed on many machines, instead of being limited to one.&lt;ref name="obrien"&gt;
O'Brien, J. &amp; Marakas, G.M.(2008) Management Information Systems (pp. 185-189). New York, NY: McGraw-Hill Irwin&lt;/ref&gt;

Two processes ensure that the distributed databases remain up-to-date and current: [[Replication (computing)|replication]] and [[Data transmission|duplication]].

# Replication involves using specialized software that looks for changes in the distributive database. Once the changes have been identified, the replication process makes all the databases look the same. The replication process can be complex and time-consuming depending on the size and number of the distributed databases. This process can also require a lot of time and computer resources.  
# Duplication, on the other hand, has less complexity. It basically identifies one database as a [[master-slave (technology)|master]] and then duplicates that database. The duplication process is normally done at a set time after hours.  This is to ensure that each distributed location has the same data.  In the duplication process, users may change only the master database. This ensures that local data will not be overwritten.

Both replication and duplication can keep the data current in all distributive locations.&lt;ref name="obrien" /&gt;

Besides distributed database replication and fragmentation, there are many other distributed database design technologies. For example, local autonomy, synchronous and asynchronous distributed database technologies. These technologies' implementations can and do depend on the needs of the business and the sensitivity/[[confidentiality]] of the data stored in the database, and the price the business is willing to spend on ensuring [[data security]], [[data consistency|consistency]] and [[data integrity|integrity]].

When discussing access to distributed databases, [[Microsoft]] favors the term '''distributed query''', which it defines in protocol-specific manner as "[a]ny SELECT, INSERT, UPDATE, or DELETE statement that references tables and rowsets from one or more external OLE DB data sources".&lt;ref&gt;
{{cite web
 |url          = http://technet.microsoft.com/en-us/library/cc966484.aspx
 |title        = TechNet Glossary
 |publisher    = Microsoft
 |accessdate   = 2013-07-16
 |quote        = distributed query[:] Any SELECT, INSERT, UPDATE, or DELETE statement that references tables and rowsets from one or more external OLE DB data sources.
}}
&lt;/ref&gt;
[[Oracle Database|Oracle]] provides a more language-centric view in which distributed queries and [[distributed transaction]]s form part of '''distributed SQL'''.&lt;ref&gt;
{{cite web
 |url          = http://docs.oracle.com/cd/E11882_01/server.112/e25789/toc.htm
 |title        = Oracle Database Concepts, 11g Release 2 (11.2)
 |last1        = Ashdown
 |first1       = Lance
 |last2        = Kyte
 |first2       = Tom
 |date=September 2011
 |publisher    = Oracle Corporation
 |accessdate   = 2013-07-17
 |quote        = Distributed SQL synchronously accesses and updates data distributed among multiple databases. [...] Distributed SQL includes distributed queries and distributed transactions. 
}}
&lt;/ref&gt;

Today the distributed [[DBMS]] market is evolving dramatically, with new, innovative entrants and incumbents supporting the growing use of unstructured data and [[NoSQL]] DBMS engines, as well as [[XML database]]s and [[NewSQL|NewSQL databases]]. These databases are increasingly supporting distributed database architecture that provides [[high availability]] and [[fault tolerance]] through [[replication (computing)|replication]] and scale out ability.  Some examples are [[Aerospike database|Aerospike]],&lt;ref&gt;{{cite web|title=Aerospike distributed database|url=http://www.aerospike.com|website=Aerospike}}&lt;/ref&gt; [[Apache Cassandra|Cassandra]],&lt;ref&gt;{{cite web |url=http://cassandra.apache.org/ |title=Apache Cassandra database menagement system |publisher=Apache.org}}&lt;/ref&gt; [[Clusterpoint]],&lt;ref&gt;{{cite web |url=http://www.clusterpoint.com |title=Clusterpoint XML distributed database |publisher=Clusterpoint}}&lt;/ref&gt; [[Clustrix|ClustrixDB]],&lt;ref&gt;{{cite web|title=Frequently Asked Questions about ClustrixDB - Clustrix Documentation|url=http://docs.clustrix.com/display/CLXDOC/Frequently+Asked+Questions+about+ClustrixDB#FrequentlyAskedQuestionsaboutClustrixDB-WhatisClustrixDB?|publisher=Clustrix, Inc.}}&lt;/ref&gt; [[Couchbase Server|Couchbase]],&lt;ref&gt;{{cite web|title=Couchbase distributed database|url=http://www.couchbase.com|website=Couchbase}}&lt;/ref&gt; [[Druid (open-source data store)]],&lt;ref&gt;{{cite web |url=http://druid.io |title=Druid distributed datastore/database |publisher=The Druid Community}}&lt;/ref&gt; [[FoundationDB]],&lt;ref&gt;
{{cite web |url=https://foundationdb.com |title=FoundationDB database |publisher=FoundationDB}}&lt;/ref&gt; [[NuoDB]],&lt;ref&gt;Clark, Jack. [http://www.theregister.co.uk/2014/02/26/nuodb_funding/ "NuoDB slurps European cash for database expansion"] The Register. Feb. 26, 2014&lt;/ref&gt; [[Riak]]&lt;ref&gt;
{{cite web |url=http://www.basho.com |title=Basho Riak Distributed database |publisher=Basho}}&lt;/ref&gt; and [[OrientDB]].&lt;ref&gt;
{{cite web |url=http://www.orientdb.com |title=OrientDB database |publisher=OrientDB}}&lt;/ref&gt; The [[Blockchain (database)|blockchain]] technology popularised by [[bitcoin]] is an implementation of a distributed database.&lt;ref&gt;{{cite news|last1=Margaret|first1=Alyson|title=How Bitcoin and the blockchain are a transformative technology |url=http://blog.blockchain.com/2015/06/23/how-bitcoin-and-the-block-chain-are-a-transformative-technology/|accessdate=23 July 2015|date=23 June 2015}}&lt;/ref&gt;

== Architecture ==
A database user accesses the distributed database through:
;Local applications
:applications which do not require data from other sites.
;Global applications
:applications which do require data from other sites.

A '''homogeneous distributed database''' has identical software and hardware running all databases instances, and may appear through a single interface as if it were a single database.  A '''heterogeneous distributed database''' may have different hardware, operating systems, database management systems, and even data models for different databases.

===Homogeneous Distributed Databases Management System===
In homogeneous distributed database, all sites have identical software and are aware of each other and agree to cooperate in processing user requests. Each site surrenders part of its autonomy in terms of right to change schema or software. A homogeneous DBMS appears to the user as a single system. The homogeneous system is much easier to design and manage. The following conditions must be satisfied for homogeneous database:
*The operating system used at each location must be same or compatible.{{According to whom|date=March 2013}}{{Elucidate|date=March 2013}}
*The data structures used at each location must be same or compatible.
*The database application (or DBMS) used at each location must be same or compatible.

===Heterogeneous DDBMS===
{{See also|Heterogeneous database system}}
In a heterogeneous distributed database, different sites may use different schema and software. Difference in schema is a major problem for query processing and transaction processing. Sites may not be aware of each other and may provide only limited facilities for cooperation in transaction processing. In heterogeneous systems, different nodes may have different hardware &amp; software and data structures at various nodes or locations are also incompatible. Different computers and operating systems, database applications or data models may be used at each of the locations. For example, one location may have the latest relational database management technology, while another location may store data using conventional files or old version of database management system. Similarly, one location may have the Windows 10 operating system, while another may have UNIX. Heterogeneous systems are usually used when individual sites use their own hardware and software. On heterogeneous system, translations are required to allow communication between different sites (or DBMS). In this system, the users must be able to make requests in a database language at their local sites. Usually the SQL database language is used for this purpose. If the hardware is different, then the translation is straightforward, in which computer codes and word-length is changed. The heterogeneous system is often not technically or economically feasible. In this system, a user at one location may be able to read but not update the data at another location.

== Important considerations ==
Care with a distributed database must be taken to ensure the following:
* The distribution is transparent &#8212; users must be able to interact with the system as if it were one logical system.  This applies to the system's performance, and methods of access among other things.
* [[Database transaction|Transaction]]s are transparent &#8212; each transaction must maintain [[database integrity]] across multiple databases.  Transactions must also be divided into sub-transactions, each sub-transaction affecting one database system.

There are two principal approaches to store a relation r in a distributed database system:

:A) [[database replication|Replication]]
:B) Fragmentation/[[Partition (database)|Partitioning]]

A) Replication: In replication, the system maintains several identical replicas of the same relation r in different sites.
:* Data is more available in this scheme.
:* Parallelism is increased when read request is served.
:* Increases overhead on update operations as each site containing the replica needed to be updated in order to maintain consistency.
:* Multi-datacenter replication provides geographical diversity, like in [[Clusterpoint]]&lt;ref&gt;
{{cite web |url=http://www.clusterpoint.com/solutions/distributed-storage |title=Clusterpoint database distributed storage multi-datacenter replication|publisher=Clusterpoint}}&lt;/ref&gt; or [[Riak]].&lt;ref&gt;
{{cite web |url=http://basho.com/tag/multi-datacenter-replication/ |title=Riak database multi-datacenter replication|publisher=Basho}}&lt;/ref&gt;

B) Fragmentation: The relation r is fragmented into several relations r&lt;sub&gt;1&lt;/sub&gt;, r&lt;sub&gt;2&lt;/sub&gt;, r&lt;sub&gt;3&lt;/sub&gt;....r&lt;sub&gt;n&lt;/sub&gt; in such a way that the actual relation could be reconstructed from the fragments and then the fragments are scattered to different locations. There are basically two schemes of fragmentation:

:* Horizontal fragmentation - splits the relation by assigning each tuple of r to one or more fragments.
:* Vertical fragmentation - splits the relation by decomposing the schema R of relation r.

A distributed database can be run by independent or even competing parties as, for example, in [[bitcoin]] or [[Hasq]].

== Advantages ==
* Management of distributed data with different levels of transparency like network transparency, fragmentation transparency, replication transparency, etc.
* Increase reliability and availability
* Easier expansion
* Reflects organizational structure &#8212; database fragments potentially stored within the departments they relate to
* Local autonomy or site autonomy &#8212; a department can control the data about them (as they are the ones familiar with it)
* Protection of valuable data &#8212; if there were ever a catastrophic event such as a fire, all of the data would not be in one place, but distributed in multiple locations
* Improved performance &#8212; data is located near the site of greatest demand, and the database systems themselves are parallelized, allowing load on the databases to be balanced among servers.  (A high load on one module of the database won't affect other modules of the database in a distributed database)
* Economics &#8212; it may cost less to create a network of smaller computers with the power of a single large computer
* Modularity &#8212; systems can be modified, added and removed from the distributed database without affecting other modules (systems)
* Reliable transactions - due to replication of the database
* Hardware, operating-system, network, fragmentation, DBMS, replication and location independence
* Continuous operation, even if some nodes go offline (depending on design)
* Distributed query processing can improve performance
* Single-site failure does not affect performance of system.
* For those systems that support full distributed transactions, operations enjoy the [[ACID]] properties:
** A-atomicity, the transaction takes place as a whole or not at all
** C-consistency, maps one consistent DB state to another
** I-isolation, each transaction sees a consistent DB
** D-durability, the results of a transaction must survive system failures

The Merge Replication Method is popularly used to consolidate the data between databases.{{citation needed|date=July 2013}}

== Disadvantages ==
* Complexity &#8212; [[Database administrator|DBAs]] may have to do extra work to ensure that the distributed nature of the system is transparent.  Extra work must also be done to maintain multiple [[disparate system]]s, instead of one big one.  Extra database design work must also be done to account for the disconnected nature of the database &#8212; for example, joins become prohibitively expensive when performed across multiple systems.
* Economics &#8212; increased complexity and a more extensive infrastructure means extra labour costs
* Security &#8212; remote database fragments must be secured, and they are not centralized so the remote sites must be secured as well.  The infrastructure must also be secured (for example, by encrypting the network links between remote sites).
* Difficult to maintain integrity &#8212; but in a distributed database, enforcing integrity over a network may require too much of the network's resources to be feasible
* Inexperience &#8212; distributed databases are difficult to work with, and in such a young field there is not much readily available experience in "proper" practice
* Lack of standards &#8212; there are no tools or methodologies yet to help users convert a centralized DBMS into a distributed DBMS{{citation needed|date=July 2013}}
* Database design more complex &#8212; In addition to traditional database design challenges, the design of a distributed database has to consider fragmentation of data, allocation of fragments to specific sites and data replication
* Additional software is required
* Operating system should support distributed environment
* [[Concurrency control]] poses a major issue. It can be solved by [[Lock (database)|locking]] and [[timestamp]]ing.
* Distributed access to data
* Analysis of distributed data

==See also==
*[[Centralized database]]
*[[Data grid]]
*[[Distributed data store]]
*[[Distributed cache]]
*[[Routing protocol]]
*[[Distributed hash table]]

==References==
{{Reflist|30em}}
{{more footnotes|date=April 2013}}
*M. T. &#214;zsu and P. Valduriez, ''Principles of Distributed Databases'' (3rd edition) (2011), Springer, ISBN 978-1-4419-8833-1
*Elmasri and Navathe, ''Fundamentals of database systems'' (3rd edition), Addison-Wesley Longman, ISBN 0-201-54263-3
*''Oracle Database Administrator's Guide 10g'' (Release 1), http://docs.oracle.com/cd/B14117_01/server.101/b10739/ds_concepts.htm

{{Databases}}

[[Category:Data management]]
[[Category:Types of databases]]
[[Category:Distributed computing architecture]]
[[Category:Applications of distributed computing]]
[[Category:Database management systems]]</text>
      <sha1>mjb296mj2vyjtxcivlo5jjra2b0mvht</sha1>
    </revision>
  </page>
  <page>
    <title>Serializability</title>
    <ns>0</ns>
    <id>4367801</id>
    <revision>
      <id>748314450</id>
      <parentid>742617910</parentid>
      <timestamp>2016-11-07T15:39:37Z</timestamp>
      <contributor>
        <ip>137.122.64.80</ip>
      </contributor>
      <comment>Typos and formatting errors</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="32740" xml:space="preserve">{{About|serializability of database transactions|serialization of objects in object-oriented languages|serialization}}

In [[concurrency control]] of [[database]]s,&lt;ref name=Bernstein87&gt;[[Phil Bernstein|Philip A. Bernstein]], Vassos Hadzilacos, Nathan Goodman (1987): [http://research.microsoft.com/en-us/people/philbe/ccontrol.aspx ''Concurrency Control and Recovery in Database Systems''] (free PDF download), Addison Wesley Publishing Company, ISBN 0-201-10715-5&lt;/ref&gt;&lt;ref name=Weikum01&gt;[[Gerhard Weikum]], Gottfried Vossen (2001): [http://www.elsevier.com/wps/find/bookdescription.cws_home/677937/description#description ''Transactional Information Systems''], Elsevier, ISBN 1-55860-508-8&lt;/ref&gt; [[transaction processing]] (transaction management), and various [[Database transaction|transactional]] applications (e.g., [[transactional memory]]&lt;ref name=Herlihy1993&gt;[[Maurice Herlihy]] and J. Eliot B. Moss. ''Transactional memory: architectural support for lock-free data structures.'' Proceedings of the 20th annual international symposium on Computer architecture (ISCA '93). Volume 21, Issue 2, May 1993.&lt;/ref&gt; and [[software transactional memory]]), both centralized and [[Distributed computing|distributed]], a transaction [[Schedule (computer science)|schedule]] is '''serializable''' if its outcome (e.g., the resulting database state) is equal to the outcome of its transactions executed serially, i.e., sequentially without overlapping in time. Transactions are normally executed concurrently (they overlap), since this is the most efficient way. Serializability is the major correctness criterion for concurrent transactions' executions. It is considered the highest level of [[isolation (computer science)|isolation]] between [[Database transaction|transactions]], and plays an essential role in [[concurrency control]]. As such it is supported in all general purpose database systems. ''[[Two-phase locking|Strong strict two-phase locking]]'' (SS2PL) is a popular serializability mechanism utilized in most of the database systems (in various variants) since their early days in the 1970s.

'''Serializability theory''' provides the formal framework to reason about and analyze serializability and its techniques. Though it is [[Mathematics|mathematical]] in nature, its fundamentals are informally (without mathematics notation) introduced below.

==Database transaction==
{{main|Database transaction}}

A '''database transaction''' is a specific intended run (with specific parameters, e.g., with transaction identification, at least) of a computer program (or programs) that accesses a database (or databases). Such a program is written with the assumption that it is running in ''isolation'' from other executing programs, i.e., when running, its accessed data (after the access) are not changed by other running programs. Without this assumption the transaction's results are unpredictable and can be wrong. The same transaction can be executed in different situations, e.g., in different times and locations, in parallel with different programs. A ''live'' transaction (i.e., exists in a computing environment with already allocated computing resources; to distinguish from a ''transaction request'', waiting to get execution resources) can be in one of three states, or phases:
#''Running'' - Its program(s) is (are) executing.
#''Ready'' - Its program's execution has ended, and it is waiting to be ''Ended (Completed)''.
#''Ended'' (or ''Completed'') - It is either ''Committed'' or ''Aborted (Rolled-back)'', depending whether the execution is considered a success or not, respectively . When committed, all its ''recoverable'' (i.e., with states that can be controlled for this purpose), ''durable'' resources (typically ''database data'') are put in their ''final'' states, states after running. When aborted, all its recoverable resources are put back in their ''initial'' states, as before running.

A failure in transaction's computing environment before ending typically results in its abort. However, a transaction may be aborted also for other reasons as well (e.g., see below).

Upon being ended (completed), transaction's allocated computing resources are released and the transaction disappears from the computing environment. However, the effects of a committed transaction remain in the database, while the effects of an aborted (rolled-back) transaction disappear from the database. The concept of ''atomic transaction'' ("all or nothing" semantics) was designed to exactly achieve this behavior, in order to control correctness in complex faulty systems.

==Correctness==

===Serializability===
'''Serializability''' is used to keep the data in the data item in a consistent state.  Serializability is a property of a transaction [[Schedule (computer science)|schedule]] (history). It relates to the ''[[Isolation (database systems)|isolation]]'' property of a [[database transaction]].
:'''Serializability''' of a schedule means equivalence (in the outcome, the database state, data values) to a ''serial schedule'' (i.e., sequential with no transaction overlap in time) with the same transactions. It is the major criterion for the correctness of concurrent transactions' schedule, and thus supported in all general purpose database systems.

:'''The rationale behind serializability''' is the following:
:If each transaction is correct by itself, i.e., meets certain integrity conditions, then a schedule that comprises any ''serial'' execution of these transactions is correct (its transactions still meet their conditions): "Serial" means that transactions do not overlap in time and cannot interfere with each other, i.e, complete ''isolation'' between each other exists. Any order of the transactions is legitimate, if no dependencies among them exist, which is assumed (see comment below). As a result, a schedule that comprises any execution (not necessarily serial) that is equivalent (in its outcome) to any serial execution of these transactions, is correct.

Schedules that are not serializable are likely to generate erroneous outcomes. Well known examples are with transactions that debit and credit accounts with money: If the related schedules are not serializable, then the total sum of money may not be preserved. Money could disappear, or be generated from nowhere. This and violations of possibly needed other [[invariant (computer science)|invariant]] preservations are caused by one transaction writing, and "stepping on" and erasing what has been written by another transaction before it has become permanent in the database. It does not happen if serializability is maintained.

If any specific order between some transactions is requested by an application, then it is enforced independently of the underlying serializability mechanisms. These mechanisms are typically indifferent to any specific order, and generate some unpredictable [[partial order]] that is typically compatible with multiple serial orders of these transactions. This partial order results from the scheduling orders of concurrent transactions' data access operations, which depend on many factors.

A major characteristic of a database transaction is ''[[Atomicity (database systems)|atomicity]]'', which means that it either ''commits'', i.e., all its operations' results take effect in the database, or ''aborts'' (rolled-back), all its operations' results do not have any effect on the database ("all or nothing" semantics of a transaction). In all real systems transactions can abort for many reasons, and serializability by itself is not sufficient for correctness. Schedules also need to possess the ''[[Schedule (computer science)#Recoverable|recoverability]]'' (from abort) property. '''Recoverability''' means that committed transactions have not read data written by aborted transactions (whose effects do not exist in the resulting database states). While serializability is currently compromised on purpose in many applications for better performance (only in cases when application's correctness is not harmed), compromising recoverability would quickly violate the database's integrity, as well as that of transactions' results external to the database. A schedule with the recoverability property (a ''recoverable'' schedule) "recovers" from aborts by itself, i.e., aborts do not harm the integrity of its committed transactions and resulting database. This is false without recoverability, where the likely integrity violations (resulting incorrect database data) need special, typically manual, corrective actions in the database.

Implementing recoverability in its general form may result in ''cascading aborts'': Aborting one transaction may result in a need to abort a second transaction, and then a third, and so on. This results in a waste of already partially executed transactions, and may result also in a performance penalty. '''[[Schedule (computer science)#Avoids cascading aborts (rollbacks)|Avoiding cascading aborts]]''' (ACA, or Cascadelessness) is a special case of recoverability that exactly prevents such phenomenon. Often in practice a special case of ACA is utilized: '''[[Schedule (computer science)#Strict|Strictness]]'''. Strictness allows an efficient database recovery from failure.

Note that the ''recoverability'' property is needed even if no database failure occurs and no database ''recovery'' from failure is needed. It is rather needed to correctly automatically handle aborts, which may be unrelated to database failure and recovery from failure.

===Relaxing serializability===

In many applications, unlike with finances, absolute correctness is not needed. For example, when retrieving a list of products according to specification, in most cases it does not matter much if a product, whose data was updated a short time ago, does not appear in the list, even if it meets the specification. It will typically appear in such a list when tried again a short time later. Commercial databases provide concurrency control with a whole range of [[isolation (computer science)#Isolation levels|isolation levels]] which are in fact (controlled) serializability violations in order to achieve higher performance. Higher performance means better transaction execution rate and shorter average transaction response time (transaction duration). ''[[Snapshot isolation]]'' is an example of a popular, widely utilized efficient relaxed serializability method with many characteristics of full serializability, but still short of some, and unfit in many situations.

Another common reason nowadays for [[Serializability#Distributed serializability|distributed serializability]] relaxation (see below) is the requirement of [[availability]] of [[internet]] products and [[Internet service provider|services]]. This requirement is typically answered by large-scale data [[Replication (computer science)|replication]]. The straightforward solution for synchronizing replicas' updates of a same database object is including all these updates in a single atomic [[distributed transaction]]. However, with many replicas such a transaction is very large, and may span several [[computer]]s and [[computer network|networks]] that some of them are likely to be unavailable. Thus such a transaction is likely to end with abort and miss its purpose.&lt;ref name=Gray1996&gt;{{cite conference
 | author = [[Jim Gray (computer scientist)|Gray, J.]]
 | coauthors = Helland, P.; [[Patrick O'Neil|O&#8217;Neil, P.]]; [[Dennis Shasha|Shasha, D.]]
 | year = 1996
 | title = The dangers of replication and a solution
 | conference = Proceedings of the 1996 [[ACM SIGMOD International Conference on Management of Data]]
 | pages = 173&#8211;182
 | url = ftp://ftp.research.microsoft.com/pub/tr/tr-96-17.pdf
 | doi = 10.1145/233269.233330
 }}&lt;/ref&gt;
Consequently, [[Optimistic replication]] (Lazy replication) is often utilized (e.g., in many products and services by [[Google]], [[Amazon.com|Amazon]], [[Yahoo]], and alike), while serializability is relaxed and compromised for [[eventual consistency]]. Again in this case, relaxation is done only for applications that are not expected to be harmed by this technique.

Classes of schedules defined by ''relaxed serializability'' properties either contain the serializability class, or are incomparable with it.

==View and conflict serializability ==

Mechanisms that enforce serializability need to execute in [[Real-time computing|real time]], or almost in real time, while transactions are running at high rates. In order to meet this requirement special cases of serializability, sufficient conditions for serializability which can be enforced effectively, are utilized.

Two major types of serializability exist: ''view-serializability'', and ''conflict-serializability''. View-serializability matches the general definition of serializability given above. Conflict-serializability is a broad special case, i.e., any schedule that is conflict-serializable is also view-serializable, but not necessarily the opposite. Conflict-serializability is widely utilized because it is easier to determine and covers a substantial portion of the view-serializable schedules. Determining view-serializability of a schedule is an [[NP-complete]] problem (a class of problems with only difficult-to-compute, excessively time-consuming known solutions).

:'''View-serializability''' of a schedule is defined by equivalence to a serial schedule (no overlapping transactions) with the same transactions, such that respective transactions in the two schedules read and write the same data values ("view" the same data values).

:'''Conflict-serializability''' is defined by equivalence to a serial schedule (no overlapping transactions) with the same transactions, such that both schedules have the same sets of respective chronologically ordered pairs of conflicting operations (same precedence relations of respective conflicting operations).

Operations upon data are ''read'' or ''write'' (a write: either ''insert'' or ''modify'' or ''delete''). Two operations are ''conflicting'', if they are of different transactions, upon the same datum (data item), and at least one of them is ''write''. Each such pair of conflicting operations has a ''conflict type'': It is either a ''read-write'', or ''write-read'', or a ''write-write'' conflict. The transaction of the second operation in the pair is said to be ''in conflict'' with the transaction of the first operation. A more general definition of conflicting operations (also for complex operations, which may consist each of several "simple" read/write operations) requires that they are [[noncommutative]] (changing their order also changes their combined result). Each such operation needs to be atomic by itself (by proper system support) in order to be considered an operation for a commutativity check. For example, read-read operations are commutative (unlike read-write and the other possibilities) and thus read-read is not a conflict. Another more complex example: the operations ''increment'' and ''decrement'' of a ''counter'' are both ''write'' operations (both modify the counter), but do not need to be considered conflicting (write-write conflict type) since they are commutative (thus increment-decrement is not a conflict; e.g., already has been supported in the old [[IBM Information Management System|IBM's IMS "fast path"]]). Only precedence (time order) in pairs of conflicting (non-commutative) operations is important when checking equivalence to a serial schedule, since different schedules consisting of the same transactions can be transformed from one to another by changing orders between different transactions' operations (different transactions' interleaving), and since changing orders of commutative operations (non-conflicting) does not change an overall operation sequence result, i.e., a schedule outcome (the outcome is preserved through order change between non-conflicting operations, but typically not when conflicting operations change order). This means that if a schedule can be transformed to any serial schedule without changing orders of conflicting operations (but changing orders of non-conflicting, while preserving operation order inside each transaction), then the outcome of both schedules is the same, and the schedule is conflict-serializable by definition.

Conflicts are the reason for blocking transactions and delays (non-materialized conflicts), or for aborting transactions due to serializability violations prevention. Both possibilities reduce performance. Thus reducing the number of conflicts, e.g., by commutativity (when possible), is a way to increase performance.

A transaction can issue/request a conflicting operation and be ''in conflict'' with another transaction while its conflicting operation is delayed and not executed (e.g., blocked by a [[Lock (computer science)|lock]]). Only executed (''materialized'') conflicting operations are relevant to ''conflict serializability'' (see more below).

==Enforcing conflict serializability==

===Testing conflict serializability===

Schedule compliance with conflict serializability can be tested with the [[precedence graph]] (''serializability graph'', ''serialization graph'', ''conflict graph'') for committed transactions of the schedule. It is the [[directed graph]] representing precedence of transactions in the schedule, as reflected by precedence of conflicting operations in the transactions.

:In the '''[[precedence graph]]'''  transactions are nodes and precedence relations are directed edges. There exists an edge from a first transaction to a second transaction, if the second transaction is ''in conflict'' with the first (see Conflict serializability above), and the conflict is '''materialized''' (i.e., if the requested conflicting operation is actually executed: in many cases a requested/issued conflicting operation by a transaction is delayed and even never executed, typically by a [[Lock (computer science)|lock]] on the operation's object, held by another transaction, or when writing to a transaction's temporary private workspace and materializing, copying to the database itself, upon commit; as long as a requested/issued conflicting operation is not executed upon the database itself, the conflict is '''non-materialized'''; non-materialized conflicts are not represented by an edge in the precedence graph).

:'''Comment:''' In many text books only ''committed transactions'' are included in the precedence graph. Here all transactions are included for convenience in later discussions.

The following observation is a '''key characterization of conflict serializability''':

:A schedule is ''conflict-serializable'' [[if and only if]] its precedence graph of ''committed transactions'' (when only ''committed'' transactions are considered) is ''[[directed acyclic graph|acyclic]]''. This means that a cycle consisting of committed transactions only is generated in the (general) precedence graph, if and only if conflict-serializability is violated.

Cycles of committed transactions can be prevented by aborting an ''undecided'' (neither committed, nor aborted) transaction on each cycle in the precedence graph of all the transactions, which can otherwise turn into a cycle of committed transactions (and a committed transaction cannot be aborted). One transaction aborted per cycle is both required and sufficient number to break and eliminate the cycle (more aborts are possible, and can happen in some mechanisms, but unnecessary for serializability). The probability of cycle generation is typically low, but nevertheless, such a situation is carefully handled, typically with a considerable overhead, since correctness is involved. Transactions aborted due to serializability violation prevention are ''restarted'' and executed again immediately.

Serializability enforcing mechanisms typically do not maintain a precedence graph as a data structure, but rather prevent or break cycles implicitly (e.g., SS2PL below).

===Common mechanism - SS2PL===
{{main|Two-phase locking}}

''Strong strict two phase locking'' (SS2PL) is a common mechanism utilized in database systems since their early days in the 1970s (the "SS" in the name SS2PL is newer though) to enforce both conflict serializability and ''[[Schedule (computer science)#Strict|strictness]]'' (a special case of recoverability which allows effective database recovery from failure) of a schedule. In this mechanism each datum is locked by a transaction before accessing it (any read or write operation): The item is marked by, associated with a ''[[lock (computer science)|lock]]'' of a certain type, depending on operation (and the specific implementation; various models with different lock types exist; in some models locks may change type during the transaction's life). As a result, access by another transaction may be blocked, typically upon a conflict (the lock delays or completely prevents the conflict from being materialized and be reflected in the precedence graph by blocking the conflicting operation), depending on lock type and the other transaction's access operation type. Employing an SS2PL mechanism means that all locks on data on behalf of a transaction are released only after the transaction has ended (either committed or aborted).

SS2PL is the name of the resulting schedule property as well, which is also called ''rigorousness''. SS2PL is a special case ([[proper subset]]) of [[Two-phase locking]] (2PL)

Mutual blocking between transactions results in a ''deadlock'', where execution of these transactions is stalled, and no completion can be reached. Thus deadlocks need to be resolved to complete these transactions' execution and release related computing resources. A deadlock is a reflection of a potential cycle in the precedence graph, that would occur without the blocking when conflicts are materialized. A deadlock is resolved by aborting a transaction involved with such potential cycle, and breaking the cycle. It is often detected using a ''[[wait-for graph]]'' (a graph of conflicts blocked by locks from being materialized; it can be also defined as the graph of non-materialized conflicts; conflicts not materialized are not reflected in the precedence graph and do not affect serializability), which indicates which transaction is "waiting for" lock release by which transaction, and a cycle means a deadlock. Aborting one transaction per cycle is sufficient to break the cycle. Transactions aborted due to deadlock resolution are ''restarted'' and executed again immediately.

===Other enforcing techniques===

Other known mechanisms include:
* [[Precedence graph]] (or Serializability graph, Conflict graph) cycle elimination
* [[Two-phase locking]] (2PL)
* [[Timestamp-based concurrency control|Timestamp ordering]] (TO)
* [[Snapshot isolation#Making Snapshot Isolation Serializable|Serializable snapshot isolation]]&lt;ref name=Cahill08&gt;Michael J. Cahill, Uwe R&#246;hm, Alan D. Fekete (2008): [http://portal.acm.org/citation.cfm?id=1376690  "Serializable isolation for snapshot databases"], ''Proceedings of the 2008 ACM SIGMOD international conference on Management of data'', pp. 729-738, Vancouver, Canada, June 2008, ISBN 978-1-60558-102-6 (SIGMOD 2008 best paper award)&lt;/ref&gt; (SerializableSI)

The above (conflict) serializability techniques in their general form do not provide recoverability. Special enhancements are needed for adding recoverability.

====Optimistic versus pessimistic techniques====

Concurrency control techniques are of three major types:
# ''Pessimistic'': In Pessimistic concurrency control a transaction blocks data access operations of other transactions upon conflicts, and conflicts are ''non-materialized'' until blocking is removed. This is done to ensure that operations that may violate serializability (and in practice also recoverability) do not occur.
# ''Optimistic'': In [[Optimistic concurrency control]] data access operations of other transactions are not blocked upon conflicts, and conflicts are immediately ''materialized''. When the transaction reaches the ''ready'' state, i.e., its ''running'' state has been completed, possible serializability (and in practice also recoverability) violation by the transaction's operations (relatively to other running transactions) is checked: If violation has occurred, the transaction is typically ''aborted'' (sometimes aborting ''another'' transaction to handle serializability violation is preferred). Otherwise it is ''committed''.
# ''Semi-optimistic'': Mechanisms that mix blocking in certain situations with not blocking in other situations and employ both materialized and non-materialized conflicts

The main differences between the technique types is the conflict types that are generated by them. A pessimistic method blocks a transaction operation upon conflict and generates a non-materialized conflict, while an optimistic method does not block and generates a materialized conflict. A semi-optimistic method generates both conflict types. Both conflict types are generated by the chronological orders in which transaction operations are invoked, independently of the type of conflict. A cycle of committed transactions (with materialized conflicts) in the ''[[precedence graph]]'' (conflict graph) represents a serializability violation, and should be avoided for maintaining serializability. A cycle of (non-materialized) conflicts in the ''[[wait-for graph]]'' represents a deadlock situation, which should be resolved by breaking the cycle. Both cycle types result from conflicts, and should be broken. At any technique type conflicts should be detected and considered, with similar overhead for both materialized and non-materialized conflicts (typically by using mechanisms like locking, while either blocking for locks, or not blocking but recording conflict for materialized conflicts). In a blocking method typically a [[context switch]]ing occurs upon conflict, with (additional) incurred overhead. Otherwise blocked transactions' related computing resources remain idle, unutilized, which may be a worse alternative. When conflicts do not occur frequently, optimistic methods typically have an advantage. With different transactions loads (mixes of transaction types) one technique type (i.e., either optimistic or pessimistic) may provide better performance than the other.

Unless schedule classes are ''inherently blocking'' (i.e., they cannot be implemented without data-access operations blocking; e.g., 2PL, SS2PL and SCO above; see chart), they can be implemented also using optimistic techniques (e.g., Serializability, Recoverability).

====Serializable multi-version concurrency control====

:See also [[Multiversion concurrency control]] (partial coverage)
:and [[Snapshot isolation#Serializable Snapshot Isolation|Serializable_Snapshot_Isolation]] in [[Snapshot isolation]]

'''Multi-version concurrency control''' (MVCC) is a common way today to increase concurrency and performance by generating a new version of a database object each time the object is written, and allowing transactions' read operations of several last relevant versions (of each object), depending on scheduling method. MVCC can be combined with all the serializability techniques listed above (except SerializableSI which is originally MVCC based). It is utilized in most general-purpose DBMS products.

MVCC is especially popular nowadays through the ''relaxed serializability'' (see above) method ''[[Snapshot isolation]]'' (SI) which provides better performance than most known serializability mechanisms (at the cost of possible serializability violation in certain cases). [[Snapshot isolation#Making Snapshot Isolation Serializable|SerializableSI]], which is an efficient enhancement of SI to make it serializable, is intended to provide an efficient serializable solution. [[Snapshot isolation#Making Snapshot Isolation Serializable|SerializableSI has been analyzed]]&lt;ref name=Cahill08/&gt;&lt;ref name=fekete2009&gt;Alan Fekete (2009), [http://www.it.usyd.edu.au/~fekete/teaching/serializableSI-Fekete.pdf "Snapshot Isolation and Serializable Execution"], Presentation, Page 4, 2009, The university of Sydney (Australia). Retrieved 16 September 2009&lt;/ref&gt; via a general theory of MVCC

==Distributed serializability==

===Overview===

'''Distributed serializability''' is the serializability of a schedule of a transactional [[distributed system]] (e.g., a [[distributed database]] system). Such system is characterized by ''[[distributed transaction]]s'' (also called ''global transactions''), i.e., transactions that span computer processes (a process abstraction in a general sense, depending on computing environment; e.g., [[operating system]]'s [[Thread (computer science)|thread]]) and possibly network nodes. A distributed transaction comprises more than one ''local sub-transactions'' that each has states as described above for a [[Serializability#Database transaction|database transaction]]. A local sub-transaction comprises a single process, or more processes that typically fail together (e.g., in a single [[processor core]]). Distributed transactions imply a need in [[Atomic commit]] protocol to reach consensus among its local sub-transactions on whether to commit or abort. Such protocols can vary from a simple (one-phase) hand-shake among processes that fail together, to more sophisticated protocols, like [[Two-phase commit]], to handle more complicated cases of failure (e.g., process, node, communication, etc. failure). Distributed serializability is a major goal of [[distributed concurrency control]] for correctness. With the proliferation of the [[Internet]], [[Cloud computing]], [[Grid computing]], and small, portable, powerful computing devices (e.g., [[smartphone]]s) the need for effective distributed serializability techniques to ensure correctness in and among distributed applications seems to increase.

Distributed serializability is achieved by implementing distributed versions of the known centralized techniques.&lt;ref name=Bernstein87 /&gt;&lt;ref name=Weikum01 /&gt; Typically all such distributed versions require utilizing conflict information (either of materialized or non-materialized conflicts, or equivalently, transaction precedence or blocking information; conflict serializability is usually utilized) that is not generated locally, but rather in different processes, and remote locations. Thus information distribution is needed (e.g., precedence relations, lock information, timestamps, or tickets). When the distributed system is of a relatively small scale, and message delays across the system are small, the centralized concurrency control methods can be used unchanged, while certain processes or nodes in the system manage the related algorithms. However, in a large-scale system (e.g., ''Grid'' and ''Cloud''), due to the distribution of such information, substantial performance penalty is typically incurred, even when distributed versions of the methods (Vs. centralized) are used, primarily due to computer and communication [[latency (engineering)|latency]]. Also, when such information is distributed, related techniques typically do not scale well. A well-known example with scalability problems is a [[distributed lock manager]], which distributes lock (non-materialized conflict) information across the distributed system to implement locking techniques.

==See also==

*[[Two-phase locking|Strong strict two-phase locking]] (SS2PL or Rigorousness).
*[[Snapshot isolation#Making Snapshot Isolation Serializable|Making snapshot isolation serializable]]&lt;ref name=Cahill08 /&gt; in [[Snapshot isolation]].
*[[Global serializability]], where the ''Global serializability problem'' and its proposed solutions are described.
* [[Linearizability]], a more general concept in [[concurrent computing]]

==Notes==
{{reflist}}

==References==
{{more footnotes|date=November 2011}}
&lt;!--Supposed sources for most of the material about centralized (vs. distributed) serializability:--&gt;
*[[Phil Bernstein|Philip A. Bernstein]], Vassos Hadzilacos, Nathan Goodman (1987): [http://research.microsoft.com/en-us/people/philbe/ccontrol.aspx ''Concurrency Control and Recovery in Database Systems''], Addison Wesley Publishing Company, ISBN 0-201-10715-5
*[[Gerhard Weikum]], Gottfried Vossen (2001): [http://www.elsevier.com/wps/find/bookdescription.cws_home/677937/description#description ''Transactional Information Systems''], Elsevier, ISBN 1-55860-508-8

[[Category:Data management]]
[[Category:Databases]]
[[Category:Concurrency control]]
[[Category:Transaction processing]]
[[Category:Distributed computing problems]]

[[el:&#931;&#949;&#953;&#961;&#953;&#959;&#960;&#959;&#953;&#951;&#963;&#953;&#956;&#972;&#964;&#951;&#964;&#945; &#931;&#965;&#947;&#954;&#961;&#959;&#973;&#963;&#949;&#969;&#957;]]</text>
      <sha1>g4ugf2nm2acc7dl596e3t5q480d9ixb</sha1>
    </revision>
  </page>
  <page>
    <title>World Wide Molecular Matrix</title>
    <ns>0</ns>
    <id>4205544</id>
    <revision>
      <id>745888372</id>
      <parentid>742207405</parentid>
      <timestamp>2016-10-23T23:54:32Z</timestamp>
      <contributor>
        <username>Me, Myself, and I are Here</username>
        <id>17619453</id>
      </contributor>
      <minor />
      <comment>/* top */ bold, LQ</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3056" xml:space="preserve">{{Refimprove|date=December 2010}}
{{No footnotes|date=December 2010}}

The '''World Wide Molecular Matrix''' ('''WWMM''') is an electronic [[Disciplinary repository|repository]] for unpublished chemical [[data]]. First proposed in 2002 by [[Peter Murray-Rust]] and his colleagues in the [[chemistry]] department at the [[University of Cambridge]] in the [[United Kingdom]], WWMM provides a free, easily searchable [[database]] for information about thousands of complicated [[molecules]], data that would otherwise remain inaccessible to [[scientists]].

Murray-Rust, a chemical [[cheminformatics|informatics]] specialist, has estimated that 80% of the results produced by chemists around the world is never published in [[scientific journals]]. Most of this data is not ground-breaking, yet it could conceivably be of use to scientists doing related projects&#8212;if they could access it. The WWMM was proposed as a solution to this problem. It would house the results of experiments on over 100,000 molecules in [[physical chemistry]], [[organic chemistry]], [[biochemistry]] and medicinal chemistry.

In other scientific fields, the need for a similar depository to house inaccessible information could be more acute. In a presentation at the "[[CERN]] Workshop on Innovations in Scholarly Communications ([[Open Archives Initiative|OAI4]])", Murray-Rust said that chemistry actually leads other fields in published data. He estimated that as much as 99% of the data in some scientific fields never reaches publication.{{Citation needed|date=December 2010|reason=This is not found in external link, OAI4 - Paine Ellsworth, ed.}}

Although scientific in nature, the WWMM is part of the broader [[Open Archives Initiative|open archives]] and [[open source]] movements, pushes to make more and more information freely available to any user via the [[Internet]] or [[www|World Wide Web]]. In his [[CERN]] presentation, Murray-Rust stated that the WWMM was a "response to the expense of [scientific] journals", and he asked the rhetorical question, "Can we win the war to make data open, or will it be absorbed into the [[publishing]] and pseudo-publishing world?" Murray-Rust and his colleagues are also responsible for the development of the Chemical Mark-up Language ([[Chemical Markup Language|CML]]), a variant of [[XML]] intended for [[chemists]].

==See also==
* [[Open Archives Initiative|The open archives initiative (OAI)]]
* [[Informatics (academic field)|The science of Informatics]]
* [[Chemical Markup Language|Chemical Mark-up language (CML)]]

==External links==
*[http://www.ch.cam.ac.uk/person/pm286 The home page of Dr. Peter Murray-Rust at the University of Cambridge]
*[http://www.escience.cam.ac.uk/projects/mi/mi_call.html The Cambridge Center for molecular informatics]
*[http://www.nesc.ac.uk/events/ahm2003/AHMCD/pdf/157.pdf An outline of the WWMM]
*[http://oai4.web.cern.ch/OAI4/ CERN Workshop on Innovations in Scholarly Communication (OAI4)]{{verify source|type=application to WWMM|date=December 2010}}

[[Category:Data management]]</text>
      <sha1>axgk4ac4n9jojdykcghzoxj4hf3340u</sha1>
    </revision>
  </page>
  <page>
    <title>Data steward</title>
    <ns>0</ns>
    <id>6212365</id>
    <revision>
      <id>762071258</id>
      <parentid>762071101</parentid>
      <timestamp>2017-01-26T13:27:13Z</timestamp>
      <contributor>
        <username>RichardWeiss</username>
        <id>193093</id>
      </contributor>
      <comment>rm all languages, the spanish is incorrect, we have wikidata now and should exclusively use that</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6738" xml:space="preserve">{{merge|Data custodian|date=February 2016}}

A '''data steward''' is a person responsible for the management and fitness of [[data element]]s  - both the content and [[metadata]]. Data stewards have a specialist role that incorporates processes, policies, guidelines and responsibilities for administering organizations' entire data in compliance with policy and/or regulatory obligations. A data steward may share some responsibilities with a [[data custodian]].

The overall objective of a data steward is [[data quality]], in regard to the key/critical data elements existing within a specific enterprise operating structure, of the elements in their respective domains. This includes capturing/documenting (meta)information for their elements (such as: definitions, related rules/governance, physical manifestation, related data models, etc. With most of these properties being specific to an attribute/concept relationship), identifying owners/custodians/various  responsibilities, relations insight pertaining to attribute quality, aiding with project requirement data facilitation and documentation of capture rules. 

Data stewards begin the [[stewardship|stewarding]] process with the identification of the elements which they will steward, with the ultimate result being standards, [[Control (disambiguation)|control]]s and [[Data entry clerk|data entry]].{{citation needed|date=October 2014}}  The steward works closely with business glossary standards analysts (for standards), with [[data architect]]/[[Data modeling|modeler]]s (for standards), with  [[Data quality|DQ]] analysts (for controls) and with [[Computer operator|operations team member]]s (good-quality data going in per business rules) while entering data.

Data stewardship roles are common when organizations attempt to exchange data precisely and consistently between computer systems and to reuse data-related resources.{{citation needed|date=October 2014}}  [[Master data management]] often{{quantify|date=October 2014}} makes references to the need for data stewardship for its implementation to succeed. Data stewardship must have precise purpose, fit for purpose or fitness.

==Data Steward Responsibilities==
A data steward ensures that each assigned data element:
# Has clear and unambiguous [[data element definition]].
# Does not conflict with other data elements in the metadata registry (removes duplicates, overlap etc.)
# Has clear enumerated value definitions if it is of type [[Code (metadata)|Code]].
# Is still being used (remove unused data elements)
# Is being used consistently in various computer systems
# Is being used, fit for purpose = Data Fitness.
# Has adequate documentation on appropriate usage and notes
# Documents the origin and sources of authority on each metadata element
# Is protected against unauthorised access or change

==Benefits of data stewardship==

Systematic data stewardship can foster fitness through:

# consistent use of data management resources
# easy mapping of data between computer systems and exchange documents
# lower costs associated with migration to (for example) [[Service Oriented Architecture]] (SOA)

Assignment of each data element to a person sometimes seems like an unimportant process. But many groups{{Which|date=July 2010}} have found that users have greater trust and usage rates in systems where they can contact a person with questions on each data element.

== Examples ==
{{Expand section|date=July 2010}}

The [http://www.epa.gov/edr [[United States Environmental Protection Agency|EPA]] metadata registry] furnishes an example of data stewardship.  Note that each data element therein has a "POC"  (point of contact).

== Data Stewardship Applications ==
A new market for data governance applications is emerging, one in which both technical and business staff &#8212; stewards &#8212; manage policies. These new applications, like previous generations, deliver a strong business glossary capability, but they don't stop there. Vendors are introducing additional features addressing the roles of business in addition to technical stewards' concerns.&lt;ref&gt;{{Cite web|url=https://www.forrester.com/report/The+Forrester+Wave+Data+Governance+Stewardship+Applications+Q1+2016/-/E-RES117915|title=The Forrester Wave&#8482;: Data Governance Stewardship Applications, Q1 2016|website=www.forrester.com|access-date=2016-12-20}}&lt;/ref&gt;

Information stewardship applications are business solutions used by business users acting in the role of information steward (interpreting and enforcing information governance policy, for example). These developing solutions represent, for the most part, an amalgam of a number of disparate, previously IT-centric tools already on the market, but are organized and presented in such a way that information stewards (a business role) can support the work of information policy enforcement as part of their normal, business-centric, day-to-day work in a range of use cases.

The initial push for the formation of this new category of packaged software came from operational use cases &#8212; that is, use of business data in and between transactional and operational business applications. This is where most of the master data management (MDM) efforts are undertaken in organizations. However, there is also now a faster-growing interest in the new data lake arena for more analytical use cases.&lt;ref&gt;{{Cite web|url=https://www.gartner.com/document/3284717?ref=TypeAheadSearch&amp;qid=744b6ad6c678d064cc2d6eb831a4c959|title=Market Guide for Information Stewardship Applications|last=De Simoni|first=Guido|date=15 April 2016|website=www.gartner.com|publisher=Gartner|access-date=}}&lt;/ref&gt;

==See also==
* [[Metadata]]
* [[Metadata registry]]
* [[Data curation]]
* [[Data element]]
* [[Data element definition]]
* [[Representation term]]
* [[ISO/IEC 11179]]

==References==
* ''Universal Meta Data Models'', by David Marco and Michael Jennings, Wiley, 2004, page 93-94 ISBN 0-471-08177-9
* ''Metadata Solution'' by Adrinne Tannenbaum, Addison Wesley, 2002, page 412
* ''Building and Managing the Meta Data Repository'', by David Marco, Wiley, 2000, pages 61&#8211;62
* ''The Data Warehouse Lifecycle Toolkit'', by [[Ralph Kimball]] et. el., Wiley, 1998, also briefly mentions the role of data steward in the context of data warehouse project management on page 70.
* ''Developing Geospatial Intelligence Stewardship for Multinational Operations'', by Jeff Thomas, US Army Command General Staff College, 2010, www.dtic.mil/dtic/tr/fulltext/u2/a524227.pdf.

==Notes==
{{reflist}}

[[Category:Data management]]
[[Category:Information technology governance]]
[[Category:Knowledge representation]]
[[Category:Library occupations]]
[[Category:Metadata]]
[[Category:Technical communication]]</text>
      <sha1>t8189qv50hzjn1o7qp7ii6exelwgvpz</sha1>
    </revision>
  </page>
  <page>
    <title>Data field</title>
    <ns>0</ns>
    <id>10082565</id>
    <revision>
      <id>650122753</id>
      <parentid>648635373</parentid>
      <timestamp>2015-03-06T09:23:09Z</timestamp>
      <contributor>
        <username>Nedrutland</username>
        <id>3556578</id>
      </contributor>
      <comment>Revert vandalism</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="463" xml:space="preserve">{{Unreferenced|date=March 2007}}
A '''data field''' is a place where you can store [[data]].  Commonly used to refer to a column in a [[database]] or a field in a [[Data entry clerk|data entry]] form or web form.

The field may contain data to be entered as well as data to be displayed.  

==See also== 
{{wiktionary|Data}}
*[[Data dictionary]]
*[[Data element]]
*[[Data acquisition]]
*[[Data hierarchy]]

[[Category:Data management]]

[[it:Campo (informatica)]]</text>
      <sha1>dwkpw0zfuvwc0bqrqmnmcu1v1i4zy84</sha1>
    </revision>
  </page>
  <page>
    <title>Comparison of ADO and ADO.NET</title>
    <ns>0</ns>
    <id>10701295</id>
    <revision>
      <id>635858892</id>
      <parentid>509011794</parentid>
      <timestamp>2014-11-29T04:58:37Z</timestamp>
      <contributor>
        <username>Cydebot</username>
        <id>1215485</id>
      </contributor>
      <minor />
      <comment>Robot - Speedily moving category .NET framework to [[:Category:.NET Framework]] per [[WP:CFDS|CFDS]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3131" xml:space="preserve">''Note: The following content requires a knowledge of [[database]] technologies.''

The following is a comparison of two different database access technologies from [[Microsoft]], namely, [[ActiveX Data Objects|ActiveX Data Objects (ADO)]] and [[ADO.NET]]. Before comparing the two technologies, it is essential to get an overview of [[Microsoft Data Access Components]] (MDAC) and the [[.NET Framework]]. [[Microsoft Data Access Components]] provide a uniform and comprehensive way of developing applications for accessing almost any data store entirely from [[Managed code#Managed and unmanaged|unmanaged code]]. The [[.NET Framework]] is an [[Virtual machine#Application virtual machine|application virtual machine]]-based software environment that provides security mechanisms, [[memory management]], and [[exception handling]] and is designed so that developers need not consider the capabilities of the specific CPU that will execute the .NET application. The .NET [[Virtual machine#Application virtual machine|application virtual machine]] turns [[intermediate language]] (IL) into machine code. High-level language compilers for [[C Sharp (programming language)|C#]], [[Visual Basic .NET|VB.NET]] and [[C++]] are provided to turn source code into IL. [[ADO.NET]] is shipped with the Microsoft NET Framework.

[[ActiveX Data Objects|ADO]] relies on [[Component Object Model|COM]] whereas [[ADO.NET]] relies on managed-providers defined by the .NET [[Common Language Runtime|CLR]]. ADO.NET does not replace ADO for the COM programmer; rather, it provides the .NET programmer with access to relational data sources, XML, and application data.

{| class="wikitable"
|-
! 
! ADO
! ADO.NET
|-
| Business Model
| Connection-oriented Models used mostly
| Disconnected models are used:Message-like Models.
|-
| Disconnected Access
| Provided by Record set
| Provided by Data Adapter and Data set
|-
| [[XML]] Support
| Limited
| Robust Support
|-
|Connection Model
|Client application needs to be connected always to data-server while working on the data, unless using client-side cursors or a disconnected Record set
|Client disconnected as soon as the data is processed. DataSet is disconnected at all times.
|-
|Data Passing
|ADO objects communicate in binary mode.
|ADO.NET uses XML for passing the data.
|-
|Control of data access behaviors
|Includes implicit behaviors that may not always be required in an application and that may therefore limit performance.
|Provides well-defined, factored components with predictable behavior, performance, and semantics.
|-
|Design-time support
|Derives information about data implicitly at run time, based on metadata that is often expensive to obtain.
|Leverages known metadata at design time in order to provide better run-time performance and more consistent run-time behavior.
|}

== References ==
* [http://msdn2.microsoft.com/en-us/library/ms973217.aspx ADO.NET for the ADO programmer]

[[Category:Data management]]
[[Category:.NET Framework]]
[[Category:Microsoft application programming interfaces]]
[[Category:SQL data access]]
[[Category:Software comparisons|ADO and ADO.NET]]</text>
      <sha1>tf10r6yvdr2z92z3s5huh62ofpa9dbr</sha1>
    </revision>
  </page>
  <page>
    <title>Content re-appropriation</title>
    <ns>0</ns>
    <id>2579709</id>
    <revision>
      <id>552574196</id>
      <parentid>499415140</parentid>
      <timestamp>2013-04-28T14:52:30Z</timestamp>
      <contributor>
        <username>BD2412</username>
        <id>196446</id>
      </contributor>
      <minor />
      <comment>/* See also */fixing links from page moves, replaced: [[Taxonomy]] &#8594; [[Taxonomy (general)|Taxonomy]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2383" xml:space="preserve">{{Orphan|date=February 2009}}
Fundamental to modern [[information architecture]]s, and driven by [http://www.webreference.com/internet/semantic/ semantic Web] technologies, '''content re-appropriation''' is the act of searching, filtering, gathering, grouping, and aggregation which allows information to be related, classified and identified.  This is achieved by applying syntactic or semantic meaning though intelligent tagging or artificial interpretation of fragmented content (see [[Resource Description Framework]]).  Hence, all information becomes valuable and interpretable.

==Domain==
Since the domain of Content applies to areas of [[software applications]], [[document]]s, and [[Computer media|media]], these can be processed though a pipeline of generation, aggregation, transform-many, and serialization (see [http://www.w3.org/TR/xml-pipeline/ XML Pipeline]).  The output of this can viewed in a medium most effect for decision making.

The desired outcomes of content re-appropriation are:

*Seamless, Integrated, and Shared User experiences
*[[Software visualization|Visualization]]
*Detection, Analysis &amp; Investigation
*[[Personalization]] unique to the User
*Inbound or Outbound [[web syndication|Syndication]] of Information
*[[Publish]] or [[Subscribe]] to Information
*Dynamically adapted output to Users medium

Essentially to make ''information'' disparities transparent to the [[user (computing)|user]] - getting to the bottom line &#8230; quickly.

==Areas of Use==
Content re-appropriation is effective across the [[Content-Tier]], that is places where Content exists:

*Identity &amp; Directory Management e.g. [[Lightweight Directory Access Protocol|LDAP]], [[Security Assertion Markup Language|SAML]] &amp; [[JNDI]]
*Content Management e.g. [http://jakarta.apache.org/slide/ Apache Slide]
*Content Systems e.g. [[File System]]s, [[E-mail]], [[Network share]]s, [[Storage Area Network|SAN]] &amp; [[Database]]
*Business Systems e.g. [[Enterprise resource planning|ERP]] &amp; [[Customer Relationship Management|CRM]]
*Data Warehouse e.g. [[OLAP]]
*Internet &amp; Web Services e.g. [[HyperText Transfer Protocol|HTTP]] &amp; [[Simple Object Access Protocol|SOAP]]
*[[Instant messenger|Presence]] and [[peer-To-Peer]]

== See also ==
* [[Knowledge visualization]]
* [[Web indexing]]
* [[Taxonomy (general)|Taxonomy]]

[[Category:Data management]]
[[Category:Technical communication]]</text>
      <sha1>8c07tgqqoqyt6815izazuhkir8tqo0k</sha1>
    </revision>
  </page>
  <page>
    <title>Data Reference Model</title>
    <ns>0</ns>
    <id>3576033</id>
    <revision>
      <id>662285168</id>
      <parentid>662275258</parentid>
      <timestamp>2015-05-14T11:14:09Z</timestamp>
      <contributor>
        <username>Melcous</username>
        <id>20472590</id>
      </contributor>
      <minor />
      <comment>Reverted 1 edit by [[Special:Contributions/62.254.171.194|62.254.171.194]] ([[User talk:62.254.171.194|talk]]) to last revision by Omnipaedista. ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3134" xml:space="preserve">[[Image:DRM Collaboration Process.jpg|thumb|320px|The DRM Collaboration Process.]]
The '''Data Reference Model''' ('''DRM''') is one of the five reference models of the [[Federal Enterprise Architecture]] (FEA). 

== Overview ==
The DRM is a framework whose primary purpose is to enable information sharing and reuse across the [[United States federal government]] via the standard description and discovery of common data and the promotion of uniform data management practices. The DRM describes artifacts which can be generated from the data architectures of federal government agencies. The DRM provides a flexible and standards-based approach to accomplish its purpose. The scope of the DRM is broad, as it may be applied within a single agency, within a [[Community of interest (computer security)|Community of Interest]] (COI)1, or cross-COI.

== Data Reference Model topics ==
=== DRM structure ===
The DRM provides a standard means by which [[data]] may be described, categorized, and shared. These are reflected within each of the DRM&#8217;s three standardization areas:

* ''Data Description'': Provides a means to uniformly describe data, thereby supporting its discovery and sharing.
* ''Data Context'': Facilitates discovery of data through an approach to the categorization of data according to taxonomies. Additionally, enables the definition of authoritative data assets within a COI.
* ''Data Sharing'': Supports the access and exchange of data where access consists of ''ad hoc'' requests (such as a query of a data asset), and exchange consists of fixed, re-occurring transactions between parties. Enabled by capabilities provided by both the Data Context and Data Description standardization areas.

===DRM Version 2 ===
The Data Reference Model version 2 released in November 2005 is a 114 page document with detailed architectural diagrams and an extensive glossary of terms.

The DRM also make many references to ISO standards specifically the [[ISO/IEC 11179]] metadata registry standard.

=== DRM usage ===
The DRM is not technically a published technical interoperability standard such as web services, it is an excellent starting point for data architects within federal and state agencies.  Any federal or state agencies that are involved with exchanging information with other agencies or that are involved in [[Data warehouse]]ing efforts should use this document as a guide.

==See also==
* [[Enterprise architecture framework]]
* [[Enterprise application integration]]
* [[Enterprise service bus]]
* [[Federal Enterprise Architecture]]
* [[ISO/IEC 11179]]
* [[Metadata publishing]]
* [[Semantic spectrum]]
* [[Semantic web]]
* [[Synonym ring]]
&lt;!--
== References ==
{{reflist}}--&gt;

==External links==
* [https://web.archive.org/web/20070617034325/http://www.defenselink.mil/cio-nii/docs/DoD_DRM_V04_5aug.pdf US Department of Defense Data Reference Model]
* [http://www.whitehouse.gov/sites/default/files/omb/assets/egov_docs/DRM_2_0_Final.pdf US Federal Enterprise Architecture Program Data Reference Model Version 2.0]
[[Category:Computer data]]
[[Category:Data management]]
[[Category:Reference models]]</text>
      <sha1>sv73w9cg9yj9ypm0fj2b6otbz2io53o</sha1>
    </revision>
  </page>
  <page>
    <title>Ontology-based data integration</title>
    <ns>0</ns>
    <id>11476249</id>
    <revision>
      <id>757143956</id>
      <parentid>749301573</parentid>
      <timestamp>2016-12-29T01:37:16Z</timestamp>
      <contributor>
        <username>Me, Myself, and I are Here</username>
        <id>17619453</id>
      </contributor>
      <comment>cap, punct., refpunct., rm items linked already, rm redlink</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6525" xml:space="preserve">'''Ontology-based data integration''' involves the use of [[ontology (computer science)|ontology]](s) to effectively combine data or information from multiple heterogeneous sources.&lt;ref name="wache"&gt;{{cite conference |author1=H. Wache |author2=T. V&#246;gele |author3=U. Visser |author4=H. Stuckenschmidt |author5=G. Schuster |author6=H. Neumann |author7=S. H&#252;bner | title=Ontology-Based Integration of Information A Survey of Existing Approaches | year=2001 | citeseerx = 10.1.1.142.4390 }}&lt;/ref&gt; It is one of the multiple [[data integration]] approaches and may be classified as Global-As-View (GAV).&lt;ref name="refone"&gt;{{cite conference | author=Maurizio Lenzerini | title=Data Integration:  A Theoretical Perspective | year=2002 | pages=243&#8211;246 | url=http://www.dis.uniroma1.it/~lenzerin/homepagine/talks/TutorialPODS02.pdf }}&lt;/ref&gt; The effectiveness of ontology based data integration is closely tied to the consistency and expressivity of the ontology used in the integration process.

==Background==
Data from multiple sources are characterized by multiple types of heterogeneity. The following hierarchy is often used:&lt;ref name="sheth"&gt;{{cite book | author=A.P. Sheth | title = Changing Focus on Interoperability in Information Systems: From System, Syntax, Structure to Semantics | booktitle=Interoperating Geographic Information Systems. M. F. Goodchild, M. J. Egenhofer, R. Fegeas, and C. A. Kottman (eds.), Kluwer Academic Publishers | year=1999 | pages=5&#8211;30 | url=http://lsdis.cs.uga.edu/library/download/S98-changing.pdf}}&lt;/ref&gt;&lt;ref&gt;[http://daks.ucdavis.edu/~ludaesch/Paper/AHM02/tutorial5.html AHM02 Tutorial 5: Data Integration and Mediation; Contributors: B. Ludaescher, I. Altintas, A. Gupta, M. Martone, R. Marciano, X. Qian]&lt;/ref&gt;
* [[Syntactic heterogeneity]]: is a result of differences in representation format of data
* Schematic or [[structural heterogeneity]]: the native model or structure to store data differ in data sources leading to structural heterogeneity. Schematic heterogeneity that particularly appears in structured databases is also an aspect of structural heterogeneity.&lt;ref name="sheth"/&gt;
* [[Semantic heterogeneity]]: differences in interpretation of the 'meaning' of data are source of semantic heterogeneity
* [[System heterogeneity]]: use of different [[operating system]], hardware platforms lead to system heterogeneity

[[ontology (computer science)|Ontologies]], as formal models of representation with explicitly defined concepts and named relationships linking them, are used to address the issue of [[semantic heterogeneity]] in data sources. In domains like [[bioinformatics]] and [[biomedicine]], the rapid development, adoption and public availability of ontologies [http://www.bioontology.org/repositories.html#obo] has made it possible for the [[data integration]] community to leverage them for [[semantic integration]] of data and information.

==The role of ontologies==

Ontologies enable the unambiguous identification of entities in heterogeneous information systems and assertion of applicable named relationships that connect these entities together. Specifically, ontologies play the following roles:

* Content Explication&lt;ref name="wache"/&gt;
The ontology enables accurate interpretation of data from multiple sources through the explicit definition of terms and relationships in the ontology.

* Query Model&lt;ref name="wache"/&gt;
In some systems like SIMS,&lt;ref name="arens"&gt;{{cite conference |author1=Y. Arens |author2=C. Hsu |author3=C.A. Knoblock | title=Query Processing in sims information mediator | year=1996 | url=http://www.isi.edu/integration/papers/arens98-agents.pdf}}&lt;/ref&gt; the query is formulated using the ontology as a global query schema.

* Verification&lt;ref name="wache"/&gt;
The ontology verifies the mappings used to integrate data from multiple sources. These mappings may either be user specified or generated by a system.

===Approaches using ontologies for data integration===
There are three main architectures that are implemented in ontology-based data integration applications,&lt;ref name="wache"/&gt; namely, 
;Single ontology approach: A single ontology is used as a global reference model in the system. This is the simplest approach as it can be simulated by other approaches.&lt;ref name="wache"/&gt; SIMS&lt;ref name="arens"/&gt;  a prominent example of this approach.  The Structured Knowledge Source Integration component of [[Cyc|Research Cyc]] is another prominent example of this approach.&lt;ref&gt;http://www.cyc.com/content/semantic-knowledge-source-integration&lt;/ref&gt;&lt;ref&gt;http://www.aaai.org/ojs/index.php/aimagazine/article/viewArticle/2299&lt;/ref&gt; (Title = Harnessing Cyc to Answer Clinical Researchers' Ad Hoc Queries)

;Multiple ontologies: Multiple ontologies, each modeling an individual data source, are used in combination for integration. Though, this approach is more flexible than the single ontology approach, it requires creation of mappings between the multiple ontologies. Ontology mapping is a challenging issue and is focus of large number of research efforts in [[computer science]] [http://www.ontologymatching.org/]. The OBSERVER system&lt;ref name="mena"&gt;{{cite conference |author1=E. Mena |author2=V. Kashyap |author3=A. Sheth |author4=A. Illarramendi | title=OBSERVER: An Approach for Query Processing in Global Information Systems based on Interoperation across Pre-existing Ontologies | year=1996 | url=http://dit.unitn.it/~p2p/RelatedWork/Matching/MKSI96.pdf}}&lt;/ref&gt; is an example of this approach.

;Hybrid approaches: The hybrid approach involves the use of multiple ontologies that subscribe to a common, top-level vocabulary.&lt;ref name="goh"&gt;{{cite conference | author=Cheng Hian Goh | title=Representing and Reasoning about Semantic Conflicts in Heterogeneous Information Systems | year=1997 | url=http://context2.mit.edu/coin/publications/goh-thesis/goh-thesis.pdf}}&lt;/ref&gt;  The top-level vocabulary defines the basic terms of the domain. Thus, the hybrid approach makes it easier to use multiple ontologies for integration in presence of the common vocabulary.

==See also==
* [[Data mapping]]
* [[Enterprise application integration]]
* [[Enterprise information integration]]
* [[Ontology mapping]]
* [[Schema matching]]

==References==

&lt;references/&gt;

==External links==
*[http://sid.cps.unizar.es/OBSERVER/ OBSERVER home page]
*[http://www.cyc.com/content/semantic-knowledge-source-integration Cyc Semantic Knowledge Source Integration (SKSI)]

[[Category:Ontology (information science)]]
[[Category:Data management]]</text>
      <sha1>q65uvjpq2t7vmaw2kildrit3buhe3yh</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Databases</title>
    <ns>14</ns>
    <id>2276471</id>
    <revision>
      <id>761378468</id>
      <parentid>732428061</parentid>
      <timestamp>2017-01-22T17:37:50Z</timestamp>
      <contributor>
        <username>JustBerry</username>
        <id>19075131</id>
      </contributor>
      <minor />
      <comment>/* top */Cleaning up... using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="309" xml:space="preserve">{{Commonscat|Databases}}
{{distinguish|Category:Database software}}

:*'''[[Database]]s'''

{{clr}}
::{{Cat main|Database}}
:::::::{{cat see also|Digital libraries}}

{{catdiffuse}}

{{Database}}
{{Databases}}

[[Category:Computer data]]
[[Category:Data management]]
[[Category:Information retrieval systems]]</text>
      <sha1>drsbmwcdombsffa1mp31jztjri4m4x1</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Computer-aided software engineering tools</title>
    <ns>14</ns>
    <id>15189720</id>
    <revision>
      <id>547616000</id>
      <parentid>391541149</parentid>
      <timestamp>2013-03-29T13:32:59Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor />
      <comment>[[User:Addbot|Bot:]] Migrating 2 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q8407536]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="118" xml:space="preserve">{{Cat main|Computer-aided software engineering}}

[[Category:Data management]]
[[Category:Computer programming tools]]</text>
      <sha1>g9quzl4o7qq2ksqp2h5xn5hsrztp0dw</sha1>
    </revision>
  </page>
  <page>
    <title>Reference data</title>
    <ns>0</ns>
    <id>15349103</id>
    <revision>
      <id>752456053</id>
      <parentid>752455472</parentid>
      <timestamp>2016-12-01T10:03:47Z</timestamp>
      <contributor>
        <username>RichardWeiss</username>
        <id>193093</id>
      </contributor>
      <comment>looks like it should be a sectiona nd we need one due to formatting issues</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3104" xml:space="preserve">{{For|use in finance|Reference data (financial markets)}}
'''Reference data''' are [[data]] that define the set of permissible values to be used by other [[data field]]s. Reference data gain in value when they are widely re-used and widely referenced. Typically, they do not change overly much in terms of definition, apart from occasional revisions. Reference data are often defined by standards organizations, such as country codes as defined in [[ISO 3166-1]].&lt;ref&gt;{{Cite web|title = IBM Redbooks {{!}} Reference Data Management|url = http://www.redbooks.ibm.com/abstracts/tips1016.html|website = www.redbooks.ibm.com|date = 2013-05-16|accessdate = 2015-12-09|language = en}}&lt;/ref&gt;&lt;ref&gt;{{Cite web|title = Reference Data Management and Master Data: Are they Related ?   (Oracle Master Data Management)|url = https://blogs.oracle.com/mdm/entry/reference_data_management_and_master|website = blogs.oracle.com|accessdate = 2015-12-09}}&lt;/ref&gt;

Examples of reference data include:
* [[Units of measurement]]
* [[Country code]]s
* Corporate codes
* [[conversion of units|Fixed conversion rates]] e.g., [[weight]], [[temperature]], and [[length]]
* [[Calendar]] structure and constraints

==Differences with master data==
Reference data should be distinguished from [[master data]], which represent key business entities such as customers and materials in all their necessary detail (e.g., for customers: number, name, address, and date of account creation). In contrast, reference data usually consist only of a list of permissible values and attached textual descriptions. A further difference between reference data and master data is that a change to the reference data values may require an associated change in business process to support the change; a change in master data will always be managed as part of existing business processes. For example, adding a new customer or sales product is part of the standard business process. However, adding a new product classification (e.g. restricted sales item) or a new customer type (e.g. gold level customer) will result in a modification to the business processes to manage those items.

==References==
&lt;references /&gt;

==Further reading==
* {{Book reference|title = Managing Reference Data in Enterprise Databases|last = Chisholm|first = Malcolm|publisher = Morgan Kaufmann Publishers|year = 2001|isbn = 1558606971|location = |pages = }}
* {{Book reference|title = Master Data Management for SaaS Applications|last = Whei-Jen|first = Chen|publisher = IBM Redbooks|year = 2014|isbn = 978-0738440040|location = |pages = }}
* {{Book reference|title = Master Data Management and Data Governance|last = Berson|first = Alex|publisher = McGraw-Hill Osborne Media|year = 2011|isbn = 978-0071744584|location = |pages = }}

==See also==
* [[Master Data]]
* [[Data modeling]]
* [[Master Data Management]]
* [[Enterprise bookmarking]]
* [[Data architecture]]
* [[Transaction data]]
* [[Code_(metadata)]]

== External links ==
* [https://msdn.microsoft.com/en-us/library/hh213066.aspx Microsoft MSDN, Reference Data Services in DQS, 2012]

[[Category:Data management]]</text>
      <sha1>qa3v6u3jg1u8zns8ofjkjjvl4vxxpsp</sha1>
    </revision>
  </page>
  <page>
    <title>Operational database</title>
    <ns>0</ns>
    <id>14190258</id>
    <revision>
      <id>759851654</id>
      <parentid>759851590</parentid>
      <timestamp>2017-01-13T14:10:02Z</timestamp>
      <contributor>
        <ip>87.175.207.226</ip>
      </contributor>
      <comment>sorted</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6498" xml:space="preserve">{{more footnotes|date=March 2013}}

Operational database management systems (also referred to as [[OLTP]] On Line Transaction Processing databases), are used to manage dynamic data in real-time. These types of databases allow you to do more than simply view archived data. Operational databases allow you to modify that data (add, change or delete data), doing it in [[Real-time computing|real-time]].

Since the early 90's, the operational database software market has been largely taken over by [[SQL]] engines. Today, the operational [[DBMS]] market (formerly [[OLTP]]) is evolving dramatically, with new, innovative entrants and incumbents supporting the growing use of unstructured data and [[NoSQL]] DBMS engines, as well as [[XML database]]s and [[NewSQL|NewSQL databases]]. Operational databases are increasingly supporting [[distributed database]] architecture that provides [[high availability]] and [[fault tolerance]] through [[replication (computing)|replication]] and scale out ability.

Recognizing the growing role of operational databases in the IT industry that is fast moving from legacy databases to real-time operational databases capable to handle distributed web and mobile demand and to address [[Big data]] challenges, in October 2013 [[Gartner]] started to publish the [[Magic Quadrant]] for Operational Database Management Systems.&lt;ref name="Gartner Magic Quadrant for Operational Database Management Systems"&gt;{{cite web|url=https://www.gartner.com/doc/2610218/magic-quadrant-operational-database-management |title=Gartner Magic Quadrant for Operational Database Management Systems|publisher=Gartner.com}}&lt;/ref&gt;

== List of Operational Databases ==

{| style="text-align: left;" class="wikitable sortable"
|-
! Database platform !! Database model !! [[SQL]] Support !! [[NoSQL]] Support !! Managed objects !! ACID-transactions
|-
| [[Aerospike database|Aerospike]] || Key&#8211;Value Store ||  No || '''Yes''' || key-value pairs || None
|-
| [[Altibase]] || Relational database || '''Yes''' || NO || tabular data || Real-time ACID transactions
|-
| [[Apache Cassandra]] || Key-value store || No || '''Yes''' || key-value pairs || None
|-
| [[Cloudant]] || Document-Oriented Database || No || '''Yes''' || JSON || None
|-
| [[Clusterpoint]] || Document-Oriented Database || '''Yes''' (essential SQL)  || '''Yes''' || XML, JSON, text data || Distributed ACID-transactions 
|-
| [[Clustrix]] || Relational Database || '''Yes''' (newSQL) || No || tabular data || ACID-transactions 
|-
| [[Couchbase]] || Document-Oriented Database || '''Yes''' (N1QL) || '''Yes''' || JSON || None
|-
| [[CouchDB]] || Document-Oriented Database || No || '''Yes''' || JSON || None 
|-
| [[EnterpriseDB]] || Relational Database || '''Yes''' || No || tabular data || ACID-transactions 
|-
| [[FoundationDB]] || Key-value store || '''Yes''' || No || key-value pairs || ACID-transactions 
|-
| [[IBM DB2]] || Relational Database || '''Yes''' || No || tabular data || ACID-transactions 
|-
| [[Ingres_(database)|Ingres]] || Relational Database || '''Yes''' || No || tabular data || ACID-transactions 
|-
| [[MarkLogic]] || Document-Oriented Database || No || '''Yes''' (XQuery) || XML || ACID-transactions
|-
| [[Microsoft SQL Server]] || Relational Database || '''Yes''' || No || tabular data || ACID-transactions
|-
| [[MongoDB]] || Document-Oriented Database || No || '''Yes''' || BSON || None
|-
| [[NuoDB]] || Relational Database || '''Yes''' (newSQL) || No || tabular data || ACID-compliant
|-
| [[Oracle Database|Oracle]] || Relational Database || '''Yes''' || No || tabular data || ACID-transactions
|-
| [[OrientDB]] || Document-oriented Database || '''Yes''' || Yes || key-value pairs || ACID-transactions&lt;ref&gt;http://orientdb.com/docs/last/Transactions.html&lt;/ref&gt;
|-
| [[Riak]] || Key-value store || No || '''Yes''' || key-value pairs || None
|-
| [[SAP HANA]] || Relational Database || '''Yes''' || No || tabular data || ACID-transactions 
|-
| [[VoltDB]] || Relational Database || '''Yes''' (newSQL) || No || tabular data || ACID-transactions

|}

== Use in business ==

Operational databases are used to store, manage and track real-time business information. For example, a company might have an operational database used to track warehouse/stock quantities. As customers order products from an online web store, an operational database can be used to keep track of how many items have been sold and when the company will need to reorder stock.  An '''operational database''' stores information about the activities of an [[organization]], for example [[customer relationship management]] transactions or financial operations, in a computer [[database]].

Operational databases allow a business to enter, gather, and retrieve large quantities of specific information, such as company legal data, financial data, call data records, personal employee information, sales data, customer data, data on assets and many other information.  An important feature of storing information in an operational database is the ability to share information across the company and over the Internet.  Operational databases can be used to manage mission-critical business data, to monitor activities, to audit suspicious transactions, or to review the history of dealings with a particular customer.  They can also be part of the actual process of making and fulfilling a purchase, for example in [[e-commerce]].

==Data warehouse terminology==

In [[Data warehouse|data warehousing]], the term is even more specific: the operational database is the one which is accessed by an [[operational system]] (for example a customer-facing website or the application used by the customer service department) to carry out regular operations of an organization. Operational databases usually use an [[online transaction processing]] database which is optimized for faster transaction processing ([[create, read, update and delete]] operations).

== See also ==
* [[Document database|Document-oriented databases]]
* [[NewSQL|NewSQL databases]]
* [[NoSQL|NoSQL databases]]
* [[XML|XML databases]]
* [[SQL|SQL databases]]
* [[Distributed database]]s

== References ==
{{Reflist|33em}}
* O&#8217;Brien, Jason., and Marakas, Gorila., (2008).  Management Information Technology Systems.  Computer Software (pp.&amp;nbsp;185). New York, New York:  McGraw-Hill

[[Category:Data warehousing]]
[[Category:Data management]]
[[Category:Information technology management]]
[[Category:Business intelligence]]
[[Category:Types of databases]]</text>
      <sha1>4w0vccaek7mtzvgl6rd2ae46zfm3y57</sha1>
    </revision>
  </page>
  <page>
    <title>Information architecture</title>
    <ns>0</ns>
    <id>185945</id>
    <revision>
      <id>762880615</id>
      <parentid>762880536</parentid>
      <timestamp>2017-01-31T05:57:44Z</timestamp>
      <contributor>
        <username>JueLinLi</username>
        <id>29760810</id>
      </contributor>
      <comment>Reverted to revision 762078237 by [[Special:Contributions/Swpb|Swpb]] ([[User talk:Swpb|talk]]). ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="10201" xml:space="preserve">{{Information science}}
'''Information architecture''' ('''IA''') is the structural design of shared [[information]] environments; the art and science of organizing and labelling [[website]]s, [[intranet]]s, [[online communities]] and [[software]] to support usability and findability; and an emerging [[community of practice]] focused on bringing principles of [[design]] and [[architecture]] to the digital landscape.&lt;ref name = "What"&gt;{{Cite journal | title = What is IA? | publisher = Information Architecture Institute | url = http://www.iainstitute.org/documents/learn/What_is_IA.pdf | format = [[PDF]] | postscript = &lt;!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. --&gt;{{inconsistent citations}}}}.&lt;/ref&gt;  Typically, it involves a [[Scientific modelling|model]] or [[concept]] of [[information]] that is used and applied to activities which require explicit details of complex [[information system]]s. These activities include [[library]] systems and [[database]] development.

Information architecture is considered to have been founded by [[Richard Saul Wurman]].&lt;ref name = "Richard Saul Wurman, Cooper-Hewitt"&gt;{{cite web|title=Richard Saul Wurman awarded for Lifetime Achievement|url=http://wurman.com/rsw/|publisher=Smithsonian Cooper-Hewitt, National Design Museum|accessdate=19 April 2014}}&lt;/ref&gt; Today there is a growing network of active IA specialists who constitute the [[Information Architecture Institute]].&lt;ref&gt;{{Cite journal | title = Join the IA Network | publisher = Information Architecture Institute | url = http://www.iainstitute.org/en/network/ | postscript = &lt;!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. --&gt;{{inconsistent citations}}}}.&lt;/ref&gt;

==Definition==
''Information architecture'' has somewhat different meanings in different branches of [[Information system|IS]] or [[Information technology|IT]]:
# The structural design of shared information environments.{{Sfn | Morville | Rosenfeld | 2007}}{{Rp|4}}
# The art and science of organizing and labeling web sites, intranets, online communities, and software to support [[findability]] and [[usability]].&lt;ref name="What"/&gt;&lt;ref&gt;Morville&amp;nbsp;&amp;amp; Rosenfeld (2007). p.&amp;nbsp;4. "The art and science of shaping information products and experienced to support usability and findability."&lt;/ref&gt;
# An emerging [[community of practice]] focused on bringing principles of design and architecture to the digital landscape.{{Sfn | Morville | Rosenfeld | 2007}}{{Rp|4}}&lt;ref&gt;Resmini, A. &amp; Rosati, L. (2012). A Brief History of Information Architecture. ''Journal of Information Architecture''. Vol. 3, No. 2. [Available at http://journalofia.org/volume3/issue2/03-resmini/]. Originally published in Resmini, A. &amp; Rosati L. (2011). ''Pervasive Information Architecture''. Morgan Kauffman. (Edited by the authors).&lt;/ref&gt;
# The combination of organization, labeling, search and navigation systems within websites and intranets.{{Sfn | Morville | Rosenfeld | 2007}}{{Rp|4}}
# Extracting required parameters/data of Engineering Designs in the process of creating a knowledge-base linking different systems and standards.  
# A subset of [[data architecture]] where usable data (a.k.a. information) is constructed in and designed or arranged in a fashion most useful or empirically holistic to the users of this data.
# The practice of organizing the information / content / functionality of a web site so that it presents the best user experience it can, with information and services being easily usable and findable (as applied to web design and development).&lt;ref&gt;{{Cite web|url=https://developer.mozilla.org/en-US/docs/Glossary/Information_architecture|title=Information Architecture|last=|first=|date=|website=|publisher=Mozilla Developer Network|access-date=}}&lt;/ref&gt;

=== Debate ===
The difficulty in establishing a common definition for "information architecture" arises partly from the term's existence in multiple fields.  In the field of [[systems design]], for example, information architecture is a component of [[enterprise architecture]] that deals with the information component when describing the structure of an enterprise.

While the definition of information architecture is relatively well-established in the field of systems design, it is much more debatable within the context of online information systems (i.e., websites). Andrew Dillon refers to the latter as the "big IA&#8211;little IA debate".&lt;ref&gt;{{Cite journal | last = Dillon | first = A | year = 2002 | title = Information Architecture in JASIST: Just where did we come from? | journal = Journal of the American Society for Information Science and Technology | volume = 53 | pages = 821&#8211;23 | issue = 10 | doi = 10.1002/asi.10090 | postscript = &lt;!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. --&gt;{{inconsistent citations}}}}.&lt;/ref&gt; In the little IA view, information architecture is essentially the application of [[information science]] to [[web design]] which considers, for example, issues of classification and information retrieval. In the big IA view, information architecture involves more than just the organization of a website; it also factors in [[user experience]], thereby considering [[usability]] issues of [[information design]].

==Information architect==
About the term '''information architect''' [[Richard Saul Wurman]] wrote: "I mean architect as used in the words ''architect of foreign policy''. I mean architect as in the creating of systemic, structural, and orderly principles to make something work &#8212; the thoughtful making of either artifact, or idea, or policy that informs because it is clear."&lt;ref&gt;Wurman, "Introduction", in: ''Information Architects'' (1997). p. 16.&lt;/ref&gt;

==Notable people in information architecture==

===Pioneers===
*[[Richard Saul Wurman]]
*[[Peter Morville]]
*[[Louis Rosenfeld]]

===First generation===
*Jorge Arango
*[[Jesse James Garrett]]
*[[Adam Greenfield]]
*[[Peter Merholz]]
*[[Eric Reiss]]
*[[Donna Spencer]]
*[[Christina Wodtke]]

===Second generation===
*[[Abby Covert]]
*[[Andrew Hinton]]
*[[Dan Klyn]]
*[[Andrea Resmini]]

===Influencers===
*[[David Weinberger]]

== See also ==
{{Div col}}
* [[Applications architecture]]
* [[Card sorting]]
* [[Chief experience officer]]
* [[Content management]]
* [[Content strategy]]
* [[Controlled vocabulary]]
* [[Data management]]
* [[Data presentation architecture]]
* [[Digital humanities]]
* [[Ecological interface design]]
* [[Enterprise information security architecture]]
* [[Faceted classification]]
* [[Human factors and ergonomics]]
* [[Informatics]]
* [[Interaction design]]
* [[Process architecture]]
* [[Site map]]
* [[Social information architecture]]
* [[Tree testing]]
* [[User experience design]]
* {{section link|Visualization (graphics)|Knowledge visualization}}
* [[Wayfinding]]
* [[Web graph]]
* [[Web literacy]] (Infrastructure)
{{Div col end}}

== References ==
{{reflist}}

== Bibliography ==
* {{Cite book | editor-last1 = Wurman | editor-first1 = Richard Saul | editor1-link = Richard Saul Wurman | isbn = 1-888-00138-0 | url = http://www.amazon.com/dp/1888001380 | year = 1997 | title = Information Architects | edition = 1st | publisher = Graphis Inc. }}
* {{Cite book | last2 = Rosenfeld | first2 = Louis | author2-link = Lou Rosenfeld | last1 = Morville | first1 = Peter | author1-link = Peter Morville | isbn = 0-596-52734-9 | url = https://books.google.com/books?id=2d2Ry2hZc2MC&amp;printsec=frontcover&amp;dq=information+architecture#v=onepage&amp;q&amp;f=false | year = 2007 | title = Information architecture for the World Wide Web | edition = 3rd|publisher = O'Reilly &amp; Associates | place = Sebastopol, CA | ref=harv}}
* {{Cite book | last1 = Brown | first1 = Peter  | isbn = 0-471-48679-5 | url = http://www.amazon.com/dp/0471486795 | year = 2003 | title = Information Architecture with XML | edition = 1st | publisher = John Wiley &amp; Sons Ltd. }}
* {{Cite book | last1 = Wodtke | first1 = Christina | author1-link = Christina Wodtke | isbn = 0-321-60080-0 | url = https://books.google.com/books?id=Tp40QFGCU2sC | year = 2009 | title = Information Architecture - Blueprints for the Web | edition = 2nd | publisher = New Riders }}
* {{Cite book | last1 = Resmini | first1 = Andrea | last2 = Rosati | first2 = Luca | isbn = 0-123-82094-4 | url = https://books.google.com/books?id=ntWc13nSiNkC | year = 2011 | title = Pervasive Information Architecture - Designing Cross-channel User Experiences | edition = 1st | publisher = Morgan Kauffman }}

== Further reading ==
* {{cite book|author1=Wei Ding|author2=Xia Lin|title=Information Architecture: The Design and Integration of Information Spaces|url=https://books.google.com/books?id=-wy3RhKoWWQC|date= 15 May 2009 | publisher=Morgan &amp; Claypool |isbn=978-1-59829-959-5}}
* {{cite book|author1=Sue Batley|title=Information Architecture for Information Professionals|url=https://books.google.com/books?id=6g0PAQAAMAAJ|date=January 2007| publisher=Woodhead Publishing |isbn=978-1-84334-233-5}}
* {{cite book|author1=Earl Morrogh|title=Information Architecture: An Emerging 21st Century Profession
|url=https://books.google.com/books?id=JzlmQgAACAAJ&amp;dq|year=2003| publisher=Prentice Hall |isbn=9780130967466}}
* {{cite book|author1=Peter Van Dijck|title=Information Architecture for Designers: Structuring Websites for Business Success |url=https://books.google.com/books?id=Wy2sb0r_udYC&amp;dq|date=August 1, 2003| publisher=Rotovision|isbn=9782880467319}}
* {{cite book|author1=Alan Gilchrist|author2=Barry Mahon|title=Information Architecture: Designing Information Environments for Purpose|url=https://books.google.com/books?id=akxqAAAAMAAJ&amp;q|year=2004| publisher=Facet|isbn=9781856044875}}

{{Semantic Web}}

[[Category:Data management]]
[[Category:Enterprise architecture]]
[[Category:Information architects]]
[[Category:Information governance]]
[[Category:Information science]]
[[Category:Information technology management]]
[[Category:Information technology]]
[[Category:Records management]]
[[Category:Technical communication]]
[[Category:Information architecture| ]]</text>
      <sha1>01t37r1ti7pg69hj4e5y3ygagws1ff8</sha1>
    </revision>
  </page>
  <page>
    <title>Retention period</title>
    <ns>0</ns>
    <id>3544624</id>
    <revision>
      <id>754590631</id>
      <parentid>717236511</parentid>
      <timestamp>2016-12-13T13:31:53Z</timestamp>
      <contributor>
        <username>Cydebot</username>
        <id>1215485</id>
      </contributor>
      <minor />
      <comment>Robot - Removing category Administration per [[WP:CFD|CFD]] at [[Wikipedia:Categories for discussion/Log/2016 October 25]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2147" xml:space="preserve">The '''retention period''' of information is an aspect of [[records management|records and information management]] (RIM) and the [[records life cycle]].  It identifies the duration of time for which the information should be maintained or "retained", irrespective of format (paper, electronic, or other).  Retention periods vary on different types of information, based on content and a variety of other factors including: internal organizational need, regulatory requirements for inspection or audit, legal statutes of limitation, involvement in litigation, taxation and financial reporting needs, as well as other factors as defined by local, regional, state, national and/or international governing entities.  

Once an applicable retention period has elapsed for a given type or series of information, and all holds/moratoriums have been released, the information is typically destroyed using an approved and effective destruction method, which renders the information completely and irreversibly unusable via any means.  Information with historical value beyond its "usable value" may be accessioned to the custody of an archive organization for permanent or extended long-term preservation.  

==Defensible retention==
''Defensible retention'' refers to the ability of an identified and applied retention period to effectively provide for the defense of the record, and its eventual destruction or accessioning when scrutinized within a court of law or by other review.

It is commonly advised by [[Records management|Records and Information Management]] (RIM) professionals that any and all retention periods applied to organizational information should be reviewed and approved for use by competent legal counsel, which represents the organization, and is familiar with the specific legal and regulatory requirements of the organization.

==Guidance and education organizations==
*[[ARMA International]]
*[[Information and Records Management Society]]

==See also==
*[[Retention schedule]]

==References==
&lt;references/&gt;

[[Category:Legal documents]]
[[Category:Data management]]
[[Category:Public records]]
[[Category:Records management]]</text>
      <sha1>cjmsfebehr3s7ov4437v51r97iaxgme</sha1>
    </revision>
  </page>
  <page>
    <title>Conference on Innovative Data Systems Research</title>
    <ns>0</ns>
    <id>21047573</id>
    <revision>
      <id>607088523</id>
      <parentid>580102254</parentid>
      <timestamp>2014-05-04T23:01:53Z</timestamp>
      <contributor>
        <username>BattyBot</username>
        <id>15996738</id>
      </contributor>
      <minor />
      <comment>changed {{Notability}} to {{Notability|Events}} &amp; [[WP:AWB/GF|general fixes]] using [[Project:AWB|AWB]] (10095)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1244" xml:space="preserve">{{multiple issues|
{{notability|Events|date=September 2011}}
{{primary sources|date=September 2011}}
}}

{{Infobox Academic Conference
 | history = 2002&#8211;
 | discipline = [[Database]]
 | abbreviation = CIDR
 | publisher = CIDR Conference
 | country= [[United States]]
 | frequency = biennial
}}
The '''Conference on Innovative Data Systems Research''' ('''CIDR''') is a biennial [[computer science]] conference focused on research into new techniques for [[data management]]. It was started in 2002 by [[Michael Stonebraker]], [[Jim Gray (computer scientist)|Jim Gray]], and [[David DeWitt]], and is held at the [[Asilomar Conference Grounds]] in [[Pacific Grove, California]].

CIDR focuses on presenting work that is more speculative, radical, or provocative than what is typically accepted by the traditional database research conferences (such as the [[International Conference on Very Large Data Bases]] (VLDB) and the [[ACM SIGMOD Conference]]).

==See also==
* [[International Conference on Very Large Data Bases]] (VLDB)
* [[ACM SIGMOD Conference]]

==External links==
* [http://www-db.cs.wisc.edu/cidr/ CIDR website]

[[Category:Data management]]
[[Category:Computer science conferences]]


{{database-stub}}
{{compu-conference-stub}}</text>
      <sha1>p45qi95t5l2xu1jl8o6gnsy9n14zd1o</sha1>
    </revision>
  </page>
  <page>
    <title>Data verification</title>
    <ns>0</ns>
    <id>22601719</id>
    <revision>
      <id>720573269</id>
      <parentid>720573248</parentid>
      <timestamp>2016-05-16T18:30:17Z</timestamp>
      <contributor>
        <username>Harry-</username>
        <id>4438764</id>
      </contributor>
      <minor />
      <comment>Reverted edits by [[Special:Contributions/168.167.94.5|168.167.94.5]] ([[User talk:168.167.94.5|talk]]): Nonconstructive editing ([[WP:HG|HG]]) (3.1.20)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="951" xml:space="preserve">Data''' Verification''' is a process in which different types of data are checked for accuracy and [[data consistency|inconsistencies]] after [[data migration]] is done.&lt;ref&gt;http://www.datacap.com/products/features/verify/&lt;/ref&gt;

It helps to determine whether data was accurately translated when data is [[data transfer|transferred]] from one source to another, is complete, and supports processes in the new system. During verification, there may be a need for a parallel run of both systems to identify areas of disparity and forestall erroneous [[data loss]].

A type of Data Verification is [[double entry]] and [[proofreading]] data. Proofreading data involves someone checking the data entered against the original document. This is also time consuming and costly.

==References==
{{reflist|2}}

==External links==
* [http://www.pcguide.com/care/bu/howVerification-c.html PC Guide article]

[[Category:Data management]]
[[Category:Data quality]]</text>
      <sha1>f9ton9h4h8m1c75py70ur94gk7ep72y</sha1>
    </revision>
  </page>
  <page>
    <title>Consumer relationship system</title>
    <ns>0</ns>
    <id>21395468</id>
    <revision>
      <id>740274277</id>
      <parentid>740274095</parentid>
      <timestamp>2016-09-20T02:36:19Z</timestamp>
      <contributor>
        <username>Me, Myself, and I are Here</username>
        <id>17619453</id>
      </contributor>
      <comment>/* See also */ add link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2013" xml:space="preserve">'''Consumer relationship systems''' ('''CRS''') are specialized [[customer relationship management]] (CRM) [[software]] applications that are used to handle a company's dealings with its customers.&lt;ref name ="Insight44-50"&gt;[http://www.nxtbook.com/nxtbooks/cmp/cmi_200709/index.php ICMI Customer Management Insight Magazine, September 2007, pp 44&#8211;50], Retrieved 11 January 2012&lt;/ref&gt;

Current consumer relationship systems integrate the [[software]] with telephone and call recording systems as well as with corporate systems for input and reporting. Customers can provide input from the company's website directly into the CRS. These systems are popular because they can deliver the 'voice of the consumer' that contributes to product quality improvement and that ultimately increases corporate profits.&lt;ref name ="Insight44-50" /&gt;

Consumer relationship systems that provide automated support as well as advanced systems may have [[artificial intelligence]] (AI) interfaces that can extract and analyse [[data]] collected, or handle basic questions and complaints.&lt;ref&gt;{{cite web|last1=Smith| first1=S.E.|title= What is Consumer Relationship System? |date= |publisher= WiseGeek.net|url= http://www.wisegeek.net/what-is-consumer-relationship-system.htm|accessdate=1 February 2013}}&lt;/ref&gt;

==History==
The first CRS was developed in the 1980s. In 1981 Michael Wilke and Robert Thornton founded Wilke/Thornton, Inc in [[Columbus, Ohio]], to develop new CRS software.&lt;ref&gt;[http://www.wilke-thornton.com/WTI/Pages/products.html Wilke/Thornton, Inc Products] Retrieved 11 January 2012&lt;/ref&gt;

==See also==
* [[ECRM]]
* [[Business intelligence]]
* [[Customer experience]]
* [[Customer intelligence]]
* [[Customer service]] &#8211; contains ISO standards
* [[Customer value maximization]]
* [[Enterprise relationship management]] (ERM)
* [[Sales force management system]]
* [[Sales intelligence]]
* [[Sales process engineering]]

== References ==
{{reflist}}

[[Category:Business intelligence]]
[[Category:Data management]]</text>
      <sha1>hvxaglisjcm466weacsx1d6csoatnpl</sha1>
    </revision>
  </page>
  <page>
    <title>Vocabulary-based transformation</title>
    <ns>0</ns>
    <id>3219147</id>
    <revision>
      <id>624461101</id>
      <parentid>618660500</parentid>
      <timestamp>2014-09-06T21:05:40Z</timestamp>
      <contributor>
        <username>Brenont</username>
        <id>4034676</id>
      </contributor>
      <minor />
      <comment>typo</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2089" xml:space="preserve">{{unreferenced|date=July 2013}}
In [[metadata]], a '''vocabulary-based transformation (VBT)''' is a transformation aided by the use of a [[semantic equivalence]] statements within a [[controlled vocabulary]].

Many organizations today require communication between two or more computers.  Although many standards exist to exchange data between computers such as [[HTML]] or [[email]], there is still much structured information that needs to be exchanged between computers that is not standardized.  The process of mapping one source of data into another is often a slow and labor-intensive process.

VBT is a possible way to avoid much of the time and cost of manual data mapping using traditional [[Extract, transform, load]] technologies.

== History ==

The term ''vocabulary-based transformation'' was first defined by Roy Shulte of the [[Gartner Group]] around May 2003 and appeared in annual "[[hype cycle|hype]]-cycle" for [[data integration|integration]].

== Application ==
VBT allows computer systems integrators to more automatically "look up" the definitions of data elements in a centralized [[data dictionary]] and use that definition and the equivalent mappings to transform that data element into a foreign [[namespace]].

The [[Web Ontology Language]] (OWL) language also support three [[semantic equivalence]] statements.

== Companies or products ==
* [[IONA Technologies]]
* [http://liaison.com/products/transform Contivo and Delta] by [http://liaison.com/ Liaison Technologies]
* enLeague Systems
* ItemField
* Unicorn Solutions
* Vitria Technology
* Zonar

== See also ==
* [[metadata]]
* [[Controlled vocabulary]]
* [[Data dictionary]]
* [[Semantic spectrum]]
* [[Semantic equivalence]]
* [[XSLT]]
* [[Enterprise Application Integration]]

==External links==
* [http://www.gartner.com/6_help/glossary/GlossaryV.jsp Gartner Glossary of Terms] Gartner definition Vocabulary-based transformation
* [http://www.sun.com/service/openwork/analyst/Gartner_Hype_Cycle.pdf Gartner Hype Cycle 2003]

{{DEFAULTSORT:Vocabulary-Based Transformation}}
[[Category:Data management]]</text>
      <sha1>ndf9nng75ldqegmz2fn6oul1cd4olir</sha1>
    </revision>
  </page>
  <page>
    <title>Hybrid array</title>
    <ns>0</ns>
    <id>24278544</id>
    <revision>
      <id>746062178</id>
      <parentid>743727892</parentid>
      <timestamp>2016-10-25T01:51:02Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* top */http&amp;rarr;https for [[Google Books]] and [[Google News]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6956" xml:space="preserve">A '''hybrid array''' is a form of [[hierarchical storage management]] that combines [[hard disk drive]]s (HDDs) with [[solid-state drive]]s (SSDs) for [[I/O]] speed improvements.

Hybrid storage arrays aim to mitigate the ever increasing price-performance gap between HDDs and [[DRAM]] by adding a non-volatile flash level to the [[memory hierarchy]].&lt;ref name="MicheloniMarelli2012"&gt;{{cite book|author1=Rino Micheloni|author2=Alessia Marelli|author3=Kam Eshghi|title=Inside Solid State Drives (SSDs)|url=https://books.google.com/books?id=S8xRtkF7hUkC&amp;pg=PA62|year=2012|publisher=Springer|isbn=978-94-007-5145-3|page=62}}&lt;/ref&gt; Hybrid arrays thus aim to lower the cost per I/O, compared to using only SSDs for storage.  Hybrid architectures can be as simple as involving a single SSD [[Cache (computing)|cache]] for desktop or laptop computers, or can be more complex as configurations for [[data center]]s and [[cloud computing]].

== Implementations ==
&lt;!-- please only add products covered in [[WP:SECONDARY]] sources at adequate depth --&gt;
Some commercial products for building hybrid arrays include:
* [[Adaptec]] demonstrated the MaxIQ series in 2009.&lt;ref&gt;{{cite web |author=Charlie Demerjian |url= http://semiaccurate.com/2009/09/09/adaptecs-maxiq-caches-raids-ssds/ |title= Adaptec's MaxIQ caches RAIDs with SSDs |publisher= SemiAccurate |date= September 9, 2009 |accessdate= October 10, 2016 }}&lt;/ref&gt;
* Apple's [[Fusion Drive]]
*  [[Linux]] software includes [[bcache]], [[dm-cache]], and [[Flashcache]] (and its fork EnhanceIO).
* Condusive's [[ExpressCache]] is marketed for laptops.
* [[EMC Corporation]] VFcache was announced in 2012.&lt;ref&gt;{{cite web |last= Larry Dignan |url= http://www.zdnet.com/blog/btl/emc-unveils-vfcache-targets-fusion-io/68657 |title=EMC unveils VFCache, targets Fusion-io |publisher= ZDNet |work= Between the Lines |date= February 5, 2012 |accessdate= October 10, 2016 }}&lt;/ref&gt;&lt;ref&gt;{{Cite news |title= One day later: EMC declares war on all-flash array, server flash card rivals: Rolls out XtremIO array, renamed VFCache |date= March 5, 2013 |work= The Register |author= Chris Mellor |url= http://www.theregister.co.uk/2013/03/05/emc_xtremsf/ |accessdate= October 10, 2016 }}&lt;/ref&gt;
* [[Fusion-io]] acquired ioTurbine in 2011,&lt;ref name="io"&gt;{{cite web |url= http://www.theregister.co.uk/2013/06/25/fusionio_spins_ioturbine_faster/ |title=Fusion-io spins up ioTurbine, enhances server flash caching |work= The Register |accessdate= October 10, 2016 }}&lt;/ref&gt; and the product line it acquired by buying NexGen in 2013.&lt;ref&gt;{{cite web|url=http://www.theregister.co.uk/2013/04/24/fusion_io_nexgen/|title=Fusion-io buys NexGen|work=theregister.co.uk |accessdate=2015-03-26}}&lt;/ref&gt;
* [[Hitachi]] Accelerated Flash Storage (HAFS) used together with the Hitachi Dynamic Tiering software&lt;ref&gt;{{citation |url=http://www.computerweekly.com/feature/Big-storage-turns-the-tide-in-the-hybrid-flash-array-market |title=Big storage turns the tide in the hybrid flash array market |work=[[Computer Weekly]] |date=September 2013 |accessdate=2015-03-26}}&lt;/ref&gt;
* [[IBM]] Flash Cache Storage Accelerator (FCSA) server software&lt;ref&gt;{{cite web|author=The SSD Guy |url=http://thessdguy.com/ibm-adds-server-side-caching/ |title=IBM Adds Server-Side Caching |publisher=The SSD Guy |date=2013-08-20 |accessdate=2013-12-23}}&lt;/ref&gt;
* Intel's [[Smart Response Technology]] for desktop
* Intel's [[Cache Acceleration Software]] for servers and workstations 
* [[LSI Corporation|LSI]] CacheCade software for their controllers&lt;ref&gt;{{cite web|url=http://www.storagereview.com/lsi_megaraid_cachecade_pro_20_review|title=LSI MegaRAID CacheCade Pro 2.0 Review |accessdate=2015-03-26 |work=storagereview.com}}&lt;/ref&gt;
* [[Marvell Technology Group|Marvell]]'s HyperDuo controllers&lt;ref&gt;{{cite web|url=http://www.cnet.com/8301-32254_1-20027657-283.html|title=Hands-on with the Marvell HyperDuo hybrid storage controller |accessdate=2015-03-26 |publisher=CBS Interactive|work=CNET}}&lt;/ref&gt;
* Microsoft's [[Automated Tiering]] (since Windows 2012 R2)
* [[NetApp]]'s Flash Cache, Flash Pool, Flash Accel&lt;ref&gt;{{cite web|url=http://www.theregister.co.uk/2012/08/21/netapp_server_flash/|title=NetApp: Flash as a STORAGE tier? You must be joking |accessdate=2015-03-26 |work=theregister.co.uk}}&lt;/ref&gt;
* [[Oracle Corporation]] markets products such as [[Exadata]] Smart Cache Flash, and the FS1 flash storage system.&lt;ref&gt;{{Cite news |title= Oracle crashes all-flash bash: Behold, our hybrid FS1 arrays: Mutant flash/disk box a pillar of storage: It's axiomatic |date= September 30, 2014 |work= The Register |author= Chris Mellor |url= http://www.theregister.co.uk/2014/09/30/the_fs1_pillar_of_oracle_arrays_storage/ |accessdate= October 10, 2016 }}&lt;/ref&gt;
* Microsoft [[ReadyBoost]] allows personal computers to [[USB flash drive]]s as cache.
* Nvelo DataPlex SSD caching software was announced in 2011,&lt;ref&gt;{{cite web |url= http://www.thessdreview.com/our-reviews/nvelo-dataplex-ssd-caching-software-review-seven-msata-ssds-prove-an-amazing-concept/  |title= NVELO Dataplex SSD Caching Software Review - Seven mSATA SSDs Prove An Amazing Concept |work= The SSD Review |date= December 4, 2011 |author= Les Tokar |accessdate= October 10, 2016 }}&lt;/ref&gt; and was acquired by [[Samsung]] in 2012.&lt;ref&gt;{{cite web |url= http://www.anandtech.com/show/6518/samsung-acquires-ssd-caching-company-nvelo |title=Samsung Acquires SSD Caching Company NVELO |publisher=AnandTech |author= Kristian V&#228;tt&#246; |date= December 16, 2012 |accessdate= October 10, 2016 }}&lt;/ref&gt;
* [[SanDisk]] FlashSoft for Windows, Linux, and [[vSphere]]&lt;ref name="io"/&gt;
* Products are offered by vendors like AMI [[StorTrends]],&lt;ref&gt;{{cite web|author=Ian Barker |url=http://betanews.com/2014/01/27/ami-stortrends-3500i-offers-high-performance-storage-for-smaller-enterprises/ |title=AMI StorTrends 3500i offers high performance storage for smaller enterprises |publisher=BetaNews |date=2014-01-27 |accessdate=2014-10-17}}&lt;/ref&gt; [[Tegile Systems]], [[Reduxio]], and [[Tintri]].&lt;ref&gt;{{cite web|url=http://www.theregister.co.uk/2013/04/09/blind_spot/ |accessdate=2015-03-26 |title=Mutant array upstarts feast on EMC, NetApp's leavings|work= The Register }}&lt;/ref&gt;
* [[ZFS]] using hybrid storage pools, are used for example in some Oracle Corporation products.&lt;ref&gt;{{cite web|url=http://www.enterprisestorageforum.com/san-nas-storage/oracles-flash-friendly-sun-zfs-storage-is-ready-for-new-sparcs.html|title=Oracle's Flash-Friendly Sun ZFS Storage Is Ready for New SPARCs|date=3 April 2013 |accessdate=2015-03-26 |work=enterprisestorageforum.com}}&lt;/ref&gt;

== See also ==
* [[Hybrid drive]]{{spaced ndash}} built-in flash cache, handled by firmware
* [[Automated tiered storage]]{{spaced ndash}} another name for hierarchical storage management
* The "[[five-minute rule]]" for caching

== References ==
{{Reflist|30em}}

[[Category:Data management]]
[[Category:Solid-state caching]]
[[Category:Memory management software]]</text>
      <sha1>3wme3jsmip0axzrum680jbqneer1l5r</sha1>
    </revision>
  </page>
  <page>
    <title>Data extraction</title>
    <ns>0</ns>
    <id>12097860</id>
    <revision>
      <id>758734714</id>
      <parentid>758615608</parentid>
      <timestamp>2017-01-07T06:46:30Z</timestamp>
      <contributor>
        <username>BG19bot</username>
        <id>14508071</id>
      </contributor>
      <minor />
      <comment>[[WP:CHECKWIKI]] error fix for #03.  Missing Reflist.  Do [[Wikipedia:GENFIXES|general fixes]] if a problem exists. -</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2747" xml:space="preserve">'''Data extraction''' is the act or process of retrieving [[data]] out of (usually [[unstructured data|unstructured]] or poorly structured) data sources for further [[data processing]] or [[data storage device|data storage]] ([[data migration]]). The [[data import|import]] into the intermediate extracting system is thus usually followed by [[data transformation]] and possibly the addition of [[metadata]] prior to [[data export|export]] to another stage in the data [[workflow]].&lt;ref&gt;[http://www.extractingdata.com Definition of data extraction.]&lt;/ref&gt;

Usually, the term data extraction is applied when ([[experiment]]al) data is first imported into a computer from primary sources, like [[measuring device|measuring]] or [[recording device]]s. Today's [[electronic device]]s will usually present an [[electrical connector]] (e.g. [[USB]]) through which '[[raw data]]' can be [[data stream|streamed]] into a [[personal computer]].

Typical unstructured data sources include web pages, emails, documents, PDFs, scanned text, mainframe reports, spool files, classifieds, etc. Which is further used for sales / marketing leads.&lt;ref&gt;[http://www.suntecdata.com/data-extraction-services.html Data Extraction Services] Retrieved, April 4, 2016&lt;/ref&gt;  Extracting data from these unstructured sources has grown into a considerable technical challenge where as historically data extraction has had to deal with changes in physical hardware formats, the majority of current data extraction deals with extracting data from these unstructured data sources, and from different software formats.  This growing process of data extraction&lt;ref&gt;[http://www.loginworks.com/blogs/web-scraping-blogs/209-web-data-extraction/ data extraction.]&lt;/ref&gt; from the web is referred to as [[Web scraping]].

The act of adding structure to unstructured data takes a number of forms
* Using text pattern matching such as [[regular expression]]s to identify small or large-scale structure e.g. records in a report and their associated data from headers and footers; 
* Using a table-based approach to identify common sections within a limited domain e.g. in emailed resumes, identifying skills, previous work experience, qualifications etc. using a standard set of commonly used headings (these would differ from language to language), e.g. Education might be found under Education/Qualification/Courses;
* Using text analytics to attempt to understand the text and link it to other information

==References==
{{Reflist}}

==External links==
* [http://www.etltools.org/extraction.html Data Extraction] as a part of the ETL process in a Data Warehousing environment

{{Data warehouse}}

{{DEFAULTSORT:Data Extraction}}
[[Category:Data management]]
[[Category:Data warehousing]]</text>
      <sha1>pbmvidmkf5dxmx9asixlo2jn3orykam</sha1>
    </revision>
  </page>
  <page>
    <title>Data integration</title>
    <ns>0</ns>
    <id>4780372</id>
    <revision>
      <id>760747007</id>
      <parentid>760746562</parentid>
      <timestamp>2017-01-18T21:10:22Z</timestamp>
      <contributor>
        <username>Sir mba</username>
        <id>11947451</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="26644" xml:space="preserve">'''Data integration''' involves combining [[data]] residing in different sources and providing users with a unified view of these data.&lt;ref name="refone"&gt;
{{cite conference | author=Maurizio Lenzerini | title=Data Integration:  A Theoretical Perspective | booktitle=PODS 2002 | year=2002 | pages=233&#8211;246 | url=http://www.dis.uniroma1.it/~lenzerin/homepagine/talks/TutorialPODS02.pdf}}&lt;/ref&gt; This process becomes significant in a variety of situations, which include both commercial (when two similar companies need to merge their [[database]]s) and scientific (combining research results from different [[bioinformatics]] repositories, for example) domains.  Data integration appears with increasing frequency as the volume and the need to share existing data [[Information explosion|explodes]].&lt;ref name="DataExplode"&gt;{{cite news | author=Frederick Lane | title=IDC: World Created 161 Billion Gigs of Data in 2006 | year=2006 | url=http://www.toptechnews.com/story.xhtml?story_id=01300000E3D0&amp;full_skip=1 }}&lt;/ref&gt;  It has become the focus of extensive theoretical work, and numerous open problems remain unsolved.

==History==
[[File:datawarehouse.png|thumb|right|Figure 1: Simple schematic for a data warehouse.  The [[Extract, transform, load|ETL]] process extracts information from the source databases, transforms it and then loads it into the data warehouse.]]

[[File:dataintegration.png|thumb|right|Figure 2: Simple schematic for a data-integration solution.  A system designer constructs a mediated schema against which users can run queries.  The [[virtual database]] interfaces with the source databases via [[Wrapper pattern|wrapper]] code if required.]]

Issues with combining [[heterogeneous]] data sources, often referred to as [[information silo]]s, under a single query interface have existed for some time. In the early 1980s, computer scientists began designing systems for interoperability of heterogeneous databases.&lt;ref&gt;{{cite news | author= John Miles Smith | title= Multibase: integrating heterogeneous distributed database systems | year=1982 | journal=AFIPS '81 Proceedings of the May 4&#8211;7, 1981, national computer conference  | pages= 487&#8211;499 |url=http://dl.acm.org/citation.cfm?id=1500483|display-authors=etal}}&lt;/ref&gt; The first data integration system driven by structured metadata was designed at the [[University of Minnesota]] in 1991, for the [[IPUMS|Integrated Public Use Microdata Series (IPUMS)]]. IPUMS used a [[data warehousing]] approach, which [[Extract, transform, load|extracts, transforms, and loads]] data from heterogeneous sources into a single view [[logical schema|schema]] so data from different sources become compatible.&lt;ref&gt;{{cite news | author= [[Steven Ruggles]], J. David Hacker, and Matthew Sobek | title= Order out of Chaos: The Integrated Public Use Microdata Series | year=1995 | journal=Historical Methods |volume=28 | pages= 33&#8211;39}}&lt;/ref&gt; By making thousands of population databases interoperable, IPUMS demonstrated the feasibility of large-scale data integration. The data warehouse approach  offers a [[Coupling (computer science)|tightly coupled]] architecture because the data are already physically reconciled in a single queryable repository, so it usually takes little time to resolve queries.&lt;ref&gt;{{cite news | author= Jennifer Widom | title= Research problems in data warehousing | year=1995 | journal=CIKM '95 Proceedings of the fourth international conference on information and knowledge management | pages= 25&#8211;30 | url=http://dl.acm.org/citation.cfm?id=221319}}&lt;/ref&gt;

The data warehouse approach is less feasible for datasets that are frequently updated, requiring the [[Extract, transform, load|ETL]] process to be continuously re-executed for synchronization.  Difficulties also arise in constructing data warehouses when one has only a query interface to summary data sources and no access to the full data.  This problem frequently emerges when integrating several commercial query services like travel or classified advertisement web applications.

{{As of | 2009}} the trend in data integration favored loosening the coupling between data{{Citation needed|date=June 2009}} and providing a unified query-interface to access real time data over a [[data mediation|mediated]] schema (see figure 2), which allows information to be retrieved directly from original databases. This is consistent with the [[Service-oriented architecture|SOA]] approach popular in that era. This approach relies on mappings between the mediated schema and the schema of original sources, and transform a query into specialized queries to match the schema of the original databases.  Such mappings can be specified in 2 ways : as a mapping from entities in the mediated schema to entities in the original sources (the "[[Global As View]]" (GAV) approach), or as a mapping from entities in the original sources to the mediated schema (the "[[Local As View]]" (LAV) approach).  The latter approach requires more sophisticated inferences to resolve a query on the mediated schema, but makes it easier to add new data sources to a (stable) mediated schema.

{{As of | 2010}} some of the work in data integration research concerns the [[semantic integration]] problem.  This problem addresses not the structuring of the architecture of the integration, but how to resolve [[semantic]] conflicts between heterogeneous data sources.  For example, if two companies merge their databases, certain concepts and definitions in their respective schemas like "earnings" inevitably have different meanings.  In one database it may mean profits in dollars (a floating-point number), while in the other it might represent the number of sales (an integer).  A common strategy for the resolution of such problems involves the use of [[ontology (computer science)|ontologies]] which explicitly define schema terms and thus help to resolve semantic conflicts. This approach represents [[ontology-based data integration]]. On the other hand, the problem of combining research results from different bioinformatics repositories requires bench-marking of the similarities, computed from different data sources, on a single criterion such as positive predictive value. This enables the data sources to be directly comparable and can be integrated even when the natures of experiments are distinct.&lt;ref&gt;{{cite journal| url=http://shubhrasankar.tripod.com/cgi-bin/combiningMultisourceIEEE.pdf  | journal=IEEE Transactions on Biomedical Engineering | title=Combining Multi-Source Information through Functional Annotation based Weighting: Gene Function Prediction in Yeast| author=Shubhra S. Ray| volume = 56 | pages=229&#8211;236 | pmid=19272921 | year=2009| issue=2 | doi=10.1109/TBME.2008.2005955|display-authors=etal}}&lt;/ref&gt;

{{As of | 2011}} it was determined that current [[data modeling]] methods were imparting data isolation into every [[data architecture]] in the form of islands of disparate data and information silos. This data isolation is an unintended artifact of the data modeling methodology that results in the development of disparate data models. Disparate data models, when instantiated as databases, form disparate databases. Enhanced data model methodologies have been developed to eliminate the data isolation artifact and to promote the development of integrated data models.&lt;ref&gt;{{cite news | author= Michael Mireku Kwakye | title= A Practical Approach To Merging Multidimensional Data Models | year=2011 | url=http://hdl.handle.net/10393/20457 }}&lt;/ref&gt;&lt;ref&gt;{{cite web | url=http://www.iri.com/pdf/RapidAce-Brochure.pdf  | title=Rapid Architectural Consolidation Engine&amp;nbsp;&#8211; The enterprise solution for disparate data models. | year=2011 }}&lt;/ref&gt; One enhanced data modeling method recasts data models by augmenting them with structural [[metadata]] in the form of standardized data entities.  As a result of recasting multiple data models, the set of recast data models will now share one or more commonality relationships that relate the structural metadata now common to these data models.  Commonality relationships are a peer-to-peer type of entity relationships that relate the standardized data entities of multiple data models.  Multiple data models that contain the same standard data entity may participate in the same commonality relationship.  When integrated data models are instantiated as databases and are properly populated from a common set of master data, then these databases are integrated.

Since 2011, [[data hub]] approaches have been of greater interest than fully structured (typically relational) Enterprise Data Warehouses. Since 2013, [[data lake]] approaches have risen to the level of Data Hubs. (See all three search terms popularity on Google Trends.&lt;ref&gt;{{cite web |title=Hub Lake and Warehouse search trends|url=https://www.google.com/trends/explore#q=enterprise%20data%20warehouse%2C%20%22data%20hub%22%2C%20%22data%20lake%22&amp;cmpt=q&amp;tz=Etc%2FGMT%2B5}}&lt;/ref&gt;) These approaches combine unstructured or varied data into one location, but do not necessarily require an (often complex) master relational schema to structure and define all data in the Hub.

==Example==
Consider a [[web application]] where a user can query a variety of information about cities (such as crime statistics, weather, hotels, demographics, etc.).  Traditionally, the information must be stored in a single database with a single schema.  But any single enterprise would find information of this breadth somewhat difficult and expensive to collect.  Even if the resources exist to gather the data, it would likely duplicate data in existing crime databases, weather websites, and census data.

A data-integration solution may address this problem by considering these external resources as [[materialized view]]s over a [[Virtual database|virtual mediated schema]], resulting in "virtual data integration".  This means application-developers construct a virtual schema&#8212;the ''mediated schema''&#8212;to best model the kinds of answers their users want.  Next, they design "wrappers" or adapters for each data source, such as the crime database and weather website.  These adapters simply transform the local query results (those returned by the respective websites or databases) into an easily processed form for the data integration solution (see figure 2).  When an application-user queries the mediated schema, the data-integration solution transforms this query into appropriate queries over the respective data sources.  Finally, the virtual database combines the results of these queries into the answer to the user's query.

This solution offers the convenience of adding new sources by simply constructing an adapter or an application software blade for them.  It contrasts with [[Extract, transform, load|ETL]] systems or with a single database solution, which require manual integration of entire new dataset into the system. The virtual ETL solutions leverage [[Virtual database|virtual mediated schema]] to implement data harmonization; whereby the data are copied from the designated "master" source to the defined targets, field by field. Advanced [[data virtualization]] is also built on the concept of object-oriented modeling in order to construct virtual mediated schema or virtual metadata repository, using [[hub and spoke]] architecture.

Each data source is disparate and as such is not designed to support reliable joins between data sources.  Therefore, data virtualization as well as data federation depends upon accidental data commonality to support combining data and information from disparate data sets.  Because of this lack of data value commonality across data sources, the return set may be inaccurate, incomplete, and impossible to validate.

One solution is to recast disparate databases to integrate these databases without the need for [[Extract, transform, load|ETL]]. The recast databases support commonality constraints where referential integrity may be enforced between databases.  The recast databases provide designed data access paths with data value commonality across databases.

==Theory==
The theory of data integration&lt;ref name="refone" /&gt; forms a subset of database theory and formalizes the underlying concepts of the problem in [[first-order logic]].  Applying the theories gives indications as to the feasibility and difficulty of data integration.  While its definitions may appear abstract, they have sufficient generality to accommodate all manner of integration systems,&lt;ref&gt;{{cite web|url=http://link.springer.com/chapter/10.1007/3-540-46093-4_14 |title=A Model Theory for Generic Schema Management}}&lt;/ref&gt; including those that include nested relational / XML databases &lt;ref&gt;{{cite web|url=http://www.vldb.org/conf/2006/p67-fuxman.pdf |title=Nested Mappings: Schema Mapping Reloaded }}&lt;/ref&gt; and those that treat databases as programs.&lt;ref&gt;{{cite web|url=http://homepages.inf.ed.ac.uk/dts/pub/psi.pdf |title=The Common Framework Initiative for algebraic specification and development of software}}&lt;/ref&gt;  Connections to particular databases systems such as Oracle or DB2 are provided by implementation-level technologies such as [[JDBC]] and are not studied at the theoretical level.

===Definitions===
Data integration systems are formally defined as a [[Triple (mathematics)|triple]] &lt;math&gt;\left \langle G,S,M\right \rangle&lt;/math&gt; where &lt;math&gt;G&lt;/math&gt; is the global (or mediated) schema, &lt;math&gt;S&lt;/math&gt; is the heterogeneous set of source schemas, and &lt;math&gt;M&lt;/math&gt; is the mapping that maps queries between the source and the global schemas.  Both &lt;math&gt;G&lt;/math&gt; and &lt;math&gt;S&lt;/math&gt; are expressed in [[formal language|languages]] over [[alphabet (computer science)|alphabets]] composed of symbols for each of their respective [[Relational database|relations]].  The [[Functional predicate|mapping]] &lt;math&gt;M&lt;/math&gt; consists of assertions between queries over &lt;math&gt;G&lt;/math&gt; and queries over &lt;math&gt;S&lt;/math&gt;.  When users pose queries over the data integration system, they pose queries over &lt;math&gt;G&lt;/math&gt; and the mapping then asserts connections between the elements in the global schema and the source schemas.

A database over a schema is defined as a set of sets, one for each relation (in a relational database).  The database corresponding to the source schema &lt;math&gt;S&lt;/math&gt; would comprise the set of sets of tuples for each of the heterogeneous data sources and is called the ''source database''.  Note that this single source database may actually represent a collection of disconnected databases.  The database corresponding to the virtual mediated schema &lt;math&gt;G&lt;/math&gt; is called the ''global database''.  The global database must satisfy the mapping &lt;math&gt;M&lt;/math&gt; with respect to the source database.  The legality of this mapping depends on the nature of the correspondence between &lt;math&gt;G&lt;/math&gt; and &lt;math&gt;S&lt;/math&gt;.  Two popular ways to model this correspondence exist: ''Global as View'' or GAV and ''Local as View'' or LAV.

[[File:GAVLAV.png|thumb|right|Figure 3: Illustration of tuple space of the GAV and LAV mappings.&lt;ref name="refseven"&gt;{{cite journal|author=Christoph Koch |title=Data Integration against Multiple Evolving Autonomous Schemata |year=2001 |url=http://www.csd.uoc.gr/~hy562/Papers/thesis_final.pdf |deadurl=yes |archiveurl=https://web.archive.org/web/20070926211342/http://www.csd.uoc.gr/~hy562/Papers/thesis_final.pdf |archivedate=2007-09-26 |df= }}&lt;/ref&gt; In GAV, the system is constrained to the set of tuples mapped by the mediators while the set of tuples expressible over the sources may be much larger and richer. In LAV, the system is constrained to the set of tuples in the sources while the set of tuples expressible over the global schema can be much larger. Therefore, LAV systems must often deal with incomplete answers.]]

GAV systems model the global database as a set of [[view (database)|views]] over &lt;math&gt;S&lt;/math&gt;.  In this case &lt;math&gt;M&lt;/math&gt; associates to each element of &lt;math&gt;G&lt;/math&gt; a query over &lt;math&gt;S&lt;/math&gt;.  [[Query optimizer|Query processing]] becomes a straightforward operation due to the well-defined associations between &lt;math&gt;G&lt;/math&gt; and &lt;math&gt;S&lt;/math&gt;.  The burden of complexity falls on implementing mediator code instructing the data integration system exactly how to retrieve elements from the source databases.  If any new sources join the system, considerable effort may be necessary to update the mediator, thus the GAV approach appears preferable when the sources seem unlikely to change.

In a GAV approach to the example data integration system above, the system designer would first develop mediators for each of the city information sources and then design the global schema around these mediators.  For example, consider if one of the sources served a weather website.  The designer would likely then add a corresponding element for weather to the global schema.  Then the bulk of effort concentrates on writing the proper mediator code that will transform predicates on weather into a query over the weather website.  This effort can become complex if some other source also relates to weather, because the designer may need to write code to properly combine the results from the two sources.

On the other hand, in LAV, the source database is modeled as a set of [[view (database)|views]] over &lt;math&gt;G&lt;/math&gt;.  In this case &lt;math&gt;M&lt;/math&gt; associates to each element of &lt;math&gt;S&lt;/math&gt; a query over &lt;math&gt;G&lt;/math&gt;.  Here the exact associations between &lt;math&gt;G&lt;/math&gt; and &lt;math&gt;S&lt;/math&gt; are no longer well-defined.  As is illustrated in the next section, the burden of determining how to retrieve elements from the sources is placed on the query processor.  The benefit of an LAV modeling is that new sources can be added with far less work than in a GAV system, thus the LAV approach should be favored in cases where the mediated schema is less stable or likely to change.&lt;ref name="refone" /&gt;

In an LAV approach to the example data integration system above, the system designer designs the global schema first and then simply inputs the schemas of the respective city information sources.  Consider again if one of the sources serves a weather website.  The designer would add corresponding elements for weather to the global schema only if none existed already.  Then programmers write an adapter or wrapper for the website and add a schema description of the website's results to the source schemas.  The complexity of adding the new source moves from the designer to the query processor.

===Query processing===
The theory of query processing in data integration systems is commonly expressed using conjunctive [[Database query language|queries]] and [[Datalog]], a purely declarative [[logic programming]] language.&lt;ref name="reffive"&gt;{{cite conference | author=[[Jeffrey D. Ullman]] | title=Information Integration Using Logical Views | booktitle=ICDT 1997 | year=1997 | pages=19&#8211;40 | url=http://www-db.stanford.edu/pub/papers/integration-using-views.ps}}&lt;/ref&gt;  One can loosely think of a [[conjunctive query]] as a logical function applied to the relations of a database such as "&lt;math&gt;f(A,B)&lt;/math&gt; where &lt;math&gt;A &lt; B&lt;/math&gt;".  If a tuple or set of tuples is substituted into the rule and satisfies it (makes it true), then we consider that tuple as part of the set of answers in the query.  While formal languages like Datalog express these queries concisely and without ambiguity, common [[SQL]] queries count as conjunctive queries as well.

In terms of data integration, "query containment" represents an important property of conjunctive queries.  A query &lt;math&gt;A&lt;/math&gt; contains another query &lt;math&gt;B&lt;/math&gt; (denoted &lt;math&gt;A \supset B&lt;/math&gt;) if the results of applying &lt;math&gt;B&lt;/math&gt; are a subset of the results of applying &lt;math&gt;A&lt;/math&gt; for any database.  The two queries are said to be equivalent if the resulting sets are equal for any database.  This is important because in both GAV and LAV systems, a user poses conjunctive queries over a ''virtual'' schema represented by a set of [[view (database)|views]], or "materialized" conjunctive queries.  Integration seeks to rewrite the queries represented by the views to make their results equivalent or maximally contained by our user's query.  This corresponds to the problem of answering queries using views ([[AQUV]]).&lt;ref name="refsix"&gt;{{cite conference | author=[[Alon Y. Halevy]] | title=Answering queries using views: A survey | booktitle=The VLDB Journal | year=2001 | pages=270&#8211;294 | url=http://www.cs.uwaterloo.ca/~david/cs740/answering-queries-using-views.pdf}}
&lt;/ref&gt;

In GAV systems, a system designer writes mediator code to define the query-rewriting.  Each element in the user's query corresponds to a substitution rule just as each element in the global schema corresponds to a query over the source.  Query processing simply expands the subgoals of the user's query according to the rule specified in the mediator and thus the resulting query is likely to be equivalent.  While the designer does the majority of the work beforehand, some GAV systems such as [http://www-db.stanford.edu/tsimmis/ Tsimmis] involve simplifying the mediator description process.

In LAV systems, queries undergo a more radical process of rewriting because no mediator exists to align the user's query with a simple expansion strategy.  The integration system must execute a search over the space of possible queries in order to find the best rewrite.  The resulting rewrite may not be an equivalent query but maximally contained, and the resulting tuples may be incomplete.  {{As of | 2009}} the MiniCon algorithm&lt;ref name="refsix" /&gt; is the leading query rewriting algorithm for LAV data integration systems.

In general, the complexity of query rewriting is [[NP-complete]].&lt;ref name="refsix" /&gt;  If the space of rewrites is relatively small this does not pose a problem&#8212;even for integration systems with hundreds of sources.

==Tools==
* Alteryx
* Analytics Canvas
* [[Capsenta]]'s Ultrawrap Platform
*[[Cloud Elements]] API Integration
* DataWatch
* [[Denodo|Denodo Platform]]
* [https://github.com/mhaghighat/dcaFuse Discriminant Correlation Analysis (DCA)]&lt;ref name="dca"&gt;M. Haghighat, M. Abdel-Mottaleb, &amp;  W. Alhalabi (2016). [http://ieeexplore.ieee.org/document/7470527/ Discriminant Correlation Analysis: Real-Time Feature Level Fusion for Multimodal Biometric Recognition]. IEEE Transactions on Information Forensics and Security, 11(9), 1984-1996.&lt;/ref&gt;
* [[elastic.io]] Integration Platform
* [http://www.hiperfabric.com HiperFabric]
* [[Lavastorm Analytics|Lavastorm]]
* [[Informatica]] Platform
* Oracle Data Integration Services
* ParseKit (enigma.io)
* Paxata
* [[RapidMiner]] Studio
* [[Red Hat]] JBoss Data Virtualization. Community project: teiid.
* [[Microsoft Azure|Azure]] Data Factory (ADF) 
* [[SQL Server Integration Services|SQL Server Integration Services (SSIS)]]
* [http://www.tmmdata.com TMMData]
* [http://www.dataladder.com Data Ladder]

==In the life sciences==
Large-scale questions in science, such as [[global warming]], [[invasive species]] spread, and [[resource depletion]], are increasingly requiring the collection of disparate data sets for [[meta-analysis]]. This type of data integration is especially challenging for ecological and environmental data because [[metadata standards]] are not agreed upon and there are many different data types produced in these fields. [[National Science Foundation]] initiatives such as [[Datanet]] are intended to make data integration easier for scientists by providing [[cyberinfrastructure]] and setting standards. The five funded [[Datanet]] initiatives are [[DataONE]],&lt;ref&gt;{{cite web|author=William Michener |url=https://www.dataone.org |title=DataONE: Observation Network for Earth |publisher=www.dataone.org | accessdate=2013-01-19|display-authors=etal}}&lt;/ref&gt; led by William Michener at the [[University of New Mexico]]; The Data Conservancy,&lt;ref&gt;{{cite web|author=Sayeed Choudhury |url=https://dataconservancy.org |title=Data Conservancy |publisher=dataconservancy.org | accessdate=2013-01-19|display-authors=etal}}&lt;/ref&gt; led by Sayeed Choudhury of [[Johns Hopkins University]]; SEAD: Sustainable Environment through Actionable Data,&lt;ref&gt;{{cite web|author=[[Margaret Hedstrom]] |url=http://sead-data.net/ |title=SEAD Sustainable Environment - Actionable Data |publisher=sead-data.net | accessdate=2013-01-19|display-authors=etal}}&lt;/ref&gt; led by [[Margaret Hedstrom]] of the [[University of Michigan]]; the DataNet Federation Consortium,&lt;ref&gt;{{cite web|author=[[Reagan Moore]] |url=http://datafed.org/ |title=DataNet Federation Consortium |publisher=datafed.org | accessdate=2013-01-19|display-authors=etal}}&lt;/ref&gt; led by Reagan Moore of the [[University of North Carolina]]; and ''Terra Populus'',&lt;ref&gt;{{cite web|author=[[Steven Ruggles]] |url=http://www.terrapop.org/ |title=Terra Populus: Integrated Data on Population and the Environment |publisher=terrapop.org | accessdate=2013-01-19|display-authors=etal}}&lt;/ref&gt; led by [[Steven Ruggles]] of the [[University of Minnesota]].  The [[Research Data Alliance]],&lt;ref&gt;{{cite web|author=[[Bill Nichols]] |url=http://rd-alliance.org/ |title=Research Data Alliance |publisher=rd-alliance.org | accessdate=2014-10-01}}&lt;/ref&gt; has more recently explored creating global data integration frameworks. The [[OpenPHACTS]] project, funded through the [[European Union]] [[Innovative Medicines Initiative]], built a drug discovery platform by linking datasets from providers such as [[European Bioinformatics Institute]], [[Royal Society of Chemistry]], [[UniProt]], [[WikiPathways]] and [[DrugBank]].

==See also==
{{div col||20em}}
* [[Business semantics management]]
* [[Core data integration]]
* [[Customer data integration]]
* [[Data curation]]
* [[Data fusion]]
* [[Data mapping]]
* [[Data wrangling]]
* [[Database model]]
* [[Dataspaces]]
* [[Edge data integration]]
* [[Enterprise application integration]]
* [[Enterprise architecture framework]]
* [[Enterprise information integration]] (EII)
* [[Enterprise integration]]
* [[Geodi]]: Geoscientific Data Integration
* [[Information integration]]
* [[Information server]]
* [[Information silo]]
* [[Integration Competency Center]]
* [[Integration Consortium]]
* [[JXTA]]
* [[Master data management]]
* [[Object-relational mapping]]
* [[Open Text]]
* [[Schema matching]]
* [[Three schema approach]]
* [[UDEF]]
* [[Web service]]
{{div col end}}

==External links==
* [https://github.com/mhaghighat/dcaFuse Discriminant Correlation Analysis (DCA)]&lt;ref name="dca"&gt;&lt;/ref&gt;

==References==
{{reflist|30em}}

{{data}}

{{DEFAULTSORT:Data Integration}}
[[Category:Data management]]</text>
      <sha1>s2vh7r8mqm7qwlbg4vjjb3hb5spw7te</sha1>
    </revision>
  </page>
  <page>
    <title>Commit (data management)</title>
    <ns>0</ns>
    <id>1626958</id>
    <revision>
      <id>717532767</id>
      <parentid>665913704</parentid>
      <timestamp>2016-04-28T08:07:46Z</timestamp>
      <contributor>
        <username>Intgr</username>
        <id>246230</id>
      </contributor>
      <minor />
      <comment>/* See also */ Simplify link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1422" xml:space="preserve">{{For|the revision control concept|Commit (revision control)}}
{{Unreferenced|date=May 2014}}

In [[computer science]] and [[data management]], a '''commit''' is the making of a set of tentative changes permanent. A popular usage is at the end of a [[database transaction|transaction]]. A ''commit'' is an act of committing.

==Data management==
A &lt;code&gt;[[COMMIT (SQL)|COMMIT]]&lt;/code&gt; statement in [[SQL]] ends a [[database transaction|transaction]] within a [[relational database management system]] (RDBMS) and makes all changes visible to other users. The general format is to issue a &lt;code&gt;[[Begin work (SQL)|BEGIN WORK]]&lt;/code&gt; statement, one or more SQL statements, and then the &lt;code&gt;COMMIT&lt;/code&gt; statement. Alternatively, a &lt;code&gt;[[Rollback (data management)|ROLLBACK]]&lt;/code&gt; statement can be issued, which undoes all the work performed since &lt;code&gt;BEGIN WORK&lt;/code&gt; was issued. A &lt;code&gt;COMMIT&lt;/code&gt; statement will also release any existing [[savepoint]]s that may be in use.

In terms of transactions, the opposite of commit is to discard the tentative changes of a transaction, a [[rollback (data management)|rollback]].

==See also==
* [[Commit (version control)]]
* [[Atomic commit]]
* [[Two-phase commit protocol]]
* [[Three-phase commit protocol]]

{{databases}}

{{DEFAULTSORT:Commit (Data Management)}}
[[Category:Data management]]
[[Category:SQL]]
[[Category:Transaction processing]]

{{comp-sci-stub}}</text>
      <sha1>3equq4wiy404bzljriv29vxvska1yvw</sha1>
    </revision>
  </page>
  <page>
    <title>Linear medium</title>
    <ns>0</ns>
    <id>676562</id>
    <revision>
      <id>691101967</id>
      <parentid>374241380</parentid>
      <timestamp>2015-11-17T17:47:19Z</timestamp>
      <contributor>
        <username>Aasasd</username>
        <id>22263090</id>
      </contributor>
      <comment>/* See also */ redirects to 'Sequential access'</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1340" xml:space="preserve">{{Unreferenced|date=December 2009}}
A '''linear medium''' is any medium which is intended to be written to or accessed in a [[linear]] fashion, literally meaning ''in a line''. 

This means that the information is written to or read from the medium in a given order, so for example a book containing a [[novel]] is intended to be read from front to back, beginning to end, and is therefore a linear medium. It may be written in the same way, but would not necessarily need to be, to be considered a linear medium. 
A book containing an [[encyclopedia]] however is a non-linear medium, as it is not necessary for the articles to be accessed (or written) in any particular order. Even though both non-linear and linear mediums have perimeters to which they are restricted, linear mediums have a set path of how to get from point A to point B, whereas non-linear mediums do not. 

Examples in technology are a pre-recorded [[videocassette]] which is usually accessed one item after another, compared with a pre-recorded [[DVD]] which can be accessed in any order.

==Types of linear medium==
* [[Scroll]]
* [[Magnetic tape data storage]]
* [[Paper tape]]
* [[Photographic film]]
* [[Novel|story book]]s
* [[Compact cassette]]s

==See also==
*[[Sequential access]] 
*[[Random access]]

{{DEFAULTSORT:Linear Medium}}
[[Category:Data management]]</text>
      <sha1>c1l13yttu7gdgdlvpa4nchfp8tgc3b9</sha1>
    </revision>
  </page>
  <page>
    <title>Point-in-time recovery</title>
    <ns>0</ns>
    <id>4576703</id>
    <revision>
      <id>733104708</id>
      <parentid>733104655</parentid>
      <timestamp>2016-08-05T11:49:10Z</timestamp>
      <contributor>
        <ip>77.56.53.183</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1105" xml:space="preserve">{{Redirect|PITR|other uses|Pitr (disambiguation){{!}}Pitr}}
{{Unreferenced stub|auto=yes|date=December 2009}}

'''Point-in-time recovery''' ('''PITR''') in the context of [[computer]]s involves systems whereby an administrator can restore or recover a set of data or a particular setting from a time in the past. Note for example [[Windows XP]]'s capability to restore operating-system settings from a past date (before data corruption occurred, for example).  [[Time Machine (OS X)|Time Machine]] for Mac OS X provides another example of point-in-time recovery.

Once PITR logging starts for a PITR-capable [[database]], a [[database administrator]] can restore that database from [[backup]]s to the state that it had at any time since.

==External links==
* [http://blog.ganneff.de/blog/2008/02/15/postgresql-continuous-archivin.html PostgreSQL Continuous Archiving and Point-In-Time Recovery (PITR) blog/article]
* [http://dev.mysql.com/doc/refman/5.5/en/point-in-time-recovery.html MySQL 5.5 Point in Time Recovery]

{{DEFAULTSORT:Point-In-Time Recovery}}
[[Category:Data management]]


{{Compu-stub}}</text>
      <sha1>ikx4o7a3k3p3hdjnlbualuxqaaro0s1</sha1>
    </revision>
  </page>
  <page>
    <title>Comparison of OLAP Servers</title>
    <ns>0</ns>
    <id>24523966</id>
    <revision>
      <id>760530402</id>
      <parentid>760529770</parentid>
      <timestamp>2017-01-17T15:56:31Z</timestamp>
      <contributor>
        <ip>62.84.129.10</ip>
      </contributor>
      <comment>/* OLAP distinctive features */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="23549" xml:space="preserve">The following tables compare general and technical information for a number of [[online analytical processing]] (OLAP) servers supporting MDX language. Please see the individual products articles for further information.

==General information==
{| class="wikitable sortable"  style="font-size: 100%; text-align: center; width: auto;"
|-
! OLAP Server
! Company
! Website
! Latest stable version
! [[Software license]]
! License Pricing
|-
! [[TM1|IBM Cognos TM1]]
| [[IBM]]
|&lt;ref&gt;{{cite web|url=http://www-01.ibm.com/software/data/cognos/index.html|title=Cognos Business Intelligence and Financial Performance Management}}&lt;/ref&gt;
| 10.2.2 FP4
| [[Proprietary software|Proprietary]]
| -
|-
! [[Essbase]]
| [[Oracle Corporation|Oracle]]
|&lt;ref&gt;{{cite web|url=http://www.oracle.com/us/solutions/ent-performance-bi/business-intelligence/essbase/index.html|title=Oracle Essbase}}&lt;/ref&gt;
| 11.1.2.4
| [[Proprietary software|Proprietary]]
| [http://www.oracle.com/us/corporate/pricing/index.htm]
|-
! [[icCube]]
| [[icCube]]
|&lt;ref&gt;{{cite web|url=http://www.icCube.com|title=icCube OLAP Server}}&lt;/ref&gt;
| 6.0
| [[Proprietary software|Proprietary]]
| community/[http://www.iccube.com//prices]
|-
! [[Jedox|Jedox OLAP Server]]
| [[Jedox]]
|&lt;ref&gt;{{cite web|url=http://www.jedox.com/en/home/overview.html |title=Jedox AG Business Intelligence |deadurl=yes |archiveurl=https://web.archive.org/web/20100514124342/http://www.jedox.com:80/en/home/overview.html |archivedate=2010-05-14 |df= }}&lt;/ref&gt;
| 7.0
| [[GNU General Public License|GPL]] v2 or [[EULA]], [[Proprietary software|Proprietary]]
| -
|-
 ! Infor BI OLAP Server
| [[Infor]]
|&lt;ref&gt;{{cite web|url=http://www.infor.com|title=Infor}}&lt;/ref&gt;
| 10.6.0
| [[Proprietary software|Proprietary]]
| -
|-
! [[Microsoft Analysis Services]]
| [[Microsoft]]
|&lt;ref&gt;{{cite web|url=http://www.microsoft.com/Sqlserver/2008/en/us/analysis-services.aspx|title=Microsoft SQL Server 2008 Analysis Services}}&lt;/ref&gt;
| 2016
| [[Proprietary software|Proprietary]]
| [http://www.microsoft.com/sqlserver/2008/en/us/pricing.aspx]
|-
! MicroStrategy Intelligence Server
| [[MicroStrategy]]
|&lt;ref&gt;{{cite web|url=http://www.microstrategy.com/Software/Products/Intelligence_Server/|title=MicroStrategy Intelligence Server}}&lt;/ref&gt;
| 9
| [[Proprietary software|Proprietary]]
| -
|-
! [[Mondrian OLAP server]]
| [[Pentaho]]
|&lt;ref&gt;{{cite web|url=http://mondrian.pentaho.org|title=Pentaho Analysis Services: Mondrian Project}}&lt;/ref&gt;
| 3.7
| [[Eclipse Public License|EPL]]
| free
|-
! [[Oracle OLAP|Oracle Database OLAP Option]]
| [[Oracle Corporation|Oracle]]
|&lt;ref&gt;{{cite web|url=http://www.oracle.com/technology/documentation/olap.html|title=Oracle OLAP Documentation}}&lt;/ref&gt;
| 11g R2
| [[Proprietary software|Proprietary]]
| [http://www.oracle.com/us/corporate/pricing/index.htm]
|-
! [[SAS System|SAS OLAP Server]]
| [[SAS Institute]]
|&lt;ref&gt;{{cite web|url=http://www.sas.com/technologies/dw/storage/mddb/index.html|title=SAS OLAP Server}}&lt;/ref&gt;
| 9.4
| [[Proprietary software|Proprietary]]
| -
|-
! [[SAP NetWeaver Business Intelligence|SAP NetWeaver BW]]
| [[SAP AG|SAP]]
|&lt;ref&gt;{{cite web|url=http://www.sap.com/usa/platform/netweaver/components/businesswarehouse/index.epx |title=Components &amp; Tools}}&lt;/ref&gt;
| 7.30
| [[Proprietary software|Proprietary]]
| -
|-
! [[Cubes (OLAP server)|Cubes]]
| [[Open source|Open source community]]
|&lt;ref&gt;{{cite web|url=http://cubes.databrewery.org|title=Cubes &#8211; Lightweight OLAP Python Toolkit}}&lt;/ref&gt;
| 1.0.1
| [[MIT License|MIT]]
| -
|}

==Data storage modes==
{| class="wikitable sortable"  style="font-size: 100%; text-align: center; width: auto;"
|-
! OLAP Server
! [[MOLAP]]
! [[ROLAP]]
! [[HOLAP]]
! In-Memory
! Offline
|-
! [[TM1|IBM Cognos TM1]]
| {{Yes}}
| {{No}}
| {{No}}
| {{No}}
| {{Yes| [http://www-01.ibm.com/support/knowledgecenter/SSVJ22_10.2.2/com.ibm.swg.ba.cognos.dsk_ug.10.2.2.doc/t_dsk_maintain_offline.html%23t_dsk_maintain_offline Cognos Insight Distributed mode]}}
|-
! [[Essbase]]
| {{Yes}}
| {{No}}
| {{No}}
| {{No}}
|
|-
! [[icCube]]
| {{Yes}}
| {{No}}
| {{No}}
| 
| {{Yes | [http://www.iccube.com/support/documentation/user_guide/using/offline_cubes.html Offline Cubes]}}
|-
! Infor BI OLAP Server
| {{Yes}}
| {{No}}
| {{No}}
| {{Yes}}
|Local cubes
|-
! [[Jedox|Jedox OLAP Server]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{No}}
| {{No}}
|-
! [[Microsoft Analysis Services]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes|Local cubes,&lt;br /&gt; [[PowerPivot|PowerPivot for Excel]],&lt;br /&gt;[[Power BI|Power BI Desktop]]}}
|-
! [[MicroStrategy|MicroStrategy Intelligence Server]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{No}}
| {{Yes|[http://www.microstrategy.com/Software/Products/User_Interfaces/Office/ MicroStrategy Office],&lt;br /&gt; [http://www.microstrategy.com/Software/Products/Service_Modules/Report_Services/ Dynamic Dashboards]}}
|-
! [[Mondrian OLAP server]]
| {{No}}
| {{Yes}}
| {{No}}
| {{No}}
|
|-
! [[Oracle OLAP|Oracle Database OLAP Option]]
| {{No}}
| {{Yes}}
| {{No}}
| {{No}}
|
|-
! [[SAS System|SAS OLAP Server]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{No}}
|
|-
! [[IBM Cognos BI]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{No}}
|
|-
! [[SAP NetWeaver Business Intelligence|SAP NetWeaver BW]]
| {{Yes}}
| {{Yes}}
| {{No}}
| {{No}}
|
|-
! [[Cubes (OLAP server)]]
| {{No}}
| {{Yes}}
| {{No}}
|
|
|-
|}

==APIs and query languages==
APIs and query languages OLAP servers support.
{| class="wikitable sortable"  style="font-size: 100%; text-align: center; width: auto;"
|-
! OLAP Server
! [[XML for Analysis]]
! [[OLE DB for OLAP]]
! [[Multidimensional Expressions|MDX]]
! [[Stored procedures]]
! Custom functions
! [[SQL]]
! [[LINQ]]&lt;ref name="linq"&gt;{{cite web|url=http://agiledesignllc.com/products|title=SSAS Entity Framework Provider}}&lt;/ref&gt;
! Visualization
! [[JSON]]
! [[REST API]]
|-
! [[Essbase]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{No}}
| {{Yes}}
| SmartView (Excel-AddIn), WebAnalysis, Financial Reports
| {{dunno}}
| {{dunno}}
|-
! [[icCube]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes|[[Java (programming language)|Java]],&lt;ref&gt;{{cite web|url=http://www.iccube.com/support/documentation/mdx_integration/java_integration.html|title=icCube Java integration documentation|publisher=[[icCube]]}}&lt;/ref&gt; [[R (programming language)|R]]&lt;ref&gt;{{cite web|url=http://www.iccube.com/support/documentation/mdx_integration/r_integration.html|title=icCube R language integration documentation|publisher=[[icCube]]}}&lt;/ref&gt;}}
| {{Yes}}
| {{No}}
| {{Yes}}
| {{Yes|[[Java (programming language)|Java]], [[Javascript]]}}
| {{dunno}}
| {{dunno}}
|-
! Infor BI OLAP Server
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes|OLAP Rules, Push Rules, Application Engine}}
| {{Yes}}
| {{Yes}}
| {{No}}
| {{Yes|Application Studio}}
| {{dunno}}
| {{dunno}}
|-
! [[Jedox|Jedox OLAP Server]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes|Cube Rules, SVS Triggers}}
| {{Yes}}
| {{No}}
| {{Yes}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
|-
! [[Microsoft Analysis Services]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes|[[.NET framework|.NET]]}}&lt;ref&gt;{{cite web|url=http://msdn.microsoft.com/en-us/library/ms176113.aspx|title=SQL Server 2008 Books Online (October 2009)Defining Stored Procedures|publisher=[[MSDN]]}}&lt;/ref&gt;
| {{Yes}}&lt;ref&gt;{{cite web|url=http://msdn.microsoft.com/en-us/library/ms145486.aspx|title=SQL Server 2008 Books Online (October 2009)Using Stored Procedures|publisher=[[MSDN]]}}&lt;/ref&gt;
| {{Yes}}&lt;ref&gt;{{cite web|url=http://support.microsoft.com/kb/218592/en-gb|title=How to perform a SQL Server distributed query with OLAP Server|publisher=[[MSDN]]}}&lt;/ref&gt;
| {{Yes}}
| {{Yes|Microsoft Excel, SharePoint, Microsoft Power BI, and 70+ other visualization tools}}&lt;ref&gt;{{cite web|url=http://www.ssas-info.com/analysis-services-client-tools-frontend|title=A collection of SSAS frontend tools|publisher=[[SSAS-info.com]]}}&lt;/ref&gt;
| {{dunno}}
| {{dunno}}
|-
! [[MicroStrategy|MicroStrategy Intelligence Server]]
| {{Yes}}
| {{No}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
|-
! [[Mondrian OLAP server]]
| {{Yes}}
| {{Yes}}&lt;ref&gt;{{cite web|url=http://www.simba.com/news/Pentaho-Simba-Partner-for-Excel-Connectivity.htm|title=Pentaho and Simba Technologies Partner to Bring World's Most Popular Open Source OLAP Project to Microsoft Excel Users}}&lt;/ref&gt;
| {{Yes}}
| {{Yes}}
| {{Yes}}&lt;ref&gt;{{cite web|url=http://mondrian.pentaho.org/documentation/schema.php#User-defined_function|title=How to Define a Mondrian Schema|publisher=Pentaho}}&lt;/ref&gt;
| {{Yes}}
| {{Yes}}
| {{No}}
| {{dunno}}
| {{dunno}}
|-
! [[Oracle OLAP|Oracle Database OLAP Option]]
| {{No}}
| {{Yes}}&lt;ref name="oraclemdx"&gt;{{cite web|url=http://www.oracle.com/us/corporate/press/036550|title=Oracle and Simba Technologies Introduce MDX Provider for Oracle OLAP}}&lt;/ref&gt;
| {{Yes}}&lt;ref name="oraclemdx"/&gt;
| {{Yes|[[Java (programming language)|Java]], PL/SQL, [[OLAP DML]]}}
| {{Yes}}
| {{Yes}}&lt;ref&gt;{{cite web|url=http://www.oracle.com/technology/products/bi/olap/11g/demos/olap_sql_demo.html|title=Querying Oracle OLAP Cubes: Fast Answers to Tough Questions Using Simple SQL}}&lt;/ref&gt;
| {{No}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
|-
! [[SAS System|SAS OLAP Server]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{No}}
| {{No}}
| {{No}}
| {{Yes}}
| {{Yes|Web Report Studio}}
| {{dunno}}
| {{dunno}}
|-
! [[SAP NetWeaver Business Intelligence|SAP NetWeaver BW]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{No}}
| {{Yes}}
| {{No}}
| {{Yes}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
|-
! [[TM1|Cognos TM1]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{No}}
| {{Yes}}
| TM1 Web/TM1 Contributor, IBM Cognos Insight, IBM Performance Modeler, IBM Cognos Cafe for Excel, Cognos BI, TM1 Perspectives for Excel
| {{dunno}}
| {{Yes}}
|-
! [[Cubes (OLAP server)|Cubes]]
| {{No}}
| {{No}}
| {{No}}
| {{No}}
| {{Yes}}
| {{No}}
| {{No}}
| Cubes Viewer&lt;ref&gt;{{cite web|url=https://github.com/jjmontesl/cubesviewer|title=Cubes Viewer|publisher=jjmontes}}&lt;/ref&gt;
| {{Yes}}
| {{dunno}}
|}

==OLAP distinctive features==

A list of OLAP features that are not supported by all vendors. All vendors support features such as parent-child, multilevel hierarchy, drilldown.

Data processing, management and performance related features:

{| class="wikitable sortable"  style="font-size: 100%; text-align: center; width: auto;"
|-
!OLAP Server
!Real Time
!Write-back
!Partitioning
!Usage Based Optimizations
!Load Balancing and Clustering
|-
! [[Essbase]]
| {{No}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
|-
! [[icCube]]
| {{Yes}}&lt;ref&gt;{{cite web|url=http://www.iccube.com/support/documentation/user_guide/walkthrough/walkthrough_rt.html|title=icCube Real Time walkthrough}}&lt;/ref&gt;
| {{Yes}}&lt;ref&gt;{{cite web|url=http://www.iccube.com/support/documentation/mdx/Update%20Cube.html|title=icCube Writeback/Update Cube}}&lt;/ref&gt;
| {{Yes}}&lt;ref&gt;{{cite web|url=http://www.iccube.com/support/documentation/user_guide/reference/partitioning_edition.html|title=icCube Partitioning}}&lt;/ref&gt;
| {{dunno}}
| {{dunno}}
|-
! Infor BI OLAP Server
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{dunno}}
| {{dunno}}
|-
! [[Jedox|Jedox OLAP Server]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{dunno}}
| {{dunno}}
|-
! [[Microsoft Analysis Services]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
|-
! [[MicroStrategy|MicroStrategy Intelligence Server]]
| {{dunno}}
| {{Yes}}&lt;ref&gt;{{cite web|url=http://www.microstrategy.com/Software/Products/Dev_Tools/SDK/extensions.asp|title=Common Extensions of the MicroStrategy Platform}}&lt;/ref&gt;
| {{Yes}}
| {{dunno}}
| {{dunno}}
|-
! [[Mondrian OLAP server]]
| {{Yes}}
| {{Yes2 | Planned}}
| {{Yes}}
| {{dunno}}
| {{dunno}}
|-
! [[Oracle OLAP|Oracle Database OLAP Option]]
| {{dunno}}
| {{Yes}}
| {{Yes}}
| {{No}}
| {{dunno}}
|-
! [[IBM Cognos TM1]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{dunno}}
| {{dunno}}
|-
! [[IBM Cognos BI]]
| {{Yes}}
| {{No}}
| {{Yes}}
| {{Yes}}
| {{dunno}}
|-
! [[SAS OLAP Server]]
| ?
| {{Yes}}
| {{Yes}}
| {{dunno}}
| {{dunno}}
|-
! [[SAP NetWeaver Business Intelligence|SAP NetWeaver BW]]
| ?
| {{Yes}}
| {{Yes}}
| {{dunno}}
| {{dunno}}
|-
! [[Cubes (OLAP server)|Cubes]]
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
|}

Data modeling features:
{| class="wikitable sortable"  style="font-size: 100%; text-align: center; width: auto;"
|-
!OLAP Server
!Semi-additive measures
!Many-to-Many 
!Multi-Cube Model
!Perspectives
!KPI
!Translations
!Named Sets
!Multi-attribute Hierarchies
!Actions
|-
! [[Essbase]]
| {{Yes}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{dunno}}
|-
! [[icCube]]
| {{Yes}}&lt;ref&gt;{{cite web|url=http://www.iccube.com/support/documentation/user_guide/schemas_cubes/facts_aggregation.html|title=icCube Aggregatin types}}&lt;/ref&gt;
| {{Yes}}&lt;ref&gt;{{cite web|url=http://www.iccube.com/support/documentation/user_guide/schemas_cubes/facts_many2many.html|title=icCube Many-to-Many}}&lt;/ref&gt;
| {{Yes}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
|-
! Infor BI OLAP Server
| {{Yes}}
| {{dunno}}
| {{Yes}}
| {{dunno}}
| {{Yes}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
|-
! [[Jedox|Jedox OLAP Server]]
| {{Yes}}
| {{Yes}}
| {{dunno}}
| {{Yes}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
|-
! [[Microsoft Analysis Services]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
|-
! [[MicroStrategy|MicroStrategy Intelligence Server]]
| {{Yes}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
|-
! [[Mondrian OLAP server]]
| {{Yes}}&lt;ref&gt;{{cite web|url=http://jira.pentaho.com/browse/MONDRIAN-962|title=Support for Non-Additive and Semi-Additive Measures}}&lt;/ref&gt;
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
|-
! [[Oracle OLAP|Oracle Database OLAP Option]]
| {{Yes}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
|-
! [[IBM Cognos TM1]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
|-
! [[IBM Cognos BI]]
| {{Yes}}
| {{Yes}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{Yes}}
| {{Yes}}
| {{dunno}}
|-
! [[SAS OLAP Server]]
| {{Yes}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
|-
! [[SAP NetWeaver Business Intelligence|SAP NetWeaver BW]]
| {{Yes}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
|-
! [[Cubes (OLAP server)|Cubes]]
| {{dunno}}
| {{Yes}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
|}

==System limits==
{| class="wikitable sortable"  style="font-size: 100%; text-align: center; width: auto;"
|-
!OLAP Server
!# cubes
!# measures
!# dimensions
!# dimensions in cube
!# hierarchies in dimension
!# levels in hierarchy
!# dimension members
|-
! [[Essbase]]&lt;ref&gt;{{cite web|url=http://docs.oracle.com/cd/E57185_01/epm.1112/essbase_db/frameset.htm?limits.html|title=Essbase Server Limits|publisher=Oracle}}&lt;/ref&gt;
| ?
| ?
| ?
| 255
| 255
| ?
| 20,000,000 (ASO), 1,000,000 (BSO)
|-
! [[icCube]]&lt;!-- Java Integer, 32 bits --&gt;
| 2,147,483,647
| 2,147,483,647
| 2,147,483,647
| ?
| 2,147,483,647
| 2,147,483,647
| 2,147,483,647
|-
! Infor BI OLAP Server
| ?
| 10,000,000
| ?
| 30
| ?
| ?
| 10,000,000
|-
! [[Jedox|Jedox OLAP Server]]
| 2^32 (32 bits) / 2^64 (64 bits)
| 2^32
| 2^32 (32 bits) / 2^64 (64 bits)
| 250
| 2^32
| 2^32
| 2^32
|-
! [[Microsoft Analysis Services]]&lt;ref&gt;{{cite web|url=http://technet.microsoft.com/en-us/library/ms365363.aspx|title=SQL Server 2008 Books Online (October 2009)Maximum Capacity Specifications (Analysis Services - Multidimensional Data)|publisher=Microsoft}}&lt;/ref&gt;
| 2,147,483,647
| 2,147,483,647
| 2,147,483,647
| 2,147,483,647 (max. number of dimensions in a database)
| 2,147,483,647
| 2,147,483,647
| 2,147,483,647 (xOLAP)
Unrestricted (In-memory)
|-
! [[MicroStrategy|MicroStrategy Intelligence Server]]&lt;!-- Unrestricted by server - based on hardware limits, infinite it's not possible ;-) --&gt;
| Unrestricted{{efn|name=fn0}}
| Unrestricted{{efn|name=fn0}}
| Unrestricted{{efn|name=fn0}}
| ?
| Unrestricted{{efn|name=fn0}}
| Unrestricted{{efn|name=fn0}}
| Unrestricted{{efn|name=fn0}}
|-
! [[SAS System|SAS OLAP Server]]&lt;ref&gt;{{cite web|url=http://support.sas.com/documentation/cdl/en/olapug/63148/HTML/default/viewer.htm#p0m66bhcbgqwjen1jyfhf6woysu3.htm|title=SAS OLAP Cube Size Specifications}}&lt;/ref&gt;
| Unrestricted{{efn|name=fn0}}
| 1024
| 128
| ?
| 128
| 19
| 4,294,967,296
|-
! [[IBM Cognos TM1]]
| Unrestricted{{efn|name=fn0}}
| Unrestricted
| Unrestricted{{efn|name=fn0}}
| 256
| Unrestricted{{efn|name=fn0}}
| Unrestricted
| Unrestricted
|}
{{notelist|notes=
{{efn|name=fn0|Please update as 'unrestricted', is just not possible}}
}}

==Security==
{| class="wikitable sortable"  style="font-size: 100%; text-align: center; width: auto;"
|-
!rowspan="2"| OLAP Server
!rowspan="2"| Authentication
!rowspan="2"| Network encryption
!rowspan="2"| On-the-Fly{{efn|name=fn1}}
!colspan="3"| Data access
|-
!Cell security
!Dimension security
!Visual totals
|-
! [[Essbase]]
| {{Yes|Essbase authentication, [[LDAP]] authentication, [[Microsoft Active Directory]]}}
| {{Yes|[[Transport Layer Security|SSL]]}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{No}}
|-
! [[icCube]]
| {{Yes|HTTP Basic/Form Authentication, Windows SSO (NTLM,Kerberos)}}
| {{Yes|[[Transport Layer Security|SSL]]}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
|-
! Infor BI OLAP Server
| {{Yes|OLAP authentication, Infor Federation Services, [[LDAP]], [[Microsoft Active Directory]]}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{dunno}}
|-
! [[Jedox|Jedox OLAP Server]]
| {{Yes|Jedox authentication, [[LDAP]], [[Microsoft Active Directory]]}}
| {{Yes|[[Transport Layer Security|SSL]]}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{dunno}}
|-
! [[Microsoft Analysis Services]]
| {{Yes|[[NTLM]], [[Kerberos (protocol)|Kerberos]]}}
| {{Yes|[[Transport Layer Security|SSL]] and [[SSPI]]}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
|-
! [[MicroStrategy|MicroStrategy Intelligence Server]]
| {{Yes|Host authentication, database authentication, [[LDAP]], &lt;br /&gt;[[Microsoft Active Directory]], [[NTLM]], SiteMinder, Tivoli, SAP, [[Kerberos (protocol)|Kerberos]]}}
| {{Yes|[[Transport Layer Security|SSL]], AES&lt;ref&gt;[http://latam.microstrategy.com/Software/Products/Intelligence_Server/features.asp MicroStrategy Intelligence Server Features]&lt;/ref&gt;}}
| ?
| {{Yes}}
| {{Yes}}
| {{Yes}}
|-
! [[Oracle OLAP|Oracle Database OLAP Option]]
| {{Yes|Oracle Database authentication}}
| {{Yes|[[Transport Layer Security|SSL]]}}
| ?
| {{Yes}}
| {{Yes}}
| {{dunno}}
|-
! [[SAS System|SAS OLAP Server]]&lt;ref&gt;{{cite web|url=http://support.sas.com/documentation/cdl/en/mdxag/59575/HTML/default/a003230130.htm|title=SAS OLAP Security Totals and Permission Conditions}}&lt;/ref&gt;
| {{Yes|Host authentication,SAS token authentication, [[LDAP]], [[Microsoft Active Directory]]}}
| {{Yes}}&lt;ref&gt;{{cite web|url=http://support.sas.com/documentation/cdl/en/bisecag/61133/HTML/default/a003275910.htm|title=How to Change Over-the-Wire Encryption Settings for SAS Servers}}&lt;/ref&gt;
| ?
| {{Yes}}
| {{Yes}}
| {{Yes}}
|-
! [[IBM Cognos TM1]]
| {{Yes|Builtin, [[LDAP]], [[Microsoft Active Directory]], [[NTLM]], IBM Cognos BI authentication}}
| {{Yes|[[Transport Layer Security|SSL]]}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
|}
{{notelist|notes=
{{efn|name=fn1|On-the-Fly : The ability to define authentication dynamically via programmatic interfaces. New users do not require restarting the server or redefining the security.}}
}}

==Operating systems==
The OLAP servers can run on the following [[operating system]]s:
{| class="wikitable sortable"  style="font-size: 100%; text-align: center; width: auto;"
|-
! OLAP Server
! Windows
! Linux
! UNIX
! z/OS
! AIX
|-
! [[Essbase]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{No}}
|-
! [[icCube]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
|-
! Infor BI OLAP Server
| {{Yes}}
| {{No}}
| {{No}}
| {{No}}
|-
! [[Jedox|Jedox OLAP Server]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{No}}
|-
! [[Microsoft Analysis Services]]
| {{Yes}}
| {{No}}
| {{No}}
| {{No}}
|-
! [[MicroStrategy|MicroStrategy Intelligence Server]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{No}}
|-
! [[Mondrian OLAP server]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
|-
! [[Oracle OLAP|Oracle Database OLAP Option]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
|-
! [[SAS System|SAS OLAP Server]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
|-
! [[SAP NetWeaver Business Intelligence|SAP NetWeaver BW]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
|-
! [[IBM Cognos TM1]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{No}}
| {{Yes}}
|-
! [[Cubes (OLAP server)|Cubes]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{No}}
|}
&lt;cite id="os_java"&gt;Note (1):&lt;/cite&gt;The server availability depends on [[JVM|Java Virtual Machine]] not on the [[operating system]]&lt;/cite&gt;

==Support information==
{| class="wikitable sortable"  style="font-size: 100%; text-align: center; width: auto;"
|-
! OLAP Server
! Issue Tracking System
! Forum/Blog
! Roadmap
! Source code
|-
! [[Essbase]]
| {{Yes|[http://support.oracle.com myOracle Support]}}
| [http://forums.oracle.com/forums/main.jspa?categoryID=84]
| [http://communities.ioug.org/Portals/2/Oracle_Essbase_Roadmap_Sep_09.pdf]
| Closed
|-
! [[icCube]]
| {{Yes | [http://issues.iccube.com/ YouTrack]}}
| |[http://www.iccube.com/forum]
| 
| Open
|-
! Infor BI OLAP Server
| {{Yes|Infor Xtreme}}
| 
| Available upon request
| Closed
|-
! [[Jedox|Jedox OLAP Server]]
| {{Yes|[http://bugs.palo.net/mantis/main_page.php Mantis]}}
| [http://www.jedox.com/community/palo-forum/board.php?boardid=9]
|
| Open
|-
! [[Microsoft Analysis Services]]
| {{Yes|[https://connect.microsoft.com/SQLServer Connect]}}
| [http://social.msdn.microsoft.com/Forums/en-US/sqlanalysisservices/threads]
| -
| Closed
|-
! [[MicroStrategy|MicroStrategy Intelligence Server]]
| {{Yes | [https://resource.microstrategy.com/Support/MainSearch.aspx MicroStrategy Resource Center]}}
| [https://resource.microstrategy.com/Forum/]
| -
| Closed
|-
! [[Mondrian OLAP server]]
| {{Yes|[http://jira.pentaho.com/browse/MONDRIAN Jira]}}
| [http://forums.pentaho.org/forumdisplay.php?f=79]
| [http://mondrian.pentaho.org/documentation/roadmap.php]
| Open
|-
! [[Oracle OLAP|Oracle Database OLAP Option]]
| {{Yes|[http://support.oracle.com myOracle Support]}}
| [http://forums.oracle.com/forums/main.jspa?categoryID=84]
|
| Closed
|-
! [[SAS System|SAS OLAP Server]]
| {{Yes|[http://support.sas.com/forums/index.jspa Support]}}
| [http://blogs.sas.com/]
|
| Closed
|-
! [[SAP NetWeaver Business Intelligence|SAP NetWeaver BW]]
| {{Yes | [http://service.sap.com/ OSS]}}
| [http://forums.sdn.sap.com/index.jspa]
| [http://esworkplace.sap.com/socoview(bD1lbiZjPTAwMSZkPW1pbg==)/render.asp?id=2270EAD629814D05A7ECECECECC8D002&amp;fragID=&amp;packageid=DEE98D07DF9FA9F1B3C7001A64D3F462]
| Closed
|-
! [[IBM Cognos TM1]]
| {{Yes | [http://ibm.com/support/servicerequest/ IBM Service Request]}}
| [http://www.tm1forum.com/viewforum.php?f=3]
|
| Closed
|-
! [[Cubes (OLAP server)|Cubes]]
| {{Yes|[https://github.com/databrewery/cubes/issues Cubes &#8211; Github Issues]}}
| [https://groups.google.com/forum/#!forum/cubes-discuss]
| [https://github.com/DataBrewery/cubes/wiki/Roadmap]
| [https://github.com/DataBrewery/cubes Open]
|}

==See also==
* [[Cubes (OLAP server)|Cubes]] (light-weight open-source OLAP server)
* [[icCube]]
* [[Palo (OLAP database)]]

==References==
{{reflist}}

{{Data warehouse}}

{{DEFAULTSORT:Comparison Of Olap Servers}}
[[Category:Online analytical processing| ]]
[[Category:Software comparisons|OLAP Servers]]
[[Category:Data management]]
[[Category:Data warehousing products]]</text>
      <sha1>j25e3psitk0shtuvr6x15qlmex7gc2f</sha1>
    </revision>
  </page>
  <page>
    <title>BBC Archives</title>
    <ns>0</ns>
    <id>23998233</id>
    <revision>
      <id>762519177</id>
      <parentid>754698446</parentid>
      <timestamp>2017-01-29T09:13:28Z</timestamp>
      <contributor>
        <username>Tim!</username>
        <id>203786</id>
      </contributor>
      <comment>removed [[Category:History of television]]; added [[Category:History of television in the United Kingdom]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="35335" xml:space="preserve">{{for|the Iron Maiden album|BBC Archives (album)}}
{{distinguish|BBC Motion Gallery}}
{{EngvarB|date=September 2013}}
{{Use dmy dates|date=September 2013}}
[[File:BBC Information and Archives Logo.svg|thumb|300px|BBC Information and Archives logo]]

'''BBC Information and Archives''' (sometimes known just as '''BBC Archives''') are collections documenting the [[BBC]]'s broadcasting history, including copies of [[BBC Television|television]] and [[BBC Radio|radio]] broadcasts, internal documents, photographs, [[BBC Online|online]] content, [[sheet music]], commercially available music, press cuttings and historic equipment.&lt;ref name="BBCArchive TV 1"/&gt; The original copies of these collections are permanently retained but are now in the process of being digitised, estimated to take until approximately 2015. Some collections are now being uploaded onto the BBC Archives website on [[BBC Online]] for viewers to see. The archive is one of the largest broadcast archives in the world with over 12 million items.&lt;ref name=Perivale1&gt;{{cite web|last=Hayes|first=Sarah|title=The new BBC Archive Centre in Perivale|url=http://www.bbc.co.uk/blogs/aboutthebbc/2011/10/the-new-bbc-archive-centre-at.shtml|work=About the BBC Blog|publisher=BBC Online|accessdate=17 January 2012}}&lt;/ref&gt;

==Overview==
The BBC Archives encompass numerous different archives containing different materials produced or acquired by the BBC. The earliest material dates back to 1890 and now consists of 1 million hours of playable material, in addition to documents, photographs and equipment.&lt;ref name="gutechweekly"&gt;{{cite news|last=Kiss|first=Jemima|title=In The BBC Archive|url=https://www.theguardian.com/technology/blog/audio/2010/aug/18/bbc-archive-roly-keating-windmill-road|work=Tech Weekly|publisher=Guardian News &amp; Media Ltd|accessdate=21 August 2010 | location=London | date=18 August 2010| archiveurl= https://web.archive.org/web/20100821165828/http://www.guardian.co.uk/technology/blog/audio/2010/aug/18/bbc-archive-roly-keating-windmill-road?| archivedate= 21 August 2010 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt; The archives contain 12 million items on 66 miles of shelving spread over several sites.&lt;ref name=gutechweekly /&gt; The stock is managed using a [[bar code]] system, which help to locate material on the shelves and also track material that has been lent out.&lt;ref name=gutechweekly /&gt; The BBC says that the budget for managing, protecting and digitising the archive accounts for only a small part of the BBC's overall spend.&lt;ref name=gutechweekly /&gt;

The BBC is engaging in an ongoing project to [[Digital reformatting|digitise]] archived programme material, converting recordings made on older [[Analog recording|analogue formats]] such as audio tape, [[videotape]] and film to electronic formats which are compatible with modern computer systems. Much of the audio-visual material was originally recorded on formats which are now obsolete and incompatible with modern broadcast equipment due to the fact that the machines used to reproduce many formats are no longer being manufactured. Additionally, some film and audio formats are slowly disintegrating, and digitisation also serves as a [[digital preservation]] programme. As of summer 2010 BBC Archive staff have spent approximately ten years digitising half of the media content&lt;ref name=gutechweekly /&gt;&lt;ref name="BBC Archive BBCInternetblog"/&gt; and due to improving work practices expect to complete the other half in five years. Current estimates suggest the digitised archive would comprise approximately 52 [[petabyte]]s of information,&lt;ref name=gutechweekly /&gt; with one programme minute of video requiring 1.4 [[gigabyte]]s of storage.&lt;ref name=gutechweekly /&gt; The BBC uses the [[Material Exchange Format]] (MXF)&lt;ref name=gutechweekly /&gt; which is an uncompressed, non-proprietary format which the BBC has been publicising to mitigate the threat of the format becoming obsolete (as digital formats can and do).&lt;ref name=gutechweekly /&gt;

The Archive digitisation a key part of the BBC's programme to engineer a fully digital and [[tapeless|tapeless production workflow]] across the entire Corporation. It was closely tied in with the ill-fated [[Digital Media Initiative]] (DMI), a scheme which ran from 2008 to 2013 and attempted to create a unified online archive search and programme production system.&lt;ref name=bbc-dmi&gt;{{cite web|title=Digital Media Initiative|url=http://www.bbc.co.uk/careers/divisions/digital-media-initiative|publisher=BBC|accessdate=15 February 2012|archiveurl=https://web.archive.org/web/20120310041000/http://www.bbc.co.uk/careers/divisions/digital-media-initiative|archivedate=10 March 2012|deadurl=yes}}&lt;/ref&gt; After spiralling development costs and project delays, the problems with DMI came to public attention during coverage of the [[death and funeral of Margaret Thatcher]] in April 2013, when it was reported that the lack of digital ingest facilities provided for [[BBC News]] staff meant that tapes had to be sent by taxi from the Perivale centre to be digitised by independent companies in central London.&lt;ref&gt;{{cite news|title=BBC's Thatcher coverage highlights problems with non-digital archives|url=https://www.theguardian.com/media/2013/apr/11/bbc-thatcher-coverage|accessdate=3 May 2013|newspaper=The Guardian|date=11 February 2012|location=London|first=Tara|last=Conlan}}&lt;/ref&gt; DMI was cancelled in 2013.&lt;ref name=bbc_abandons&gt;{{cite news|title=BBC abandons &#163;100m digital project|url=http://www.bbc.co.uk/news/entertainment-arts-22651126|accessdate=25 May 2013|newspaper=BBC News|date=24 May 2013}}&lt;/ref&gt;

The BBC Archive website was relaunched online in 2008 and has provided newly released historical material regularly since then.&lt;ref&gt;{{cite web|last=Sangster|first=Jim|title=A new homepage for BBC Archive|url=http://www.bbc.co.uk/blogs/bbcinternet/2010/05/a_new_homepage_for_bbc_archive.html|work=BBC Internet Blog|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt; The BBC works in partnership with the [[British Film Institute]] (BFI), [[The National Archives]] and other partners in working with and using the materials.&lt;ref name=gutechweekly /&gt; A related project called "Genome" is expected to complete in 2011 and will make programme listings dating back to 1923, sourced from ''[[The Radio Times]]'', available to search online.&lt;ref name=gutechweekly /&gt;

In July 2008, [[Roly Keating]] was appointed Director of Archive Content,&lt;ref&gt;{{cite web|url=http://www.bbc.co.uk/pressoffice/pressreleases/stories/2008/07_july/22/archive.shtml |title=Roly Keating appointed as Director of Archive Content |publisher=BBC Press Office|date=22 July 2008 |accessdate=1 July 2011}}&lt;/ref&gt; with responsibility for increasing public access to the BBC&#8217;s archives. In October 2008, Keating appointed [[Tony Ageh]] Controller of Archive Development with "specific responsibility for developing ways of making the archive easily understandable and accessible to users".&lt;ref&gt;{{cite web|url=http://www.bbc.co.uk/pressoffice/pressreleases/stories/2008/10_october/10/ageh.shtml|publisher=BBC Press Office|date=10 October 2008|accessdate=1 July 2011|title=Tony Ageh appointed Controller of Archive Development}}&lt;/ref&gt;

In 2012, BBC Archive Development produced a book - primarily aimed as BBC staff - titled 'BBC Archive Collections: What's In The Archive And How To Use Them'.&lt;ref&gt;'BBC Archive Collections: What's In The Archives, And How To Use Them' Edited by Jake Berger https://www.dropbox.com/s/rz1o57nzlsf1v04/BBC%20Archive%20Collections%20Guide%202012.pdf?dl=0&lt;/ref&gt;  This book describes the BBC's archive collections and offers guidance around on how items from the collections can be reused online.  The book's references to 'Fabric', a system due to be delivered by the [[Digital Media Initiative]] are no longer accurate as the project was cancelled.

==Buildings==
From 1968 to 2010 the BBC Archive was housed at the Archive centre in Windmill Road, [[Brentford]], in [[W postcode area|west London]].&lt;ref name=Perivale1/&gt;&lt;ref name="BBC Archive BBCInternetblog"/&gt;&lt;ref name="Perivale 2 AtBBCblog"/&gt; The condition of the building deteriorated over the years and suffered occasional flooding incidents, and eventually the Archive was relocated to a new centre at Perivale Park, [[Perivale]], three miles north of the old site.&lt;ref name="Perivale 2 AtBBCblog"/&gt;&lt;ref name="Perivale Centre R4 Blog"/&gt; The new BBC Archive Centre was opened in Summer 2010 and all material was successfully moved by March 2011.&lt;ref name="Perivale 2 AtBBCblog"/&gt;&lt;ref name="Perivale 3 S&amp;Pblog"/&gt; The cost of the refurbishment and of the move was approximately &#163;16.6 million.&lt;ref name=Perivale1/&gt;&lt;ref name="Perivale 2 AtBBCblog"&gt;{{cite news|last=Skinner|first=Peter|title=A new home for the BBC Archive|url=http://www.bbc.co.uk/blogs/aboutthebbc/2010/08/a-warm-balmy-afternoon-in.shtml|accessdate=19 January 2012|newspaper=BBC About the BBC Blog|date=20 August 2010}}&lt;/ref&gt;&lt;ref name="Perivale Centre R4 Blog"&gt;{{cite news|last=Bolton|first=Roger|title=Tears in Perivale &#8211; Feedback in the archives|url=http://www.bbc.co.uk/blogs/radio4/2011/09/tears_in_perivale_feedback_in_the_archives.html|accessdate=19 January 2012|newspaper=BBC Radio 4 and 4 Extra Blog|date=23 September 2011}}&lt;/ref&gt;&lt;ref name="Perivale 3 S&amp;Pblog"&gt;{{cite news|last=Kane|first=Chris|title=Preserving the past at Perivale|url=http://www.bbc.co.uk/blogs/spacesandplaces/2011/03/preserving_the_past_at_perival.shtml|accessdate=19 January 2012|newspaper=BBC Spaces &amp; Places Blog|date=9 March 2011}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://downloads.bbc.co.uk/foi/classes/disclosure_logs/rfi20111170_cost_of_new_archive_centre.pdf|title=Freedom of Information Act 2000 &#8211; RFI20111170|last=Jupe|first=Steve|date=20 October 2011|work=Freedom of Information Request|publisher=BBC|accessdate=7 February 2012}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=The BBC Archive Centre has moved|url=http://www.bbc.co.uk/commissioning/news/the-bbc-archive-centre-has-moved.shtml|work=BBC Commissioning|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt;

Material is stored in thirteen vaults,&lt;ref name="Perivale 2 AtBBCblog"/&gt; controlled to match the best climate for the material inside them,&lt;ref name=Perivale1/&gt;&lt;ref name=gutechweekly /&gt;&lt;ref name="BBC Archive BBCInternetblog"/&gt;&lt;ref name="Perivale 2 AtBBCblog"/&gt; and named after a different BBC personality depending on the content contained in them.&lt;ref name="Perivale 2 AtBBCblog"/&gt; In addition to the vaults, new editing and workrooms have been added so that the material can easily be transferred between formats as well as viewed and restored.&lt;ref name="Perivale 2 AtBBCblog"/&gt; The building has also been fitted with fire suppression systems to protect the archive in the event of an incident at the centre, so the total loss of the archive is avoided.&lt;ref name="Perivale Centre R4 Blog"/&gt;

==Television Archive==
The '''BBC Television Archive''' contains over 600,000 hours of television broadcast material&lt;ref name="BBCArchive TV 1"&gt;{{cite web|last=Lee|first=Adam|title=BBC Television Archive &#8211; What's in the BBC Archive|url=http://www.bbc.co.uk/archive/tv_archive.shtml|work=BBC Archive &#8211; Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt; located on 600,000/650,000 film reels&lt;ref name="Windmill Road"&gt;{{cite web|title=A Tour of the BBC Archive at Windmill Road|url=https://www.youtube.com/watch?v=S3Z2djrAW2M|publisher=[[BBC]]|accessdate=23 July 2015|date=13 Aug 2010}}&lt;/ref&gt;&lt;ref name="BBC Archive BBCInternetblog"&gt;{{cite news|last=Williams|first=Adrian|title=Safeguarding the BBC's archive|url=http://www.bbc.co.uk/blogs/bbcinternet/2010/08/safeguarding_the_bbcs_archive.html|accessdate=19 January 2012|newspaper=BBC Internet Blog|date=18 August 2010}}&lt;/ref&gt;&lt;ref name="BBCArchive PTV 4"&gt;{{cite web|last=Williams|first=Adrian|title=Preserving the Television Archive &#8211; Film|url=http://www.bbc.co.uk/archive/preserving.shtml?chapter=4|work=BBC Archives &#8211; Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt; and 2.4/2.7 million videotapes.&lt;ref name="BBC Archive BBCInternetblog"/&gt;&lt;ref name="Windmill Road"/&gt; The archive itself holds extensive material from approximately the mid-1970s onwards, when important recordings at the broadcaster were retained for the future.&lt;ref name="BBCArchives TV 6"&gt;{{cite web|last=Lee|first=Adam|title=BBC Television Archive &#8211; When did the BBC start to ensure that important broadcasts were not destroyed|url=http://www.bbc.co.uk/archive/tv_archive.shtml?chapter=6|work=BBC Archives &#8211; Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt;

Recordings from before this date are less comprehensively preserved; the process of [[Kinescope|telerecording]] was originally invented in 1947&lt;ref name="BBCArchive TV 2"&gt;{{cite web|last=Lee|first=Adam|title=BBC Television Archive &#8211; Why aren't there many recordings from the early days of television|url=http://www.bbc.co.uk/archive/tv_archive.shtml?chapter=2|work=BBC Archive &#8211; Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt; while videotape recording was gradually introduced from the late 1950s onwards,&lt;ref name="BBC Archive TV 4"&gt;{{cite web|last=Lee|first=Adam|title=BBC Television Archive &#8211; When did the BBC start recording programmes regularly|url=http://www.bbc.co.uk/archive/tv_archive.shtml?chapter=4|work=BBC Archive &#8211; Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt; but due to the expense of the tapes,&lt;ref name="BBCArchives PTV 8"&gt;{{cite web|last=Williams|first=Adrian|title=Preserving the Television Archive &#8211; Why was videotape invented|url=http://www.bbc.co.uk/archive/preserving.shtml?chapter=8|work=BBC Archives &#8211; Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt; recording was seen for production use only with recordings subsequently being [[wiping|wiped]].&lt;ref name="BBC Archive TV 4"/&gt; or telerecordings being junked. The exceptions in the early years were usually occasions of great importance, such as the [[coronation of Queen Elizabeth II]].&lt;ref name="BBCArchive TV 2"/&gt; In addition, numerous programmes at the time were broadcast 'live' and so utilised no recording procedure in the production process.&lt;ref name="BBCArchive TV 2"/&gt; The earliest item in the collection is from 1936.&lt;ref name="BBC Archive BBCInternetblog"/&gt;&lt;ref name="Perivale 3 S&amp;Pblog"/&gt;&lt;ref name="BBCArchives PTV 6"&gt;{{cite web|last=Williams|first=Adrian|title=Preserving the Television Archive &#8211; The oldest BBC Television film clip|url=http://www.bbc.co.uk/archive/preserving.shtml?chapter=6|work=BBC Archives &#8211; Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt;

In 2013 there were 340,000 D3 Tapes but the hardware they have could only transfer up to 130,000 D3 tapes.&lt;ref name=Digitising&gt;{{cite web|title=Digitising the BBC archive|url=http://www.bbc.co.uk/academy/technology/article/art20130704121742520|publisher=[[BBC]]|accessdate=23 July 2015|date=2013}}&lt;/ref&gt; The BBC has had to be very selective of what they are transferring.&lt;ref name=Digitising/&gt;

Before anything is put into the archive a team of Digitisation Operators watch and listen to programs looking for problems with the tapes or transfers.&lt;ref name="Sarah Bello"&gt;{{cite web|title=Sarah Bello, BBC Archive|url=https://www.youtube.com/watch?v=MyG1lSdsKQs|publisher=[[BBC]]|accessdate=23 July 2015|date=11 March 2013}}&lt;/ref&gt; 

Today, the majority of programmes are kept, including news, entertainment, drama and a selection of other long-running programmes such as quiz shows.&lt;ref name="BBCArchive TV 7"&gt;{{cite web|last=Lee|first=Adam|title=BBC Television Archive &#8211; How does the BBC decide what to keep in its archive today|url=http://www.bbc.co.uk/archive/tv_archive.shtml?chapter=7|work=BBC Archive &#8211; Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt; The remaining material from the television archive is offered to the [[British Film Institute]] prior to being disposed of.&lt;ref name="BBCArchive TV 8"&gt;{{cite web|last=Lee|first=Adam|title=BBC Television Archive &#8211; Does the BBC offer recordings it's not keeping for the archive to anyone else|url=http://www.bbc.co.uk/archive/tv_archive.shtml?chapter=8|work=BBC Archives &#8211; Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt;

==Sound Archive==
{{main|BBC Sound Archive}}

The '''BBC Sound Archive''' contains the archived output from the BBC's radio output. Widespread recordings exist in the archive from the mid-1930s, when recording of programmes and speeches were kept for rebroadcast; the catalyst for this was the launch of the [[BBC Empire Service]] in 1932 and the subsequent rebroadcast of speeches from political leaders at a time convenient in the different time zones.&lt;ref name="BBCArchive - Radio 3"&gt;{{cite web|last=Rooks|first=Simon|title=BBC Sound Archive &#8211; Why did the BBC start making recordings|url=http://www.bbc.co.uk/archive/sound_archive.shtml?chapter=3|work=BBC Archives &#8211; Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt; Prior to this, the broadcast of recordings was seen as being false to the listener and was avoided.&lt;ref name="BBCArchives Radio 2"&gt;{{cite web|last=Rooks|first=Simon|title=BBC Sound Archive &#8211; Why aren't there many recordings from the early days of radio|url=http://www.bbc.co.uk/archive/sound_archive.shtml?chapter=2|work=BBC Archives &#8211; Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt; Any recordings made were frequently disposed of and it was the efforts of [[Marie Slocombe]], who founded the Sound Archive in 1937 when she retained recordings of prominent figures in the country, that the archive became into being officially when she was appointed the Sounds Recording Librarian in 1941.&lt;ref name="BBCArchives - PRadio 6"&gt;{{cite web|last=Weaver|first=Julia|title=Preserving the Sound Archive &#8211; How did the Sound Archive begin|url=http://www.bbc.co.uk/archive/preserving_sound.shtml?chapter=6|work=BBC Archives &#8211; Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt; Today, all of the BBC's radio output is recorded for re-use,&lt;ref name="BBCArchive - Radio 8"&gt;{{cite web|last=Rooks|first=Simon|title=BBC Sound Archive &#8211; Does the BBC keep copies of all programmes today|url=http://www.bbc.co.uk/archive/sound_archive.shtml?chapter=8|work=BBC Archives &#8211; Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt; with approximately 66% of output being preserved in the Archives;&lt;ref name="BBCArchive - Radio 8"/&gt; programmes involving guests or live performances from artists are kept&lt;ref name="BBCArchive - Radio 8"/&gt; whereas programmes in which the DJ plays commercially available music are only sampled and not kept entirely.&lt;ref name="BBCArchive - Radio 8"/&gt; Prior to any material being disposed of, the material is offered to the [[British Library Sound Archive]].&lt;ref name="BBCArchive TV 8"/&gt;

The archive consists of a number of different formats including 200 [[Phonograph cylinder|wax cylinders]],&lt;ref name="BBCArchive - PRadio 3"&gt;{{cite web|last=Weaver|first=Julia|title=Preserving the Sound Archive &#8211; What are the earliest sound recordings|url=http://www.bbc.co.uk/archive/preserving_sound.shtml?chapter=3|work=BBC Archives &#8211; Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt; numerous [[gramophone record]]s made from both [[shellac]] and [[vinyl]]&lt;ref name="BBCArchives - PRadio 4"&gt;{{cite web|last=Weaver|first=Julia|title=Preserving the Sound Archive &#8211; Discs|url=http://www.bbc.co.uk/archive/preserving_sound.shtml?chapter=4|work=BBC Archives &#8211; Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt; as well as numerous more recordings on [[Reel-to-reel audio tape recording|tape]], CD and on [[digital audio tape]] (DAT).&lt;ref name="BBCArchive PRadio 5"&gt;{{cite web|last=Weaver|first=Julia|title=Preserving the Sound Archive &#8211; Tape|url=http://www.bbc.co.uk/archive/preserving_sound.shtml?chapter=5|work=BBC Archives &#8211; Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt; The difficulty of these different formats is the availability of the machines required to play them; some of the vinyl records in the archive are 16 inches in size and require large [[phonograph]] units to play,&lt;ref name="BBCArchives - PRadio 4"/&gt; while the players for the wax cylinders and DATs are no longer in production.&lt;ref name="BBCArchive PRadio 5"/&gt; There are 700,00 vinyl records, 180,000 78's records, 400,000 [[LP record]] and 350,000 [[Compact disc|Cd's]] in the archive.&lt;ref name="Windmill Road"/&gt;

The oldest item is a wax cylinder containing a recording made by [[Florence Nightingale]], recorded on 30 July 1890.&lt;ref name="BBCArchive - PRadio 3"/&gt; Another unique item is the gramophone record from [[Mary of Teck|Queen Mary]]'s [[doll house]], which is approximately an inch in size and had the [[God Save the Queen|national anthem]] on it.&lt;ref name="BBCArchives - PRadio 4"/&gt;

The Sound Archive is based at the new BBC Archive Centre in Perivale, along with the television archive,&lt;ref name=Perivale1/&gt;&lt;ref name="Perivale 3 S&amp;Pblog"/&gt; and was previously based at Windmill Road, Brentford.

==Written Archives==
The '''BBC Written Archive''' contains all the internal written documents and communications from the corporation from the launch in 1922 to the present day.&lt;ref name="Written Archives BBC Story"&gt;{{cite web|title=The Written Archives|url=http://www.bbc.co.uk/historyofthebbc/contacts/wac.shtml|work=The BBC Story|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt;&lt;ref name="BBCArchive Written 1"&gt;{{cite web|last=Kavanagh|first=Jacquie|title=BBC Written Archives &#8211; What are the BBC Written Archives|url=http://www.bbc.co.uk/archive/written.shtml|work=BBC Archives &#8211; Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt; Its collections shed light into the behind the scenes workings of the corporation and also elaborate on the difficulties of getting a television or radio programme to or off the air as the case may be.&lt;ref name="BBC Archive Written 3"&gt;{{cite web|last=Kavanagh|first=Jacquie|title=BBC Written Archives &#8211; What do the documents reveal|url=http://www.bbc.co.uk/archive/written.shtml?chapter=3|work=BBC Archives &#8211; Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt; The archive guidelines state that access to files post-1980 is restricted due to the current nature of the files; the general exception to this rule are documents such as scripts and Programme as Broadcast records.

The Written Archives are located at the BBC Written Archive Centre in [[Caversham, Berkshire]], near [[Reading, Berkshire|Reading]].&lt;ref name="Written Archives BBC Story"/&gt; The centre houses the archive on four and a half miles of shelving along with reading rooms. The centre is different from the other BBC Archives in that the centre opens for writers and academic researchers in higher education.&lt;ref name="Written Archives BBC Story"/&gt;

==Photographic Library==
The '''BBC Photographic Library''' is responsible for approximately 10 million images,&lt;ref name="BBCArchive Photo 1"&gt;{{cite web|last=Dewar|first=Natalie|title=Photographic Library &#8211; What's in the BBC Photo Library|url=http://www.bbc.co.uk/archive/photo_library.shtml|work=BBC Archives &#8211; Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt; dating back to 1922,&lt;ref&gt;{{cite web|title=BBC Pictures|url=http://www.bbc.co.uk/mediacentre/pictures/index.html|work=BBC Media Centre|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt; created for publicity purposes and subsequently kept for future use.&lt;ref name="BBCArchive Photo 2"&gt;{{cite web|last=Dewar|first=Natalie|title=Photographic Library &#8211; Why does the BBC have photographs|url=http://www.bbc.co.uk/archive/photo_library.shtml?chapter=2|work=BBC Archives &#8211; Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt; In addition to programme promotion, a large number of images are of historic events which are often incorporate into the daily news bulletins; as a result, half the photographic library team work specifically with these images.&lt;ref name="BBCArchive Photo 4"&gt;{{cite web|last=Dewar|first=Natalie|title=Photographic Library &#8211; The Team|url=http://www.bbc.co.uk/archive/photo_library.shtml?chapter=4|work=BBC Archives &#8211; Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt; The images themselves are kept as originals in the archive, with digitisation only utilised when a specific image is required for use, when the image is sent in a digital format.&lt;ref name="BBCArchive Photo 5"&gt;{{cite web|last=Dewar|first=Natalie|title=Photographic Library &#8211; What format are the images stored on|url=http://www.bbc.co.uk/archive/photo_library.shtml?chapter=5|work=BBC Archives &#8211; Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt; Copies of images are also used in case any images are damaged, notable due to [[vinegar syndrome]].&lt;ref name="BBCArchive Photo 6"&gt;{{cite web|last=Dewar|first=Natalie|title=Photographic Library &#8211; Preservation|url=http://www.bbc.co.uk/archive/photo_library.shtml?chapter=6|work=BBC Archives &#8211; Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt; The BBC Photographic library itself is based within [[BBC Television Centre]], London.

The most popular images from the Archive include [[Colin Firth]] in ''[[Pride and Prejudice (1995 TV series)|Pride and Prejudice]]'', [[Michael Parkinson]] interviewing [[Muhammad Ali]], [[Jimmy Savile]] presenting the first ''[[Top of the Pops]]'', [[Martin Bashir]] interviewing [[Diana, Princess of Wales]] and a picture of [[Delia Derbyshire]] at work in the Radiophonic workshop at the BBC.&lt;ref name="BBCArchive Photo 8"&gt;{{cite web|last=Dewar|first=Natalie|title=Photographic Library &#8211; Our Top 10|url=http://www.bbc.co.uk/archive/photo_library.shtml?chapter=8|work=BBC Archive &#8211; Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt;

==Heritage Collection==
The '''BBC Heritage Collection''' is the newest of the BBC Archives and holds a variety of historic broadcast technology, art, props and merchandise.&lt;ref name="BBCArchives Heritage 2"&gt;{{cite web|last=O'Connell|first=Rory|title=BBC Heritage Collection &#8211; Where do the items come from|url=http://www.bbc.co.uk/archive/heritage.shtml?chapter=2|work=BBC Archives &#8211; Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt; The collection was created out of personal collections and bequeaths by former staff members, as the BBC had no formal policy on the heritage collection until c.2003.&lt;ref name="BBCArchives Heritage 2"/&gt;

The collection includes, amongst other items, the BBC One Noddy Globe and clock,&lt;ref name="BBCArchive Heritage 3"&gt;{{cite web|last=O'Connell|first=Rory|title=BBC Heritage Collection &#8211; Broadcast technology|url=http://www.bbc.co.uk/archive/heritage.shtml?chapter=3|work=BBC Archives &#8211; Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt; a [[BBC-Marconi Type A]] microphone,&lt;ref name="BBCArchive Heritage 3"/&gt; an early [[crystal radio]] made by the [[British Broadcasting Company]],&lt;ref name="BBCArchive Heritage 3"/&gt; a [[405-line television system|Marconi/EMI camera]] used in the early [[BBC Television]] experiments,&lt;ref name="BBCArchive Heritage 3"/&gt; a [[BBC Micro]] computer&lt;ref name="BBCArchive Heritage 3"/&gt; and a selection of items used to create [[Foley (filmmaking)|Foley]].&lt;ref name="BBCArchive Heritage 3"/&gt; In addition to all the broadcast technology, art is also kept, namely the portraits of all the BBC [[Director-General of the BBC|Director General]]s,&lt;ref name="BBCArchive Heritage 4"&gt;{{cite web|last=O'Connell|first=Rory|title=BBC Heritage Collection &#8211; Art|url=http://www.bbc.co.uk/archive/heritage.shtml?chapter=4|work=BBC Archives &#8211; Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt; as well as props including an original [[TARDIS]] from ''[[Doctor Who]]''&lt;ref name="BBCArchive Heritage 5"&gt;{{cite web|last=O'Connell|first=Rory|title=BBC Heritage Collection &#8211; Costumes and Props|url=http://www.bbc.co.uk/archive/heritage.shtml?chapter=5|work=BBC Archives &#8211; Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt; and the children's television puppet [[Gordon the Gopher]].&lt;ref name="BBCArchive Heritage 5"/&gt;

The heritage collection itself has no one permanent home, as the majority of objects are on display, either around BBC properties or on loan to museums or other collections; the most notable museum housing the collection is the [[National Media Museum]] in [[Bradford]].&lt;ref name="BBCArchive Heritage 8"&gt;{{cite web|last=O'Connell|first=Rory|title=BBC Heritage Collection &#8211; Where can I see items from the collection|url=http://www.bbc.co.uk/archive/heritage.shtml?chapter=8|work=BBC Archives &#8211; Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt;

==Archive Treasure Hunt==
At the turn of the millennium, the BBC launched the '''BBC Archive Treasure Hunt''', a public appeal to recover pre-1980s lost BBC radio and television productions.&lt;ref&gt;{{cite web|url=http://www.bbc.co.uk/cult/treasurehunt/about/about.shtml |title=BBC Online &#8211; Cult &#8211; Treasure Hunt &#8211; About the Campaign |publisher=Bbc.co.uk |date= |accessdate=30 July 2010| archiveurl= https://web.archive.org/web/20100721235531/http://www.bbc.co.uk/cult/treasurehunt/about/about.shtml| archivedate= 21 July 2010 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt; Original material, featuring several popular programmes were lost due to the practice of [[wiping]], because of copyright issues and for technological reasons.&lt;ref&gt;{{cite web|url=http://www.bbc.co.uk/cult/treasurehunt/about/lost.shtml |title=BBC Online &#8211; Cult &#8211; Treasure Hunt &#8211; About the Campaign |publisher=Bbc.co.uk |date= |accessdate=30 July 2010}}&lt;/ref&gt;&lt;ref&gt;{{cite web|author=Stuart Douglas - www.thiswaydown.org |url=http://www.btinternet.com/~m.brown1/bbchunt.htm |title=missing episodes articles |publisher=Btinternet.com |date=7 July 1965 |accessdate=30 July 2010| archiveurl= https://web.archive.org/web/20100814103420/http://www.btinternet.com/~m.brown1/bbchunt.htm| archivedate= 14 August 2010 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt;

The resolution of this appeal was that over one hundred productions were recovered&lt;ref&gt;{{cite web|url=http://fiatifta.org/aboutfiat/news/old/2001/2001-04/03.light.html |title=No 4 2001 &#8211; Missing Believed Wiped |publisher=Fiat/Ifta |date= |accessdate=30 July 2010| archiveurl= https://web.archive.org/web/20100716032923/http://fiatifta.org/aboutfiat/news/old/2001/2001-04/03.light.html| archivedate= 16 July 2010 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt; including ''[[The Men from the Ministry]]'', ''[[Something to Shout About (film)|Something To Shout About]]'', ''[[Man and Superman]]'', ''[[The Doctor's Dilemma (play)|The Doctor's Dilemma]]'', ''[[I'm Sorry, I'll Read That Again]]'', ''[[Hancock's Half Hour]]'', ''[[I'm Sorry, I Haven't A Clue]]'' and ''[[The Ronnie Corbett Thing]]'' in addition to recording sessions with [[Elton John]], [[Ringo Starr]] and [[Paul Simon]].&lt;ref name="BBCTH"&gt;{{cite web|url=http://www.bbc.co.uk/cult/treasurehunt/about/listoffinds.shtml |title=BBC Online &#8211; Cult &#8211; Treasure Hunt &#8211; List of Finds |publisher=Bbc.co.uk |date= |accessdate=30 July 2010}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.allbusiness.com/services/motion-pictures/4848337-1.html |title='hunt' Unearths BBC Treasures From Radio, Tv &amp;#124; Business solutions from |publisher=AllBusiness.com |date=9 November 2001 |accessdate=30 July 2010}}&lt;/ref&gt; Also, the Peter Sellers Estate Collection donated numerous recordings featuring [[Peter Sellers]].&lt;ref name="BBCTH"/&gt;

==Creative Archive Licence==
The BBC together with the [[British Film Institute]], the [[Open University]], [[Channel 4]] and [[Teachers' TV]] formed a collaboration, named the Creative Archive Licence Group, to create a copyright licence for the re-release of archived material.&lt;ref name=CAL&gt;{{cite web|title=Creative Archive pilot|url=http://www.bbc.co.uk/creativearchive/|publisher=BBC|accessdate=31 March 2016}}&lt;/ref&gt;

The Licence was a trial, launched in 2005, and notable for the re-release of part of the [[BBC News]]' archive and programmes made by the [[BBC Natural History Unit]] for creative use by the public. While artists and teachers were encouraged to use the content to create works of their own, the terms of the licence were restrictive compared to [[copyleft]] licences. Use of Creative Archive content for commercial, "endorsement, campaigning, defamatory or derogatory purposes" was forbidden, any derivative works were to be released under the same licence, and content was only  to be used within the UK.&lt;ref name=CAL/&gt;&lt;ref&gt;{{cite web|title=Creative Archive License|url=http://news.bbc.co.uk/1/hi/help/4527506.stm|publisher=BBC|accessdate=17 January 2012}}&lt;/ref&gt; The trial ended in 2006 following a review by the [[BBC Trust]] and works released under the licence were withdrawn.&lt;ref name=CAL/&gt;

==Voices from the archives==
Voices from the Archives is a former [[BBC]] project, launched in partnership with [[BBC Four]] that provided free access to audio interviews with various notable people and professions from a variety of political, religious and social backgrounds. The website ceased to be updated in June 2005, and the concept was instead adopted by [[BBC Radio 4]] as a collection of film interviews from various programmes.

==Programme catalog==
{{main|BBC Programme Catalogue}}
Over the years there the BBC has used various Programme catalog databases to keep a record of the programmes in the archives. Internal databases include [[Infax]] and [[BBC Fabric|Fabric]], and publicly accessible databases include [[BBC Genome]] and [http://www.bbc.co.uk/programmes BBC Programmes].

==See also==
{{portal|BBC}}
* [[BBC Genome Project]]
* [[Lost film]]
* [[Film preservation]]
* [[Missing Believed Wiped]]
* [[Telerecording]]
* [[Doctor Who missing episodes|''Doctor Who'' missing episodes]]
* [[Timeline of the BBC]]

==References==
{{reflist|2}}

==External links==

===BBC Archives===
*{{bbc.co.uk|id=archive|title=BBC Archives}}
*{{bbc.co.uk|id=bbcfour/collections|title=BBC Four &#8211; Collections}}
*{{bbc.co.uk|id=archive/archive_pioneers|title=BBC Archive collection &#8211; Archive Pioneers: Saviours of sound at the BBC}}
*{{bbc.co.uk|id=programmes|title=BBC Programmes}}
*{{bbc.co.uk|id=informationandarchives|title=BBC Information and Archives}}
* [https://www.theguardian.com/technology/blog/audio/2010/aug/18/bbc-archive-roly-keating-windmill-road Tech Weekly podcast: In the BBC archives] from ''[[The Guardian]]'' website.
* [https://www.dropbox.com/s/rz1o57nzlsf1v04/BBC%20Archive%20Collections%20Guide%202012.pdf?dl=0 BBC Archive Collections: What's In The Archives, And How To Use Them]

===Wiped Material===
* [http://www.missing-episodes.com/ British TV Missing Episodes Index]
* [http://www.wipednews.com/ Wiped News.Com &#8211; A news and features website devoted to missing TV, Film &amp; Radio]

{{BBC}}

[[Category:BBC]]
[[Category:BBC New Media|Archives]]
[[Category:Data management]]
[[Category:Broadcasting websites]]
[[Category:British websites]]
[[Category:BBC offices, studios and buildings|Archives]]
[[Category:Organisations based in Reading, Berkshire]]
[[Category:History of television in the United Kingdom]]
[[Category:History of radio]]
[[Category:BBC history]]
[[Category:Year of establishment missing]]
[[Category:Archives in Berkshire]]
[[Category:Television archives]]</text>
      <sha1>1j5euqw83ymiltdatnus7k813c0zcws</sha1>
    </revision>
  </page>
  <page>
    <title>DMAPI</title>
    <ns>0</ns>
    <id>8947566</id>
    <revision>
      <id>621913548</id>
      <parentid>621913486</parentid>
      <timestamp>2014-08-19T13:31:55Z</timestamp>
      <contributor>
        <ip>119.151.72.60</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="809" xml:space="preserve">'''Data Management API''' ('''DMAPI''') is the interface defined in the [[X/Open]] document "Systems Management: Data Storage Management (XDSM) API" dated February 1997. [[XFS]], IBM [[JFS (file system)|JFS]], [[VxFS]], [[AdvFS]], [[StorNext]] and [[GPFS]] file systems support DMAPI for [[Hierarchical storage management|Hierarchical Storage Management]] (HSM).

== External links ==
* [http://pubs.opengroup.org/onlinepubs/9657099/ Systems Management: Data Storage Management (XDSM) API]
* [http://publib.boulder.ibm.com/infocenter/clresctr/vxrx/topic/com.ibm.cluster.gpfs34.dmapi.doc/bl1dmp_BookMap_xtoc.html GPFS V3.4 Data Management API Guide]
* [http://oss.sgi.com/projects/xfs/ Open Source XFS Source code with DMAPI Implementation and Test Suite ]

[[Category:Data management]]

{{compu-storage-stub}}</text>
      <sha1>61pohqdz3ytzm84stx64klc54r46rma</sha1>
    </revision>
  </page>
  <page>
    <title>Grid-oriented storage</title>
    <ns>0</ns>
    <id>11962687</id>
    <revision>
      <id>749335160</id>
      <parentid>742871218</parentid>
      <timestamp>2016-11-13T20:34:42Z</timestamp>
      <contributor>
        <username>TGCP</username>
        <id>11261013</id>
      </contributor>
      <comment>/* top */ [[grid storage]] more used for energy</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6044" xml:space="preserve">{{primary sources|article|date=March 2009}}

'''Grid-oriented Storage''' ('''GOS''') was a term used for data storage by a university project during the era when the term [[grid computing]] was popular.

== Description ==
GOS was a successor of the term [[network-attached storage]] (NAS). GOS systems contained hard disks, often [[RAID]]s (redundant arrays of independent disks), like traditional file servers. 
[[Image:gosongrid.jpg |thumb |upright=1.4]]

GOS was designed to deal with long-distance, cross-domain and single-image file operations, which is typical in Grid environments. GOS behaves like a file server via the file-based GOS-FS protocol to any entity on the grid. Similar to [[Advanced Resource Connector|GridFTP]], GOS-FS integrates a parallel stream engine and [[Grid Security Infrastructure]] (GSI). 

Conforming to the universal VFS (Virtual Filesystem Switch), GOS-FS can be pervasively used as an underlying platform to best utilize the increased transfer bandwidth and accelerate the [[Network File System (protocol)|NFS]]/[[CIFS]]-based applications. GOS can also run over [[SCSI]], [[Fibre Channel]] or [[iSCSI]], which does not affect the acceleration performance, offering both file level protocols and block level protocols for [[storage area network]] (SAN) from the same system.

In a grid infrastructure, resources may be geographically distant from each other, produced by differing manufacturers, and have differing access control policies. This makes access to grid resources dynamic and conditional upon local constraints. Centralized management techniques for these resources are limited in their scalability both in terms of execution efficiency and fault tolerance. Provision of services across such platforms requires a distributed resource management mechanism and the peer-to-peer clustered GOS appliances allow a single storage image to continue to expand, even if a single GOS appliance reaches its capacity limitations. The cluster shares a common, aggregate presentation of the data stored on all participating GOS appliances. Each GOS appliance manages its own internal storage space. The major benefit of this aggregation is that clustered GOS storage can be accessed by users as a single mount point. 

GOS products fit the thin-server categorization. Compared with traditional &#8220;fat server&#8221;-based storage architectures, thin-server GOS appliances deliver numerous advantages, such as the alleviation of potential network/grid bottle-necks, CPU and OS optimized for I/O only, ease of installation, remote management and minimal maintenance, low cost and Plug and Play, etc. Examples of similar innovations include NAS, printers, fax machines, routers and switches.

An [[Apache server]] has been installed in the GOS operating system, ensuring an HTTPS-based communication between the GOS server and an administrator via a Web browser. Remote management and monitoring makes it easy to set up, manage, and monitor GOS systems.

== History ==
[[Frank Zhigang Wang]] and Na Helian proposed a funding proposal to the UK government titled &#8220;Grid-Oriented Storage (GOS): Next Generation Data Storage System Architecture for the Grid Computing Era&#8221; in 2003. The proposal was approved and granted one million pounds{{citation needed|date=March 2009}} in 2004. The first prototype was constructed in 2005 at Centre for Grid Computing, Cambridge-Cranfield High Performance Computing Facility. The first conference presentation was at IEEE Symposium on Cluster Computing and Grid (CCGrid), 9&#8211;12 May 2005, Cardiff, UK. As one of the five best work-in-progress, it was included in the IEEE Distributed Systems Online. In 2006, the GOS architecture and its implementations was published in IEEE Transactions on Computers, titled &#8220;Grid-oriented Storage: A Single-Image, Cross-Domain, High-Bandwidth Architecture&#8221;.  
Starting in January 2007, demonstrations were presented at [[Princeton University]], Cambridge University Computer Lab and others.
By 2013, the Cranfield Centre still used future tense for the project.&lt;ref name="Cranfield CGC"&gt;{{cite web |url= http://www.cranfield.ac.uk/soe/departments/appliedmaths/gridcomputing/index.html |title= Centre for Grid Computing |accessdate= June 14, 2013 |publisher=Cranfield University}} &lt;!--  --&gt;&lt;/ref&gt;

[[Peer-to-peer file sharing]]s use similar techniques.

==Notes==
{{reflist}}

==Further reading==
* Frank Wang, Na Helian, Sining Wu, Yuhui Deng, Yike Guo, Steve Thompson, Ian Johnson, Dave Milward &amp; Robert Maddock, Grid-Oriented Storage, IEEE Distributed Systems Online,  Volume 6,  Issue 9, Sept. 2005.
* Frank Wang, Sining Wu, Na Helian, Andy Parker, Yike Guo, Yuhui Deng, Vineet Khare, Grid-oriented Storage: A Single-Image, Cross-Domain, High-Bandwidth Architecture, IEEE Transaction on Computers, Vol.56, No.4, pp.&amp;nbsp;474&#8211;487, 2007.
* Frank Zhigang Wang, Sining Wu, Na Helian, An Underlying Data-Transporting Protocol for Accelerating Web Communications, International Journal of Computer Networks, Elsevier, 2007.
* Frank Zhigang Wang, Sining Wu, Na Helian, Yuhui Deng, Vineet Khare, Chris Thompson and Michael Parker, Grid-based Data Access to Nucleotide Sequence Database with 6x Improvement in Response Times, New Generation Computing, No.2, Vol.25, 2007.
* Frank Wang, Yuhui Deng, Na Helian, Evolutionary Storage: Speeding up a Magnetic Disk by Clustering Frequent Data, IEEE Transactions on Magnetics, Issue.6, Vol.43, 2007.
* Frank Zhigang Wang, Na Helian, Sining Wu, Yuhui Deng, Vineet Khare, Chris Thompson and Michael Parker, Grid-based Storage Architecture for Accelerating Bioinformatics Computing, Journal of VLSI Signal Processing Systems, No.1, Vol.48, 2007.
* Yuhui Deng and   Frank Wang, A Heterogeneous Storage Grid Enabled by Grid Service, ACM Operating System Review, No.1, Vol.41, 2007.
* Yuhui Deng &amp; Frank Wang, Optimal Clustering Size of Small File Access in Network Attached Storage Device, Parallel Processing Letters, No.1, Vol.17, 2007.

{{DEFAULTSORT:Grid-Oriented Storage}}
[[Category:Data management]]</text>
      <sha1>76wfrkipp9t7s77qjknrxusi2eg9f9f</sha1>
    </revision>
  </page>
  <page>
    <title>Change data capture</title>
    <ns>0</ns>
    <id>3557729</id>
    <revision>
      <id>760392514</id>
      <parentid>760392298</parentid>
      <timestamp>2017-01-16T18:34:53Z</timestamp>
      <contributor>
        <ip>187.39.108.160</ip>
      </contributor>
      <comment>/* Push versus pull */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="11133" xml:space="preserve">{{No footnotes|date=March 2016}}
In [[database]]s, '''change data capture''' (CDC) is a set of software [[Design pattern (computer science)|design patterns]] used to determine (and track) the data that has changed so that action can be taken using the changed data. Also, Change data capture (CDC) is an approach to data integration that is based on the identification, capture and delivery of the changes made to enterprise data sources.

CDC solutions occur most often in [[data warehouse|data-warehouse]] environments since capturing and preserving the state of data across time is one of the core functions of a data warehouse, but CDC can be utilized in any database or data repository system.

== Methodology ==

System developers can set up CDC mechanisms in a number of ways and in any one or a combination of system layers from application logic down to physical storage.

In a simplified CDC context, one computer system has data believed to have changed from a previous point in time, and a second computer system needs to take action based on that changed data.  The former is the source, the latter is the target.  It is possible that the source and target are the same system physically, but that would not change the design pattern logically.

Not uncommonly, multiple CDC solutions can exist in a single system.

=== Timestamps on rows ===
Tables whose changes must be captured may have a column that represents the time of '''last''' change.  Names such as LAST_UPDATE, etc. are common.  Any row in any table that has a timestamp in that column that is more recent than the last time data was captured is considered to have changed.

=== Version Numbers on rows ===
Database designers give tables whose changes must be captured a column that contains a version number.  Names such as VERSION_NUMBER, etc. are common.  When data in a row changes, its version number is updated to the current version.  A supporting construct such as a reference table with the current version in it is needed.  When a change capture occurs, all data with the latest version number is considered to have changed.  When the change capture is complete, the reference table is updated with a new version number.

Three or four major techniques exist for doing CDC with version numbers, the above paragraph is just one.

==== Use in Optimistic Locking ====
Version numbers can be useful with [[optimistic locking]] in ACID transactional or [[Relational database management system|relational database management systems (RDMBS)]]. For an example in read-then-update scenarios for [[Create, read, update and delete|CRUD]] applications in [[relational database management system]]s, a row is first read along with the state of its version number; in a separate transaction, a [[Update (SQL)|SQL UPDATE]] statement is executed along with an additional [[Where (SQL)|WHERE clause]] that includes the version number found from the initial read. If no record was updated, it usually means that the version numbers didn't match because some other action/transaction had already updated the row and consequently its version number. Several [[Object relational mapper|object relational mapping]] tools use this method to detect for optimistic locking scenarios (including [[Hibernate (Java)|Hibernate]]).

=== Status indicators on rows ===
This technique can either supplement or complement timestamps and versioning. It can configure an alternative if, for example, a status column is set up on a table row indicating that the row has changed (e.g. a boolean column that, when set to true, indicates that the row has changed). Otherwise, it can act as a complement to the previous methods, indicating that a row, despite having a new version number or an earlier date, still shouldn't be updated on the target (for example, the data may require human validation).

=== Time/Version/Status on rows ===
This approach combines the three previously discussed methods.  As noted, it is not uncommon to see multiple CDC solutions at work in a single system, however, the combination of time, version, and status provides a particularly powerful mechanism and programmers should utilize them as a trio where possible.  The three elements are not redundant or superfluous.  Using them together allows for such logic as, "Capture all data for version 2.1 that changed between 6/1/2005 12:00 a.m. and 7/1/2005 12:00 a.m. where the status code indicates it is ready for production."

=== Triggers on tables ===
May include a [[observer pattern|publish/subscribe]] pattern to communicate the changed data to multiple targets.  In this approach, [[Database trigger|triggers]] log events that happen to the transactional table into another queue table that can later be "played back".  For example, imagine an Accounts table, when transactions are taken against this table, triggers would fire that would then store a history of the event or even the deltas into a separate queue table.  The queue table might have schema with the following fields: Id, TableName, RowId, TimeStamp, Operation.  The data inserted for our Account sample might be:  1, Accounts, 76, 11/02/2008 12:15am, Update.
More complicated designs might log the actual data that changed.  This queue table could then be "played back" to replicate the data from the source system to a target.

[More discussion needed]

An example of this technique is the pattern known as the [[log trigger]].

=== Event Programming ===
Coding a change into an application at appropriate points is another method that can give  intelligent discernment that data changed.  Although this method involves programming vs. more easily implemented "dumb" triggers, it may provide more accurate and desirable CDC, such as only after a COMMIT, or only after certain columns changed to certain values - just what the target system is looking for.

=== Log scanners on databases ===
Most database management systems manage a [[transaction log]] that records changes made to the database contents and to [[metadata]]. By scanning and interpreting the contents of the database transaction log one can capture the changes made to the database in a non-intrusive manner.

Using transaction logs for change data capture offers a challenge in that the structure, contents and use of a transaction log is specific to a database management system. Unlike data access, no standard exists for transaction logs. Most database management systems do not document the internal format of their transaction logs, although some provide programmatic interfaces to their transaction logs (for example: Oracle, DB2, SQL/MP, SQL/MX and SQL Server 2008).

Other challenges in using transaction logs for change data capture include:

* Coordinating the reading of the transaction logs and the archiving of log files (database management software typically archives log files off-line on a regular basis).
* Translation between physical storage formats that are recorded in the transaction logs and the logical formats typically expected by database users (e.g., some transaction logs save only minimal buffer differences that are not directly useful for change consumers).
* Dealing with changes to the format of the transaction logs between versions of the database management system.
* Eliminating uncommitted changes that the database wrote to the transaction log and later [[Rollback (data management)|rolled back]].
* Dealing with changes to the metadata of tables in the database.

CDC solutions based on transaction log files have distinct advantages that include:

* minimal impact on the database (even more so if one uses [[log shipping]] to process the logs on a dedicated host).
* no need for programmatic changes to the applications that use the database.
* low [[Latency (engineering)|latency]] in acquiring changes.
* [[data integrity|transactional integrity]]: log scanning can produce a change stream that replays the original transactions in the order they were committed. Such a change stream include changes made to all tables participating in the captured transaction.
* no need to change the database schema

== Confounding factors ==
As often occurs in complex domains, the final solution to a CDC problem  may have to balance many competing concerns.

=== Unsuitable source systems ===

Change data capture both increases in complexity and reduces in value if the source system saves [[metadata]] changes when the data itself is not modified.  For example, some [[Data model]]s track the user who last looked at but did not change the data in the same structure as the data.  This results in [[Noise (signal processing)|noise]] in the Change Data Capture.

=== Tracking the capture ===

Actually tracking the changes depends on the data source.  If the data is being persisted in a modern [[database]] then Change Data Capture is a simple matter of permissions.  Two techniques are in common use:
# Tracking changes using [[Database Trigger]]s
# Reading the [[transaction log]] as, or shortly after, it is written.

If the data is not in a modern database, Change Data Capture becomes a programming challenge.

=== Push versus pull ===
* '''Push''': the source process creates a snapshot of changes within its own process and delivers rows downstream. The downstream process uses the snapshot, creates its own subset and delivers them to the next process.
* '''Pull''': the target that is immediately downstream from the source, prepares a request for data from the source. The downstream target delivers the snapshot to the next target, as in the push model.

=== Alternatives ===

Sometimes the [[Slowly changing dimension]] is used as a method, this is an example:
[[File:Scd model.png|frame|center|Scd model]]

==See also==

* [[Slowly Changing Dimension]]
* [[Referential integrity]]

== References ==
{{reflist}}

==External links==
* [https://github.com/linkedin/databus/wiki LinkedIn Databus]
* [http://www.informaticacloud.com/images/whitepapers/data%20replication%20best%20practices.pdf Data Replication as a Service Best Practices]
* [https://web.archive.org/web/20110902084451/http://www.attunity.com:80/attunity_stream Attunity Change Data Capture (CDC)]
* [http://www-01.ibm.com/software/data/infosphere/change-data-capture/  IBM Infosphere CDC]
* [https://web.archive.org/web/20060523023144/http://www.oracle.com:80/technology/oramag/oracle/03-nov/o63tech_bi.html Tutorial on setting up CDC in Oracle 9i]
* [http://social.technet.microsoft.com/wiki/contents/articles/how-to-enable-sql-azure-change-data-capture.aspx Tutorial on setting up SQL Azure Change Data Capture]
* [http://msdn2.microsoft.com/en-us/library/bb522489(SQL.100).aspx Details of the CDC facility included in Microsoft Sql Server 2008 Feb '08 CTP]
* [http://www.jumpmind.com/products/symmetricds/features SymmetricDS - Heterogeneous, Cross Platform CDC]
* [http://www.gamma-soft.com/wp/index.php?page_id=30 Gamma-Soft CDC Technology]
* [http://www.talend.com/download/talend-open-studio?qt-product_tos_download=3#qt-product_tos_download Talend]

{{DEFAULTSORT:Change Data Capture}}
[[Category:Computer data]]
[[Category:Data management]]</text>
      <sha1>lr4yj0hsc0qbibwkv666wl89h700hkg</sha1>
    </revision>
  </page>
  <page>
    <title>Technical data management system</title>
    <ns>0</ns>
    <id>20092666</id>
    <revision>
      <id>754896932</id>
      <parentid>753285847</parentid>
      <timestamp>2016-12-15T02:58:47Z</timestamp>
      <contributor>
        <username>Just a guy from the KP</username>
        <id>9994896</id>
      </contributor>
      <minor />
      <comment>softwares &#8594; software</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="22913" xml:space="preserve">{{Orphan|date=February 2009}}
A '''Technical Data Management System''' (TDMS) is essentially a [[Document management system]] (DMS) pertaining to the management of technical and [[engineering drawing]]s and documents. Often the data are contained in 'records' of various forms, such as on paper, microfilms or digital media. Hence technical [[data management]] is also concerned with record management involving technical data. Proper Technical Document [[Management system|Management Systems]] are essential for executions within large organisations with large scale projects involving engineering. For example, TDMS is a vital function for the successful management of Integrated Steel Plants (ISP), Automobile factories, Aero-space facilities, Infrastructure companies, City Corporations, Research Organisations, etc. In such organisations, Technical Archives or Technical Documentation Centres are created as central facilities for effective management of technical data and records.
[[File:Information processing system (english).svg|alt= A simplified example of information flow within a Technical Data Management System|thumb|A simplified example of information flow within a Technical Data Management System]]
TDMS functions are similar to that of conventional archive functions in concepts, except that the archived materials in this case are essentially engineering drawings, survey maps, [[Specification|technical specifications]], plant and equipment data sheets, feasibility reports, project reports, operation and maintenance manuals, standards, etc.

Document registration, indexing, repository management, reprography, etc. are parts of TDMS.  Various kinds of sophisticated technologies such as document scanners, microfilming and digitization camera units, wide format printers, digital plotters, software, etc. are available now, making TDMS functions an easier process than previous times.

== Crucial Constituents of a Technical Data Management System ==
Technical data refers to both scientific and technical information recorded and presented in any form or manner (excluding financial and management information).&lt;ref&gt;{{Cite web|url = http://www.businessdictionary.com/definition/technical-data.html|title = What is technical data? Definition and meaning|date = 2015-11-03|accessdate = 2015-11-03|website = BusinessDictionary.com|publisher = WebFinance, Inc|last = |first = }}&lt;/ref&gt; A Technical Data Management System is created within an organisation for archiving and sharing information such as [[technical specifications]], datasheets and drawings. Similar to other types of data management system, a Technical Data Management System consists of the 4 crucial constituents mentioned below.

=== Data planning ===
Data plans (long-term or short-term) are constructed as the first essential step of a proper and complete TDMS. It is created to ultimately help with the 3 other constituents, Data Acquisition, Data Management and Data sharing. A proper data plan should not exceed 2 pages and should address the following basics:&lt;ref&gt;{{Cite web|url = https://www.libraries.psu.edu/psul/pubcur/what_is_dm.html#data-planning|title = Data planning|date = 2015-11-03|accessdate = 2015-11-03|website = Data Curation|publisher = Penn State University Libraries|last = |first = }}&lt;/ref&gt;
* Types of data (samples, experiment results, reports, drawings, etc.) and [[Metadata]] (Data that summarizes and describes other data. In this case, it refers to details such as sample sizes, experiment conditions and procedures, dates of reports, explanations of drawings, etc.)&lt;ref&gt;{{Cite web|url = http://whatis.techtarget.com/definition/metadata|title = metadata|date = July 2014|accessdate = 2015-11-03|website = WhatIs.com|publisher = Search engine optimization (SEO)|last = Rouse|first = Margaret}}&lt;/ref&gt; 
* Means of researches and collections of data (field works, experiments in production lines, etc.)
* Costs of researches
* Policies for access, sharing (re-use within the organisation and re-distribution to the public)
* Proposals for archiving data and maintaining access to it

=== Data Acquisition ===
Raw Data is collected from Primary Sites of the organisations through the use of modern Technologies.&lt;ref name=":0"&gt;{{Cite web|url = http://sine.ni.com/cs/app/doc/p/id/cs-13019#|title = By using powerful default components, TDM, NI DataFinder, and DIAdem, and without using a database, we considerably reduced our creation and maintenance costs.|date = 2015-11-03|accessdate = 2015-11-03|website = National Instruments|publisher = a-solution GmbH|last = Finkl|first = Karl}}&lt;/ref&gt; Please reference the table below for examples.&lt;ref name=":0" /&gt;
{| class="wikitable"
!Organisations
!Raw Data
!Primary Sites
!Technologies
|-
|Integrated steel plants, Automobile factories
|Feasibility reports, Equipment datasheets, etc.
|Test rigs and Controls
|Transiting software to digitize data and Input software for recording report results and details on datasheets
|-
|Aero-space facilities
|Engineering drawings, Operation manuals, maintenance logs, etc.
|Engineering labs
|Scanners for engineering drawings, Input software for maintenance logs
|-
|City corporations
|Survey maps, Population reports, etc.
|City to be mapped and City that involves the research
|Digital cameras for survey maps, Input software for statistics of population
|}
The data collected is then transferred to Technical Data Centres for Data Management.

=== Data Management ===
After Data Acquisition, data is sorted out, whilst useful data is archived, unwanted data is disposed. When managing and archiving data, the features below of the data are considered.&lt;ref&gt;{{Cite web|url = https://www.libraries.psu.edu/psul/pubcur/what_is_dm.html#data-management|title = Data Management|date = 2015-11-03|accessdate = 2015-11-03|website = Data Curation|publisher = Penn State University Libraries|last = |first = }}&lt;/ref&gt;
* Names, labels, values and descriptions for variables and records. (In the case of TDMS, one example is names of equipments on an equipment datasheet)
* Derived data from the original data, with code, algorithm or command file used to create them. (In the case of TDMS, one example is an expectation report derived from the analysis of an equipment datasheet)
* [[Metadata]] associates with the data being archived

=== Data Sharing ===
Archived and managed data are accessible to rightful entities. A proper and complete TDMS should share data to a suitable extent, under suitable security, in order to achieve optimal usage of data within the organisation. It aims for easy access when reused by other researchers and hence it enhances other research processes. Data is often referred in other tests and [[Specification (technical standard)|technical specifications]], where new analysis is generated, managed and archived again. As a result, data is flowing within the organisation under effective management through the use of TDMS.&lt;ref&gt;{{Cite web|url = https://www.libraries.psu.edu/psul/pubcur/what_is_dm.html#data-sharing|title = Data Sharing|date = 2015-11-03|accessdate = 2015-11-03|website = Data Curation|publisher = Penn State University Libraries|last = |first = }}&lt;/ref&gt;

== Advantages and disadvantages of usage of Technical Data Management Systems ==
There are strengths and weakness when using Technical Data Management Systems (TDMS) to archive data. Some of the advantages and disadvantages are listed below.&lt;ref&gt;{{Cite web|url = https://razorleaf.com/solutions/technologies/product-data-management/|title = Product Data Management / Technical Data Management (PDM/TDM)|date = 2015-11-03|accessdate = 2015-11-03|website = Razorleaf Solutions|publisher = Razorleaf Corporation|last = |first = }}&lt;/ref&gt;&lt;ref&gt;{{Cite web|url = http://arxiv.org/ftp/arxiv/papers/1008/1008.1321.pdf|title = Contributions of PDM Systems in Organiza- tional Technical Data Management|date = 2015-11-03|accessdate = 2015-11-03|website = |publisher = Mechanical Engineering Informatics and Virtual Product Development Division (MIVP),  Vienna University of Technology|last1 = Ahmed|first1 = Zeeshan|last2 = Gerhard|first2 = Detlef}}&lt;/ref&gt;&lt;ref&gt;{{Cite web|url = http://www.flosim.com/calcium.aspx|title = Calcium - technical data management|date = 2015-11-03|accessdate = 2015-11-03|website = Flow Simulation|publisher = Flow Simulation Ltd.|last = |first = }}&lt;/ref&gt;

=== Advantages ===

==== 1. Faster and easier data management ====

Since TDMS is integrated into the organisation's systems, whenever workers develop data files (SolidWorks, AutoCAD, Microsoft Word, etc.), they can also archive and manage data, linking what they need to their current work, at the same time they can also update the archives with useful data. This speeds up working processes and makes them more efficient.

==== 2. Increased security ====

All data files are centralized, hence internal and external data leakages are less likely to happen, and the data flow is more closely monitored. As a result, data in the organisation is more secured.

==== 3. Increased collaboration within the organisation ====

Since the data files are centralized and the data flow within the organisation increases, researchers and workers within the organisation are able to work on joint projects. More complex tasks can be performed for higher yields.

==== 4. Compatible to various formats of data ====

TDMS is compatible to many formats of data, from basic data like Microsoft Words to complex data like voice data. This enhances the quality of the management of data archived.

=== Disadvantages ===

==== 1. Higher financial costs ====

Implementing TDMS into the organisation's systems involves monetary costs. Maintenance costs certain amount of human resources and money as well. These resources involve opportunity costs as they can be utilized in other aspects.

==== 2. Lower stability ====

Since TDMS manages and centralizes all the data the organisation processes, it links the working processes within the whole organisation together. It also increases the vulnerability of the organisation data network. If TDMS is not stable enough or when it is exposed to hacker and virus attacks, the organisation's data flow might shut down completely, affecting the work in an organisation-wide scale and leading to a lower stability as results.

== Comparison between Traditional Data Management Approaches and Technical Data Management Systems ==
Test engineers and researchers are facing great challenges in turning complex test results and simulation data into usable information for higher yields of firms. These challenges are listed below.&lt;ref&gt;{{Cite web|url = http://www.ni.com/white-paper/7389/en/|title = From Raw Data to Engineering Results: The NI Technical Data Management Solution|date = 2015-10-13|accessdate = 2015-11-03|website = |publisher = National Instruments|last = |first = }}&lt;/ref&gt;
* Increase in complication of designs
* Reduced in time and budgets available
* Higher quality is demanded
[[File:Logo oracle.jpg|alt= A company logo for Oracle|thumb|A company logo for Oracle]]

=== Traditional Data Management Approaches ===
Many organisations are still applying the conventional file management systems, due to the difficulty in building a proper and complete archives for data management.

The first approach is the simple file-folder system. This costs the problem of ineffectiveness as workers and researchers have to manually go through numerous layers of systems and files for the target data. Moreover, the target data may contain files with different formats and these files may not be stored in the same machine. These files are also easily lost if renamed or moved to another location.

The second approach is conventional databases such as Oracle. These databases are capable of enabling easy search and access of data. However, a great drawback is that huge effort for preparing and modeling the data is required. For large-scale projects, huge monetary costs are induced, and extra IT human resources must be employed for constant handling, expanding and maintaining the inflexible system, which is custom for specific tasks, instead of all tasks. In the long-term, it is not cost-effective.

=== Technical Data Management Systems(TDMS) ===
TDMS is developed based on 3 principles, flexible and organized file storage, self-scaling hybrid data index, and an interactive post-processing environment. The system in practical, mainly consists of 3 components, data files with essential and relevant [[Metadata]], data finders for organizing and managing data regardless of files formats, and, a software of searching, analyzing and reporting. With [[Metadata]] attached to original data files, the data finder can identify different related data files during searches, even if they are in different file formats. TDMS hence allows researchers to search for data like browsing the Internet. Last but not least, it can adapt to changes and update itself according to the changes, unlike databases.

== Comparison between Strong Information Systems and Weak Information Systems ==
Complex organizations may need large amounts of technical information, which can be distributed among several independent archives. Existing approaches span from &#8220;no integration&#8221; to &#8220;strong integration&#8221;, that is based on a common database or product model. The so-called &#8220;Weak Information Systems&#8221; (WIS)&lt;ref&gt;{{cite conference |url=http://www.marcolazzari.it/publications/weak-information-systems-for-technical-data-management-preprint.pdf |title=Weak information systems for technical data management |first= |last1=Salvaneschi |first1=Paolo |last2=Lazzari |first2=Marco |year=1997 |conference=Worldwide ECCE Symposium on computers in the practice of building and civil engineering |location=Lahti, Finland |pages=310&#8211;314 |access-date=2015-11-29 }}&lt;/ref&gt; lie somewhere in the middle. Their basic concept is to add to the pre-existing information a new layer of multiple partial models of products and processes, so that it is possible to reuse existing databases, to reduce the development from scratch, and to provide evolutionary paths relevant for the development of the WIS. Each partial model may include specific knowledge and it acts as a way to structure and access the information according to a specific user view.
The comparison between strong and weak information systems may be summarized as follows:
{| class="wikitable"
!Strong information systems
!Weak information systems
|-
|Common data model
|Multiple specific integration models
|-
|Database oriented architecture
|Integration of multiple data sources by adding integration layers
|-
|One shot design
|Growing process
|-
|Redesign of legacy systems
|Integration of legacy systems
|}
The architecture of a weak information  system is composed of:
* information sources (databases, computational programs, ...);
* the integration layer.
The integration layer comprises the following sub-layers:
* abstraction layer (information models);
* communication layer between models and information sources;
* communication layer between models and humans (human-computer interface).

== Technical Data Management Systems in terms of regulations in different countries ==
In some countries, such as in the US, record and document management are considered very vital functions, and much stress is given in the management of Technical Archives. Records and documents coming under the public domain are governed by appropriate laws.&lt;ref&gt;{{Cite web|url = http://apps.americanbar.org/lpm/lpt/articles/tch01093.shtml|title = Document Management in the Digital Law Office|date = January 2009|accessdate = 2015-11-03|website = Law Practice Today|publisher = American Bar Association|last1 = Best|last2 = Foster|first1 = Steven J.|first2 = Debbie}}&lt;/ref&gt; However, this has not been so in many underdeveloped and [[Developing country|developing nations]]. For example, India enacted the ' Public Records Act'&lt;ref&gt;{{Cite web|url = http://nationalarchives.nic.in/writereaddata/html_en_files/html/public_records93.html|title = THE PUBLIC RECORDS ACT, 1993 (India)|date = 1993-12-22|accessdate = 2015-11-03|website = |publisher = Government of India|last = MOHANPURIA|first = K.L.}}&lt;/ref&gt; in 1993. However, many in the country are not aware of the existence of such a law or its importance.

== Applications and Examples of Technical Data Management Systems ==
Technical Data Management Systems (TDMS) are widely applied across the globe, in different sectors. Some of the examples are listed below.
* Voith Hydro tests models of the power plant turbines, including 4 main program parts, engine characteristics values, oscillation and cavitation, and transfer data from 1 program part to the next one using TDMS.&lt;ref name=":0" /&gt;
* Danburykline created a knowledge and data platform, SOROS, which is following the wiki based approach. It aims to represent data in accessible and simple forms.&lt;ref&gt;{{Cite web|url = http://danburykline.co.uk/DKWP/?page_id=967|title = Knowledge &amp; Technical Data Management|date = 2015-11-03|accessdate = 2015-11-03|website = |publisher = Danburykline|last = |first = }}&lt;/ref&gt;
* Berghof develops and provides a TDMS to simplify and manage data for development of firms including automobile firms. This TDMS enables reserve of data, centralization of data volumes on an online server. It is also compatible to Windows PC and many other systems.&lt;ref&gt;{{Cite web|url = http://www.berghof.com/en/products/test-engineering/technical-data-management/|title = Data availability|date = 2015-11-03|accessdate = 2015-11-03|website = Test engineering Technical data management|publisher = Berghof|last = |first = }}&lt;/ref&gt;
* This journal proposes the use of [[Cloud database|Cloud]] TDMS in third world countries for higher education purposes. Republic of Sudan is the model in this journal. Some of the solutions mentioned include online course delivery and online assignments and tests for greater class participation. Weaknesses mentioned include high financial costs and the fact that underdeveloped countries have not enough infrastructure to support such proposal.&lt;ref&gt;{{Cite journal|url = http://airccse.org/journal/ijdms/papers/7315ijdms02.pdf|title = Cloud Computing Architecture for higher education in the third world countries (Republic of the Sudan as model)|last = Adrees|first = Mohmed Sirelkhtem|date = June 2015|journal = International Journal of Database Management Systems ( IJDMS )|doi = 10.5121/ijdms.2015.7302|pmid = |access-date = 2015-11-03|volume = 7|last3 = Sheta|last2 = Omer|first2 = Majzoob Kamal Aldein|first3 = Osama E.|issue = 3}}&lt;/ref&gt;
* This journal is about [[text simplification]]. The purpose of this text simplification project in the journal is to simplify high level knowledge in English, so that students in high level studies who do not have sufficient English foundations can learn about these knowledge and data more easily. The method to do so suggested by the journal is to introduce a TDMS that can transform complicated English words into easier words. A problem with this project is that the Internet is flooded with useless information and it is very difficult to sort out useful information for simplification.&lt;ref&gt;{{Cite journal|url = http://airccse.org/journal/ijdms/papers/7415ijdms01.pdf|title = Software feasibility study to transform complex scientific written knowledge to a clear, rationale and simple language|last = Khandelwal|first = Manoj|date = August 2015|journal = International Journal of Database Management System (IJDMS)|doi = 10.5121/ijdms.2015.7401|pmid = |access-date = 2015-11-03|last2 = Jafarabad|first2 = Mohammad|issue = 4|volume = 7}}&lt;/ref&gt;
* This journal mentions about [[River basin|River Basin]] Information System (RBIS), which monitors data of different parts of a river basin, in order to identify which parts of the basin are gauging. Data is dynamic and lots of information has to be taken, which is impossible to do it manually. RBIS can help with this but one current weakness is that there are only 2 synoptic stations working (Kara and Niamtougou), whilst the rest are out of order.&lt;ref&gt;{{Cite journal|url = http://airccse.org/journal/ijdms/papers/7115ijdms02.pdf|title = An information for integrated land and water resources management in the Kara River Basin (Togo and Benin)|last = BADJANA|first = H&#232;ou Mal&#233;ki|date = February 2015|journal = International Journal of Database Management System (IJDMS)|doi = 10.5121/ijdms.2015.7102|pmid = |access-date = 2015-11-03|last2 = ZANDER|first2 = Franziska|issue = 1|volume = 7|last3 = KRALISCH|first3 = Sven|last4 = HELMSCHROT|first4 = J&#246;rg|last5 = FL&#220;GEL|first5 = Wolfgang-Albert}}&lt;/ref&gt;

== Data mining ==
Data mining is an important criteria in constructing a technical Data Management System. For example, in building a E-commence platform, TDMS is needed to search and display information about the products.&lt;ref&gt;{{Cite journal|url = http://airccse.org/journal/ijdms/papers/7115ijdms01.pdf|title = Web-mining on Indonesia E-commerce site: Lazada and Rakuten|last = Simanjuntak|first = Humasak|date = February 2015|journal = International Journal of Database Management System (IJDMS)|doi = 10.5121/ijdms.2015.7101|pmid = |access-date = 2015-11-03|last2 = Sibarani|first2 = Novitasari|issue = 1|volume = 7|last3 = inaga|first3 = Bambang|last4 = Hutabarat|first4 = Novalina}}&lt;/ref&gt; It indicates that it is essential to gather information from other systems, to archive and manage it properly, and finally, to share it to users.

== See also ==
[[Data management system|Data Management System]]

[[Data mining]]

[[Database]]

[[Information Systems Research]]

== Further reading ==
http://airccse.org/journal/ijdms/papers/7115ijdms03.pdf&lt;ref&gt;{{Cite journal|url = http://airccse.org/journal/ijdms/papers/7115ijdms03.pdf|title = Mining closed sequential patterns in large sequence databases|last = Raju|first = V. Purushothama|date = February 2015|journal = International Journal of Database Management System (IJDMS)|doi = 10.5121/ijdms.2015.7103|pmid = |access-date = 2015-11-03|last2 = Varma|first2 = G.P. Saradhi|issue = 1|volume = 7}}&lt;/ref&gt;

https://seer.lcc.ufmg.br/index.php/jidm&lt;ref&gt;{{Cite journal|url = https://seer.lcc.ufmg.br/index.php/jidm|title = JOURNAL OF INFORMATION AND DATA MANAGEMENT|date = February 2015|journal = JOURNAL OF INFORMATION AND DATA MANAGEMENT|doi = |pmid = |access-date = 2015-11-03|volume = 6|issue = 1|editor-last = Traina Junior|editor-first = Caetano|editor2-last = Cordeiro|editor2-first = Robson L. F.|editor3-last = Amo|editor3-first = Sandra de|display-editors = 3 |editor4-last = Davis|editor4-first = Clodoveu|issn = 2178-7107}}&lt;/ref&gt;

== External links ==
* http://airccse.org/journal/ijdms/Editorialboard.html

==References==

&lt;references&gt;
&lt;/references&gt;

&lt;nowiki/&gt;

[[Category:Data management]]
[[Category:Document management systems]]
[[Category:Systems engineering]]</text>
      <sha1>p0ares3jmtb5a40qkkuuq9ygkcmoivf</sha1>
    </revision>
  </page>
  <page>
    <title>Global serializability</title>
    <ns>0</ns>
    <id>11861063</id>
    <revision>
      <id>762614467</id>
      <parentid>760531997</parentid>
      <timestamp>2017-01-29T20:58:54Z</timestamp>
      <contributor>
        <username>GreenC bot</username>
        <id>27823944</id>
      </contributor>
      <minor />
      <comment>Reformat 1 archive link. [[User:Green Cardamom/WaybackMedic_2.1|Wayback Medic 2.1]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="35296" xml:space="preserve">{{Technical|date=January 2017}}
In [[concurrency control]] of ''[[database]]s'', ''[[transaction processing]]'' (''transaction management''), and other transactional [[Distributed computing|distributed applications]], '''Global serializability''' (or '''Modular serializability''') is a property of a ''global schedule'' of [[Database transaction|transactions]]. A global schedule is the unified [[schedule (computer science)|schedule]] of all the individual database (and other [[transactional object]]) schedules in a multidatabase environment (e.g., [[federated database]]). Complying with global serializability means that the global schedule is ''[[serializable (databases)|serializable]]'', has the ''[[serializability]]'' property, while each component database (module) has a serializable schedule as well. In other words, a collection of serializable components provides overall system serializability, which is usually incorrect. A need in correctness across databases in multidatabase systems makes global serializability a major goal for ''[[global concurrency control]]'' (or ''modular concurrency control''). With the proliferation of the [[Internet]], [[Cloud computing]], [[Grid computing]], and small, portable, powerful computing devices (e.g., [[smartphone]]s), as well as increase in [[systems management]] sophistication, the need for atomic distributed transactions and thus effective global serializability techniques, to ensure correctness in and among distributed transactional applications, seems to increase.

In a [[federated database system]] or any other more loosely defined multidatabase system, which are typically distributed in a communication network, transactions span multiple (and possibly [[Distributed database|distributed]]) databases. Enforcing global serializability in such system, where different databases may use different types of [[concurrency control]], is problematic. Even if every local schedule of a single database is serializable, the global schedule of a whole system is not necessarily serializable. The massive communication exchanges of conflict information needed between databases to reach [[Serializability#View and conflict serializability|conflict serializability]] globally would lead to unacceptable performance, primarily due to computer and communication [[latency (engineering)|latency]]. Achieving global serializability effectively over different types of concurrency control has been [[Open problem|open]] for several years. ''[[Commitment ordering]]'' (or Commit ordering; CO), a serializability technique publicly introduced in 1991 by [[Yoav Raz]] from [[Digital Equipment Corporation]] (DEC), provides an effective general solution for global ([[Serializability#View and conflict serializability|conflict]]) serializability across any collection of database systems and other [[transactional object]]s, with possibly different concurrency control mechanisms. CO does not need the distribution of conflict information, but rather utilizes the already needed (unmodified) [[atomic commitment]] protocol messages without any further communication between databases. It also allows [[Optimistic concurrency control|optimistic]] (non-blocking) implementations. CO generalizes ''[[Two-phase locking|Strong strict two phase locking]]'' (SS2PL), which in conjunction with the ''[[Two-phase commit protocol|Two-phase commit]]'' (2PC) protocol is the [[de facto standard]] for achieving global serializability across (SS2PL based) database systems. As a result, CO compliant database systems (with any, different concurrency control types) can transparently join existing SS2PL based solutions for global serializability. The same applies also to all other multiple (transactional) object systems that use atomic transactions and need global serializability for correctness (see examples above; nowadays such need is not smaller than with database systems, the origin of atomic transactions).

The most significant aspects of CO that make it a uniquely effective general solution for global serializability are the following:
#Seamless, low overhead integration with any concurrency control mechanism, with neither changing any transaction's operation scheduling or blocking it, nor adding any new operation.
#[[Heterogeneity]]: Global serializability is achieved across multiple [[transactional objects]] (e.g., [[database management system]]s) with different (any) concurrency control mechanisms, without interfering with the mechanisms' operations.
#[[Modularity]]: Transactional objects can be added and removed transparently.
#[[Commitment ordering#CO is a necessary condition for global serializability across autonomous database systems|Autonomy]] of transactional objects: No need of conflict or equivalent information distribution (e.g., local precedence relations, locks, timestamps, or tickets; no object needs other object's information).
#[[Scalability]]: With "normal" global transactions, [[computer network]] size and number of transactional objects can increase unboundedly with no impact on performance, and
#Automatic global deadlock resolution.

All these aspects, except the first two, are also possessed by the popular [[Two-phase locking|SS2PL]], which is a (constrained, blocking) special case of CO and inherits many of CO's qualities.

==The global serializability problem==

===Problem statement===

The difficulties described above translate into the following problem:
:Find an efficient (high-performance and [[fault tolerant]]) method to enforce ''Global serializability'' (global conflict serializability) in a heterogeneous distributed environment of multiple autonomous database systems. The database systems may employ different [[concurrency control]] methods. No limitation should be imposed on the operations of either local transactions (confined to a single database system) or [[distributed transaction|global transactions]] (span two or more database systems).

===Quotations===
Lack of an appropriate solution for the global serializability problem has driven researchers to look for alternatives to [[serializability]] as a correctness criterion in a multidatabase environment (e.g., see ''[[Global serializability#Relaxing global serializability|Relaxing global serializability]]'' below), and the problem has been characterized as difficult and ''[[open problem|open]]''. The following two quotations demonstrate the mindset about it by the end of the year 1991, with similar quotations in numerous other articles:

*"Without knowledge about local as well as global transactions, it is highly unlikely that efficient global concurrency control can be provided... Additional complications occur when different component DBMSs [Database Management Systems] and the FDBMSs [Federated Database Management Systems] support different concurrency mechanisms... It is unlikely that a theoretically elegant solution that provides conflict serializability without sacrificing performance (i.e., concurrency and/or response time) and [[availability]] exists."&lt;ref&gt;Amit Sheth, James Larson (1990): [http://www.informatik.uni-trier.de/~ley/db/journals/csur/ShethL90.html  "Federated Database Systems for Managing Distributed, Heterogeneous, and Autonomous Databases"], ''ACM Computing Surveys'', Vol. 22, No 3, pp. 183-236, September 1990 (quotation from page 227)&lt;/ref&gt;

[[Commitment ordering]],&lt;ref name=Raz1992/&gt;&lt;ref name=Raz1994/&gt; publicly introduced in May 1991 (see below), provides an efficient [[Elegance|elegant]] general solution, from both practical&lt;ref name=Raz1990/&gt;&lt;ref name=Raz1991/&gt; and [[Theory|theoretical]]&lt;ref name=Raz2009/&gt; points of view, to the global serializability problem across database systems with possibly different concurrency control mechanisms. It provides conflict serializability with no negative effect on availability, and with no worse performance than the [[de facto standard]] for global serializability, CO's special case [[Two-phase locking#Strong strict two-phase locking|Strong strict two-phase locking]] (SS2PL). It requires knowledge about neither local nor global transactions.

*"Transaction management in a heterogeneous, distributed database system is a difficult  issue. The main problem is that each of the local database management systems may be using a different type of concurrency control scheme. Integrating this is a challenging problem, made worse if we wish to preserve the local autonomy of each of the local databases, and allow local and global transactions to execute in parallel. One simple solution is to restrict global transactions to retrieve-only access. However, the issue of reliable transaction management in the general case, where global and local transactions are allowed to both read and write data, is [[open problem|still open]]."&lt;ref&gt;[[Abraham Silberschatz]], [[Michael Stonebraker]], and [[Jeffrey Ullman]] (1991): [http://www.informatik.uni-trier.de/~ley/db/journals/cacm/SilberschatzSU91.html  "Database Systems: Achievements and Opportunities"], ''Communications of the ACM'', Vol. 34, No. 10, pp. 110-120, October 1991 (quotation from page 120)&lt;/ref&gt;

The commitment ordering solution comprises effective integration of autonomous database management systems with possibly different concurrency control mechanisms. This while local and global transactions execute in parallel without restricting any read or write operation in either local or global transactions, and without compromising the systems' autonomy.

Even in later years, after the public introduction of the Commitment ordering general solution in 1991, the problem still has been considered by many unsolvable:

*"We present a transaction model for multidatabase systems with autonomous component systems, coined heterogeneous 3-level transactions. It has become evident that in such a system the requirements of guaranteeing full [[ACID]] properties and full local autonomy can not be reconciled..."&lt;ref&gt;Peter Muth (1997): [http://portal.acm.org/citation.cfm?id=264226  "Application Specific Transaction Management in Multidatabase Systems"], ''Distributed and Parallel Databases'', Volume 5, Issue 4, pp. 357 - 403, October 1997, {{ISSN|0926-8782}} (quotation from the article's Abstract)&lt;/ref&gt;

The quotation above is from a 1997 article proposing a relaxed global serializability solution (see ''[[Global serializability#Relaxing global serializability|Relaxing global serializability]]'' below), and referencing [[Commitment ordering]] (CO) articles. The CO solution supports effectively both full [[ACID]] properties and full local autonomy, as well as meeting the other requirements posed above in the ''[[Global serializability#Problem statement|Problem statement]]'' section, and apparently has been misunderstood.

Similar thinking we see also in the following quotation from a 1998 article:

*"The concept of serializability has been the traditionally accepted correctness criterion in database systems. However in multidatabase systems (MDBSs), ensuring global serializability is a difficult task. The difficulty arises due to the heterogeneity of the concurrency control protocols used by the participating local database management systems (DBMSs), and the desire to preserve the autonomy of the local DBMSs. In general, solutions to the global serializability problem result in executions with a low degree of concurrency. The alternative, relaxed serializability, may result in data inconsistency."&lt;ref name=Shar1998&gt;Sharad Mehrotra, Rajeev Rastogi, Henry Korth, [[Abraham Silberschatz]] (1998):
[http://portal.acm.org/citation.cfm?id=277629 "Ensuring Consistency in Multidatabases by Preserving Two-Level Serializability"], ''ACM Transactions on Database Systems'' (TODS), Vol. 23, No. 2, pp. 199-230, June 1998 (quotation from the article's Abstract)&lt;/ref&gt;

Also the above quoted article proposes a relaxed global serializability solution, while referencing the CO work. The CO solution for global serializability both bridges between different concurrency control protocols with no substantial concurrency reduction (and typically minor, if at all), and maintains the autonomy of local DBMSs. Evidently also here CO has been misunderstood. This misunderstanding continues to 2010 in a textbook by some of the same authors, where the same relaxed global serializability technique, ''Two level serializability'', is emphasized and described in detail, and CO is not mentioned at all.&lt;ref name=Silber2010&gt;[[Abraham Silberschatz|Avi Silberschatz]], Henry F Korth, S. Sudarshan (2010): [http://highered.mcgraw-hill.com/sites/0073523321/  ''Database System Concepts''], 6th Edition, McGraw-Hill, ISBN 0-07-295886-3&lt;/ref&gt;

On the other hand, the following quotation on CO appears in a 2009 book:&lt;ref name=Bern2009&gt;[[Phil Bernstein|Philip A. Bernstein]], Eric Newcomer (2009): [http://www.elsevierdirect.com/product.jsp?isbn=9781558606234 ''Principles of Transaction Processing'', 2nd Edition],  Morgan Kaufmann (Elsevier), June 2009, ISBN 978-1-55860-623-4 (quotation from page 145)&lt;/ref&gt;

*"Not all concurrency control algorithms use locks... Three other techniques are timestamp ordering, serialization graph testing, and commit ordering. '''Timestamp ordering''' assigns each transaction a timestamp and ensures that conflicting operations execute in timestamp order. '''Serialization graph testing''' tracks conflicts and ensures that the serialization graph is acyclic. '''Commit ordering''' ensures that conflicting operations are consistent with the relative order in which their transactions commit, which can enable interoperability of systems using different concurrency control mechanisms."

:'''Comments:'''
#Beyond the common locking based algorithm SS2PL, which is a CO variant itself, also additional variants of CO that use locks exist, (see below). However, generic, or "pure" CO does not use locks.
#Since CO mechanisms order the commit events according to conflicts that already have occurred, it is better to describe CO as "'''Commit ordering''' ensures that the relative order in which transactions commit is consistent with the order of their respective conflicting operations."

The characteristics and properties of the CO solution are discussed below.

===Proposed solutions===
Several solutions, some partial, have been proposed for the global serializability problem. Among them:

* ''Global [[serializability#Testing conflict serializability|conflict graph]]'' (serializability graph, [[precedence graph]]) ''checking''
* ''Distributed [[Two phase locking]]'' (Distributed 2PL)
* ''Distributed [[Timestamp-based concurrency control|Timestamp ordering]]''
* ''Tickets'' (local logical timestamps which define local total orders, and are propagated to determine global partial order of transactions)
* ''Commitment ordering''

===Technology perspective===
The problem of global serializability has been a quite intensively researched subject in the late 1980s and early 1990s. ''Commitment ordering'' (CO) has provided an effective general solution to the problem, insight into it, and understanding about possible generalizations of ''[[serializability#Common mechanism - SS2PL|strong strict two phase locking]]'' (SS2PL), which practically and almost exclusively has been utilized (in conjunction with the ''[[Two-phase commit protocol]]'' (2PC) ) since the 1980s to achieve global serializability across databases. An important side-benefit of CO is the automatic ''global deadlock'' resolution that it provides (this is applicable also to distributed SS2PL; though global deadlocks have been an important research subject for SS2PL, automatic resolution has been overlooked, except in the CO articles, until today (2009)). At that time quite many commercial database system types existed, many non-relational, and databases were relatively very small. Multi database systems were considered a key for database scalability by database systems interoperability, and global serializability was urgently needed. Since then the tremendous progress in computing power, storage, and communication networks, resulted in [[Order of magnitude|orders of magnitude]] increases in both centralized databases' sizes, transaction rates, and remote access to database capabilities, as well as blurring the boundaries between centralized computing and distributed one over fast, low-latency local networks (e.g., [[Infiniband]]). These, together with progress in database vendors' distributed solutions (primarily the popular SS2PL with 2PC based, a [[de facto standard]] that allows interoperability among different vendors' (SS2PL-based) databases; both SS2PL and 2PC technologies have gained substantial expertise and efficiency), [[workflow]] management systems, and [[database replication]] technology, in most cases have provided satisfactory and sometimes better [[information technology]] solutions without multi database atomic [[distributed transaction]]s over databases with different concurrency control (bypassing the problem above). As a result, the sense of urgency that existed with the problem at that period, and in general with high-performance distributed atomic transactions over databases with different concurrency control  types, has reduced. However, the need in concurrent distributed atomic transactions as a fundamental element of reliability exists in distributed systems also beyond database systems, and so the need in global serializability as a fundamental correctness criterion for such transactional systems (see also [[Serializability#Distributed serializability|Distributed serializability]] in [[Serializability]]). With the proliferation of the [[Internet]], [[Cloud computing]], [[Grid computing]], small, portable, powerful computing devices (e.g., [[smartphone]]s), and sophisticated [[systems management]] the need for effective global serializability techniques to ensure correctness in and among distributed transactional applications seems to increase, and thus also the need in Commitment ordering (including the popular for databases special case SS2PL; SS2PL, though, does not meet the requirements of many other transactional objects).

==The commitment ordering solution==
{{POV-section|Commitment ordering|date=November 2011}}
{{main|Commitment ordering}}
{{main|The History of Commitment Ordering}}

Commitment ordering&lt;ref name=Raz1992&gt;[[Yoav Raz]] (1992): [http://www.informatik.uni-trier.de/~ley/db/conf/vldb/Raz92.html "The Principle of Commitment Ordering, or Guaranteeing Serializability in a Heterogeneous Environment of Multiple Autonomous Resource Managers Using Atomic Commitment"] {{webarchive |url=https://web.archive.org/web/20070523182950/http://www.informatik.uni-trier.de/~ley/db/conf/vldb/Raz92.html |date=May 23, 2007 }}, ''Proc. of the Eighteenth Int. Conf. on Very Large Data Bases'' (VLDB), pp. 292-312, Vancouver, Canada, August 1992. (also DEC-TR 841, [[Digital Equipment Corporation]], November 1990) 
&lt;/ref&gt;&lt;ref name=Raz1994&gt;Yoav Raz (1994): [http://linkinghub.elsevier.com/retrieve/pii/0020019094900051 "Serializability by Commitment Ordering"], ''Information Processing Letters'', [http://www.informatik.uni-trier.de/~ley/db/journals/ipl/ipl51.html#Raz94 Volume 51, Number 5],  pp. 257-264, September 1994. (Received August 1991)&lt;/ref&gt; (or Commit ordering; CO) is the only high-performance, [[fault tolerant]], [[Serializability#View and conflict serializability|conflict serializability]] providing solution that has been proposed as a fully distributed (no central computing component or data-structure are needed), general mechanism that can be combined seamlessly with any local (to a database) [[concurrency control]] mechanism (see [[Commitment ordering#Summary|technical summary]]). Since the CO property of a schedule is a [[necessary condition]] for global serializability of [[Commitment ordering#CO is a necessary condition for global serializability across autonomous database systems|''autonomous databases'']] (in the context of concurrency control), it provides the only general solution for autonomous databases (i.e., if autonomous databases do not comply with CO, then global serializability may be violated). Seemingly by sheer luck, the CO solution possesses many attractive properties: 
 
#does not interfere with any transaction's operation, particularly neither block, restrict nor delay any data-access operation (read or write) for either local or [[distributed transaction|global]] transactions (and thus does not cause any extra aborts); thus allows seamless integration with any concurrency control mechanism.
#allows [[Optimistic concurrency control|optimistic]] implementations (''non-blocking'', i.e., non data access blocking).
#allows [[heterogeneity]]: Global serializability is achieved across multiple [[transactional objects]] with different (any) concurrency control mechanisms, without interfering with the mechanisms' operations.
#allows [[modularity]]: Transactional objects can be added and removed transparently.
#allows full [[ACID]] transaction support.
#maintains each database's [[Commitment ordering#CO is a necessary condition for global serializability across autonomous database systems|autonomy]], and does not need any concurrency control information distribution (e.g., local precedence relations, locks, timestamps, or tickets).
#does not need any knowledge about the transactions.
#requires no communication overhead since it only uses already needed, unmodified ''[[atomic commitment]]'' protocol messages (any such protocol; using [[fault tolerant]] atomic commitment protocols and database systems makes the CO solution fault tolerant).
#automatically resolves global [[deadlock]]s due to [[lock (computer science)|locking]].
#[[Scalability|scales up]] effectively with [[computer network]] size and number of databases, almost without any negative impact on performance, since each global transaction is typically confined to certain relatively small numbers of databases and network nodes.
#requires no additional, artificial transaction access operations (e.g., "take [[Timestamp-based concurrency control|timestamp]]" or "take ticket"), which typically result in  additional, artificial conflicts that reduce concurrency.
#requires low overhead.

The only overhead incurred by the CO solution is locally detecting conflicts (which is already done by any known serializability mechanism, both pessimistic and optimistic) and locally ordering in each database system both the (local) commits of local transactions and the voting for atomic commitment of global transactions. Such overhead is low. The net effect of CO may be some delays of commit events (but never more delay than SS2PL, and on the average less). This makes CO instrumental for global concurrency control of multidatabase systems (e.g., [[federated database system]]s). The underlying ''Theory of Commitment ordering'',&lt;ref name=Raz2009&gt;Yoav Raz (2009): [http://sites.google.com/site/yoavraz2/home/theory-of-commitment-ordering Theory of Commitment Ordering - Summary] GoogleSites - Site of Yoav Raz. Retrieved 1 Feb, 2011.&lt;/ref&gt; part of [[Serializability]] theory, is both sound and [[Scientific method#Hypothesis development|elegant]] (and even [[Mathematical beauty|"mathematically beautiful"]]; referring to structure and dynamics of conflicts, graph cycles, and deadlocks), with interesting implications for transactional [[Distributed computing|distributed applications]].

All the qualities of CO in the list above, except the first three, are also possessed by SS2PL, which is a special case of CO, but blocking and constraining. This partially explains the popularity of SS2PL as a solution (practically, the only solution, for many years) for achieving global serializability. However, property 9 above, automatic resolution of global deadlocks, has not been noticed for SS2PL in the database research literature until today (2009; except in the CO publications). This, since the phenomenon of voting-deadlocks in such environments and their automatic resolution by the [[atomic commitment]] protocol has been overlooked.

Most existing database systems, including all major commercial database systems, are ''[[serializability#Common mechanism - SS2PL|strong strict two phase locking (SS2PL)]]'' based and already CO compliant. Thus they can participate in a [[commitment ordering#Summary|CO based solution for global serializability in multidatabase environments]] without any modification (except for the popular ''[[Multiversion concurrency control|multiversioning]]'', where additional CO aspects should be considered). Achieving global serializability across SS2PL based databases using atomic commitment (primarily using ''[[two phase commit]], 2PC'') has been employed for many years (i.e., using the same CO solution for a specific special case; however, no reference is known prior to CO, that notices this special case's automatic global deadlock resolution by the atomic commitment protocol's [[commitment ordering#Exact characterization of voting-deadlocks by global cycles|augmented-conflict-graph]] global cycle elimination process). Virtually all existing distributed transaction processing environments and supporting products rely on SS2PL and provide 2PC. As a matter of fact SS2PL together with 2PC have become a [[de facto standard]]. This solution is a homogeneous concurrency control one, suboptimal (when both Serializability and [[Schedule (computer science)#Strict|Strictness]] are needed; see [[Commitment ordering#Strict CO (SCO)|Strict commitment ordering]]; SCO) but still quite effective in most cases, sometimes at the cost of increased computing power needed relatively to the optimum. (However, for better performance [[Serializability#Relaxing serializability|relaxed serializability]] is used whenever applications allow). It allows inter-operation among SS2PL-compliant different database system types, i.e., allows heterogeneity in aspects other than concurrency control. SS2PL is a very constraining schedule property, and "takes over" when combined with any other property. For example, when combined with any [[Concurrency control#Concurrency control mechanisms|optimistic property]], the result is not optimistic anymore, but rather characteristically SS2PL. On the other hand, CO does not change data-access scheduling patterns at all, and ''any'' combined property's characteristics remain unchanged. Since also CO uses atomic commitment (e.g., 2PC) for achieving global serializability, as SS2PL does, any CO compliant database system or transactional object can transparently join existing SS2PL based environments, use 2PC, and maintain global serializability without any environment change. This makes CO a straightforward, natural generalization of SS2PL for any conflict serializability based database system, for all practical purposes.

Commitment ordering has been quite widely known inside the ''[[transaction processing]]'' and ''[[database]]s'' communities at ''[[Digital Equipment Corporation]]'' (DEC) since 1990. It has been under ''company confidentiality'' due to [[patent]]ing&lt;ref name=Raz1990&gt;Yoav Raz (1990): [http://yoavraz.googlepages.com/DEC-CO-MEMO-90-11-16.pdf  ''On the Significance of Commitment Ordering''] - Call for patenting, Memorandum, [[Digital Equipment Corporation]], November 1990.&lt;/ref&gt;
&lt;ref name=Raz1991&gt;
Yoav Raz: US patents [http://patft1.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=3&amp;f=G&amp;l=50&amp;co1=AND&amp;d=PTXT&amp;s1=%22commitment+ordering%22.TI.&amp;OS=TTL/ 5,504,899]  [http://patft1.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=2&amp;f=G&amp;l=50&amp;co1=AND&amp;d=PTXT&amp;s1=%22commitment+ordering%22.TI.&amp;OS=TTL/ 5,504,900]   [http://patft1.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;co1=AND&amp;d=PTXT&amp;s1=%22commitment+ordering%22.TI.&amp;OS=TTL/ 5,701,480]&lt;/ref&gt; processes. CO was disclosed outside of DEC by lectures and technical reports' distribution to database researches in May 1991, immediately after its first patent filing. It has been misunderstood by many database researchers years after its introduction, which is evident by the quotes above from articles in 1997-1998 referencing Commitment ordering articles. On the other hand, CO has been utilized extensively as a solution for global serializability in works on [[Transactional processes]],
&lt;ref&gt;Heiko Schuldt, Hans-J&#246;rg Schek, and Gustavo Alonso (1999): [http://portal.acm.org/citation.cfm?id=853907 "Transactional Coordination Agents for Composite Systems"], In ''Proceedings of the 3rd International Database Engineering and Applications Symposium'' (IDEAS&#8217;99), IEEE Computer Society Press, Montrteal, Canada, pp. 321&#8211;331.&lt;/ref&gt;
&lt;ref&gt;Klaus Haller, Heiko Schuldt, Can T&#252;rker (2005): [http://portal.acm.org/citation.cfm?doid=1099554.1099563 "Decentralized coordination of transactional processes in peer-to-peer environments",] ''Proceedings of the 2005 ACM CIKM, International Conference on Information and Knowledge Management'', pp. 28-35, Bremen, Germany, October 31 - November 5, 2005, ISBN 1-59593-140-6&lt;/ref&gt; and more recently in the related '''Re:GRIDiT''', 
&lt;ref&gt;Laura Cristiana Voicu, Heiko Schuldt, Fuat Akal, Yuri Breitbart, Hans J&#246;rg Schek (2009): [http://dbis.cs.unibas.ch/publications/2009/grid2009/dbis_publication_view  "Re:GRIDiT &#8211; Coordinating Distributed Update Transactions on Replicated Data in the Grid"], ''10th IEEE/ACM International Conference on Grid Computing (Grid 2009)'', Banff, Canada, 2009/10.&lt;/ref&gt;
&lt;ref&gt;Laura Cristiana Voicu and Heiko Schuldt (2009): [http://dbis.cs.unibas.ch/publications/2009/clouddb09/dbis_publication_view  "How Replicated Data Management in the Cloud can benefit from a Data Grid Protocol &#8212; the Re:GRIDiT Approach"], ''Proceedings of the 1st International Workshop on Cloud Data Management (CloudDB 2009)'', Hong Kong, China, 2009/11.&lt;/ref&gt;
which is an approach for transaction management in the converging [[Grid computing]] and [[Cloud computing]]. 
See more in ''[[The History of Commitment Ordering]]''.

==Relaxing global serializability==
Some techniques have been developed for '''relaxed global serializability''' (i.e., they do not guarantee global serializability; see also ''[[Serializability#Relaxing serializability|Relaxing serializability]]''). Among them (with several publications each):

* ''Quasi serializability''&lt;ref name=Du1989&gt;Weimin Du and Ahmed K. Elmagarmid (1989): [http://www.informatik.uni-trier.de/~ley/db/conf/vldb/DuE89.html  "Quasi Serializability: a Correctness Criterion for Global Concurrency Control in InterBase"], ''Proceedings of the Fifteenth International Conference on Very Large Data Bases'' (VLDB), August 22&#8211;25, 1989, Amsterdam, The Netherlands, pp. 347-355, Morgan Kaufmann, ISBN 1-55860-101-5&lt;/ref&gt;
* ''Two-level serializability''&lt;ref name=Shar1998 /&gt;

While local (to a database system) relaxed serializability methods compromise ''serializability'' for performance gain (and are utilized only when the application can tolerate possible resulting inaccuracies, or its integrity is unharmed), it is unclear that various proposed ''relaxed global serializability'' methods which compromise ''global serializability'', provide any performance gain over ''commitment ordering'' which guarantees global serializability. Typically, the declared intention of such methods has not been performance gain over effective global serializability methods (which apparently have been unknown to the inventors), but rather correctness criteria alternatives due to lack of a known effective global serializability method. Oddly, some of them were introduced years after CO had been introduced, and some even quote CO without realizing that it provides an effective global serializability solution, and thus without providing any performance comparison with CO to justify them as alternatives to global serializability for some applications (e.g., ''Two-level serializability''&lt;ref name=Shar1998 /&gt;). ''Two-level serializability'' is even presented as a major global concurrency control method in a 2010 edition of a text-book on databases&lt;ref name=Silber2010/&gt; (authored by two of the original authors of Two-level serializability, where one of them, [[Abraham Silberschatz|Avi Silberschatz]], is also an author of the original ''[[The History of Commitment Ordering#AESO is modified to Strong recoverability (CO)|Strong recoverability]]'' articles). This book neither mentions CO nor references it, and strangely, apparently does not consider CO a valid ''Global serializability'' solution.

Another common reason nowadays for Global serializability relaxation is the requirement of [[availability]] of [[internet]] products and [[Internet service provider|services]]. This requirement is typically answered by large scale data [[Replication (computer science)|replication]]. The straightforward solution for synchronizing replicas' updates of a same database object is including all these updates in a single atomic [[distributed transaction]]. However, with many replicas such a transaction is very large, and may span several [[computer]]s and [[computer network|networks]] that some of them are likely to be unavailable. Thus such a transaction is likely to end with abort and miss its purpose.&lt;ref name=Gray1996&gt;{{cite conference
 | author = [[Jim Gray (computer scientist)|Gray, J.]]
 |author2=Helland, P. |author3=[[Patrick O'Neil|O&#8217;Neil, P.]] |author4=[[Dennis Shasha|Shasha, D.]]
 | year = 1996
 | title = The dangers of replication and a solution
 | conference = Proceedings of the 1996 [[ACM SIGMOD International Conference on Management of Data]]
 | pages = 173&#8211;182
 | url = ftp://ftp.research.microsoft.com/pub/tr/tr-96-17.pdf
 | doi = 10.1145/233269.233330
 }}&lt;/ref&gt;
Consequently, [[Optimistic replication]] (Lazy replication) is often utilized (e.g., in many products and services by [[Google]], [[Amazon.com|Amazon]], [[Yahoo]], and alike), while Global serializability is relaxed and compromised for [[Eventual consistency]]. In this case relaxation is done only for applications that are not expected to be harmed by it.

Classes of schedules defined by ''relaxed global serializability'' properties either contain the global serializability class, or are incomparable with it. What differentiates techniques for ''relaxed global conflict serializability'' (RGCSR) properties from those of ''relaxed conflict serializability'' (RCSR) properties that are not RGCSR is typically the different way ''global cycles'' (span two or more databases) in the ''global conflict graph'' are handled. No distinction between global and local cycles exists for RCSR properties that are not RGCSR. RCSR contains RGCSR. Typically RGCSR techniques eliminate local cycles, i.e., provide ''local serializability'' (which can be achieved effectively by regular, known [[concurrency control]] methods), however, obviously they do not eliminate all global cycles (which would achieve global serializability).

==References==
{{reflist|33em}}

{{DEFAULTSORT:Global Serializability}}
[[Category:Data management]]
[[Category:Databases]]
[[Category:Transaction processing]]
[[Category:Concurrency control]]</text>
      <sha1>6kbogavwkurhbujphsrcdyn24s4r9ms</sha1>
    </revision>
  </page>
  <page>
    <title>Customer data management</title>
    <ns>0</ns>
    <id>31501606</id>
    <revision>
      <id>695948693</id>
      <parentid>669195217</parentid>
      <timestamp>2015-12-19T21:19:32Z</timestamp>
      <contributor>
        <username>NickPenguin</username>
        <id>703140</id>
      </contributor>
      <comment>rm merge tag, no consensus to merge</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4170" xml:space="preserve">'''Customer data management (CDM)''' is the ways in which businesses keep track of their customer information and survey their [[customer base]] in order to obtain feedback. CDM embraces a range of software or [[cloud computing]] applications designed to give large organizations rapid and efficient access to customer data. Surveys and data can be centrally located and widely accessible within a company, as opposed to being warehoused in separate departments. CDM encompasses the collection, analysis, organizing, reporting and sharing of customer information throughout an organization. Businesses need a thorough understanding of their customers&#8217; needs if they are to retain and increase their customer base. Efficient CDM solutions provide companies with the ability to deal instantly with customer issues and obtain immediate feedback. As a result, [[customer retention]] and [[customer satisfaction]] can show dramatic improvement. According to a recent study by [[Aberdeen Group]] inc.: "Above-average and best-in-class companies... attain greater than 20% annual improvement in retention rates, revenues, data accuracy and partner/customer satisfaction rates."&lt;ref&gt;Smalltree, Hannah (2006) [http://searchcrm.techtarget.com/news/1212337/Best-practices-in-managing-customer-data]&lt;/ref&gt;

== Customer data management and cloud computing ==

Cloud computing offers an attractive choice for CDM in many companies due to its accessibility and [[Cost efficiency|cost-effectiveness]]. Businesses can decide who, within their company, should have the ability to create, adjust, analyze or share customer information. In December 2010, 52% of [[Information technology|Information Technology]] (IT) professionals worldwide were deploying, or planning to deploy, cloud computing;&lt;ref&gt;Cisco.com (December 2010) [http://newsroom.cisco.com/dlls/2010/prod_120810.html]&lt;/ref&gt; this percentage is far higher in many countries.

== Uses for management ==

'''Customer data management'''
* should provide a cost-effective, user-friendly solution for [[marketing]], research, sales, [[human resources]] and IT departments
* enables companies to create and email online surveys, reports and newsletters
* encompasses and simplifies [[customer relationship management]] (CRM) and [[Customer feedback management services|customer feedback management]] (CFM)

== Background ==

Customer data management, as a term, was coined in the 1990s, pre-dating the alternative term [[enterprise feedback management]] (EFM). Customer data management (CDM) was introduced as a software solution that would replace earlier disc-based or paper-based surveys and [[spreadsheet]] data. Initially, CDM solutions were marketed to businesses as software, specific to one company, and often to one department within that company. This was superseded by [[application service  provider]]s (ASPs) where software was hosted for [[end user]] organizations, thus avoiding the necessity for IT professionals to deploy and support software. However, ASPs with their single-tenancy architecture were, in turn, superseded by [[software as a service]] (SaaS), engineered for multi-tenancy. By 2007 SaaS applications, giving businesses on-demand access to their customer information, were rapidly gaining popularity compared with ASPs. Cloud computing now includes SaaS and many prominent CDM providers offer cloud-based applications to their clients.

In recent years, there has been a push away from the term EFM, with many of those working in this area advocating the slightly updated use of CDM. The return to the term CDM is largely based on the greater need for clarity around the solutions offered by companies, and on the desire to retire terminology veering on techno-jargon that customers may have a hard time understanding.&lt;ref&gt;InSiteSystems.com (December, 2010) [http://www.insitesystems.com/systems/blogs/the-problem-with-efm.html]&lt;/ref&gt;

== References ==
&lt;!--- See [[Wikipedia:Footnotes]] on how to create references using &lt;ref&gt;&lt;/ref&gt; tags which will then appear here automatically --&gt;
{{Reflist}}

&lt;!--- Categories ---&gt;
[[Category:Articles created via the Article Wizard]]
[[Category:Data management]]</text>
      <sha1>f691y9d2juj5vjogm01pv3foi6ldjuf</sha1>
    </revision>
  </page>
  <page>
    <title>Contrast set learning</title>
    <ns>0</ns>
    <id>34055690</id>
    <revision>
      <id>722587737</id>
      <parentid>721153417</parentid>
      <timestamp>2016-05-29T03:21:41Z</timestamp>
      <contributor>
        <username>Dcirovic</username>
        <id>11795905</id>
      </contributor>
      <minor />
      <comment>clean up using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="16164" xml:space="preserve">'''Contrast set learning''' is a form of [[association rule learning]] that seeks to identify meaningful differences between separate groups by reverse-engineering the key predictors that identify for each particular group. For example, given a set of attributes for a pool of students (labeled by degree type), a contrast set learner would identify the ''contrasting'' features between students seeking bachelor's degrees and those working toward PhD degrees.

== Overview ==

A common practice in [[data mining]] is to [[Statistical classification|classify]], to look at the attributes of an object or situation and make a guess at what category the observed item belongs to. As new evidence is examined (typically by feeding a ''training set'' to a learning [[algorithm]]), these guesses are re&#64257;ned and improved. Contrast set learning works in the opposite direction. While classi&#64257;ers read a collection of data and collect information that is used to place new data into a series of discrete categories, contrast set learning takes the category that an item belongs to and attempts to reverse engineer the statistical evidence that identifies an item as a member of a class. That is, contrast set learners seek rules associating attribute values with changes to the class distribution.&lt;ref name="bay01"&gt;{{cite journal
 |author1=Stephen Bay |author2=Michael Pazzani | year = 2001
 | title = Detecting group differences: Mining contrast sets
 | journal = Data Mining and Knowledge Discovery
 | volume = 5
 | issue= 3
 | pages= 213&#8211;246
 | url= http://wotan.liu.edu/docis/lib/musl/rclis/dbl/dmiknd/(2001)5%253A3%253C213%253ADGDMCS%253E/www.isle.org%252F~sbay%252Fpapers%252Fstucco.dmkd.pdf
 }}
&lt;/ref&gt; They seek to identify the key predictors that contrast one classification from another.

For example, an aerospace engineer might record data on test launches of a new rocket. Measurements would be taken at regular intervals throughout the launch, noting factors such as the trajectory of the rocket, operating temperatures, external pressures, and so on. If the rocket launch fails after a number of successful tests, the engineer could use contrast set learning to distinguish between the successful and failed tests. A contrast set learner will produce a set of association rules that, when applied, will indicate the key predictors of each failed tests versus the successful ones (the temperature was too high, the wind pressure was too high, etc.).

Contrast set learning is a form of [[association rule learning]].&lt;ref name="webb03"&gt;{{cite conference
 |author1=GI Webb |author2=S. Butler |author3=D. Newlands | year = 2003
 | title = On Detecting Differences Between Groups
 | conference = KDD'03 Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining
| url= http://portal.acm.org/citation.cfm?id=956781
 }}
&lt;/ref&gt; Association rule learners typically offer rules linking attributes commonly occurring together in a training set (for instance, people who are enrolled in four-year programs and take a full course load tend to also live near campus). Instead of &#64257;nding rules that describe the current situation, contrast set learners seek rules that differ meaningfully in their distribution across groups (and thus, can be used as predictors for those groups).&lt;ref name="bay99"&gt;{{cite conference
 |author1=Stephen Bay |author2=Michael Pazzani | year = 1999
 | title = Detecting change in categorical data: mining contrast sets
 | conference = KDD '99 Proceedings of the fifth ACM SIGKDD international conference on Knowledge discovery and data mining
 }}
&lt;/ref&gt; For example, a contrast set learner could ask, &#8220;What are the key identifiers of a person with a bachelor's degree or a person with a PhD, and how do people with PhD's and bachelor&#8217;s degrees differ?&#8221;

Standard [[Classification in machine learning|classifier]] algorithms, such as [[C4.5]], have no concept of class importance (that is, they do not know if a class is "good" or "bad"). Such learners cannot bias or filter their predictions towards certain desired classes. As the goal of contrast set learning is to discover meaningful differences between groups, it is useful to be able to target the learned rules towards certain classifications. Several contrast set learners, such as MINWAL&lt;ref name="cai98"&gt;{{cite conference
 |author1=C.H. Cai |author2=A.W.C. Fu |author3=C.H. Cheng |author4=W.W. Kwong | year = 1998
 | title = Mining association rules with weighted items
 | conference = Proceedings of International Database Engineering and Applications Symposium (IDEAS 98)
 | url= http://appsrv.cse.cuhk.edu.hk/~kdd/assoc_rule/paper_chcai.pdf
 }}
&lt;/ref&gt; or the family of TAR algorithms,&lt;ref name="hu03"/&gt;&lt;ref name="burlet07"&gt;{{cite conference
 |author1=K. Gundy-Burlet |author2=J. Schumann |author3=T. Barrett |author4=T. Menzies | year = 2007
 | title = Parametric analysis of ANTARES re-entry guidance algorithms using advanced test generation and data analysis
 | conference = In 9th International Symposium on Arti&#64257;cial Intelligence, Robotics and Automation in Space
 }}
&lt;/ref&gt;&lt;ref name="gay10"&gt;{{cite journal
 |author1=Gregory Gay |author2=Tim Menzies |author3=Misty Davies |author4=Karen Gundy-Burlet | year = 2010
 | title = Automatically Finding the Control Variables for Complex System Behavior
 | journal = Automated Software Engineering
 | volume = 17
 | issue= 4
 | url= http://www.greggay.com/pdf/10tar3.pdf 
 }}
&lt;/ref&gt; assign weights to each class in order to focus the learned theories toward outcomes that are of interest to a particular audience. Thus, contrast set learning can be though of as a form of weighted class learning.&lt;ref name="menzies03"&gt;{{cite journal
 |author1=T. Menzies |author2=Y. Hu | year = 2003
 | title = Data Mining for Very Busy People
 | journal = IEEE Computer
 | volume = 36
 | issue= 11
 | pages= 22&#8211;29
 | url= http://menzies.us/pdf/03tar2.pdf
 | doi=10.1109/mc.2003.1244531
 }}
&lt;/ref&gt;

=== Example: Supermarket Purchases ===

The differences between standard classification, association rule learning, and contrast set learning can be illustrated with a simple supermarket metaphor. In the following small dataset, each row is a supermarket transaction and each "1" indicates that the item was purchased (a "0" indicates that the item was not purchased):

{| class="wikitable"
|-
! ''Hamburger'' !! ''Potatoes'' !! ''Foie Gras'' !! ''Onions'' !! ''Champagne'' !! ''Purpose of Purchases''
|-
| 1 || 1 || 0 || 1 || 0 || Cookout
|-
| 1 || 1 || 0 || 1 || 0 || Cookout
|-
| 0 || 0 || 1 || 0 || 1 || Anniversary
|-
| 1 || 1 || 0 || 1 || 0 || Cookout
|-
| 1 || 1 || 0 || 0 || 1 || Frat Party
|}

Given this data,
* Association rule learning may discover that customers that buy onions and potatoes together are likely to also purchase hamburger meat.
* Classification may discover that customers that bought onions, potatoes, and hamburger meats were purchasing items for a cookout.
* Contrast set learning may discover that the major difference between customers shopping for a cookout and those shopping for an anniversary dinner are that customers acquiring items for a cookout purchase onions, potatoes, and hamburger meat (and ''do not purchase'' foie gras or champagne).

== Treatment learning ==

&lt;!--[[File:TreatmentLearningExample.png|thumb|200px|Example of a treatment produced based on data collected while riding a bicycle. This treatment states that an optimal riding speed can be obtained while the hill slope is constrained to between &#8722;10 and 0% and the cadence is between 1.4 and 2.5. {{deletable image-caption|date=December 2011}}]]--&gt;

Treatment learning is a form of weighted contrast-set learning that takes a single ''desirable'' group and contrasts it against the remaining ''undesirable'' groups (the level of desirability is represented by weighted classes).&lt;ref name="hu03"&gt;{{cite book
 | author = Y. Hu
 | year = 2003
 | title = Treatment learning: Implementation and application
 | type= Master's thesis
 | publisher=Department of Electrical Engineering, University of British Columbia
 }}
&lt;/ref&gt; The resulting "treatment" suggests a set of rules that, when applied, will lead to the desired outcome.

Treatment learning differs from standard contrast set learning through the following constraints:
* Rather than seeking the differences between all groups, treatment learning specifies a particular group to focus on, applies a weight to this desired grouping, and lumps the remaining groups into one "undesired" category.
* Treatment learning has a stated focus on minimal theories. In practice, treatment are limited to a maximum of four constraints (i.e., rather than stating all of the reasons that a rocket differs from a skateboard, a treatment learner will state one to four major differences that predict for rockets at a high level of statistical significance).

This focus on simplicity is an important goal for treatment learners. Treatment learning seeks the ''smallest'' change that has the ''greatest'' impact on the class distribution.&lt;ref name="menzies03"/&gt;

Conceptually, treatment learners explore all possible subsets of the range of values for all attributes. Such a search is often infeasible in practice, so treatment learning often focuses instead on quickly pruning and ignoring attribute ranges that, when applied, lead to a class distribution where the desired class is in the minority.&lt;ref name="gay10"/&gt;

=== Example: Boston housing data ===

The following example demonstrates the output of the treatment learner TAR3 on a dataset of housing data from the city of [[Boston]] (a nontrivial public dataset with over 500 examples). In this dataset, a number of factors are collected for each house, and each house is classified according to its quality (low, medium-low, medium-high, and high). The ''desired'' class is set to "high", and all other classes are lumped together as undesirable.

The output of the treatment learner is as follows:

&lt;code&gt;
 Baseline class distribution:
 low: 29%
 medlow: 29%
 medhigh: 21%
 high: 21%

 Suggested Treatment: [PTRATIO=[12.6..16), RM=[6.7..9.78)]

 New class distribution:
 low: 0%
 medlow: 0%
 medhigh: 3%
 high: 97%
&lt;/code&gt;

With no applied treatments (rules), the desired class represents only 21% of the class distribution. However, if one filters the data set for houses with 6.7 to 9.78 rooms and a neighborhood parent-teacher ratio of 12.6 to 16, then 97% of the remaining examples fall into the desired class (high-quality houses).

== Algorithms ==

There are a number of algorithms that perform contrast set learning. The following subsections describe two examples.

=== STUCCO ===

The STUCCO contrast set learner&lt;ref name="bay01"/&gt;&lt;ref name="bay99"/&gt; treats the task of learning from contrast sets as a [[Tree traversal|tree search]] problem where the root node of the tree is an empty contrast set. Children are added by specializing the set with additional items picked through a canonical ordering of attributes (to avoid visiting the same nodes twice). Children are formed by appending terms that follow all existing terms in a given ordering. The formed tree is searched in a breadth-first manner. Given the nodes at each level, the dataset is scanned and the support is counted for each group. Each node is then examined to determine if it is significant and large, if it should be pruned, and if new children should be generated. After all significant contrast sets are located, a post-processor selects a subset to show to the user - the low order, simpler results are shown first, followed by the higher order results which are "surprising and significantly different.&lt;ref name="bay99"/&gt;"

The support calculation comes from testing a null hypothesis that the contrast set support is equal across all groups (i.e., that contrast set support is ''independent of group membership''). The support count for each group is a frequency value that can be analyzed in a contingency table where each row represents the truth value of the contrast set and each column variable indicates the group membership frequency. If there is a difference in proportions between the contrast set frequencies and those of the null hypothesis, the algorithm must then determine if the differences in proportions represent a relation between variables or if it can be attributed to random causes. This can be determined through a [[Chi-squared test|chi-square test]] comparing the observed frequency count to the expected count.

Nodes are pruned from the tree when all specializations of the node can never lead to a significant and large contrast set. The decision to prune is based on:
* The minimum deviation size: The maximum difference between the support of any two groups bust be greater than a user-specified threshold.
* Expected cell frequencies: The expected cell frequencies of a contingency table can only decrease as the contrast set is specialized. When these frequencies are too small, the validity of the chi-square test is violated.
* &lt;math&gt;\chi^2&lt;/math&gt; bounds: An upper bound is kept on the distribution of a statistic calculated when the null hypothesis is true. Nodes are pruned when it is no longer possible to meet this cutoff.

=== TAR3 ===

The TAR3&lt;ref name="burlet07"/&gt;&lt;ref name="schumann09"&gt;{{cite conference
 |author1=J. Schumann |author2=K. Gundy-Burlet |author3=C. Pasareanu |author4=T. Menzies |author5=A. Barrett | year = 2009
 | title = Software V&amp;V support by parametric analysis of large software simulation systems
 | conference = Proceedings of the 2009 IEEE Aerospace Conference
 }}
&lt;/ref&gt; weighted contrast set learner is based on two fundamental concepts - the '''lift''' and '''support''' of a rule set.

The lift of a set of rules is the change that some decision makes to a set of examples after imposing that decision (i.e., how the class distribution shifts in response to the imposition of a rule). TAR3 seeks the smallest set of rules which induces the biggest changes in the sum of the weights attached to each class multiplied by the frequency at which each class occurs. The lift is calculated by dividing the score of the set in which the set of rules is imposed by the score of the baseline set (i.e., no rules are applied). Note that by reversing the lift scoring function, the TAR3 learner can also select for the remaining classes and reject the target class.

It is problematic to rely on the lift of a rule set alone. Incorrect or misleading data noise, if correlated with failing examples, may result in an overfitted rule set. Such an overfitted model may have a large lift score, but it does not accurately re&#64258;ect the prevailing conditions within the dataset. To avoid overfitting, TAR3 utilizes a support threshold and rejects all rules that fall on the wrong side of this threshold. Given a target class, the support threshold is a user-supplied value (usually 0.2) which is compared to the ratio of the frequency of the target class when the rule set has been applied to the frequency of that class in the overall dataset. TAR3 rejects all sets of rules with support lower than this threshold.

By requiring both a high lift and a high support value, TAR3 not only returns ideal rule sets, but also favors smaller sets of rules. The fewer rules adopted, the more evidence that will exist supporting those rules.

The TAR3 algorithm only builds sets of rules from attribute value ranges with a high heuristic value. The algorithm determines which ranges to use by &#64257;rst determining the lift score of each attribute&#8217;s value ranges. These individual scores are then sorted and converted into a cumulative probability distribution. TAR3 randomly selects values from this distribution, meaning that low-scoring ranges are unlikely to be selected. To build a candidate rule set, several ranges are selected and combined. These candidate rule sets are then scored and sorted. If no improvement is seen after a user-defined number of rounds, the algorithm terminates and returns the top-scoring rule sets.

== References ==
{{Reflist}}

{{DEFAULTSORT:Contrast Set Learning}}
[[Category:Data management]]
[[Category:Data mining]]</text>
      <sha1>ej44g5zxcmej82tux2g7tgeneull81r</sha1>
    </revision>
  </page>
  <page>
    <title>Machine-readable data</title>
    <ns>0</ns>
    <id>34578263</id>
    <revision>
      <id>738689561</id>
      <parentid>738451281</parentid>
      <timestamp>2016-09-10T13:56:49Z</timestamp>
      <contributor>
        <username>Owen Ambur</username>
        <id>3035628</id>
      </contributor>
      <comment>/* See also */ Added link to Machine-Readable Documents</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3212" xml:space="preserve">'''Machine-readable data''' is [[data]] (or [[metadata]]) which is in a format that can be understood by a [[computer]].

There are two types; human-readable data that is [[markup language|marked up]] so that it can also be read by machines (examples; [[microformat]]s, [[RDFa]], [[HTML]]) or data file formats intended principally for processing by machines ([[Resource Description Framework|RDF]], [[XML]], [[JSON]]).

''Machine readable'' is not synonymous with ''digitally accessible''.  A digitally accessible document may be online, making it easier for a human to access it via a computer, but unless the relevant data is available in a machine readable format, it will be much harder to use the computer to extract, transform and process that data.&lt;ref&gt;{{cite web
 | url=https://www.data.gov/developers/blog/primer-machine-readability-online-documents-and-data
 | title=A Primer on Machine Readability for Online Documents and Data
 | work=Data.gov
 | date=2012-09-24
 | accessdate=2015-02-27 }}
&lt;/ref&gt;

For purposes of implementation of the [[Government Performance and Results Act]] (GPRA) Modernization Act, the [[Office of Management and Budget]] (OMB) defines "machine readable" as follows:  "Format in a standard computer language (not English text) that can be read automatically by a web browser or computer system. (e.g.; xml). Traditional word processing documents and portable document format (PDF) files are easily read by humans but typically are difficult for machines to interpret. Other formats such as extensible markup language (XML), (JSON), or spreadsheets with header columns that can be exported as comma separated values (CSV) are machine readable formats. As HTML is a structural markup language, discreetly labeling parts of the document, computers are able to gather document components to assemble Tables of Content, outlines, literature search bibliographies, etc. It is possible to make traditional word processing documents and other formats machine readable but the documents must include enhanced structural elements."&lt;ref&gt;[http://www.whitehouse.gov/sites/default/files/omb/assets/a11_current_year/s200.pdf OMB Circular A-11, Part 6], Preparation and Submission of Strategic Plans, Annual Performance Plans, and Annual Program Performance Reports&lt;/ref&gt;

==References==
{{reflist}}

==See also==

* [[Open data]]
* [[Linked data]]
* [[Machine-Readable Documents]]
* [[Human-readable medium]]
* [http://xml.fido.gov/stratml/references/PL111-532StratML.htm#SEC10 Section 10] of the [[Government Performance and Results Act|GPRA]] Modernization Act (GPRAMA), which requires U.S. federal agencies to publish their strategic and performance plans and reports in machine-readable format, like [[Strategy Markup Language]] (StratML)
* President Obama's [http://xml.fido.gov/stratml/carmel/EOOMRDwStyle.xml Executive Order] Making Open and Machine Readable the New Default for Government Information
* [http://xml.fido.gov/stratml/carmel/M-13-13wStyle.xml#_78e85ef4-b91c-11e2-bf2b-79d279ad226c OMB M-13-13], Open Data Policy: Managing Information as an Asset, which requires agencies to use open, machine-readable, data format standards
[[Category:Data management]]


{{Comp-stub}}</text>
      <sha1>ccg3hblttqhhqn9beqcvzh1of5zzqrk</sha1>
    </revision>
  </page>
  <page>
    <title>Electronic lab notebook</title>
    <ns>0</ns>
    <id>1616185</id>
    <revision>
      <id>750728389</id>
      <parentid>741454683</parentid>
      <timestamp>2016-11-21T14:08:46Z</timestamp>
      <contributor>
        <username>Deb</username>
        <id>1219</id>
      </contributor>
      <comment>/* See also */ remove red link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="10621" xml:space="preserve">An '''electronic lab notebook''' (also known as electronic laboratory notebook, or ELN) is a [[computer program]] designed to replace paper [[lab notebook|laboratory notebook]]s.  Lab notebooks in general are used by [[scientist]]s, [[engineer]]s, and [[technician]]s to document [[research]], [[experiment]]s, and procedures performed in a laboratory.  A lab notebook is often maintained to be a [[legal document]] and may be used in a [[court of law]] as [[evidence (law)|evidence]].  Similar to an [[inventor's notebook]], the lab notebook is also often referred to in [[patent]] prosecution and [[intellectual property]] [[litigation]].

Electronic lab notebooks are a fairly new technology and offer many benefits to the user as well as organizations. For example: electronic lab notebooks are easier to search upon, simplify data copying and backups, and support collaboration amongst many users.&lt;ref&gt;{{cite conference |
title = A Collaborative Electronic Notebook |
first = James | 
last = Myers |author2=Elena Mendoza |author3=Bonnie Hoopes |
journal = Proceedings of the IASTED International Conference on Internet and Multimedia Systems and Applications |
year = 2001 
}}&lt;/ref&gt;  
ELNs can have fine-grained access controls, and can be more secure than their paper counterparts.&lt;ref&gt;{{
cite conference | 
last= Myers | 
first = James | 
year = 2003 | 
journal = Proceedings of the 2003 International Symposium On Collaborative Technologies and Systems | 
title = Collaborative Electronic Notebooks as Electronic Records:Design Issues for the Secure Electronic Laboratory Notebook (ELN) | 
url = http://collaboratory.emsl.pnl.gov/resources/publications/papers/seceln(final1)1-22Nov.pdf
}}&lt;/ref&gt;  They also allow the direct incorporation of data from instruments, replacing the practice of printing out data to be stapled into a paper notebook.&lt;ref&gt;{{Cite journal | last1 = Perkel | first1 = J. M. | title = Coding your way out of a problem | journal = Nature Methods | volume = 8 | issue = 7 | pages = 541&#8211;543 | year = 2011 | pmid = 21716280 | doi = 10.1038/nmeth.1631}}&lt;/ref&gt;

==Types==
ELNs can be divided into two categories:

* "Specific ELNs" contain features designed to work with specific applications, scientific instrumentation or data types.
* "[[Cross-disciplinary]] ELNs" or "Generic ELNs" are designed to support access to all data and information that needs to be recorded in a lab notebook.

Solutions range from specialized programs designed from the ground up for use as an ELN, to modifications or direct use of more general programs.  Examples of using more general software include using [[OpenWetWare]], a [[MediaWiki]] install (running the same software that Wikipedia uses), as an ELN, or the use of general note taking software such as OneNote as an ELN.&lt;ref&gt;{{Cite journal | last1 = Perkel | first1 = J. M. | title = Coding your way out of a problem | journal = Nature Methods | volume = 8 | issue = 7 | pages = 541&#8211;543 | year = 2011 | pmid = 21716280 | doi = 10.1038/nmeth.1631}}.&lt;/ref&gt;

ELN's come in many different forms. They can be standalone programs, use a client-server model, or be entirely web-based. Some use a lab-notebook approach, others resemble a blog.

A good many variations on the "ELN" acronym have appeared.&lt;ref&gt;{{Cite web|url=http://cerf-notebook.com/articles/eln-glossary/|title=Lab Notebook (ELN) Glossary - CERF|date=2016-02-16|language=en-US|access-date=2016-08-20}}&lt;/ref&gt; Differences between systems with different names are often subtle, with considerable functional overlap between them. Examples include "ERN" (Electronic Research Notebook), "ERMS" (Electronic Resource (or Research or Records) Management System (or Software) and SDMS (Scientific Data (or Document) Management System (or Software). Ultimately, these types of systems all strive to do the same thing: Capture, record, centralize and protect scientific data in a way that is highly searchable, historically accurate, and legally stringent, and which also promotes secure collaboration, greater efficiency, reduced mistakes and lowered total research costs.

==Objectives==
A good electronic laboratory notebook should offer a secure environment to protect the integrity of both data and process, whilst also affording the flexibility to adopt new processes or changes to existing processes without recourse to further software development. The package architecture should be a modular design, so as to offer the benefit of minimizing validation costs of any subsequent changes that you may wish to make in the future as your needs change.

A good electronic laboratory notebook should be an "out of the box" solution that, as standard, has fully configurable forms to comply with the requirements of regulated analytical groups through to a sophisticated ELN for inclusion of structures, spectra, chromatograms, pictures, text, etc. where a preconfigured form is less appropriate. All data within the system may be stored in a database (e.g. MySQL, MS-SQL, Oracle) and be fully searchable. The system should enable data to be collected, stored and retrieved through any combination of forms or ELN that best meets the requirements of the user.

The application should enable secure forms to be generated that accept laboratory data input via PCs and/or laptops / palmtops, and should be directly linked to electronic devices such as laboratory balances, pH meters, etc.  Networked or wireless communications should be accommodated for by the package which will allow data to be interrogated, tabulated, checked, approved, stored and archived to comply with the latest regulatory guidance and legislation.  A system should also include a scheduling option for routine procedures such as equipment qualification and study related timelines. It should include configurable qualification requirements to automatically verify that instruments have been cleaned and calibrated within a specified time period, that reagents have been quality-checked and have not expired, and that workers are trained and authorized to use the equipment and perform the procedures.

==Regulatory and legal aspects==
The laboratory accreditation criteria found in the [[ISO 17025]] standard needs to be considered for the protection and computer backup of electronic records. These criteria can be found specifically in clause 4.13.1.4 of the standard.&lt;ref&gt;"ISO/IEC 17025:2005 - General Requirements for the Competence of Testing and Calibration Laboratories." ISO - International Organization for Standardization. Web. 16 Nov. 2011. &lt;http://www.iso.org/iso/Catalogue_detail?csnumber=39883&gt;.&lt;/ref&gt;

Electronic lab notebooks used for development or research in regulated industries, such as medical devices or pharmaceuticals, are expected to comply with FDA regulations related to software validation.  The purpose of the regulations is to ensure the integrity of the entries in terms of time, authorship, and content.  Unlike ELNs for patent protection, FDA is not concerned with patent interference proceedings, but is concerned with avoidance of falsification.  Typical provisions related to software validation are included in the medical device regulations at 21 CFR 820 (et seq.)&lt;ref&gt;United States. Food and Drug Administration. Department of Health and Human Resources. 1 Food and Drugs - Subchapter H Medical Devices - Part 820 System RegCode of Federal Regulations - Title 2ulation. FDA.gov, 7 Oct. 1996. Web. &lt;http://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfcfr/cfrsearch.cfm?cfrpart=820&gt;.&lt;/ref&gt; and [[Title 21 CFR Part 11]].&lt;ref&gt;United States. Food and Drug Administration. Department of Health and Human Resources. Code of Federal Regulations - Title 21 Part 11 Electronic Records; Electronic Signatures. FDA.gov. Authority: 21 U.S.C. 321-393; 42 U.S.C. 262., 20 Mar. 1997. Web. 16 Nov. 2011. &lt;http://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfcfr/cfrsearch.cfm?cfrpart=11&gt;.&lt;/ref&gt;  Essentially, the requirements are that the software has been designed and implemented to be suitable for its intended purposes.  Evidence to show that this is the case is often provided by a Software Requirements Specification (SRS) setting forth the intended uses and the needs that the ELN will meet; one or more testing protocols that, when followed, demonstrate that the ELN meets the requirements of the specification and that the requirements are satisfied under worst-case conditions.  Security, audit trails, prevention of unauthorized changes without substantial collusion of otherwise independent personnel (i.e., those having no interest in the content of the ELN such as independent quality unit personnel) and similar tests are fundamental.  Finally, one or more reports demonstrating the results of the testing in accordance with the predefined protocols are required prior to release of the ELN software for use.  If the reports show that the software failed to satisfy any of the SRS requirements, then corrective and preventive action ("CAPA") must be undertaken and documented.  Such CAPA may extend to minor software revisions, or changes in architecture or major revisions.  CAPA activities need to be documented as well.

Aside from the requirements to follow such steps for regulated industry, such an approach is generally a good practice in terms of development and release of any software to assure its quality and fitness for use.  There are standards related to software development and testing that can be applied (see ref.).

==See also==
* [[List of ELN software packages]]	
* [[Data management]]
* [[Laboratory informatics]]
* [[Scientific management]]

==References==
{{reflist}}

== Further reading ==
* {{Cite journal 
| last1 = Taylor | first1 = K. T. 
| title = The status of electronic laboratory notebooks for chemistry and biology 
| journal = Current opinion in drug discovery &amp; development 
| volume = 9 
| issue = 3 
| pages = 348&#8211;353 
| year = 2006 
| pmid = 16729731
}}
* {{Cite journal | last1 = Rubacha | first1 = M. | last2 = Rattan | first2 = A. K. | last3 = Hosselet | first3 = S. C. | doi = 10.1016/j.jala.2009.01.002 | title = A Review of Electronic Laboratory Notebooks Available in the Market Today | journal = Journal of Laboratory Automation | volume = 16 | issue = 1 | pages = 90&#8211;98 | year = 2011 | pmid =  21609689| pmc = }}

{{DEFAULTSORT:Electronic Lab Notebook}}
[[Category:Electronic lab notebook]]
[[Category:Research]]
[[Category:Science software]]
[[Category:Scientific documents]]
[[Category:Notebooks]]
[[Category:Electronic documents]]
[[Category:Data management]]
[[Category:Content management systems]]
[[Category:Data management software]]</text>
      <sha1>h0szlanaajetcxu8axsdg9p45m6ist4</sha1>
    </revision>
  </page>
  <page>
    <title>Data warehouse</title>
    <ns>0</ns>
    <id>7990</id>
    <revision>
      <id>759338969</id>
      <parentid>755857297</parentid>
      <timestamp>2017-01-10T16:14:47Z</timestamp>
      <contributor>
        <username>Paul2520</username>
        <id>19295592</id>
      </contributor>
      <comment>filled in references</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="29470" xml:space="preserve">{{multiple issues|
{{Refimprove|date=February 2008}}
{{Citation style|date=September 2013}}
}}

[[File:Data warehouse overview.JPG|thumb|200px|Data Warehouse Overview]]

In [[computing]], a '''data warehouse''' ('''DW''' or '''DWH'''), also known as an '''enterprise data warehouse''' ('''EDW'''), is a system used for [[Business reporting|reporting]] and [[data analysis]], and is considered a core component of [[business intelligence]].&lt;ref&gt;Dedi&#263;, N. and Stanier C., 2016., "An Evaluation of the Challenges of Multilingualism in Data Warehouse Development" in 18th International Conference on Enterprise Information Systems - ICEIS 2016, p. 196.&lt;/ref&gt; DWs are central repositories of integrated data from one or more disparate sources. They store current and historical data and are used for creating analytical reports for knowledge workers throughout the enterprise. Examples of reports could range from annual and quarterly comparisons and trends to detailed daily sales analysis.

The data stored in the warehouse is [[upload]]ed from the [[operational system]]s (such as marketing or sales). The data may pass through an [[operational data store]] for additional operations before it is used in the DW for reporting.

==Types of systems==
;[[Data mart]]:  A data mart is a simple form of a data warehouse that is focused on a single subject (or functional area), hence they draw data from a limited number of sources such as sales, finance or marketing. Data marts are often built and controlled by a single department within an organization. The sources could be internal operational systems, a central data warehouse, or external data.&lt;ref&gt;{{cite web |url=http://docs.oracle.com/html/E10312_01/dm_concepts.htm |title=Data Mart Concepts |publisher=Oracle |year=2007}}&lt;/ref&gt; Denormalization is the norm for data modeling techniques in this system. Given that data marts generally cover only a subset of the data contained in a data warehouse, they are often easier and faster to implement.

{| class="wikitable"
|+ Difference between data warehouse and {{nowrap|data mart}}
|-
! Data warehouse
! Data mart
|-
| enterprise-wide data
| department-wide data
|-
| multiple subject areas
| single subject area
|-
| difficult to build
| easy to build
|-
| takes more time to build
| less time to build
|-
| larger memory
| limited memory
|}

'''Types of data marts'''
* Dependent data mart
* Independent data mart
* Hybrid data mart
;[[Online analytical processing]] (OLAP): OLAP is characterized by a relatively low volume of transactions. Queries are often very complex and involve aggregations. For OLAP systems, response time is an effectiveness measure. OLAP applications are widely used by [[Data Mining]] techniques. OLAP databases store aggregated, historical data in multi-dimensional schemas (usually star schemas). OLAP systems typically have data latency of a few hours, as opposed to data marts, where latency is expected to be closer to one day. The OLAP approach is used to analyze multidimensional data from multiple sources and perspectives. The three basic operations in OLAP are : Roll-up (Consolidation), Drill-down and Slicing &amp; Dicing.&lt;ref name=dwh&gt;{{cite web |url=https://intellipaat.com/tutorial/data-warehouse-tutorial/ |title=Data Warehousing Tutorial For Beginners |publisher=Intellipaat}}&lt;/ref&gt;

;[[Online transaction processing]] (OLTP): OLTP is characterized by a large number of short on-line transactions (INSERT, UPDATE, DELETE). OLTP systems emphasize very fast query processing and maintaining [[data integrity]] in multi-access environments. For OLTP systems, effectiveness is measured by the number of transactions per second. OLTP databases contain detailed and current data. The schema used to store transactional databases is the entity model (usually [[Third normal form|3NF]]).&lt;ref&gt;{{cite web |url=http://datawarehouse4u.info/OLTP-vs-OLAP.html |title=OLTP vs. OLAP |year=2009 |website=Datawarehouse4u.Info |quote=We can divide IT systems into transactional (OLTP) and analytical (OLAP). In general we can assume that OLTP systems provide source data to data warehouses, whereas OLAP systems help to analyze it.}}&lt;/ref&gt; Normalization is the norm for data modeling techniques in this system.

;Predictive analysis: Predictive analysis is about [[pattern recognition|finding]] and quantifying hidden patterns in the data using complex mathematical models that can be used to [[prediction|predict]] future outcomes.  Predictive analysis is different from OLAP in that OLAP focuses on historical data analysis and is reactive in nature, while predictive analysis focuses on the future. These systems are also used for CRM ([[customer relationship management]]).

==Software tools==
The typical extract-transform-load ([[Extract, transform, load|ETL]])-based data warehouse uses [[Staging (data)|staging]], [[data integration]], and access layers to house its key functions. The staging layer or staging database stores raw data extracted from each of the disparate source data systems. The integration layer integrates the disparate data sets by transforming the data from the staging layer often storing this transformed data in an [[operational data store]] (ODS) database. The integrated data are then moved to yet another database, often called the data warehouse database, where the data is arranged into hierarchical groups often called dimensions and into facts and aggregate facts. The combination of facts and dimensions is sometimes called a [[star schema]].  The access layer helps users retrieve data.&lt;ref name=IJCA96Patil&gt;{{cite journal |url=http://www.ijcaonline.org/proceedings/icwet/number9/2131-db195 |author1=Patil, Preeti S. |author2=Srikantha Rao |author3=Suryakant B. Patil |title=Optimization of Data Warehousing System: Simplification in Reporting and Analysis |work=IJCA Proceedings on International Conference and workshop on Emerging Trends in Technology (ICWET) |year=2011 |volume=9 |issue=6 |pages=33&#8211;37 |publisher=Foundation of Computer Science}}&lt;/ref&gt;

This definition of the data warehouse focuses on data storage. The main source of the data is cleaned, transformed, catalogued and made available for use by managers and other business professionals for [[data mining]], [[OLAP|online analytical processing]], [[market research]] and [[decision support]].&lt;ref&gt;Marakas &amp; O'Brien 2009&lt;/ref&gt; However, the means to retrieve and analyze data, to [[Extract, transform, load|extract, transform, and load]] data, and to manage the [[data dictionary]] are also considered essential components of a data warehousing system. Many references to data warehousing use this broader context. Thus, an expanded definition for data warehousing includes [[business intelligence tools]], tools to extract, transform, and load data into the repository, and tools to manage and retrieve [[metadata]].

==Benefits==
A data warehouse maintains a copy of information from the source transaction systems. This architectural complexity provides the opportunity to:
* Integrate data from multiple sources into a single database and data model. Mere congregation of data to single database so a single query engine can be used to present data is an ODS.
* Mitigate the problem of database isolation level lock contention in transaction processing systems caused by attempts to run large, long running, analysis queries in transaction processing databases.
* Maintain [[Provenance#Data provenance|data history]], even if the source transaction systems do not.
* Integrate data from multiple source systems, enabling a central view across the enterprise. This benefit is always valuable, but particularly so when the organization has grown by merger.
* Improve [[data quality]], by providing consistent codes and descriptions, flagging or even fixing bad data.
* Present the organization's information consistently.
* Provide a single common data model for all data of interest regardless of the data's source.
* Restructure the data so that it makes sense to the business users.
* Restructure the data so that it delivers excellent query performance, even for complex analytic queries, without impacting the [[operational system]]s.
* Add value to operational business applications, notably [[customer relationship management]] (CRM) systems.
*Make decision&#8211;support queries easier to write.
*Optimized data warehouse architectures allow data scientists to organize and disambiguate repetitive data.&lt;ref&gt;{{Cite web|url=https://www.idera.com/resourcecentral/whitepapers/modern-data-architecture|title=Modern Data Architecture {{!}} IDERA|last=|first=|date=|website=www.idera.com|publisher=|access-date=2016-09-18}}&lt;/ref&gt;

==Generic environment==

The environment for data warehouses and marts includes the following:

* Source systems that provide data to the warehouse or mart;
* Data integration technology and processes that are needed to prepare the data for use;
* Different architectures for storing data in an organization's data warehouse or data marts;
*Different tools and applications for the variety of users;
*Metadata, data quality, and governance processes must be in place to ensure that the warehouse or mart meets its purposes.

In regards to source systems listed above, Rainer{{clarify|reason=Who is Rainer?|date=December 2014}} states, "A common source for the data in data warehouses is the company's operational databases, which can be relational databases".&lt;ref name=rainer2012&gt;{{cite book|last=Rainer|first=R. Kelly|title=Introduction to Information Systems: Enabling and Transforming Business, 4th Edition (Kindle Edition)|date=2012-05-01|publisher=Wiley|pages=127, 128, 130, 131, 133}}&lt;/ref&gt;

Regarding data integration, Rainer states, "It is necessary to extract data from source systems, transform them, and load them into a data mart or warehouse".&lt;ref name=rainer2012/&gt;

Rainer discusses storing data in an organization&#8217;s data warehouse or data marts.&lt;ref name=rainer2012 /&gt;

Metadata are data about data. "IT personnel need information about data sources; database, table, and column names; refresh schedules; and data usage measures".&lt;ref name=rainer2012 /&gt;

Today, the most successful companies are those that can respond quickly and flexibly to market changes and opportunities. A key to this response is the effective and efficient use of data and information by analysts and managers.&lt;ref name=rainer2012 /&gt; A "data warehouse" is a repository of historical data that are organized by subject to support decision makers in the organization.&lt;ref name=rainer2012 /&gt; Once data are stored in a data mart or warehouse, they can be accessed.

==History==
The concept of data warehousing dates back to the late 1980s&lt;ref&gt;{{cite web|url=http://www.computerworld.com/databasetopics/data/story/0,10801,70102,00.html |title=The Story So Far |date=2002-04-15 |accessdate=2008-09-21 |deadurl=yes |archiveurl=https://web.archive.org/web/20080708182105/http://www.computerworld.com/databasetopics/data/story/0,10801,70102,00.html |archivedate=2008-07-08 |df= }}&lt;/ref&gt; when IBM researchers Barry Devlin and Paul Murphy developed the "business data warehouse". In essence, the data warehousing concept was intended to provide an architectural model for the flow of data from operational systems to [[decision support system|decision support environments]]. The concept attempted to address the various problems associated with this flow, mainly the high costs associated with it. In the absence of a data warehousing architecture, an enormous amount of redundancy was required to support multiple decision support environments. In larger corporations it was typical for multiple decision support environments to operate independently. Though each environment served different users, they often required much of the same stored data. The process of gathering, cleaning and integrating data from various sources, usually from long-term existing operational systems (usually referred to as [[legacy system]]s), was typically in part replicated for each environment. Moreover, the operational systems were frequently reexamined as new decision support requirements emerged. Often new requirements necessitated gathering, cleaning and integrating new data from "[[data mart]]s" that were tailored for ready access by users.

Key developments in early years of data warehousing were:
* 1960s &#8211; [[General Mills]] and [[Dartmouth College]], in a joint research project, develop the terms ''dimensions'' and ''facts''.&lt;ref name="kimball16"&gt;Kimball 2002, pg. 16&lt;/ref&gt;
* 1970s &#8211; [[ACNielsen]] and IRI provide dimensional data marts for retail sales.&lt;ref name="kimball16" /&gt;
* 1970s &#8211; [[Bill Inmon]] begins to define and discuss the term: Data Warehouse.{{citation needed|date=June 2014}}
* 1975 &#8211; [[Sperry Univac]] introduces [[MAPPER]] (MAintain, Prepare, and Produce Executive Reports) is a database management and reporting system that includes the world's first [[Fourth-generation programming language|4GL]]. First platform designed for building Information Centers (a forerunner of contemporary Enterprise [[Data Warehousing]] platforms)
* 1983 &#8211; [[Teradata]] introduces a database management system specifically designed for decision support.
* 1984 &#8211; [[Metaphor Computer Systems]], founded by [[David Liddle]] and Don Massaro, releases Data Interpretation System (DIS). DIS was a hardware/software package and GUI for business users to create a database management and analytic system.
* 1988 &#8211; Barry Devlin and Paul Murphy publish the article ''An architecture for a business and information system'' where they introduce the term "business data warehouse".&lt;ref&gt;{{cite journal|url=http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=5387658|title=An architecture for a business and information system|journal=IBM Systems Journal | doi=10.1147/sj.271.0060|volume=27|pages=60&#8211;80}}&lt;/ref&gt;
* 1990 &#8211; Red Brick Systems, founded by [[Ralph Kimball]], introduces Red Brick Warehouse, a database management system specifically for data warehousing.
* 1991 &#8211; Prism Solutions, founded by [[Bill Inmon]], introduces Prism Warehouse Manager, software for developing a data warehouse.
* 1992 &#8211; [[Bill Inmon]] publishes the book ''Building the Data Warehouse''.&lt;ref&gt;{{cite book|last=Inmon|first=Bill|title=Building the Data Warehouse|year=1992|publisher=Wiley|isbn=0-471-56960-7}}&lt;/ref&gt;
* 1995 &#8211; The Data Warehousing Institute, a for-profit organization that promotes data warehousing, is founded.
* 1996 &#8211; [[Ralph Kimball]] publishes the book ''The Data Warehouse Toolkit''.&lt;ref name=":0"&gt;{{cite book|title=The Data Warehouse Toolkit|last=Kimball|first=Ralph|publisher=Wiley|year=2011|isbn=9780470149775|page=237}}&lt;/ref&gt;
* 2012 &#8211; [[Bill Inmon]] developed and made public technology known as "textual disambiguation". Textual disambiguation applies context to raw text and reformats the raw text and context into a standard data base format. Once raw text is passed through textual disambiguation, it can easily and efficiently be accessed and analyzed by standard business intelligence technology. Textual disambiguation is accomplished through the execution of textual ETL. Textual disambiguation is useful wherever raw text is found, such as in documents, Hadoop, email, and so forth.

==Information storage==

===Facts===
A fact is a value or measurement, which represents a fact about the managed entity or system.

Facts as reported by the reporting entity are said to be at raw level. E.g. in a mobile telephone system, if a BTS (base transceiver station) received 1,000 requests for traffic channel allocation, it allocates for 820 and rejects the remaining then it would report 3 '''facts''' or measurements to a management system:
* tch_req_total = 1000
* tch_req_success = 820
* tch_req_fail = 180

Facts at the raw level are further aggregated to higher levels in various [[Dimension (data warehouse)|dimensions]] to extract more service or business-relevant information from it. These are called aggregates or summaries or aggregated facts.

For instance, if there are 3 BTSs in a city, then the facts above can be aggregated from the BTS to the city level in the network dimension. For example:

* &lt;math&gt;tch\_req\_success\_city = tch\_req\_success\_bts1 + tch\_req\_success\_bts2 + tch\_req\_success\_bts3&lt;/math&gt;
* &lt;math&gt;avg\_tch\_req\_success\_city = (tch\_req\_success\_bts1 + tch\_req\_success\_bts2 + tch\_req\_success\_bts3) / 3&lt;/math&gt;

===Dimensional versus normalized approach for storage of data===
There are three or more leading approaches to storing data in a data warehouse&amp;nbsp;&#8212; the most important approaches are the dimensional approach and the normalized approach.

The dimensional approach refers to [[Ralph Kimball]]&#8217;s approach in which it is stated that the data warehouse should be modeled using a Dimensional Model/[[star schema]]. The normalized approach, also called the [[Third normal form|3NF]] model (Third Normal Form) refers to Bill Inmon's approach in which it is stated that the data warehouse should be modeled using an E-R model/normalized model.

In a [[Star schema|dimensional approach]], [[transaction data]] are partitioned into "facts", which are generally numeric transaction data, and "[[dimension (data warehouse)|dimensions]]", which are the reference information that gives context to the facts. For example, a sales transaction can be broken up into facts such as the number of products ordered and the total price paid for the products, and into dimensions such as order date, customer name, product number, order ship-to and bill-to locations, and salesperson responsible for receiving the order.

A key advantage of a dimensional approach is that the data warehouse is easier for the user to understand and to use. Also, the retrieval of data from the data warehouse tends to operate very quickly.&lt;ref name=":0" /&gt; Dimensional structures are easy to understand for business users, because the structure is divided into measurements/facts and context/dimensions. Facts are related to the organization&#8217;s business processes and operational system whereas the dimensions surrounding them contain context about the measurement (Kimball, Ralph 2008). Another advantage offered by dimensional model is that it does not involve a relational database every time. Thus, this type of modeling technique is very useful for end-user queries in data warehouse.&lt;ref name=dwh /&gt;

The main disadvantages of the dimensional approach are the following:
# In order to maintain the integrity of facts and dimensions, loading the data warehouse with data from different operational systems is complicated.
# It is difficult to modify the data warehouse structure if the organization adopting the dimensional approach changes the way in which it does business.

In the normalized approach, the data in the data warehouse are stored following, to a degree, [[database normalization]] rules. Tables are grouped together by ''subject areas'' that reflect general data categories (e.g., data on customers, products, finance, etc.). The normalized structure divides data into entities, which creates several tables in a relational database. When applied in large enterprises the result is dozens of tables that are linked together by a web of joins. Furthermore, each of the created entities is converted into separate physical tables when the database is implemented (Kimball, Ralph 2008){{Citation needed|date=November 2013}}.
The main advantage of this approach is that it is straightforward to add information into the database. Some disadvantages of this approach are that, because of the number of tables involved, it can be difficult for users to join data from different sources into meaningful information and to access the information without a precise understanding of the sources of data and of the [[data structure]] of the data warehouse.

Both normalized and dimensional models can be represented in entity-relationship diagrams as both contain joined relational tables. The difference between the two models is the degree of normalization (also known as [[Database normalization#Normal forms|Normal Forms]]). These approaches are not mutually exclusive, and there are other approaches. Dimensional approaches can involve normalizing data to a degree (Kimball, Ralph 2008).

In ''Information-Driven Business'',&lt;ref&gt;{{cite book|last=Hillard|first=Robert|title=Information-Driven Business|year=2010|publisher=Wiley|isbn=978-0-470-62577-4}}&lt;/ref&gt; Robert Hillard proposes an approach to comparing the two approaches based on the information needs of the business problem. The technique shows that normalized models hold far more information than their dimensional equivalents (even when the same fields are used in both models) but this extra information comes at the cost of usability. The technique measures information quantity in terms of [[Entropy (information theory)|information entropy]] and usability in terms of the Small Worlds data transformation measure.&lt;ref&gt;{{cite web|url=http://mike2.openmethodology.org/wiki/Small_Worlds_Data_Transformation_Measure |title=Information Theory &amp; Business Intelligence Strategy - Small Worlds Data Transformation Measure - MIKE2.0, the open source methodology for Information Development |publisher=Mike2.openmethodology.org |date= |accessdate=2013-06-14}}&lt;/ref&gt;

==Design methods==
{{refimprove section|date=July 2015}}

===Bottom-up design===
In the ''bottom-up'' approach, [[data mart]]s are first created to provide reporting and analytical capabilities for specific [[business process]]es. These data marts can then be integrated to create a comprehensive data warehouse. The data warehouse bus architecture is primarily an implementation of "the bus", a collection of [[Dimension (data warehouse)#Types|conformed dimension]]s and [[Facts (data warehouse)#Types|conformed fact]]s, which are dimensions that are shared (in a specific way) between facts in two or more data marts.&lt;ref&gt;{{Cite web|url=http://decisionworks.com/2003/09/the-bottom-up-misnomer/|title=The Bottom-Up Misnomer - DecisionWorks Consulting|website=DecisionWorks Consulting|language=en-US|access-date=2016-03-06}}&lt;/ref&gt;

===Top-down design===
The ''top-down'' approach is designed using a normalized enterprise [[data model]]. [[Data element|"Atomic" data]], that is, data at the greatest level of detail, are stored in the data warehouse. Dimensional data marts containing data needed for specific business processes or specific departments are created from the data warehouse.&lt;ref name="ReferenceA"&gt;Gartner, Of Data Warehouses, Operational Data Stores, Data Marts and Data Outhouses, Dec 2005&lt;/ref&gt;

===Hybrid design===
Data warehouses (DW) often resemble the [[hub and spokes architecture]]. [[Legacy system]]s feeding the warehouse often include [[customer relationship management]] and [[enterprise resource planning]], generating large amounts of data. To consolidate these various data models, and facilitate the [[extract transform load]] process, data warehouses often make use of an [[operational data store]], the information from which is parsed into the actual DW. To reduce data redundancy, larger systems often store the data in a normalized way. Data marts for specific reports can then be built on top of the DW.

The DW database in a hybrid solution is kept on [[third normal form]] to eliminate [[data redundancy]]. A normal relational database, however, is not efficient for business intelligence reports where dimensional modelling is prevalent. Small data marts can shop for data from the consolidated warehouse and use the filtered, specific data for the fact tables and dimensions required. The DW provides a single source of information from which the data marts can read, providing a wide range of business information. The hybrid architecture allows a DW to be replaced with a [[master data management]] solution where operational, not static information could reside.

The [[Data Vault Modeling]] components follow hub and spokes architecture. This modeling style is a hybrid design, consisting of the best practices from both third normal form and [[star schema]]. The Data Vault model is not a true third normal form, and breaks some of its rules, but it is a top-down architecture with a bottom up design. The Data Vault model is geared to be strictly a data warehouse. It is not geared to be end-user accessible, which when built, still requires the use of a data mart or star schema based release area for business purposes.

==Versus operational system==
Operational systems are optimized for preservation of [[data integrity]] and speed of recording of business transactions through use of [[database normalization]] and an [[entity-relationship model]]. Operational system designers generally follow the [[Codd's 12 rules|Codd rules]] of [[database normalization]] in order to ensure data integrity. Codd defined five increasingly stringent rules of normalization. Fully normalized database designs (that is, those satisfying all five Codd rules) often result in information from a business transaction being stored in dozens to hundreds of tables. [[Relational database]]s are efficient at managing the relationships between these tables. The databases have very fast insert/update performance because only a small amount of data in those tables is affected each time a transaction is processed. Finally, in order to improve performance, older data are usually periodically purged from operational systems.

Data warehouses are optimized for analytic access patterns.  Analytic access patterns generally involve selecting specific fields and rarely if ever 'select *' as is more common in operational databases.  Because of these differences in access patterns, operational databases (loosely, OLTP) benefit from the use of a row-oriented DBMS whereas analytics databases (loosely, OLAP) benefit from the use of a [[column-oriented DBMS]].  Unlike operational systems which maintain a snapshot of the business, data warehouses generally maintain an infinite history which is implemented through ETL processes that periodically migrate data from the operational systems over to the data warehouse.

==Evolution in organization use==
These terms refer to the level of sophistication of a data warehouse:

; Offline operational data warehouse: Data warehouses in this stage of evolution are updated on a regular time cycle (usually daily, weekly or monthly) from the operational systems and the data is stored in an integrated reporting-oriented data
; Offline data warehouse: Data warehouses at this stage are updated from data in the operational systems on a regular basis and the data warehouse data are stored in a data structure designed to facilitate reporting.
; On time data warehouse: Online Integrated Data Warehousing represent the real time Data warehouses stage data in the warehouse is updated for every transaction performed on the source data
; Integrated data warehouse: These data warehouses assemble data from different areas of business, so users can look up the information they need across other systems.&lt;ref&gt;{{cite web |url=http://www.tech-faq.com/data-warehouse.html |title=Data Warehouse }}&lt;/ref&gt;

==See also==
{{colbegin|3}}
*[[Accounting intelligence]]
*[[Anchor modeling]]
*[[Business intelligence]]
*[[Business intelligence tools]]
*[[Data integration]]
*[[Data mart]]
*[[Data mining]]
*[[Data presentation architecture]]
*[[Data scraping]]
*[[Data warehouse appliance]]
*[[Database management system]]
*[[Decision support system]]
*[[Data vault modeling]]
*[[Executive information system]]
*[[Extract, transform, load]]
*[[Master data management]]
*[[Online analytical processing]]
*[[Online transaction processing]]
*[[Operational data store]]
*[[Semantic warehousing]]
*[[Snowflake schema]]
*[[Software as a service]]
*[[Star schema]]
*[[Slowly changing dimension]]
*[[Data warehouse automation]]
{{colend}}

==References==
{{Reflist|30em|&lt;!--refs=
&lt;ref name=ahsan&gt;
{{Cite journal
|last1=Abdullah
|first1=Ahsan
|title=Analysis of mealybug incidence on the cotton crop using ADSS-OLAP (Online Analytical Processing) tool, Volume 69, Issue 1
|journal= Computers and Electronics in Agriculture
|year=2009
|pages=59&#8211;72
|doi=10.1016/j.compag.2009.07.003
|volume=69
}}
&lt;/ref&gt;--&gt;
}}

==Further reading==
* Davenport, Thomas H. and Harris, Jeanne G. ''Competing on Analytics: The New Science of Winning'' (2007) Harvard Business School Press. ISBN 978-1-4221-0332-6
* Ganczarski, Joe. ''Data Warehouse Implementations: Critical Implementation Factors Study'' (2009) [[VDM Verlag]] ISBN 3-639-18589-7 ISBN 978-3-639-18589-8
* Kimball, Ralph and Ross, Margy. ''The Data Warehouse Toolkit'' Third Edition (2013) Wiley, ISBN 978-1-118-53080-1
* Linstedt, Graziano, Hultgren. ''The Business of Data Vault Modeling'' Second Edition (2010) Dan linstedt, ISBN 978-1-4357-1914-9
* William Inmon. ''Building the Data Warehouse'' (2005) John Wiley and Sons, ISBN 978-8-1265-0645-3

==External links==
* [http://www.kimballgroup.com/html/articles.html Ralph Kimball articles]
* [http://www.ijcaonline.org/archives/number3/77-172 International Journal of Computer Applications]
* [http://dwreview.com/DW_Overview.html Data Warehouse Introduction]

{{data}}
{{Data warehouse}}

{{Authority control}}

{{DEFAULTSORT:Data Warehouse}}
[[Category:Business intelligence]]
[[Category:Data management]]
[[Category:Data warehousing| ]]
[[Category:Information technology management]]</text>
      <sha1>ojlbhcfc2n3zvyh79lllk6fwpoiqxu2</sha1>
    </revision>
  </page>
  <page>
    <title>Single customer view</title>
    <ns>0</ns>
    <id>37040021</id>
    <revision>
      <id>761320533</id>
      <parentid>760820213</parentid>
      <timestamp>2017-01-22T08:33:09Z</timestamp>
      <contributor>
        <username>RichardWeiss</username>
        <id>193093</id>
      </contributor>
      <comment>more detail</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2779" xml:space="preserve">A '''Single Customer View''' is an aggregated, consistent and holistic representation of the [[data]] known by an organisation about its customers&lt;ref&gt;[http://www.experian.co.uk/assets/about-us/white-papers/single-customer-view-whitepaper.pdf Exploiting the Single Customer  View to Maximise the Value of Customer Relationships]&lt;/ref&gt;&lt;ref&gt;[http://www.marketingweek.co.uk/driving-value-from-the-single-customer-view/3015497.article Driving value from the single customer view]&lt;/ref&gt; that can be viewed in one place, such as a single page.&lt;ref&gt;[https://spotlessdata.com/blog/data-driven-marketing Data-driven marketing]&lt;/ref&gt; The advantage to an organisation of attaining this unified view comes from the ability it gives to analyse past behaviour in order to better target and personalise future customer interactions.&lt;ref&gt;[http://www.atominsight.com/about-us/blog/single-customer-view-essential Why a single customer view is essential]&lt;/ref&gt; A single customer view is also considered especially relevant where organisations engage with customers through [[multichannel marketing]], since customers expect those interactions to reflect a consistent understanding of their history and preferences.&lt;ref&gt;[http://econsultancy.com/uk/blog/9612-the-impact-of-a-single-customer-view-on-consumer-behaviour-infographic The impact of a single customer view on consumer behaviour: infographic]&lt;/ref&gt; However, some commentators have challenged the idea that a single view of customers across an entire organisation is either natural or meaningful, proposing that the priority should instead be consistency between the multiple views that arise in different contexts.

Where representations of a customer are held in more than one [[data set]], achieving a single customer view can be difficult: firstly because customer identity must be traceable between the records held in those systems, and secondly because anomalies or discrepancies in the customer data must be [[data cleansing|data cleansed]].&lt;ref&gt;[http://www.atominsight.com/about-us/blog/single-customer-view-hard Why building a single customer view isn&#8217;t as easy as you might think]&lt;/ref&gt; As such, the acquisition by an organisation of a single customer view is one potential outcome of successful [[master data management]]. Since 31 December, 2010, maintaining a single customer view has become mandatory for [[United Kingdom]] banks and other deposit takers due to new rules introduced by the [[Financial Services Compensation Scheme]].&lt;ref&gt;[https://www.fscs.org.uk/industry/single-customer-view/ Single Customer View]&lt;/ref&gt;

==References==
{{reflist}}

[[Category:Identity management]]
[[Category:Business intelligence]]
[[Category:Data management]]
[[Category:Data warehousing]]
[[Category:Information technology management]]</text>
      <sha1>7t0lkfmjp3x794vl7a8241cmvuwqzsn</sha1>
    </revision>
  </page>
  <page>
    <title>Consistency (database systems)</title>
    <ns>0</ns>
    <id>1140830</id>
    <revision>
      <id>762789251</id>
      <parentid>762780282</parentid>
      <timestamp>2017-01-30T19:25:29Z</timestamp>
      <contributor>
        <username>Andy Dingley</username>
        <id>3606755</id>
      </contributor>
      <comment>rv unsourced fragment</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4025" xml:space="preserve">{{merge from|Data consistency|date=November 2014}}{{About||Consistency in distributed systems as defined in the CAP Theorem|CAP theorem}}

'''Consistency''' in [[database systems]] refers to the requirement that any given [[database transaction]] must change affected data only in allowed ways. Any data written to the database must be valid according to all defined rules, including [[Integrity constraints|constraints]], [[Cascading rollback|cascades]], [[Database trigger|triggers]], and any combination thereof.  This does not guarantee correctness of the transaction in all ways the application programmer might have wanted (that is the responsibility of application-level code) but merely that any programming errors cannot result in the violation of any defined rules.

==As an ACID guarantee==
Consistency is one of the four guarantees that define [[ACID]] [[database transaction|transactions]]; however, significant ambiguity exists about the nature of this guarantee. It is defined variously as:
* The guarantee that any transactions started in the future necessarily see the effects of other transactions committed in the past&lt;ref name="CAP Theorem Paper"&gt;http://webpages.cs.luc.edu/~pld/353/gilbert_lynch_brewer_proof.pdf "Brewer&#8217;s Conjecture and the Feasibility of Consistent, Available, Partition-Tolerant Web Services"&lt;/ref&gt;&lt;ref name="Ports et al"&gt;{{cite journal | url=http://drkp.net/papers/txcache-osdi10.pdf | title=Transactional Consistency and Automatic Management in an Application Data Cache |author1=Ports, D.R.K |author2=Clements, A.T |author3=Zhang, I |author4=Madden, S |author5=Liskov, B. | journal=MIT CSAIL}}&lt;/ref&gt;
* The guarantee that [[Relational database#Constraints|database constraints]] are not violated, particularly once a transaction commits&lt;ref name="Haerder &amp; Reuter"&gt;{{cite journal | url=http://www.minet.uni-jena.de/dbis/lehre/ws2005/dbs1/HaerderReuter83.pdf | title=Principles of Transaction-Oriented Database Recovery |author1=Haerder, T |author2=Reuter, A. | journal=Computing Surveys |date=December 1983  | volume=15 | issue=4 | pages=287&#8211;317}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://databases.about.com/od/specificproducts/a/acid.htm|title=The ACID Model|author=Mike Chapple|work=About}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://msdn.microsoft.com/en-us/library/aa480356.aspx|title=ACID properties|publisher=}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.techopedia.com/definition/23949/atomicity-consistency-isolation-durability-acid|title=What is ACID in Databases? - Definition from Techopedia|author=Cory Janssen|work=Techopedia.com}}&lt;/ref&gt;
* The guarantee that operations in transactions are performed accurately, correctly, and with validity, with respect to application semantics&lt;ref&gt;{{cite web|url=http://www.iso.org/iso/home/store/catalogue_ics/catalogue_detail_ics.htm?csnumber=27614|title=ISO/IEC 10026-1:1998 - Information technology -- Open Systems Interconnection -- Distributed Transaction Processing -- Part 1: OSI TP Model|publisher=}}&lt;/ref&gt;

As these various definitions are not mutually exclusive, it is possible to design a system that guarantees "consistency" in every sense of the word, as most [[relational database management system]]s in common use today arguably do.

==As a CAP trade-off==

The [[CAP theorem]] is based on three trade-offs, one of which is "atomic consistency" (shortened to "consistency" for the acronym), about which the authors note, "Discussing atomic consistency is somewhat different than talking about an ACID database, as database consistency refers to transactions, while atomic consistency refers only to a property of a single request/response operation sequence. And it has a different meaning than the Atomic in ACID, as it subsumes the database notions of both Atomic and Consistent."&lt;ref name="CAP Theorem Paper" /&gt;

==See also==
* [[Consistency model]]
* [[CAP theorem]]
* [[Eventual consistency]]

==References==
{{reflist}}

{{DEFAULTSORT:Consistency (Database Systems)}}
[[Category:Data management]]
[[Category:Transaction processing]]</text>
      <sha1>qch9svk3ngj6av0asrkl6x4fzwhkwzk</sha1>
    </revision>
  </page>
  <page>
    <title>Inverted index</title>
    <ns>0</ns>
    <id>3125116</id>
    <revision>
      <id>750712977</id>
      <parentid>750712463</parentid>
      <timestamp>2016-11-21T11:49:17Z</timestamp>
      <contributor>
        <username>Zazpot</username>
        <id>632368</id>
      </contributor>
      <comment>/* Bibliography */ Improve poorly formatted references</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6701" xml:space="preserve">In [[computer science]], an '''inverted index''' (also referred to as '''postings file''' or '''inverted file''') is an [[index (database)|index data structure]] storing a mapping from content, such as words or numbers, to its locations in a [[Table (database)|database file]], or in a document or a set of documents (named in contrast to a [[Forward Index]], which maps from documents to content).  The purpose of an inverted index is to allow fast [[full text search]]es, at a cost of increased processing when a document is added to the database.  The inverted file may be the database file itself, rather than its [[Index (database)|index]]. It is the most popular data structure used in [[document retrieval]] systems,&lt;ref&gt;{{Harvnb |Zobel|Moffat|Ramamohanarao|1998| Ref=none }}&lt;/ref&gt; used on a large scale for example in [[search engine]]s.  Additionally, several significant general-purpose [[Mainframe computer|mainframe]]-based [[database management systems]] have used inverted list architectures, including [[ADABAS]], [[DATACOM/DB]], and [[Model 204]].

There are two main variants of inverted indexes: A '''record-level inverted index''' (or '''inverted file index''' or just '''inverted file''') contains a list of references to documents for each word. A '''word-level inverted index''' (or '''full inverted index''' or '''inverted list''') additionally contains the positions of each word within a document.&lt;ref name="isbn0-201-39829-X-p192"&gt;{{Harvnb |Baeza-Yates|Ribeiro-Neto|1999| p=192 | Ref=BYR99 }}&lt;/ref&gt; The latter form offers more functionality (like [[phrase search]]es), but needs more processing power and space to be created.

==Applications==

The inverted index [[data structure]] is a central component of a typical [[Index (search engine)|search engine indexing algorithm]]. A goal of a search engine implementation is to optimize the speed of the query: find the documents where word X occurs. Once a [[Search engine indexing#The forward index|forward index]] is developed, which stores lists of words per document, it is next inverted to develop an inverted index. Querying the forward index would require sequential iteration through each document and to each word to verify a matching document. The time, memory, and processing resources to perform such a query are not always technically realistic.  Instead of listing the words per document in the forward index, the inverted index data structure is developed which lists the documents per word.

With the inverted index created, the query can now be resolved by jumping to the word ID (via [[random access]]) in the inverted index.

In pre-computer times, [[Concordance (publishing)|concordances]] to important books were manually assembled.  These were effectively inverted indexes with a small amount of accompanying commentary that required a tremendous amount of effort to produce.

In bioinformatics, inverted indexes are very important in the [[sequence assembly]] of short fragments of sequenced DNA. One way to find the source of a fragment is to search for it against a reference DNA sequence. A small number of mismatches (due to differences between the sequenced DNA and reference DNA, or errors) can be accounted for by dividing the fragment into smaller fragments&#8212;at least one subfragment is likely to match the reference DNA sequence. The matching requires constructing an inverted index of all substrings of a certain length from the reference DNA sequence. Since the human DNA contains more than 3 billion base pairs, and we need to store a DNA substring for every index and a 32-bit integer for index itself, the storage requirement for such an inverted index would probably be in the tens of gigabytes.

==See also==
* [[Index (search engine)]]
* [[Reverse index]]
* [[Vector space model]]

== Bibliography ==
*{{cite book |last= Knuth |first= D. E. |authorlink= Donald Knuth |title= [[The Art of Computer Programming]] |publisher= [[Addison-Wesley]] |edition= Third |year= 1997 |origyear= 1973 |location= [[Reading, Massachusetts]] |isbn= 0-201-89685-0 |ref= Knu97 |chapter= 6.5. Retrieval on Secondary Keys}}
*{{cite journal|last1= Zobel |first1= Justin |last2=Moffat |first2=Alistair |last3=Ramamohanarao |first3=Kotagiri |date=December 1998 |title= Inverted files versus signature files for text indexing |journal= ACM Transactions on Database Systems |volume= 23 |issue= 4 |pages=453&#8211;490 |publisher= [[Association for Computing Machinery]] |location= New York |doi= 10.1145/296854.277632 |url= |accessdate= }}
*{{cite journal|last1= Zobel |first1= Justin |last2=Moffat |first2=Alistair |date=July 2006 |title= Inverted Files for Text Search Engines |journal= ACM Computing Surveys |volume= 38 |issue= 2 |page=6|publisher= [[Association for Computing Machinery]] |location= New York |doi= 10.1145/1132956.1132959 |url= |accessdate= }}
*{{cite book |last= Baeza-Yates | first = Ricardo |authorlink=Ricardo Baeza-Yates |author2=Ribeiro-Neto, Berthier |title= Modern information retrieval |publisher= Addison-Wesley Longman |location= [[Reading, Massachusetts]] |year= 1999 |isbn= 0-201-39829-X |oclc= |doi= |ref= BYR99 |page= 192 }}
*{{cite journal |last= Salton | first = Gerard |author2=Fox, Edward A. |author3=Wu, Harry  |title= Extended Boolean information retrieval |publisher= ACM |year= 1983
|journal = Commun. ACM |volume = 26 |issue = 11 |doi= 10.1145/182.358466 |page=1022 }}
*{{cite book |title=Information Retrieval: Implementing and Evaluating Search Engines  |url=http://www.ir.uwaterloo.ca/book/ |publisher=MIT Press |year=2010 |location=Cambridge, Massachusetts |isbn= 978-0-262-02651-2 |author8=Stefan B&amp;uuml;ttcher, Charles L. A. Clarke, and Gordon V. Cormack}}

==References==
{{Reflist}}

==External links==
*[https://xlinux.nist.gov/dads/HTML/invertedIndex.html NIST's Dictionary of Algorithms and Data Structures: inverted index]
*[http://mg4j.di.unimi.it Managing Gigabytes for Java] a free full-text search engine for large document collections written in Java.
*[http://lucene.apache.org/java/docs/ Lucene] - Apache Lucene is a full-featured text search engine library written in Java.
*[http://sphinxsearch.com/ Sphinx Search] - Open source high-performance, full-featured text search engine library used by craigslist and others employing an inverted index.
*[http://rosettacode.org/wiki/Inverted_Index Example implementations] on [[Rosetta Code]]
* [http://www.vision.caltech.edu/malaa/software/research/image-search/ Caltech Large Scale Image Search Toolbox]: a Matlab toolbox implementing Inverted File Bag-of-Words image search.

[[Category:Data management]]
[[Category:Search algorithms]]
[[Category:Database index techniques]]
[[Category:Substring indices]]</text>
      <sha1>0ake1ml4qryliozkopdv1puxvpmyr05</sha1>
    </revision>
  </page>
  <page>
    <title>Clustered file system</title>
    <ns>0</ns>
    <id>10459749</id>
    <revision>
      <id>758821001</id>
      <parentid>746471842</parentid>
      <timestamp>2017-01-07T19:41:02Z</timestamp>
      <contributor>
        <username>Goose121</username>
        <id>26303422</id>
      </contributor>
      <comment>Removed merge template</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="13843" xml:space="preserve">{{distinguish|Data cluster}}
{{redirect2|Network filesystem|Parallel file system|the Sun NFS protocol|Network File System|the IBM GPFS protocol|IBM General Parallel File System}}
{{multiple|
{{refimprove|date=December 2015}}
{{cleanup|date=December 2013|reason=Merges need to be smoothed over}}
}}

A '''clustered file system''' is a [[file system]] which is shared by being simultaneously [[Mount (computing)|mounted]] on multiple [[Server (computing)|servers]].  There are several approaches to [[computer cluster|clustering]], most of which do not employ a clustered file system (only [[direct attached storage]] for each node).  Clustered file systems can provide features like location-independent addressing and redundancy which improve reliability or reduce the complexity of the other parts of the cluster.  '''Parallel file systems''' are a type of clustered file system that spread data across multiple storage nodes, usually for redundancy or performance.&lt;ref&gt;http://www.dell.com/downloads/global/power/ps2q05-20040179-Saify-OE.pdf&lt;/ref&gt;

== {{Anchor|SHARED-DISK}}Shared-disk file system ==
A '''shared-disk file system''' uses a [[storage area network|storage-area network]] (SAN) to allow multiple computers to gain  direct disk access at the [[Block (data storage)|block level]].  Access control and translation from file-level operations that applications use to block-level operations used by the SAN must take place on the client node.  The most common type of clustered file system, the shared-disk file system &amp;mdash;by adding mechanisms for [[concurrency control]]&amp;mdash;provides a consistent and [[serialization|serializable]] view of the file system, avoiding corruption and unintended [[data loss]] even when multiple clients try to access the same files at the same time. Shared-disk file-systems commonly employ some sort of [[Fencing (computing)|fencing]] mechanism to prevent data corruption in case of node failures, because an unfenced device can cause data corruption if it loses communication with its sister nodes and tries to access the same information other nodes are accessing.

The underlying storage area network may use any of a number of block-level protocols, including [[SCSI]], [[iSCSI]], [[HyperSCSI]], [[ATA over Ethernet]] (AoE), [[Fibre Channel]], [[network block device]], and [[InfiniBand]].

There are different architectural approaches to a shared-disk filesystem. Some distribute file information across all the servers in a cluster (fully distributed). Others utilize a centralized [[metadata]] server. Both achieve the same result of enabling all servers to access all the data on a shared storage device.{{Citation needed|date=December 2009}}

=== Examples ===
{{Div col||25em}}
* [[Blue Whale Clustered file system]] (BWFS)
* [[Silicon Graphics]] (SGI) clustered file system ([[CXFS]])
* [[Veritas Cluster File System]]
* DataPlow [[Nasan]] File System
* [[IBM General Parallel File System]] (GPFS)
* [[LizardFS]]
* [[Lustre (file system)|Lustre]]
* Microsoft [[Cluster Shared Volumes]] (CSV)
* [[OCFS|Oracle Cluster File System]] (OCFS)
* PolyServe storage solutions
* [[Quantum Corporation|Quantum]] [[StorNext File System|StorNext]] File System (SNFS), ex ADIC, ex CentraVision File System (CVFS)
* Red Hat [[Global File System]] (GFS)
* Sun [[QFS]]
* TerraScale Technologies TerraFS
* Versity VSM
* [[VMware VMFS]]
* [[Xsan]]
{{Div col end}}

=={{Anchor|DISTRIBUTED-FS}}Distributed file systems==
''Distributed file systems'' do not share block level access to the same storage but use a network [[protocol (computing)|protocol]].&lt;ref&gt;Silberschatz, Galvin (1994). ''Operating System concepts'', chapter 17 ''Distributed file systems''. Addison-Wesley Publishing Company. ISBN 0-201-59292-4.&lt;/ref&gt;&lt;ref name="ostep-1"&gt;{{citation|title=Sun's Network File System|url=http://pages.cs.wisc.edu/~remzi/OSTEP/dist-nfs.pdf|publisher= Arpaci-Dusseau Books|date = 2014|first1 = Remzi H.|last1 =Arpaci-Dusseau|first2=Andrea C.|last2 = Arpaci-Dusseau}}&lt;/ref&gt;  These are commonly known as ''network file systems'', even though they are not the only file systems that use the network to send data.{{citation needed|date=March 2013}}  Distributed file systems can restrict access to the file system depending on [[access list]]s or [[Capability-based security|capabilities]] on both the servers and the clients, depending on how the protocol is designed.

The difference between a distributed file system and a [[distributed data store]] is that a distributed file system allows files to be accessed using the same interfaces and semantics as local files{{snd}} for example, mounting/unmounting, listing directories, read/write at byte boundaries, system's native permission model.  Distributed data stores, by contrast, require using a different API or library and have different semantics (most often those of a database).{{cn|date=June 2016}}

A distributed file system may also be created by software implementing IBM's [[Distributed Data Management Architecture]] (DDM), in which programs running on one computer use local interfaces and semantics to create, manage and access files located on other networked computers.  All such client requests are trapped and converted to equivalent messages defined by the DDM. Using protocols also defined by the DDM, these messages are transmitted to the specified remote computer on which a DDM server program interprets the messages and uses the file system interfaces of that computer to locate and interact with the specified file.

===Design goals===
Distributed file systems may aim for "transparency" in a number of aspects.  That is, they aim to be "invisible" to client programs, which "see" a system which is similar to a local file system.  Behind the scenes, the distributed file system handles locating files, transporting data, and potentially providing other features listed below.

* ''Access transparency'' is that clients are unaware that files are distributed and can access them in the same way as local files are accessed.
* ''Location transparency''; a consistent name space exists encompassing local as well as remote files. The name of a file does not give its location.
* ''Concurrency transparency''; all clients have the same view of the state of the file system. This means that if one process is modifying a file, any other processes on the same system or remote systems that are accessing the files will see the modifications in a coherent manner.
* ''Failure transparency''; the client and client programs should operate correctly after a server failure.
* ''Heterogeneity''; file service should be provided across different hardware and operating system platforms.
* ''Scalability''; the file system should work well in small environments (1 machine, a dozen machines) and also scale gracefully to huge ones (hundreds through tens of thousands of systems).
* ''Replication transparency''; to support scalability, we may wish to replicate files across multiple servers. Clients should be unaware of this.
* ''Migration transparency''; files should be able to move around without the client's knowledge.

===History===
The [[Incompatible Timesharing System]] used virtual devices for transparent inter-machine file system access in the 1960s.  More file servers were developed in the 1970s. In 1976 [[Digital Equipment Corporation]] created the [[File Access Listener]] (FAL), an implementation of the [[Data Access Protocol]] as part of [[DECnet]] Phase II which became the first widely used network file system. In 1985 [[Sun Microsystems]] created the file system called "[[Network File System (protocol)|Network File System]]" (NFS) which became the first widely used [[Internet Protocol]] based network file system.&lt;ref name="ostep-1" /&gt;  Other notable network file systems are [[Andrew File System]] (AFS), [[Apple Filing Protocol]] (AFP), [[NetWare Core Protocol]] (NCP), and [[Server Message Block]] (SMB) which is also known as Common Internet File System (CIFS).

In 1986, [[IBM]] announced client and server support for Distributed Data Management Architecture (DDM) for the [[System/36]], [[System/38]], and IBM mainframe computers running [[CICS]].  This was followed by the support for [[IBM Personal Computer]], [[AS/400]], IBM mainframe computers under the [[MVS]] and [[VSE (operating system)|VSE]] operating systems, and [[FlexOS]].   DDM also became the foundation for [[DRDA|Distributed Relational Database Architecture]], also known as DRDA.

=== Examples ===
{{Main|List of file systems#Distributed file systems|l1 = List of distributed file systems}}
{{Div col||25em}}
* [[BeeGFS]] (Fraunhofer)
* [[Ceph (software)|Ceph]] (Inktank, Red Hat, SUSE)
* [[Distributed File System (Microsoft)|Windows Distributed File System (DFS)]] (Microsoft)
* [[Infinit (file system)|Infinit]]
* [[Gfarm file system|GfarmFS]]
* [[GlusterFS]] (Red Hat)
* [[Google file system|GFS]] (Google Inc.)
* [[Hadoop distributed file system|HDFS]] (Apache Software Foundation)
* [[InterPlanetary File System|IPFS]]
* iRODS
* [[LizardFS]] (Skytechnology)
* [[Moose File System|MooseFS]] (Core Technology / Gemius)
* [[ObjectiveFS]]
* [[OneFS]] (EMC Isilon)
* OpenIO
* [[OrangeFS]] (Clemson University, Omnibond Systems), formerly [[Parallel Virtual File System]]
* [[Panasas|Panfs]] (Panasas)
* [[Parallel Virtual File System]] (Clemson University, Argonne National Laboratory, Ohio Supercomputer Center)
* [[RozoFS]] (Rozo Systems)
* Torus (CoreOS)
* [[XtreemFS]]
{{Div col end}}

==Network-attached storage==
{{Main|Network-attached storage}}

Network-attached storage (NAS) provides both storage and a file system, like a shared disk file system on top of a storage area network (SAN).  NAS typically uses file-based protocols (as opposed to block-based protocols a SAN would use) such as [[Network File System (protocol)|NFS]] (popular on [[UNIX]] systems), SMB/CIFS ([[Server Message Block|Server Message Block/Common Internet File System]]) (used with MS Windows systems), [[Apple Filing Protocol|AFP]] (used with [[Macintosh|Apple Macintosh]] computers), or NCP (used with [[Novell Open Enterprise Server|OES]] and [[NetWare|Novell NetWare]]).

==Design considerations==

===Avoiding single point of failure===

The failure of disk hardware or a given storage node in a cluster can create a [[single point of failure]] that can result in [[data loss]] or unavailability.  [[Fault tolerance]] and high availability can be provided through [[Replication (computing)|data replication]] of one sort or another, so that data remains intact and available despite the failure of any single piece of equipment.  For examples, see the lists of [[List of file systems#Distributed fault-tolerant file systems|distributed fault-tolerant file systems]] and [[List of file systems#Distributed parallel fault-tolerant file systems|distributed parallel fault-tolerant file systems]].

===Performance===

A common [[performance]] [[measurement]] of a clustered file system is the amount of time needed to satisfy service requests.  In conventional systems, this time consists of a disk-access time and a small amount of [[Central processing unit|CPU]]-processing time.  But in a clustered file system, a remote access has additional overhead due to the distributed structure.  This includes the time to deliver the request to a server, the time to deliver the response to the client, and for each direction, a CPU overhead of running the [[communication protocol]] [[software]].

===Concurrency===

Concurrency control becomes an issue when more than one person or client is accessing the same file or block and want to update it.  Hence updates to the file from one client should not interfere with access and updates from other clients. This problem is more complex with file systems due to concurrent overlapping writes, where different writers write to overlapping regions of the file concurrently.&lt;ref&gt;Pessach, Yaniv (2013). ''Distributed Storage: Concepts, Algorithms, and Implementations''. ISBN 978-1482561043.&lt;/ref&gt; This problem is usually handled by [[concurrency control]] or [[lock (computer science)|locking]] which may either be built into the file system or provided by an add-on protocol.

==History==
IBM mainframes in the 1970s could share physical disks and file systems if each machine had its own channel connection to the drives' control units. In the 1980s, [[Digital Equipment Corporation]]'s [[TOPS-20]] and [[OpenVMS]] clusters (VAX/ALPHA/IA64) included shared disk file systems.{{Citation needed|date=May 2016}}

==See also==
{{Div col||25em}}
* [[Network-attached storage]]
* [[Storage area network]]
* [[Shared resource]]
* [[Direct-attached storage]]
* [[Peer-to-peer file sharing]]
* [[Disk sharing]]
* [[Distributed data store]]
* [[Distributed file system for cloud]]
* [[Global file system]]
* [[Gopher (protocol)]]
* [[List of file systems#Distributed file systems|List of distributed file systems]]
* [[CacheFS]]
* [[RAID]]
{{Div col end}}

==References==
{{reflist}}

==Further reading==
* [http://www.cloudbus.org/reports/DistributedStorageTaxonomy.pdf A Taxonomy of Distributed Storage Systems]
* [http://trac.nchc.org.tw/grid/raw-attachment/wiki/jazz/09-05-22/A_Taxonomy_and_Survey_on_Distributed_File_Systems.pdf A Taxonomy and Survey on Distributed File Systems]
* [http://www.cis.upenn.edu/~bcpierce/courses/dd/papers/satya89survey.ps A survey of distributed file systems]
* [http://www.snia-europe.org/objects_store/Christian_Bandulet_SNIATutorial%20Basics_EvolutionFileSystems.pdf The Evolution of File Systems]

{{File systems|state=collapsed}}

[[Category:Computer file systems]]
[[Category:Shared disk file systems| ]]
[[Category:Storage area networks]]
[[Category:Distributed file systems| ]]
[[Category:Data management]]
[[Category:Distributed data storage]]
[[Category:Network file systems]]</text>
      <sha1>gf3rwvd42jmdodkazhwpy8r4npx3isw</sha1>
    </revision>
  </page>
  <page>
    <title>System of record</title>
    <ns>0</ns>
    <id>2100046</id>
    <revision>
      <id>753419366</id>
      <parentid>740941382</parentid>
      <timestamp>2016-12-07T01:36:37Z</timestamp>
      <contributor>
        <username>Mgibby5</username>
        <id>17240541</id>
      </contributor>
      <minor />
      <comment>grammar, caps.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3114" xml:space="preserve">A '''system of record''' (SOR) or '''Source System of Record''' (SSoR) is a [[Data Management|data management]] term for an [[information]] storage system (commonly implemented on a [[computer system]]) that is the authoritative data source for a given [[data element]] or piece of information. The need to identify systems of record can become acute in organizations where [[management information system]]s have been built by taking output data from multiple source systems, re-processing this data, and then re-presenting the result for a new business use.

In these cases, multiple information systems may disagree about the same piece of information. These disagreements may stem from semantic differences, differences in opinion, use of different sources, differences in the timing of the [[Extract, transform, load|ETL]] extracts that create the data they report against, or may simply be the result of bugs. 

The [[integrity]] and [[validity]] of any data set is open to question when there is no [[tracing (software)|traceable]] connection to a good source, such as a known System of Record. Where the integrity of the data is vital, if there is an agreed system of record, the data element must either be linked to, or extracted directly from it. In other cases, the provenance and estimated data quality should be documented. 

The "system of record" approach is a good fit for environments where both:
* there is a single authority over all data consumers, and 
* all consumers have similar needs 
In diverse environments, one instead needs to support the presence of multiple opinions. Consumers may accept different authorities or may differ on what constitutes an authoritative source -- researchers may prefer carefully vetted data, while tactical military systems may require the most recent credible report.

==See also==
* [[Single Source of Truth]] practice of using one source for a particular data element
* [[Privacy Act of 1974]] United States law including requirement for agencies to publish System Of Records Notices (SORN) in the [[Federal Register]] to identify the system and describe the use of individuals data.
* [[Master Data Management]] defining the handling of master data
* [[Systems of Engagement]] &#8212; more decentralized systems that incorporate technologies which encourage peer interactions

==References==
* {{cite web |title=The System of Record in the Global Data Warehouse | url=http://www.information-management.com/issues/20030501/6645-1.html  |publisher = Information Management |accessdate=2007-12-18 |author=[[Bill Inmon]]  |date=May 2003 |work= }}
* {{cite web 
| title = The Golden Copy 
| url = http://adam.goucher.ca/?p=72
| last = Goucher
| first = Adam
| date = 2006-04-26
| accessdate = 2013-04-30}}
* {{cite web
| title = The Move from Systems of Record to Systems Of Engagement
| url = http://www.forbes.com/sites/joshbersin/2012/08/16/the-move-from-systems-of-record-to-systems-of-engagement/
| last = Bersin
| first = Josh
| date = 2012-08-16
| accessdate = 2013-04-30}}


[[Category:Information systems]]
[[Category:Data management]]


{{compu-stub}}</text>
      <sha1>ck4sf64teutbm27qeiwzv8vw98mqc6n</sha1>
    </revision>
  </page>
  <page>
    <title>Single source of truth</title>
    <ns>0</ns>
    <id>6831362</id>
    <revision>
      <id>757107301</id>
      <parentid>733619064</parentid>
      <timestamp>2016-12-28T21:06:08Z</timestamp>
      <contributor>
        <ip>69.4.118.251</ip>
      </contributor>
      <comment>SPoT is NOT the same thing as SSoT.  SPOT is where sources merge to create a single point of truth and a POINT of Truth isn't always a or the SOURCE.  May be out dated but not sure there is really a SSOT these days since our enterprises are so diverse.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7081" xml:space="preserve">{{Unreferenced|date=December 2009}}

In [[information systems]] design and theory, '''single source of truth''' ('''SSOT'''), is the practice of structuring information models and associated [[Database schema|schemata]] such that every data element is stored exactly once (e.g., in no more than a single row of a single table).  Any possible linkages to this data element (possibly in other areas of the relational schema or even in distant [[federated database|federated databases]]) are by [[Reference (computer science)|reference]] only.  Because all other locations of the data just refer back to the primary "source of truth" location, updates to the data element in the primary location propagate to the entire system without the possibility of a duplicate value somewhere being forgotten.

Deployment of an SSOT architecture is becoming increasingly important in enterprise settings where incorrectly linked duplicate or de-normalized data elements (a direct consequence of intentional or unintentional [[denormalization]] of any explicit data model) poses a risk for retrieval of outdated, and therefore incorrect, information.  A common example would be the [[electronic health record]], where it is imperative to accurately validate patient identity against a single referential repository, which serves as the SSOT.  Duplicate representations of data within the enterprise would be implemented by the use of [[pointer (computer programming)|pointer]]s rather than duplicate database tables, rows, or cells.  This ensures that data updates to elements in the authoritative location are comprehensively distributed to all [[federated database]] constituencies in the larger overall enterprise architecture.{{fact|date=July 2012}}

SSOT systems provide data that is authentic, relevant, and referable.&lt;ref&gt;IBM Smarter Planet - Operational risk management for financial services[http://www.ibm.com/smarterplanet/za/en/banking_technology/nextsteps/solution/Z017038Z16405R75.html]&lt;/ref&gt;

==Implementation==
The "ideal" implementation of SSOT as described above is rarely possible in most enterprises. This is because many organisations have multiple information systems, each of which needs access to data relating to the same entities (e.g., customer). Often these systems are purchased "off-the-shelf" from vendors and cannot be modified in non-trivial ways. Each of these various systems therefore needs to store its own version of common data or entities, and therefore each system must retain its own copy of a record (hence immediately violating the SSOT approach defined above). For example, an ERP (enterprise resource planning) system (such as SAP or Oracle e-Business Suite) may store a customer record; the CRM (customer relationship management) system also needs a copy of the customer record (or part of it) and the warehouse despatch system might also need a copy of some or all of the customer data (e.g., shipping address). In cases where vendors do not support such modifications, it is not always possible to replace these records with pointers to the SSOT.

For organisations (with more than one information system) wishing to implement a Single Source of Truth (without modifying all but one master system to store pointers to other systems for all entities), three supporting technologies are commonly used:{{fact|date=July 2012}}

*[[Enterprise service bus]] (ESB)
*[[Master data management]] (MDM)
*[[Data warehouse]] (DW)

===Enterprise service bus (ESB)===
An enterprise service bus (ESB) allows any number of systems in an organisation to receive updates of data that has changed in another system. To implement a Single Source of Truth, a single source system of correct data for any entity must be identified. Changes to this entity (creates, updates, and deletes) are then published via the ESB; other systems which need to retain a copy of that data subscribe to this update, and update their own records accordingly. For any given entity, the master source must be identified (sometimes called the Golden Record). It should be noted that any given system could publish (be the source of truth for) information on a particular entity (e.g., customer) and also subscribe to updates from another system for information on some other entity (e.g., product).{{fact|date=July 2012}}

An alternative approach is point-to-point data updates, but these become exponentially more expensive to maintain as the number of systems increases, and this approach is increasingly out of favour as an IT architecture.{{fact|date=July 2012}}

===Master data management (MDM)===
An MDM system can act as the source of truth for any given entity that might not necessarily have an alternative "source of truth" in another system. Typically the MDM acts as a hub for multiple systems, many of which could allow (be the source of truth for) updates to different aspects of information on a given entity. For example, the CRM system may be the "source of truth" for most aspects of the customer, and is updated by a call centre operator. However, a customer may (for example) also update their address via a customer service web site, with a different back-end database from the CRM system. The MDM application receives updates from multiple sources, acts as a broker to determine which updates are to be regarded as authoritative (the Golden Record) and then syndicates this updated data to all subscribing systems. The MDM application normally requires an ESB to syndicate its data to multiple subscribing systems.&lt;ref&gt;BAYT Job Site - June 2014[http://www.bayt.com/en/specialties/q/7370/what-are-the-top-business-processes-and-applications-that-need-master-data-management/]&lt;/ref&gt;
[[Customer Data Integration]] (CDI), as a common application of Master Data Management, is sometimes abbreviated CDI-MDM.{{fact|date=July 2012}}

===Data warehouse (DW)===
While the primary purpose of a data warehouse is to support reporting and analysis of data that has been combined from multiple sources, the fact that such data has been combined (according to business logic embedded in the [[Extract, transform, load|data transformation and integration processes]]) means that the data warehouse is often used as a ''de facto'' SSOT. Generally, however, the data available from the data warehouse is not used to update other systems; rather the DW becomes the "single source of truth" for reporting to multiple stakeholders. In this context, the Data Warehouse is more correctly referred to as a "[[single version of the truth]]" since other versions of the truth exist in its operational data sources (no data originates in the DW;  it is simply a reporting mechanism for data loaded from operational systems).{{fact|date=July 2012}}

==See also==
*[[Don't repeat yourself]] (DRY)
*[[SOLID (object-oriented design)]]
*[[Database normalization]]
*[[Single version of the truth]]
*[[System of record]]

==References==
{{reflist}}

==External links==


{{DEFAULTSORT:Single Source Of Truth}}
[[Category:Data modeling]]
[[Category:Database normalization]]
[[Category:Data management]]</text>
      <sha1>2p4drvlotdg4yulxms33xotftpxjne3</sha1>
    </revision>
  </page>
  <page>
    <title>Abstraction (software engineering)</title>
    <ns>0</ns>
    <id>60491</id>
    <revision>
      <id>754117328</id>
      <parentid>754115292</parentid>
      <timestamp>2016-12-10T23:04:57Z</timestamp>
      <contributor>
        <username>Greenrd</username>
        <id>15476</id>
      </contributor>
      <comment>/* Specification languages */ Specification phase is specific to waterfall projects, agile projects do not have a specification phase as such</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="30265" xml:space="preserve">{{Refimprove|date=June 2011}}
{{Quote box|quote=The essence of abstractions is preserving information that is relevant in a given context, and forgetting information that is irrelevant in that context.|source=&#8211; John V. Guttag&lt;ref&gt;{{Cite book | edition = Spring 2013 | publisher = The MIT Press | isbn = 9780262519632 | last = Guttag | first = John V. | title = Introduction to Computation and Programming Using Python | location = Cambridge, Massachusetts | date = 2013-01-18}}&lt;/ref&gt;|width=25%}}

In [[software engineering]] and [[computer science]], '''abstraction''' is a technique for arranging complexity of computer systems. It works by establishing a level of complexity on which a person interacts with the system, suppressing the more complex details below the current level. The programmer works with an idealized interface (usually well defined) and can add additional levels of functionality that would otherwise be too complex to handle. For example, a programmer writing code that involves numerical operations may not be interested in the way numbers are represented in the underlying hardware (e.g. whether they're ''16 bit'' or ''32 bit [[Integer (computer science)|integers]]''), and where those details have been suppressed it can be said that they were ''abstracted away'', leaving simply ''numbers'' with which the programmer can work. In addition, a task of sending an email message across continents would be extremely complex if the programmer had to start with a piece of fiber optic cable and basic hardware components. By using layers of complexity that have been created to abstract away the physical cables and network layout, and presenting the programmer with a virtual data channel, this task is manageable.

Abstraction can apply to control or to data: '''Control abstraction''' is the abstraction of actions while '''data abstraction''' is that of [[data structures]].

* Control abstraction involves the use of [[subroutine]]s and [[control flow]] abstractions
* Data abstraction allows handling pieces of data in meaningful ways. For example, it is the basic motivation behind the [[datatype]].

The notion of an [[object (computer science)|object]] in object-oriented programming can be viewed as a way to combine abstractions of data and code.

The same abstract definition can be used as a common [[Interface (computer science)|interface]] for a family of objects with different implementations and behaviors but which share the same meaning. The [[Inheritance (computer science)|inheritance]] mechanism in object-oriented programming can be used to define an [[Class (computer science)#Abstract|abstract class]] as the common interface.

The recommendation that programmers use abstractions whenever suitable in order to avoid duplication (usually [[code duplication|of code]]) is known as the [[Abstraction principle (computer programming)|abstraction principle]]. The requirement that a programming language provide suitable abstractions is also called the abstraction principle.

==Rationale==
Computing mostly operates independently of the concrete world: The hardware implements a [[model of computation]] that is interchangeable with others. The software is structured in [[software architecture|architecture]]s to enable humans to create the enormous systems by concentrating on a few issues at a time. These architectures are made of specific choices of abstractions. [[Greenspun's Tenth Rule]] is an [[aphorism]] on how such an architecture is both inevitable and complex.

A central form of abstraction in computing is language abstraction: new artificial languages are developed to express specific aspects of a system. ''[[Modeling languages]]'' help in planning. ''[[Computer language]]s'' can be processed with a computer. An example of this abstraction process is the generational development of [[programming language]]s from the [[First-generation programming language|machine language]] to the [[Second-generation programming language|assembly language]] and the [[Third-generation programming language|high-level language]]. Each stage can be used as a stepping stone for the next stage. The language abstraction continues for example in [[scripting language]]s and [[domain-specific programming language]]s.

Within a programming language, some features let the programmer create new abstractions. These include [[subroutine]]s, [[module (programming)|modules]], [[polymorphism (computer science)|polymorphism]], and [[software component]]s. Some other abstractions such as [[software design pattern]]s and [[software architecture#Architecture examples|architectural styles]] remain invisible to a [[translator (computing)|translator]] and operate only in the design of a system.

Some abstractions try to limit the range of concepts a programmer needs to be aware of, by completely hiding the abstractions that they in turn are built on. The software engineer and writer [[Joel Spolsky]] has criticised these efforts by claiming that all abstractions are ''[[leaky abstraction|leaky]]'' &#8212; that they can never completely hide the details below;&lt;ref&gt;{{cite web|last1=Spolsky|first1=Joel|title=The Law of Leaky Abstractions|url=http://www.joelonsoftware.com/articles/LeakyAbstractions.html}}&lt;/ref&gt; however, this does not negate the usefulness of abstraction.

Some abstractions are designed to interoperate with other abstractions - for example, a programming language may contain a [[foreign function interface]] for making calls to the lower-level language.

==Language features==

===Programming languages===
{{Main|Programming language}}

Different programming languages provide different types of abstraction, depending on the intended applications for the language. For example:

* In [[object-oriented programming language]]s such as [[C++]], [[Object Pascal]], or [[Java (programming language)|Java]], the concept of '''abstraction''' has itself become a declarative statement &#8211; using the [[keyword (computer programming)|keyword]]s ''&lt;code&gt;virtual&lt;/code&gt;'' (in [[C++]]) or ''&lt;code&gt;abstract&lt;/code&gt;''&lt;ref name="Oracle Java abstract"&gt;{{cite web|title=Abstract Methods and Classes|url=http://docs.oracle.com/javase/tutorial/java/IandI/abstract.html|website=The Java&#8482; Tutorials|publisher=Oracle|accessdate=4 September 2014}}&lt;/ref&gt; and ''&lt;code&gt;interface&lt;/code&gt;''&lt;ref name="Oracle Java interface"&gt;{{cite web|title=Using an Interface as a Type|url=http://docs.oracle.com/javase/tutorial/java/IandI/interfaceAsType.html|website=The Java&#8482; Tutorials|publisher=Oracle|accessdate=4 September 2014}}&lt;/ref&gt; (in [[Java (programming language)|Java]]). After such a declaration, it is the responsibility of the programmer to implement a [[Class (computer science)|class]] to instantiate the [[Object (computer science)|object]] of the declaration.
* [[Functional programming language]]s commonly exhibit abstractions related to functions, such as [[lambda abstraction]]s (making a term into a function of some variable) and [[higher-order function]]s (parameters are functions). &lt;!-- This has to be merged in the following sections. --&gt;
* Modern members of the Lisp programming language family such as [[Clojure]], [[Scheme (programming language)|Scheme]] and [[Common Lisp]] support [[Macro (computer science)#Syntactic macros|macro systems]] to allow syntactic abstraction. Other programming languages such as [[Scala (programming language)|Scala]] also have macros, or very similar [[metaprogramming]] features (for example, [[Haskell (programming language)|Haskell]] has [[Template Haskell]], and [[OCaml]] has [[MetaOCaml]]). These can allow a programmer to eliminate [[boilerplate code]], abstract away tedious function call sequences, implement new [[Control flow|control flow structures]], and implement [[Domain-specific language|Domain Specific Languages (DSLs)]], which allow domain-specific concepts to be expressed in concise and elegant ways. All of these, when used correctly, improve both the programmer's efficiency and the clarity of the code by making the intended purpose more explicit. A consequence of syntactic abstraction is also that any Lisp dialect and in fact almost any programming language can, in principle, be implemented in any modern Lisp with significantly reduced (but still non-trivial in some cases) effort when compared to "more traditional" programming languages such as [[Python (programming language)|Python]], [[C (programming language)|C]] or [[Java (programming language)|Java]].

===Specification methods===
{{Main|Formal specification}}

Analysts have developed various methods to formally specify software systems.  Some known methods include:

* Abstract-model based method (VDM, Z);
* Algebraic techniques (Larch, CLEAR, OBJ, ACT ONE, CASL);
* Process-based techniques (LOTOS, SDL, Estelle);
* Trace-based techniques (SPECIAL, TAM);
* Knowledge-based techniques (Refine, Gist).

===Specification languages===
{{Main|Specification language}}

Specification languages generally rely on abstractions of one kind or another, since specifications are typically defined earlier in a project, (and at a more abstract level) than an eventual implementation. The [[Unified Modeling Language|UML]] specification language, for example, allows the definition of ''abstract'' classes, which in a waterfall project, remain abstract during the architecture and specification phase of the project.

==Control abstraction==
{{Main|Control flow}}

Programming languages offer control abstraction as one of the main purposes of their use. Computer machines understand operations at the very low level such as moving some bits from one location of the memory to another location and producing the sum of two sequences of bits. Programming languages allow this to be done in the higher level. For example, consider this statement written in a [[Pascal (programming language)|Pascal]]-like fashion:

:&lt;code&gt;a := (1 + 2) * 5&lt;/code&gt;

To a human, this seems a fairly simple and obvious calculation (''"one plus two is three, times five is fifteen"''). However, the low-level steps necessary to carry out this evaluation, and return the value "15", and then assign that value to the variable "a", are actually quite subtle and complex. The values need to be converted to binary representation (often a much more complicated task than one would think) and the calculations decomposed (by the compiler or interpreter) into assembly instructions (again, which are much less intuitive to the programmer: operations such as shifting a binary register left, or adding the binary complement of the contents of one register to another, are simply not how humans think about the abstract arithmetical operations of addition or multiplication). Finally, assigning the resulting value of "15" to the variable labeled "a", so that "a" can be used later, involves additional 'behind-the-scenes' steps of looking up a variable's label and the resultant location in physical or virtual memory, storing the binary representation of "15" to that memory location, etc.

Without control abstraction, a programmer would need to specify ''all'' the register/binary-level steps each time they simply wanted to add or multiply a couple of numbers and assign the result to a variable. Such duplication of effort has two serious negative consequences:

# it forces the programmer to constantly repeat fairly common tasks every time a similar operation is needed
# it forces the programmer to program for the particular hardware and instruction set

===Structured programming===
{{Main|Structured programming}}

Structured programming involves the splitting of complex program tasks into smaller pieces with clear flow-control and interfaces between components, with reduction of the complexity potential for side-effects.

In a simple program, this may aim to ensure that loops have single or obvious exit points and (where possible) to have single exit points from functions and procedures.

In a larger system, it may involve breaking down complex tasks into many different modules. Consider a system which handles payroll on ships and at shore offices:

* The uppermost level may feature a menu of typical end-user operations.
* Within that could be standalone executables or libraries for tasks such as signing on and off employees or printing checks.
* Within each of those standalone components there could be many different source files, each containing the program code to handle a part of the problem, with only selected interfaces available to other parts of the program. A sign on program could have source files for each data entry screen and the database interface (which may itself be a standalone third party library or a statically linked set of library routines).
*Either the database or the payroll application also has to initiate the process of exchanging data with between ship and shore, and that data transfer task will often contain many other components.

These layers produce the effect of isolating the implementation details of one component and its assorted internal methods from the others. Object-oriented programming embraces and extends this concept.

==Data abstraction==
{{Main|Abstract data type}}

Data abstraction enforces a clear separation between the ''abstract'' properties of a [[data type]] and the ''concrete'' details of its implementation. The abstract properties are those that are visible to client code that makes use of the data type&#8212;the ''interface'' to the data type&#8212;while the concrete implementation is kept entirely private, and indeed can change, for example to incorporate efficiency improvements over time. The idea is that such changes are not supposed to have any impact on client code, since they involve no difference in the abstract behaviour.

For example, one could define an [[abstract data type]] called ''lookup table'' which uniquely associates ''keys'' with ''values'', and in which values may be retrieved by specifying their corresponding keys. Such a lookup table may be implemented in various ways: as a [[hash table]], a [[binary search tree]], or even a simple linear [[List (computing)|list]] of (key:value) pairs. As far as client code is concerned, the abstract properties of the type are the same in each case.

Of course, this all relies on getting the details of the interface right in the first place, since any changes there can have major impacts on client code. As one way to look at this: the interface forms a ''contract'' on agreed behaviour between the data type and client code; anything not spelled out in the contract is subject to change without notice.

&lt;!-- This makes no sense to me. [[User:TakuyaMurata|Taku]] 07:13, 19 June 2005 (UTC) --&gt;
Languages that implement data abstraction include [[Ada programming language|Ada]] and [[Modula-2]]. [[Object-oriented]] languages are commonly claimed{{By whom|date=March 2009}} to offer data abstraction; however, their [[Inheritance (computer science)|inheritance]] concept tends to put information in the interface that more properly belongs in the implementation; thus, changes to such information ends up impacting client code, leading directly to the [[Fragile binary interface problem]].

==Abstraction in object oriented programming==
{{Main|Object (computer science)}}

In [[object-oriented programming]] theory, '''abstraction''' involves the facility to define objects that represent abstract "actors" that can perform work, report on and change their state, and "communicate" with other objects in the system. The term [[Encapsulation (object-oriented programming)|encapsulation]] refers to the hiding of [[state (computer science)|state]] details, but extending the concept of ''data type'' from earlier programming languages to associate ''behavior'' most strongly with the data, and standardizing the way that different data types interact, is the beginning of '''abstraction'''.  When abstraction proceeds into the operations defined, enabling objects of different types to be substituted, it is called [[polymorphism (computer science)|polymorphism]]. When it proceeds in the opposite direction, inside the types or classes, structuring them to simplify a complex set of relationships, it is called [[Delegation (object-oriented programming)|delegation]] or [[Inheritance (computer science)|inheritance]].

Various object-oriented programming languages offer similar facilities for abstraction, all to support a general strategy of [[polymorphism (computer science)|polymorphism]] in object-oriented programming, which includes the substitution of one [[type in object-oriented programming|type]] for another in the same or similar role. Although not as generally supported, a [[configuration in object-oriented programming|configuration]] or image or package may predetermine a great many of these [[name binding|bindings]] at [[compile-time]], [[link-time]], or [[loadtime]]. This would leave only a minimum of such bindings to change at [[Run time (program lifecycle phase)|run-time]].

[[Common Lisp Object System]] or [[Self (programming language)|Self]], for example, feature less of a class-instance distinction and more use of delegation for [[polymorphism in object-oriented programming|polymorphism]]. Individual objects and functions are abstracted more flexibly to better fit with a shared functional heritage from [[Lisp programming language|Lisp]].

C++ exemplifies another extreme: it relies heavily on [[generic programming|templates]] and [[method overloading|overloading]] and other static bindings at compile-time, which in turn has certain flexibility problems.

Although these examples offer alternate strategies for achieving the same abstraction, they do not fundamentally alter the need to support abstract nouns in code - all programming relies on an ability to abstract verbs as functions, nouns as data structures, and either as processes.

Consider for example a sample [[Java (programming language)|Java]] fragment to represent some common farm "animals" to a level of abstraction suitable to model simple aspects of their hunger and feeding. It defines an &lt;code&gt;Animal&lt;/code&gt; class to represent both the state of the animal and its functions:

&lt;source lang=java&gt;
public class Animal extends LivingThing
{
     private Location loc;
     private double energyReserves;

     public boolean isHungry() {
         return energyReserves &lt; 2.5;
     }
     public void eat(Food food) {
         // Consume food
         energyReserves += food.getCalories();
     }
     public void moveTo(Location location) {
         // Move to new location
         this.loc = location;
     }
}
&lt;/source&gt;
With the above definition, one could create objects of type &lt;tt&gt;Animal&lt;/tt&gt; and call their methods like this:

&lt;source lang=java&gt;
thePig = new Animal();
theCow = new Animal();
if (thePig.isHungry()) {
    thePig.eat(tableScraps);
}
if (theCow.isHungry()) {
    theCow.eat(grass);
}
theCow.moveTo(theBarn);
&lt;/source&gt;
In the above example, the class ''&lt;code&gt;Animal&lt;/code&gt;'' is an abstraction used in place of an actual animal, ''&lt;code&gt;LivingThing&lt;/code&gt;'' is a further abstraction (in this case a generalisation) of ''&lt;code&gt;Animal&lt;/code&gt;''.

If one requires a more differentiated hierarchy of animals &#8212; to differentiate, say, those who provide milk from those who provide nothing except meat at the end of their lives &#8212; that is an intermediary level of abstraction, probably DairyAnimal (cows, goats) who would eat foods suitable to giving good milk, and MeatAnimal (pigs, steers) who would eat foods to give the best meat-quality.

Such an abstraction could remove the need for the application coder to specify the type of food, so s/he could concentrate instead on the feeding schedule. The two classes could be related using [[Inheritance (computer science)|inheritance]] or stand alone, and the programmer could define varying degrees of [[polymorphism (computer science)|polymorphism]] between the two types. These facilities tend to vary drastically between languages, but in general each can achieve anything that is possible with any of the others. A great many operation overloads, data type by data type, can have the same effect at compile-time as any degree of inheritance or other means to achieve polymorphism. The class notation is simply a coder's convenience.

===Object-oriented design===
{{Main|Object-oriented design}}

Decisions regarding what to abstract and what to keep under the control of the coder become the major concern of object-oriented design and [[domain analysis]]&amp;mdash;actually determining the relevant relationships in the real world is the concern of [[object-oriented analysis]] or [[legacy analysis]].

In general, to determine appropriate abstraction, one must make many small decisions about scope (domain analysis), determine what other systems one must cooperate with (legacy analysis), then perform a detailed object-oriented analysis which is expressed within project time and budget constraints as an object-oriented design. In our simple example, the domain is the barnyard, the live pigs and cows and their eating habits are the legacy constraints, the detailed analysis is that coders must have the flexibility to feed the animals what is available and thus there is no reason to code the type of food into the class itself, and the design is a single simple Animal class of which pigs and cows are instances with the same functions. A decision to differentiate DairyAnimal would change the detailed analysis but the domain and legacy analysis would be unchanged&amp;mdash;thus it is entirely under the control of the programmer, and we refer to abstraction in object-oriented programming as distinct from abstraction in domain or legacy analysis.

==Considerations==
When discussing [[formal semantics of programming languages]], [[formal methods]] or [[abstract interpretation]], '''abstraction''' refers to the act of considering a less detailed, but safe, definition of the observed program behaviors. For instance, one may observe only the final result of program executions instead of considering all the intermediate steps of executions. Abstraction is defined to a '''concrete''' (more precise) model of execution.

Abstraction may be '''exact''' or '''faithful''' with respect to a property if one can answer a question about the property equally well on the concrete or abstract model. For instance, if we wish to know what the result of the evaluation of a mathematical expression involving only integers +, -, &#215;, is worth [[modular arithmetic|modulo]] ''n'', we need only perform all operations modulo ''n'' (a familiar form of this abstraction is [[casting out nines]]).

Abstractions, however, though not necessarily '''exact''', should be '''sound'''. That is, it should be possible to get sound answers from them&amp;mdash;even though the abstraction may simply yield a result of [[undecidable problem|undecidability]]. For instance, we may abstract the students in a class by their minimal and maximal ages; if one asks whether a certain person belongs to that class, one may simply compare that person's age with the minimal and maximal ages; if his age lies outside the range, one may safely answer that the person does not belong to the class; if it does not, one may only answer "I don't know".

The level of abstraction included in a programming language can influence its overall [[usability]]. The [[Cognitive dimensions]] framework includes the concept of ''abstraction gradient'' in a formalism. This framework allows the designer of a programming language to study the trade-offs between abstraction and other characteristics of the design, and how changes in abstraction influence the language usability.

Abstractions can prove useful when dealing with computer programs, because non-trivial properties of computer programs are essentially [[undecidable problem|undecidable]] (see [[Rice's theorem]]). As a consequence, automatic methods for deriving information on the behavior of computer programs either have to drop termination (on some occasions, they may fail, crash or never yield out a result), soundness (they may provide false information), or precision (they may answer "I don't know" to some questions).

Abstraction is the core concept of [[abstract interpretation]]. [[Model checking]] generally takes place on abstract versions of the studied systems.

==Levels of abstraction==
{{Main|Abstraction layer}}

Computer science commonly presents ''levels'' (or, less commonly, ''layers'') of abstraction, wherein each level represents a different model of the same information and processes, but uses a system of expression involving a unique set of objects and compositions that apply only to a particular domain.
&lt;ref&gt;[[Luciano Floridi]], [http://www.cs.ox.ac.uk/activities/ieg/research_reports/ieg_rr221104.pdf ''Levellism and the Method of Abstraction'']
IEG &#8211; Research Report 22.11.04&lt;/ref&gt;
Each relatively abstract, "higher" level builds on a relatively concrete, "lower" level, which tends to provide an increasingly "granular" representation. For example, gates build on electronic circuits, binary on gates, machine language on binary, programming language on machine language, applications and operating systems on programming languages. Each level is embodied, but not determined, by the level beneath it, making it a language of description that is somewhat self-contained.

===Database systems===
{{Main|Database management system}}

Since many users of database systems lack in-depth familiarity with computer data-structures, database developers often hide complexity through the following levels:

[[Image:Data abstraction levels.png|thumb|Data abstraction levels of a database system]]

'''Physical level:''' The lowest level of abstraction describes ''how'' a system actually stores data. The physical level describes complex low-level data structures in detail.

'''Logical level:''' The next higher level of abstraction describes ''what'' data the database stores, and what relationships exist among those data. The logical level thus describes an entire database in terms of a small number of relatively simple structures. Although implementation of the simple structures at the logical level may involve complex physical level structures, the user of the logical level does not need to be aware of this complexity. This referred to as [[physical data independence]]. [[Database administrator]]s, who must decide what information to keep in a database, use the logical level of abstraction.

'''View level:''' The highest level of abstraction describes only part of the entire database. Even though the logical level uses simpler structures, complexity remains because of the variety of information stored in a large database. Many users of a database system do not need all this information; instead, they need to access only a part of the database. The view level of abstraction exists to simplify their interaction with the system. The system may provide many [[view (database)|view]]s for the same database.

===Layered architecture===
{{Main|Abstraction layer}}
The ability to provide a [[design]] of different levels of abstraction can

* simplify the design considerably
* enable different role players to effectively work at various levels of abstraction
* support the portability of [[software artifact]]s (model-based ideally)

[[Systems design]] and [[Business process modeling|business process design]] can both use this. Some [[Software modeling|design processes]] specifically generate designs that contain various levels of abstraction.

Layered architecture partitions the concerns of the application into stacked groups (layers).
It is a technique used in designing computer software, hardware, and communications in which system or network components are isolated in layers so that changes can be made in one layer without affecting the others.

==See also==
* [[Abstraction principle (computer programming)]]
* [[Abstraction inversion]] for an anti-pattern of one danger in abstraction
* [[Abstract data type]] for an abstract description of a set of data
* [[Algorithm]] for an abstract description of a computational procedure
* [[Bracket abstraction]] for making a term into a function of a variable
* [[Data modeling]] for structuring data independent of the processes that use it
* [[Encapsulation (object-oriented programming)|Encapsulation]] for abstractions that hide implementation details
* [[Greenspun's Tenth Rule]] for an aphorism about an (the?) optimum point in the space of abstractions
* [[Higher-order function]] for abstraction where functions produce or consume other functions
* [[Lambda abstraction]] for making a term into a function of some variable
* [[List of abstractions (computer science)]]
* [[Program refinement|Refinement]] for the opposite of abstraction in computing

==References==
{{Reflist}}
{{FOLDOC}}

==Further reading==
{{refbegin}}
* {{cite book|author1=Harold Abelson|author2=Gerald Jay Sussman|author3=Julie Sussman|title=Structure and Interpretation of Computer Programs|url=http://mitpress.mit.edu/sicp/full-text/book/book-Z-H-10.html|accessdate=22 June 2012|edition=2|date=25 July 1996|publisher=MIT Press|isbn=978-0-262-01153-2}}
* {{cite web|last=Spolsky|first=Joel|title=The Law of Leaky Abstractions|url=http://www.joelonsoftware.com/articles/LeakyAbstractions.html|work=Joel on Software|date=11 November 2002}}
* [http://www.cs.cornell.edu/courses/cs211/2006sp/Lectures/L08-abstraction/08_abstraction.html Abstraction/information hiding] - CS211 course, Cornell University.
* {{cite book|author=Eric S. Roberts|title=Programming Abstractions in C A Second Course in Computer Science|date=1997}}
* {{cite web|last=Palermo|first=Jeffrey|title=The Onion Architecture|url=http://jeffreypalermo.com/blog/the-onion-architecture-part-1/|work=Jeffrey Palermo|date=29 July 2008}} 
{{refend}}

==External links==
* [https://sites.google.com/site/simulationarchitecture/ SimArch] example of layered architecture for distributed simulation systems.

{{Use dmy dates|date=June 2011}}

{{DEFAULTSORT:Abstraction (computer science)}}
[[Category:Data management]]
[[Category:Programming paradigms]]
[[Category:Articles with example Java code]]
[[Category:Abstraction]]</text>
      <sha1>02reazahfcduvc7xhkos91cwkicknpt</sha1>
    </revision>
  </page>
  <page>
    <title>Quality of Data (QoD)</title>
    <ns>0</ns>
    <id>42426440</id>
    <revision>
      <id>723788746</id>
      <parentid>699755417</parentid>
      <timestamp>2016-06-05T07:19:03Z</timestamp>
      <contributor>
        <username>Dcirovic</username>
        <id>11795905</id>
      </contributor>
      <minor />
      <comment>refs using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3791" xml:space="preserve">{{Orphan|date=January 2016}}

'''Quality-of-Data (QoD)''' is a designation coined by L. Veiga, that specifies and describes the required Quality of Service of a distributed storage system from the Consistency point of view of its data.
. It can be used to support [[Big data|Big Data]] management frameworks, Workflow management, and HPC systems (mainly for data replication and consistency). It takes into account data semantics, namely Time interval of data freshness, Sequence of tolerable number of outstanding versions of the data read before refresh, and Value divergence allowed before displaying it. Initially it was based in a model from an existing research work regarding vector-field Consistency,&lt;ref&gt;{{cite conference |author1=Nuno Santos |author2=Lu&#237;s Veiga |author3=Paulo Ferreira | year=2007 | title=Vector-Field Consistency for Adhoc Gaming| booktitle = ACM/IFIP/Usenix Middleware Conference 2007 | url=http://www.gsd.inesc-id.pt/~lveiga/msc-08-09/vfc-middleware-07.pdf | format=PDF}}&lt;/ref&gt; awarded the best-paper prize in the ACM/IFIP/Usenix Middleware Conference 2007 and later enhanced for increased scalability and fault-tolerance.&lt;ref&gt;{{cite conference |author1=Lu&#237;s Veiga |author2=Andr&#233; Negr&#227;o |author3=Nuno Santos |author4=Paulo Ferreira | year=2010 | title=Unifying Divergence Bounding and Locality Awareness in Replicated Systems with Vector-Field Consistency | booktitle = JISA, Journal of Internet Services and Applications, Volume 1, Number 2, 95-115, Springer, 2010 | url=http://www.gsd.inesc-id.pt/~lveiga/vfc-JISA-2010.pdf | format=PDF}}&lt;/ref&gt;

This consistency model has been successfully applied and proven in Big Data key/value store [[Apache HBase]],&lt;ref group="nb"&gt;url=https://hbase.apache.org&lt;/ref&gt; initially designed as a [[middleware]]&lt;ref&gt;{{cite conference |author1=Sergio Est&#233;ves |author2=Jo&#227;o Silva |author3=Lu&#237;s Veiga  |last-author-amp=yes | year=2013 | title=Quality-of-service for consistency of data geo-replication in cloud computing  | booktitle = Euro-Par 2012 Parallel Processing. Springer Berlin Heidelberg, 2012. 285-297 | url=http://www.gsd.inesc-id.pt/~sesteves/papers/vfc3-europar12.pdf | format=PDF}}&lt;/ref&gt; module seating between clusters from separate data centres. The HBase-QoD coupling &lt;ref&gt;{{cite conference |author1=&#193;lvaro Garc&#237;a-Recuero |author2=Sergio Est&#233;ves |author3=Lu&#237;s Veiga | year=2013 | title=Quality-of-Data for Consistency Levels in Geo-replicated Cloud Data Stores  | booktitle = IEEE CloudCom 2013 | url=http://www.inesc-id.pt/ficheiros/publicacoes/9253.pdf | format=PDF}}&lt;/ref&gt; minimises bandwidth usage and optimises resources allocation during replication achieving the desired consistency level at a more fine-grained level.

QoD is defined by the three-dimensions of vector k=(&#952;,&#963;,&#957;), but with a broader view of the issue, applicable also to large-scale data management techniques in regards to their timely delivery.&lt;ref group="nb"&gt;&lt;sub&gt;url=http://www-01.ibm.com/software/data/quality/&lt;/sub&gt;&lt;/ref&gt;

== Other Descriptions ==

Quality-of-Data should not be confused with other definitions for Data Quality such as
&lt;ref&gt;{{cite conference |author1=Richard Y. Wang | year= 1992 | title=Toward quality data : an attribute-based approach | booktitle=Decision Support Systems 13, MIT  | url=http://web.mit.edu/tdqm/www/tdqmpub/Toward%20Quality%20Data.pdf | format=PDF}}&lt;/ref&gt;
&lt;ref&gt;{{cite conference |author1=George A. Mihaila |author2=Louiqa Raschid |author3=Mar&#237;a-Esther Vidal | year= 2000 | title=Using Quality of Data Metadata for Source Selection and Ranking | booktitle =  | url=http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.34.9361 | format=}}&lt;/ref&gt;
- Completeness
- Validity
- Accuracy

== Notes ==
&lt;references group="nb"/&gt;

== References ==
&lt;references/&gt;

[[Category:Data management]]</text>
      <sha1>195rmbcklpswptyhucqsoplelwz34ue</sha1>
    </revision>
  </page>
  <page>
    <title>Geospatial metadata</title>
    <ns>0</ns>
    <id>7419799</id>
    <revision>
      <id>737970273</id>
      <parentid>737969570</parentid>
      <timestamp>2016-09-06T04:16:41Z</timestamp>
      <contributor>
        <username>Annbeaumaris</username>
        <id>29111955</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="15596" xml:space="preserve">'''Geospatial metadata''' (also '''geographic metadata''', or simply  '''metadata''' when used in a geographic context) is a type of [[metadata]] that is applicable to objects that have an explicit or implicit [[Geography|geographic]] extent, i.e. are associated with some position on the surface of the [[globe]]. Such objects may be stored in a [[geographic information system]] (GIS) or may simply be documents, data-sets, images or other objects, services, or related items that exist in some other native environment but whose features may be appropriate to describe in a (geographic) metadata catalog (may also be known as a data directory, data inventory, etc.).

==Definition==
I'''SO 19115:2013 "Geographic Information - Metadata'''"&lt;ref name=":0"&gt;{{Cite web|url=https://www.iso.org/obp/ui/#iso:std:iso:19115:-1:ed-1:v1:en|title=ISO 19115-1:2014(en)|last=International Organization for Standardization|first=|date=2014-04-01|website=ISO|publisher=|access-date=2016-04-01}}&lt;/ref&gt; from [[ISO/TC 211]], the industry standard for geospatial metadata, describes its scope as follows:

"''[This standard] provides information about the identification, the extent, the quality, the spatial and temporal aspects, the content, the spatial reference, the portrayal, distribution, and other properties of digital geographic data and services"&lt;ref name=":0" /&gt;''

ISO 19115:2013 also provides for non-digital mediums: ''"[t]hough this part of [https://www.iso.org/obp/ui/#iso:std:iso:19115:en ISO 19115] is applicable to digital data and services, its principles can be extended to many other types of resources such as maps, charts, and textual documents as well as non-geographic data.''" &lt;ref name=":0" /&gt;

'''The U.S. Federal Geographic Data Committee (FGDC)''' describes geospatial metadata as follows:

"''A metadata record is a file of information, usually presented as an XML document, which captures the basic characteristics of a data or information resource. It represents the who, what, when, where, why and how of the resource. Geospatial metadata commonly document geographic digital data such as Geographic Information System (GIS) files, geospatial databases, and earth imagery but can also be used to document geospatial resources including data catalogs, mapping applications, data models and related websites. Metadata records include core library catalog elements such as Title, Abstract, and Publication Data; geographic elements such as Geographic Extent and Projection Information; and database elements such as Attribute Label Definitions and Attribute Domain Values.''" &lt;ref&gt;{{Cite web|url=http://www.fgdc.gov/metadata|title=Geospatial Metadata &#8212; Federal Geographic Data Committee|website=www.fgdc.gov|access-date=2016-04-01}}&lt;/ref&gt;

==History==
The growing appreciation of the value of geospatial metadata through the 1980s and 1990s led to the development of a number of initiatives to collect metadata according to a variety of formats either within agencies, communities of practice, or countries/groups of countries. For example, [[NASA]]'s "DIF" metadata format was developed during an Earth Science and Applications Data Systems Workshop in 1987,&lt;ref&gt;[http://gcmd.nasa.gov/User/difguide/whatisadif.html Gene Major and Lola Olsen: "A short history of the DIF". On GCMD website, visited 16 October 2006]&lt;/ref&gt; and formally approved for adoption in 1988. Similarly, the U.S. FGDC developed its geospatial metadata standard over the period 1992&#8211;1994.&lt;ref&gt;[http://libraries.mit.edu/guides/subjects/metadata/standards/fgdc.html MIT Libraries Guide: "Federal Geographic Data Committee (FGDC) Metadata". On MIT Libraries website, visited 16 October 2006]
&lt;/ref&gt; The Spatial Information Council of Australia and New Zealand (ANZLIC),&lt;ref&gt;
{{cite web
| url         = http://anzlic.gov.au/sites/default/files/files/ANZLICmetadataProfileGuidelines_v1-2.pdf
| title       = ANZLIC Metadata Profile Guidelines version 1.2 July 2011
| year        = 2011
| publisher   = ANZLIC
| accessdate  = 2011-04-11
| quote       = ANZLIC[:] The Spatial Information Council of Australia and New Zealand (formerly known as the Australia New Zealand Land Information Council)
}}
&lt;/ref&gt; a combined body representing spatial data interests in Australia and New Zealand, released version 1 of its "metadata guidelines" in 1996.&lt;ref&gt;[http://anzlic.gov.au/resources/anzlic-metadata-profile ANZLIC Metadata Guidelines: Core metadata elements for geographic data in Australia and New Zealand, Version 2 (February 2001)]&lt;/ref&gt; [[ISO/TC 211]] undertook the task of harmonizing the range of formal and ''de facto'' standards over the approximate period 1999&#8211;2002, resulting in the release of '''ISO 19115''' "'''Geographic Information - Metadata'''" in 2003 and a subsequent revision in 2013. {{As of | 2011}} individual countries, communities of practice, agencies, etc. have started re-casting their previously used metadata standards as "profiles" or recommended subsets of ISO 19115, occasionally with the inclusion of additional metadata elements as formal extensions to the ISO standard. The growth in popularity of Internet technologies and data formats, such as [[Extensible Markup Language]] (XML) during the 1990s led to the development of mechanisms for exchanging geographic metadata on the [[World Wide Web|web]]. In 2004, the [[Open Geospatial Consortium]] released the current version (3.1) of [[Geography Markup Language]] (GML), an XML grammar for expressing geospatial features and corresponding metadata. With the growth of the [[Semantic Web]] in the 2000s, the geospatial community has begun to develop [[Ontology (computer science)|ontologies]] for representing semantic geospatial metadata. Some examples include the [http://www.ordnancesurvey.co.uk/oswebsite/ontology/ Hydrology and Administrative ontologies] developed by the [[Ordnance Survey]] in the [[United Kingdom]].

==ISO 19115: Geographic information - Metadata==
ISO 19115 is a standard of the International Organization for Standardization (ISO).&lt;ref&gt;ISO 19115 Geographic Information - Metadata. International Organization for Standardization (ISO), Geneva, 2003&lt;/ref&gt; The standard is part of the [[ISO/TC 211|ISO geographic information suite of standards]] (19100 series). ISO 19115 and its parts define how to describe geographical information and associated services, including contents, spatial-temporal purchases, data quality, access and rights to use.

The objective of this International Standard is to provide a clear procedure for the description of digital geographic data-sets so that users will be able to determine whether the data in a holding will be of use to them and how to access the data. By establishing a common set of metadata terminology, definitions and extension procedures, this standard promotes the proper use and effective retrieval of geographic data.&lt;ref&gt;{{cite web|title=ISO 19115 Metadata Factsheet|url=http://www.isotc211.org/Outreach/Overview/Factsheet_19115.pdf|publisher=AG Outreach|accessdate=2012-11-22}}&lt;/ref&gt;

ISO 19115 was revised in 2013 to accommodate growing use of the internet for metadata management, as well as add many new categories of metadata elements (referred to as codelists) and the ability to limit the extent of metadata use temporally or by user.&lt;ref&gt;{{Cite web|url=https://wiki.earthdata.nasa.gov/display/NASAISO/NASA+Metadata+and+the+New+ISO+19115-1+Capabilities|title=NASA Metadata and the New ISO 19115-1 Capabilities - NASA ISO for EOSDIS - Earthdata Wiki|website=wiki.earthdata.nasa.gov|access-date=2016-04-01}}&lt;/ref&gt;

{{Expand section|date=June 2012}}

==ISO 19139 Geographic information Metadata XML schema implementation==
ISO 19139:2012 &lt;ref&gt;{{Cite web|url=https://www.iso.org/obp/ui/#iso:std:iso:ts:19139:-2:ed-1:v1:en|title=ISO/TS 19139-2:2012(en)|last=International Organization for Standardization|first=|date=2012-12-15|website=ISO|publisher=|access-date=2016-04-01}}&lt;/ref&gt; provides the XML implementation schema for ISO 19115 specifying the metadata record format and may be used to describe, validate, and exchange geospatial metadata prepared in XML.&lt;ref&gt;[http://marinemetadata.org/references/iso19139 "ISO 19139 Geographic information Metadata XML schema implementation"], Marine Metadata Interoperability Project&lt;/ref&gt;

The standard is part of the [[ISO/TC 211|ISO geographic information suite of standards]] (19100 series), and provides a spatial metadata XML (spatial metadata eXtensible Mark-up Language (smXML)) encoding, an XML schema implementation derived from ISO 19115, Geographic information &#8211; Metadata. The metadata includes information about the identification, constraint, extent, quality, spatial and temporal reference, distribution, lineage, and maintenance of the digital geographic data-set.

{{Expand section|date=June 2012}}

==Metadata directories==
Also known as metadata catalogues or data directories.

(need discussion of, and subsections on GCMD, FGDC metadata gateway, ASDD, European and Canadian initiatives, etc. etc.)
* [http://gisinventory.net GIS Inventory] &#8211; National GIS Inventory System which is maintained by the US-based [[National States Geographic Information Council|National States Geographic Information Council (NSGIC)]] as a tool for the entire US GIS Community. Its primary purpose is to track data availability and the status of geographic information system (GIS) implementation in state and local governments to aid the planning and building of statewide spatial data infrastructures (SSDI). The Random Access Metadata for Online Nationwide Assessment (RAMONA) database is a critical component of the GIS Inventory. RAMONA moves its FGDC-compliant metadata (CSDGM Standard) for each data layer to a web folder and a Catalog Service for the Web (CSW) that can be harvested by Federal programs and others. This provides far greater opportunities for discovery of user information. The GIS Inventory website was originally created in 2006 by NSGIC under award NA04NOS4730011 from the Coastal Services Center, National Oceanic and Atmospheric Administration, U.S. Department of Commerce. The Department of Homeland Security has been the principal funding source since 2008 and they supported the development of the Version 5 during 2011/2012 under Order Number HSHQDC-11-P-00177. The Federal Emergency Management Agency and National Oceanic and Atmospheric Administration have provided additional resources to maintain and improve the GIS Inventory. Some US Federal programs require submission of CSDGM-Compliant Metadata for data created under grants and contracts that they issue. The GIS Inventory provides a very simple interface to create the required Metadata. 
* [http://gcmd.nasa.gov GCMD] - Global Change Master Directory's goal is to enable users to locate and obtain access to Earth science data sets and services relevant to global change and Earth science research. The GCMD database holds more than 20,000 descriptions of Earth science data sets and services covering all aspects of Earth and environmental sciences.
* [http://earthdata.nasa.gov/echo ECHO] - The EOS Clearing House (ECHO) is a spatial and temporal metadata registry, service registry, and order broker. It allows users to more efficiently search and access data and services through the [http://reverb.earthdata.nasa.gov/echo Reverb Client] or Application Programmer Interfaces (APIs). ECHO stores metadata from a variety of science disciplines and domains, totalling over 3400 Earth science data sets and over 118&amp;nbsp;million granule records.
* [http://www.gogeo.ac.uk/gogeo/ GoGeo] - GoGeo is a service run by [[EDINA]] (University of Edinburgh) and is supported by [[Jisc]]. GoGeo allows users to conduct geographically targeted searches to discover geospatial datasets. GoGeo searches many data portals from the HE and FE community and beyond. GoGeo also allows users to create standards compliant metadata through its Geodoc metadata editor.

==Geospatial metadata tools==
There are many commercial GIS or geospatial products that support metadata viewing and editing on GIS resources. For example, [[ESRI]]'s [[ArcGIS]] Desktop, [[SOCET GXP]], [[Autodesk]]'s AutoCAD Map 3D 2008, [[Arcitecta]]'s [[Mediaflux]] and [[Intergraph]]'s [[GeoMedia]] support geospatial metadata extensively.

[http://gisinventory.net GIS Inventory] is a free web-based tool that provides a very simple interface to create geospatial metadata. Participants create a profile and document their data layers through a survey-style interface. The GIS Inventory produces metadata that is compliant with the Federal Content Standard for Digital Geospatial Metadata (CSDGM). The GIS Inventory is also capably of ingesting already completed metadata through document upload and web server connectivity. Through the GIS Inventory web services, metadata are automatically shared with US Federal agencies.

[http://geonetwork-opensource.org GeoNetwork opensource] is a comprehensive [[Free and Open Source Software]] solution to manage and publish geospatial metadata and services based on international metadata and catalog standards. The software is part of the [[Open Source Geospatial Foundation]]'s software stack.

[http://geocat.net/bridge GeoCat Bridge] allows to edit, validate and directly publish metadata from [[ArcGIS]] Desktop to [http://geonetwork-opensource.org GeoNetwork] (and generic CSW catalogs) and publishes data as map services on [http://geoserver.org GeoServer]. Several metadata profiles are supported.

[[pycsw]] is an OGC CSW server implementation written in Python. pycsw fully implements the OpenGIS Catalogue Service Implementation Specification ([[Catalog Service for the Web|Catalogue Service for the Web]]). The project is certified OGC Compliant, and is an OGC Reference Implementation.

[http://catmdedit.sourceforge.net/ CATMDEdit]
terraCatalog
ArcCatalog
ArcGIS Server Portal
[http://geonetwork-opensource.org GeoNetwork opensource]
[http://www.conterra.de/en/products/sdi/terracatalog/index.shtm IME]
[http://www.intelec.ca/html/en/technologies/m3cat.html M3CAT MetaD]
[http://www.gigateway.org.uk/metadata/metagenie.html MetaGenie]
Parcs Canada Metadata Editor
Mapit/CADit
NOKIS Editor

{{Expand section|date=June 2008}}

==References==
&lt;references/&gt;
[http://anzlic.gov.au/sites/default/files/files/ANZLICmetadataProfileGuidelines_v1-2.pdf ANZLIC Metadata Profile Version 1.2 (viewed July 2011)]

==External links==
*[http://www.fgdc.gov/metadata FGDC metadata page]
*[http://gcmd.nasa.gov/ Global Change Master Directory(GCMD)]
*[http://wiki.milcord.com/wiki/Geospatial_Exploitation_of_Motion_Imagery Geospatial Exploitation of Motion Imagery] is a geospatially aware and integrated Intelligent Video Surveillance (IVS) software system targeted at real-time and forensic video analytic and mining applications that require low-resolution detection, tracking, and classification of moving objects (people and vehicles) in outdoor, wide-area scenes.
*[http://www.iso.org/iso/en/CatalogueDetailPage.CatalogueDetail?CSNUMBER=26020 ISO 19115:2003 Geographic information -- Metadata]
*[http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=32557 Geographic information -- Metadata -- XML schema implementation ]
*[http://www.earthdatamodels.org/designs/metadata_BGS.html EarthDataModels design for Metadata] is a logical data model and physical implementation of a Spatial Metadata Database, based on ISO19115 and is INSPIRE compliant.

{{use dmy dates|date=January 2011}}

[[Category:Data management]]
[[Category:Metadata]]
[[Category:Geographic data and information]]</text>
      <sha1>sdc2fvo47quks8hzqgsqn4696r8uuyo</sha1>
    </revision>
  </page>
  <page>
    <title>Vinelink.com</title>
    <ns>0</ns>
    <id>42814178</id>
    <revision>
      <id>760897181</id>
      <parentid>665032048</parentid>
      <timestamp>2017-01-19T18:29:24Z</timestamp>
      <contributor>
        <username>Kimberley Murry</username>
        <id>30142502</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3292" xml:space="preserve">'''''Vinelink.com''''' (VINE) is a national website in the [[United States]] that allows victims of crime, and the general public, to track the movements of prisoners held by the various [[US states|states]] and [[Territories of the United States|territories]]. The first four letters in the websites name, "vine", are an acronym for "Victim Information and Notification Everyday". Vinelink.com displays information, based on the information provided by the various states' departments of correction and other law enforcement agencies, on whether an inmate is in custody, has been released, has been granted parole or probation, or has escaped from custody. In some cases, the website will reveal whether a defendant has been granted parole or probation, but then subsequently violated conditions of their release and become a fugitive.&lt;ref&gt;{{cite web | url=http://www.correct.state.ak.us/probation-parole/vine|title=Automated Victim Notification System (VINE)|publisher=[[Alaska Department of Corrections]]|accessdate=2014-05-20}}&lt;/ref&gt; Information provided on Vinelink.com represents [[metadata]], in that the website lists a defendants custody status; but does not list what the individual is charged with, their criminal history, or the amount of their bail, if applicable.

[[Internet]] users accessing the Vinelink.com website choose from a map of states and provinces within the United States where they wish to perform a search for an inmate. The user may then search for an individual using the inmate or parolees name, or by entering the inmates specific department of corrections inmate number, if known. When the inmates custody status changes, users who have registered to be notified of such changes will be notified via email, phone or both.&lt;ref&gt;{{cite web | url=http://www.doj.state.or.us/victims/pages/vine.aspx|title=Victim Information and Notification Everyday (VINE)|publisher=[[Oregon Department of Justice]]|date=2013-02-03| accessdate=2014-05-20}}&lt;/ref&gt; This information is currently released upon request, without the website requesting reasons for the users search or requiring payment, as [[public records]] available to the [[general public]].

Inmate information is available for most states, and for [[Puerto Rico]], on the website. The states of [[Arizona]], [[Georgia (U.S. state)|Georgia]], [[Massachusetts]], [[Montana]], [[New Hampshire]] and [[West Virginia]] provide very limited information on the site. The states of [[Kansas]], [[Maine]] and [[South Dakota]] do not participate in the VINE system.&lt;ref&gt;{{cite web | url=http://www.theledger.com/article/20130203/news/130209793| title=Who to Call: VINElink|publisher=[[The Ledger]]|date=2010| accessdate=2014-05-20}}&lt;/ref&gt; The website does not provide data on prisoners detained by the [[United States federal government]].

==References==
==External links==
* [http://www.vinelink.com official website] 

==See also==
Visit [http://www.bop.gov/inmateloc/ the official Federal Bureau of Prisons website] to search for inmates being held by the [[United States Federal Bureau of Prisons]]

[[Category:Data management]]
[[Category:Government services web portals in the United States]]
[[Category:Law enforcement websites]]
[[Category:Metadata]]
[[Category:Public records]]


{{website-stub}}
{{Crime-stub}}</text>
      <sha1>0qvcm5rcrs9w3qxupwbuzfh879i9f8l</sha1>
    </revision>
  </page>
  <page>
    <title>Head/tail Breaks</title>
    <ns>0</ns>
    <id>42933069</id>
    <revision>
      <id>760464914</id>
      <parentid>760429490</parentid>
      <timestamp>2017-01-17T04:02:37Z</timestamp>
      <contributor>
        <ip>82.152.154.105</ip>
      </contributor>
      <comment>/* Color rendering DEM */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="14361" xml:space="preserve">{{Multiple issues|
{{technical|date=June 2014}}
{{COI|date=July 2014}}
}}

[[File:Patterns1024Cities2.jpg|thumb|500px|1024 cities that follow exactly Zipf's law, which implies that the first largest city is size 1, the second largest city is size 1/2, the third largest city is size 1/3, ... and the smallest city is size 1/1024. The left pattern is produced by head/tail breaks, while the right one by natural breaks, also known as [[Jenks natural breaks optimization]].]]
'''Head/tail breaks''' is a new [[clustering algorithm]] scheme for data with a heavy-tailed distribution such as [[power laws]] and [[lognormal distributions]]. The heavy-tailed distribution can be simply referred to the scaling pattern of far more small things than large ones, or alternatively numerous smallest, a very few largest, and some in between the smallest and largest. The classification is done through dividing things into large (or called the head) and small (or called the tail) things around the arithmetic mean or average, and then recursively going on for the division process for the large things or the head until the notion of far more small things than large ones is no longer valid, or with more or less similar things left only.&lt;ref name="Jiang1"&gt;Jiang, Bin (2013a). "Head/tail breaks: A new classification scheme for data with a heavy-tailed distribution", ''The Professional Geographer'', 65 (3), 482 &#8211; 494.&lt;/ref&gt; Head/tail breaks is not just for classification, but also for visualization of big data by keeping the head, since the head is self-similar to the whole. Head/tail breaks can be applied not only to vector data such as points, lines and polygons, but also to raster data like digital elevation model (DEM). 

==Motivation==
The head/tail breaks is mainly motivated by inability of conventional classification methods such as equal intervals, quantiles, geometric progressions, standard deviation, and natural breaks - commonly known as [[Jenks natural breaks optimization]] for revealing the underlying scaling pattern of far more small things than large ones. Note that the notion of far more small things than large one is not only referred to geometric property, but also to topological and semantic properties. In this connection, the notion should be interpreted as far more unpopular (or less-connected) things than popular (or well-connected) ones, or far more meaningless things than meaningful ones.

==Method==
Given some variable X that demonstrates a heavy-tailed distribution, there are far more small x than large ones. Take the average of all xi, and obtain the first mean m1. Then calculate the second mean for those xi greater than m1, and obtain m2. In the same recursive way, we can get m3 depending on whether the ending condition of no longer far more small x than large ones is met. For simplicity, we assume there are three means, m1, m2, and m3. This classification leads to four classes: [minimum, m1], (m1, m2], (m2, m3], (m3, maximum]. In general, it can be represented as a recursive function as follows:

     Recursive function '''Head/tail Breaks''':
     Break the input data (around mean or average) into the head and the tail;  
     // the head for data values greater the mean
     // the tail for data values less the mean
     while (head &lt;= 40%):
         '''Head/tail Breaks'''(head);
     End Function

The resulting number of classes is referred to as ht-index, an alternative index to [[fractal dimension]] for characterizing complexity of fractals or geographic features: the higher the ht-index, the more complex the fractals.&lt;ref name="Jiang2"&gt;Jiang, Bin and Yin Junjun (2014). "Ht-index for quantifying the fractal or scaling structure of geographic features", ''Annals of the Association of American Geographers'', 104(3), 530&#8211;541.&lt;/ref&gt; Recently, a more sensitive ht-index, namely CRG-index,&lt;ref&gt;{{Cite journal|title = CRG Index: A more sensitive ht-index for enabling dynamic views of geographic features|url = http://dx.doi.org/10.1080/00330124.2015.1099448|journal = The Professional Geographer|date = 2015-12-09|issn = 0033-0124|pages = 1&#8211;13|volume = 0|issue = 0|doi = 10.1080/00330124.2015.1099448|first = Peichao|last = Gao|first2 = Zhao|last2 = Liu|first3 = Meihui|last3 = Xie|first4 = Kun|last4 = Tian|first5 = Gang|last5 = Liu}}&lt;/ref&gt; has been developed, and it is able to capture slight changes which ht-index is unable to. Thus while ht-index is an integer, CRG-index is a real number. A PostgreSQL function for calculating ht-index can be found here.&lt;ref&gt;{{Cite journal|title = A PostgreSQL function for calculating the ht-index|url = https://www.researchgate.net/publication/287533541_A_PostgreSQL_function_for_calculating_the_ht-index?channel=doi&amp;linkId=56777e5b08aebcdda0e962fe&amp;showFulltext=true|date = 2015-01-01|doi = 10.13140/RG.2.1.3041.0324|first = Peichao Gao|last = Kun Tian}}&lt;/ref&gt;

=== Threshold or its sensitivity ===
The criterion to stop the iterative classification process using the head/tail breaks method is that the remaining data (i.e., the head part) are not heavy-tailed, or simply, the head part is no longer a minority (i.e., the proportion of the head part is no longer less than a threshold such as 40%). This threshold is suggested to be 40% by Jiang et al. (2013),&lt;ref name="Jiang3" /&gt; just as the codes above (i.e., head &lt;= 40%). But sometimes a larger threshold, for example 50% or more, can be used, as Jiang and Yin (2014)&lt;ref name="Jiang2" /&gt; noted in another article: "this condition can be relaxed for many geographic features, such as 50 percent or even more". However, all heads' percentage on average must be smaller than 40% (or 41, 42%), indicating far more small things than large ones. This sensitivity issue deserves further research in the future.

=== Rank-size plot and RA index ===
A good tool to display the scaling pattern, or the heavy-tailed distribution, is the rank-size plot, which is a scatter plot to display a set of values according to their ranks. With this tool, a new index &lt;ref&gt;{{Cite journal|last=Gao|first=Peichao|last2=Liu|first2=Zhao|last3=Tian|first3=Kun|last4=Liu|first4=Gang|date=2016-03-10|title=Characterizing Traf&#64257;c Conditions from the Perspective of Spatial-Temporal Heterogeneity|url=http://www.mdpi.com/2220-9964/5/3/34|journal=ISPRS International Journal of Geo-Information|language=en|volume=5|issue=3|pages=34|doi=10.3390/ijgi5030034}}&lt;/ref&gt; termed as the ratio of areas (RA) in a rank-size plot was defined to characterize the scaling pattern. The RA index has been successfully used in the estimation of traffic conditions. However, it should be noted that the RA index can only be used as a complementary method to the ht-index, because it is ineffective to capture the scaling structure of geographic features.

==Applications==
Instead of more or less similar things, there are far more small things than large ones surrounding us. Given the ubiquity of the scaling pattern, head/tail breaks is found to be of use to statistical mapping, map generalization, cognitive mapping and even perception of beauty
.&lt;ref name="Jiang3"&gt;Jiang, Bin, Liu, Xintao and Jia, Tao (2013). "Scaling of geographic space as a universal rule for map generalization", ''Annals of the Association of American Geographers'', 103(4), 844 &#8211; 855.&lt;/ref&gt;&lt;ref name="Jiang4"&gt;Jiang, Bin (2013b). "The image of the city out of the underlying scaling of city artifacts or locations", ''Annals of the Association of American Geographers'', 103(6), 1552-1566.&lt;/ref&gt;&lt;ref name="Jiang5"&gt;Jiang, Bin and Sui, Daniel (2014). "A new kind of beauty out of the underlying scaling of geographic space", ''The Professional Geographer'', 66(4), 676&#8211;686&lt;/ref&gt; It helps visualize big data, since big data are likely to show the scaling property of far more small things than large ones. The visualization strategy is to recursively drop out the tail parts until the head parts are clear or visible enough.&lt;ref name="Jiang6"&gt;Jiang, Bin (2015). "Head/tail breaks for visualization of city structure and dynamics", ''Cities'', 43, 69 - 77.&lt;/ref&gt; In addition, it helps delineate cities or natural cities to be more precise from various geographic information such as street networks, social media geolocation data, and nighttime images.

=== Characterizing the imbalance ===
As the head/tail breaks method can be used iteratively to obtain head parts of a data set, this method actually captures the underlying hierarchy of the data set. For example, if we divide the array (19, 8, 7, 6, 2, 1, 1, 1, 0) with the head/tail breaks method, we can get two head parts, i.e., the first head part (19, 8, 7, 6) and the second head part (19). These two head parts as well as the original array form a three-level hierarchy:

the 1st level (19),

the 2nd level (19, 8, 7, 6), and

the 3rd level (19, 8, 7, 6, 2, 1, 1, 1, 0).

The number of levels of the above-mentioned hierarchy is actually a characterization of the imbalance of the example array, and this number of levels has been termed as the ht-index.&lt;ref name="Jiang2" /&gt; With the ht-index, we are able to compare degrees of imbalance of two data sets. For example, the ht-index of the example array (19, 8, 7, 6, 2, 1, 1, 1, 0) is 3, and the ht-index of another array (19, 8, 8, 8, 8, 8, 8, 8, 8) is 2. Therefore, the degree of imbalance of the former array is higher than that of the latter array.
[[File:Natural_cities_of_Germany,_created_from_points_of_interest.jpg|thumb|250px|right|The left panel pattern contains 50,000 natural cities, which can be put into 7 hierarchical levels. It looks like a hair ball. Instead of showing all the 7 hierarchical levels, we show 4 top levels, by dropping out 3 low levels. Now with the right panel, the scaling pattern of far more small cities than large ones emerges. It is important to note that the right pattern (or the remaining part after dropping out the tails) is self-similar to the whole (or the left pattern). Thus the right pattern reflects the underlying structure of the left one, and enables us to see the whole.]]
[[File:Headtail breaks of American DEM.jpg|thumb|250px|The scaling pattern of US terrain surface is distorted by the natural breaks, but revealed by the head/tail breaks.]]

=== Delineating natural cities ===
The term &#8216;natural cities&#8217; refers to the human settlements or human activities in general on Earth&#8217;s surface that are naturally or objectively defined and delineated from massive geographic information based on head/tail division rule, a non-recursive form of head/tail breaks.&lt;ref name="Jiang7"&gt;Jiang, Bin and Miao, Yufan (2015). "The evolution of natural cities from the perspective of location-based social media", ''The Professional Geographer'', 67(2), 295 - 306.&lt;/ref&gt;&lt;ref name="Long"&gt;Long, Ying (2016). "Redefining Chinese city system with emerging new data", ''Applied Geography'', 75, 36 - 48.&lt;/ref&gt;  Such geographic information could be from various sources, such as massive street junctions &lt;ref name="Long"/&gt; and street ends, a massive number of street blocks, nighttime imagery and social media users&#8217; locations etc. Distinctive from conventional cities, the adjective &#8216;natural&#8217; could be explained not only by the sources of natural cities, but also by the approach to derive them. Natural cities are derived from a meaningful cutoff averaged from a massive amount of units extracted from geographic information.&lt;ref name="Jiang6"/&gt; Those units vary according to different kinds of geographic information, for example the units could be area units for the street blocks and pixel values for the nighttime images. A '''[http://www.arcgis.com/home/item.html?id=47b1d6fdd1984a6fae916af389cdc57d natural cities model]''' has been created using ArcGIS model builder,&lt;ref name="Ren"&gt;Ren, Zheng (2016). "Natural cities model in ArcGIS", ''http://www.arcgis.com/home/item.html?id=47b1d6fdd1984a6fae916af389cdc57d''.&lt;/ref&gt; it follows the same process of deriving natural cities from location-based social media,&lt;ref name="Jiang7"/&gt; namely, building up huge triangular irregular network (TIN) based on the point features (street nodes in this case) and regarding the triangles which are smaller than a mean value as the natural cities.

=== Color rendering DEM ===
Current color renderings for DEM or density map are essentially based on conventional classifications such as natural breaks or equal intervals, so they disproportionately exaggerate high elevations or high densities. As a matter of fact, there are not so many high elevations or high-density locations.&lt;ref name="Jiang8"&gt;Jiang, Bin (2015). "Geospatial analysis requires a different way of thinking: The problem of spatial heterogeneity", ''GeoJournal'', 80(1), 1-13.&lt;/ref&gt; It was found that coloring based head/tail breaks is more favorable than those by other classifications &lt;ref name="Wu"&gt;Wu, Jou-Hsuan (2015). "Examining the new kind of beauty using the human being as a measuring instrument", ''http://www.diva-portal.org/smash/get/diva2:805296/FULLTEXT01.pdf''.&lt;/ref&gt;

== Software implementations ==
The following implementations are available under [[Free and open-source software|Free/Open Source Software]] licenses.
* '''[https://github.com/digmaa/HeadTailBreaks HT calculator]''': a winform application for obtaining related metrics of head/tail breaks applying on a single data array.
* '''[http://jsfiddle.net/mhkeller/5yATK/ HT in JavaScript]''': a JavaScript implementation for applying head/tail breaks on a single data array.
* '''[http://fromto.hig.se/~bjg/axwoman/ HT Mapping tool]''': a function in the free plug-in Axwoman 6.3 to [[ArcMap]] 10.2 that conducts geo-data symbolization automatically based on the head/tail breaks classification.
* '''[https://github.com/chad-m/head_tail_breaks_algorithm HT in Python]''': Python and JavaScript code for the head/tail breaks algorithm. It's works great for choropleth map coloring.

==References==
{{one author|date=June 2014}}
{{Reflist}}

==Further reading==
{{Further reading cleanup|date=June 2014}}
* Lin, Yue (2013), A comparison study on natural and head/tail breaks involving digital elevation models. http://www.diva-portal.org/smash/get/diva2:658963/FULLTEXT02.pdf
* Wu, Jou-Hsuan (2015), The mirror of the self test: http://sharon19891101.wix.com/mirror-of-the-self

{{DEFAULTSORT:Head tail Breaks}}
[[Category:Data management]]
[[Category:Cartography]]</text>
      <sha1>seruqmirs4txlpsha8vh7lwmsavjhei</sha1>
    </revision>
  </page>
  <page>
    <title>Personal, Inc.</title>
    <ns>0</ns>
    <id>43509883</id>
    <revision>
      <id>742529608</id>
      <parentid>742419464</parentid>
      <timestamp>2016-10-04T07:16:07Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor />
      <comment>[[WP:CHECKWIKI]] error fixes using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="23354" xml:space="preserve">{{Infobox company |
| name=Personal, Inc.
| logo=[[File:Personal, Inc. logo.png]]
| type=[[Private company|Private]]
| foundation=2009
| location=[[Washington, D.C.]], US
| industry=[[Internet]]
| homepage={{URL|www.personal.com}}
}}

'''Personal''' (also referred to as Personal.com or Personal, Inc.) was a consumer [[Personal Data Service]] and [[identity management system]] for individuals to aggregate, manage and reuse their own data. It was re-launched in May 2016 as a collaborative data management and security solution for the workplace called TeamData.&lt;ref&gt;{{Cite web|url=http://www.prnewswire.com/news-releases/personalcom-becomes-teamdata-300275063.html|title=Personal.com Becomes TeamData|last=TeamData|website=www.prnewswire.com|access-date=2016-10-03}}&lt;/ref&gt;

Personal's consumer products included: the Data Vault with Cloud Sync for secure management and sharing of data and documents between an individual and other individuals, companies, sites, apps and devices; and Data Imports to import information from third parties, including [[Social networking services|social media services]], companies and the [[United States Department of Education|U.S. Department of Education]], and  the Fill It App for automated completion of web and mobile forms, logins and checkouts.

The Personal platform supported user-centric [[DataPortability|data management and portability]] for over 1,200 different types (or fields) of structured, machine-readable, human-readable data. The platform also provided tools and APIs for developers and companies to integrate Fill It and the Data Vault into their websites and applications, primarily to give data back to their customers so they can autofill web and mobile forms.

==History==
Personal was founded in 2009 in [[Washington, DC]] by the management team that built The Map Network, which was acquired by [[Nokia]]/[[Navteq|NAVTEQ]] in 2006.&lt;ref name=acquisition&gt;{{cite web |url=http://www.directionsmag.com/pressreleases/navteq-announces-agreement-to-acquire-the-map-network/110396 |title=NAVTEQ Announces Agreement to Acquire The Map Network |date=6 December 2006 |website=Directions Magazine |accessdate=21 August 2014}}&lt;/ref&gt; Founded in 1999, The Map Network (previously called URHere.com) built the first platform for places and events to create and distribute digital online and mobile maps, location data and content. The Map Network served as the official mapping solution for over 100 cities and thousands of events and venues, from the NFL Super Bowl to the Democratic and Republican National Conventions to the Smithsonian Institution. It also produced the most-used interactive map of 9/11 relief and rescue efforts.&lt;ref name=acquisition /&gt;&lt;ref&gt;{{cite web |url=http://www.prnewswire.com/news-releases/interactive-relief-and-rescue-map-aids-in-nyc-response-72052587.html |title=Interactive Relief and Rescue Map Aids in NYC Response |date=17 September 2001 |website=PR Newswire |accessdate=25 August 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://spatialnews.geocomm.com/dailynews/2001/sep/11/ |title=The Geospatial Industry's Response to Terrorism |date=11 September 2001 |website=GeoCommunity |accessdate=26 August 2014}}&lt;/ref&gt;

Called a &#8220;life management platform&#8221; by [[The Economist]]&lt;ref&gt;{{cite web |url=http://www.economist.com/blogs/babbage/2011/11/personal-data |title=A life-management platform? |last=L. |first=G. |date=17 November 2011 |website=The Economist |accessdate=8 August 2014}}&lt;/ref&gt; and a &#8220;personal encrypted cloud service&#8221; by TIME for its user-centric approach to data,&lt;ref name="time.com"&gt;{{cite web|url=http://time.com/3069834/how-to-take-control-of-your-personal-data/|title=How to Take Control of Your Personal Data|last=Stokes|first=Natasha|date=1 August 2014|website=Time Inc.|accessdate=8 August 2014}}&lt;/ref&gt; the company has been associated with both the [[Infomediary]] model originated in 1999 by [[John Hagel III]] and Mark Singer, as well as the [[Vendor relationship management|vendor relationship management (VRM)]] model developed by Doc Searls. Personal closed $7.6m in funding in December 2010, including [[Steve Case]]&#8217;s Revolution Ventures, Grotech Ventures, [[Allen &amp; Company]], [[Ted Leonsis]], Neil Ashe and [[Jonathan Miller (businessman)|Jonathan Miller]].&lt;ref&gt;{{cite web |url=http://techcrunch.com/2011/01/06/personal-raises-7m-from-steve-case-and-others-to-help-consumers-protect-their-digital-data/ |title=Personal Raises $7M From Steve Case And Others To Help Consumers Protect Their Digital Data |last=Rao |first=Leena |date=6 January 2011 |website=TechCrunch |accessdate=8 August 2014}}&lt;/ref&gt;

Personal was early to embrace &#8220;small data,&#8221; which it defines as &#8220;big data for the benefit of individuals.&#8221;&lt;ref&gt;{{cite web |url=http://blog.personal.com/2012/03/the-era-of-small-data-begins/ |title=The Era of Small Data Begins |last=Green |first=Shane |date=6 March 2012 |website=Personal |accessdate=20 August 2014 }}&lt;/ref&gt; The term &#8220;small data&#8221; may have been originally coined by [[Jeremie Miller]] of Sing.ly, who mentioned it in a talk at the Web 2.0 Summit in November 2011 and is cited in ''The Intention Economy''.&lt;ref&gt;{{cite web |url=http://siliconprairienews.com/2011/11/watch-jeremie-miller-present-singly-at-the-web-2-0-summit/ |title=Watch Jeremie Miller present Singly at the Web 2.0 Summitt |first=Michael |date=9 November 2011 |website=Silicon Prairie News |accessdate=8 August 2014}}&lt;/ref&gt; In 2011, Personal was a part of the first group of companies to join the [[Personal Data Ecosystem Consortium]]'s ''Startup Circle.''&lt;ref&gt;{{cite web |url=http://pde.cc/startup-circle/#2011 |title=Members of the PDEC Startup Circle |website=Personal Data Ecosystem Consortium |accessdate=20 August 2014}}&lt;/ref&gt; A Small Data [[Meetup (website)|Meetup]] group has also formed in New York City, bringing together technology, legal and business experts to exchange ideas about user-centric and user-driven models for internet products and services.&lt;ref&gt;http://www.meetup.com/smalldata/&lt;/ref&gt; Personal ultimately raised $24 million, including $4.5m from Bill Miller of [[Legg Mason]] and [[Esther Dyson]] of EDventures in October 2013.&lt;ref&gt;{{cite web |url=http://www.reuters.com/article/2013/10/15/idUS412005883920131015 |title=Personal raises $4.5 million to be the personal data vault we so desperately need |date=15 October 2013 |website=Reuters |accessdate=8 August 2014}}&lt;/ref&gt;

==Products and services ==

===Overview===
The Personal Platform was a privacy- and security-by-design platform for individuals to manage and reuse their own data and information. The Fill It app was a 1-click form-filling solution for web and mobile logins, checkouts and forms, and the Data Vault app served as the main cloud-based repository for a user's data. Personal helped individuals take control and benefit from their information while knowing that the information in their Data Vault remained legally theirs and could not be used without their permission.&lt;ref&gt;{{Cite web|url=http://www.zdnet.com/article/intel-execs-on-big-data-and-privacy-its-a-balancing-act/|title=Intel execs on big data and privacy: It's a balancing act {{!}} ZDNet|last=King|first=Rachel|website=ZDNet|access-date=2016-10-03}}&lt;/ref&gt;

===Data Vault with Cloud Sync===
Personal spent two years building the Personal Platform before launching its Data Vault product in beta in November 2011. Following [[Privacy by Design]] principles, Personal only enabled users to see or share the sensitive data and all the files they stored in their Data Vault. Such information was encrypted, and could only be decrypted with a user&#8217;s password. Only users could choose and know their passwords to their vault because Personal did not store user passwords &#8211; and therefore could not reset them without deleting a user&#8217;s sensitive data and all files stored in their vault.&lt;ref&gt;{{cite web |url=http://www.ipc.on.ca/images/Resources/pbd-pde.pdf |title=Privacy by Design and the Emerging Personal Data Ecosystem |last=Cavoukian |first=Ann |last2=Green |first2=Shane |date=October 2012 |website=Office of the Information and Privacy Commissioner |accessdate=8 August 2014}}&lt;/ref&gt; All Personal apps and services were linked to a user&#8217;s private Data Vault.

The Data Vault featured automatic synchronization of data and files added on any device logged into Personal. It also featured a &#8220;Secure Share&#8221; function that created a live, private network, allowing registered users to share access to data and files through an exchange of encrypted keys without the risk of transmitting the data or files through non-secure, direct means. It also allowed users to immediately update data across their own network and revoke access to it when they choose.

Personal launched its [[Android (operating system)|Android]] app on November 30, 2011.&lt;ref name=mashable&gt;{{cite web |url=http://mashable.com/2011/11/17/personal/ |title=Never Fill Out a Form Again? Personal Seeks to Be the Data Vault for Your Private Information |last=Parr |first=Ben |date=17 November 2011 |website=Mashable |accessdate=8 August 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=https://www.personal.com/s/pages/news/personal-android-release/ |title=Personal Releases Android App for Its Private, Personal Network and Data Vault Service |date=30 November 2011 |website=Personal, Inc. |accessdate=8 August 2014}}&lt;/ref&gt; The [[iOS]] Data Vault app was released on May 7, 2012.&lt;ref&gt;{{cite web |url=http://techcrunch.com/2012/05/07/personal-takes-its-secure-vault-for-all-of-your-private-digital-data-mobile-with-ios-app/ |title=Personal Takes Its Secure Vault For All Of Your Private, Digital Data Mobile WIth iOS App |last=Rao |first=Leena |date=7 May 2012 |website=TechCrunch |accessdate=8 August 2014}}&lt;/ref&gt; Personal officially launched its [[Application programming interface|application programming interface (APIs)]] on October 2, 2012 at the Mashery Business of APIs Conference.&lt;ref&gt;{{cite web |url=https://www.personal.com/s/pages/news/personal-launches-personal-platform/ |title=Personal Launches 'Personal Platform at Business of APIs Conference, Opening APIs for Developers |date=2 October 2012 |website=Personal, Inc. |accessdate=8 August 2014}}&lt;/ref&gt; A review by [[CNET]] highlighted the challenges of getting people to trust such a new service with their sensitive data and spending the time required entering enough data to make it useful.&lt;ref&gt;{{cite web |url=http://www.cnet.com/news/what-hump-personals-private-database-faces-challenges/ |title=What hump? Personal's private database faces challenges |last=Needleman |first=Rafe |date=30 November 2011 |website=CNET |accessdate=8 August 2014}}&lt;/ref&gt;

===Fill It App and Form Index===
When the Data Vault was launched in November 2011, ''[[Mashable]]'' posed the question: &#8220;Never Fill Out a Form Again?&#8221;&lt;ref name=mashable /&gt; The [[World Economic Forum]] in its February 2013 report highlighted the possibility of saving 10 billion hours globally &#8220;and improv[ing] the delivery of public and private sector services&#8221; through automated form-filling tools, specifically citing Personal&#8217;s Fill It app.&lt;ref&gt;{{cite web |url=http://www3.weforum.org/docs/WEF_IT_UnlockingValuePersonalData_CollectionUsage_Report_2013.pdf |title=Unlocking the Value of Personal Data: From Collection to Usage |date=February 2013 |website=World Economic Forum |accessdate=8 August 2014}}&lt;/ref&gt; In January 2013, Personal launched Fill It in beta as a web bookmarklet for automatic form-filling.&lt;ref&gt;{{cite web |url=http://www.digitaltrends.com/web/personal-coms-new-fill-it-feature-makes-quick-work-of-long-web-forms/#!bBp6iJ |title=PERSONAL.COM'S NEW FILL IT APP MAKES QUICK WORK OF LONG ONLINE FORMS |last=Couts |first=Andrew |date=16 January 2013 |website=Digital Trends |accessdate=8 August 2014}}&lt;/ref&gt;

On June 11, 2014, Personal released Fill It as a web extension and announced that it was publishing an index of over 140,000 1-click online forms at www.fillit.com.&lt;ref name=extensionlaunch&gt;{{cite web |url=http://tech.co/dc-based-startup-personal-launches-fill-it-for-quick-and-safe-auto-filling-on-online-forms-2014-06 |title=DC-Based Personal Launches Fill It for Quick and Safe Auto-Filling on Online Forms |last=Barba |first=Ronald |date=16 June 2014 |website=Tech Cocktail |accessdate=8 August 2014}}&lt;/ref&gt; The company also announced that a mobile version of the product will launch later in the year. According to a story in ''[[Tech Cocktail]]'' about the launch, Personal&#8217;s &#8220;web extension and mobile app are able to support over 1,200 different types of reusable data, even enabling them to unlock more confidential information so they can complete longer forms, including patient registrations, job applications, event registrations, school admissions, insurance and bank applications, and government forms.&#8221;&lt;ref name=extensionlaunch /&gt; In November 2014, a mobile version of Fill It was launched that could autofill mobile forms using APIs.&lt;ref&gt;{{Cite web|url=http://tech.co/personal-launches-fill-it-mobile-2014-11|title=Personal Launches Fill It Mobile at #pii2014|date=2014-11-13|language=en-US|access-date=2016-10-03}}&lt;/ref&gt;

Personal&#8217;s form portal ultimately indexed more than 500,000 forms with three components, which, together, allowed data to be captured and reused across any of the forms: (1) a form graph, which mapped individual form fields to the Personal ontology; (2) a semantic layer, which determined how data was required on a form (e.g. one field vs. three fields for a U.S. telephone number); and (3) a correlations graph, which helped individuals match their specific data to a form without looking at the data value (e.g. knowing which phone number is a mobile phone number, which address is a billing address, or that a person uses their middle name as a first name on most forms).&lt;ref&gt;{{cite web |url=http://tech.co/dc-startup-personal-university-data-privacy-security-2014-08 |title=Personal Launches "Personal University," a Video Series on Data Privacy and Security |last=Barba |first=Ronald |date=8 August 2014 |website=Tech Cocktail |accessdate=8 August 2014}}&lt;/ref&gt;

===Monetizing personal data===
With the [[initial public offering]] of [[Facebook]] in May 2012, there was media interest in the question of the monetary value of personal data and whether tools and services might emerge to help consumers monetize their own data. Personal was frequently cited as a company that could potentially offer such a service. Articles and pieces focusing on this subject have appeared in ''[[The New York Times]]'', ''[[AdWeek]]'', the ''[[MIT Technology Review]]'', and on ''[[CNN]]'' and ''[[National Public Radio]]''.&lt;ref&gt;{{cite web |url=http://www.nytimes.com/2012/02/13/technology/start-ups-aim-to-help-users-put-a-price-on-their-personal-data.html?_r=0 |title=Start-Ups Seek to Help Users Put a Price on Their Personal Data |last=Brustein |first=Joshua |date=12 February 2012 |website=The New York Times |accessdate=8 August 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://www.technologyreview.com/view/426235/is-personal-data-the-new-currency/ |title=Is Personal Data the New Currency? |last=Zax |first=David |date=30 November 2011 |website=MIT Technology Review |accessdate=8 August 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://edition.cnn.com/2012/02/24/tech/web/owning-your-data-online |title=Manage (and make cash with?) your data online |last=Gross |first=Doug |date=27 February 2012 |website=CNN |accessdate=8 August 2014}}&lt;/ref&gt; Company Co-founder and CEO Shane Green was quoted as saying that &#8220;the average American consumer would soon be able to realize over $1,000 per year&#8221; by granting limited, anonymous access to their data to marketers, but that figure was never supported by Green or the company.&lt;ref&gt;{{cite web |url=http://www.ft.com/cms/s/2/61c4c378-60bd-11e2-a31a-00144feab49a.html#axzz39ptB1It4 |title=Data mining offers rich seam |last=Palmer |first=Maija |date=18 February 2013 |website=Financial Times |accessdate=8 August 2014}}&lt;/ref&gt;

==Reception and Re-launch as TeamData==
Personal was the first online consumer-facing company to be named an Ambassador for [[Privacy by Design]] for its technical, business and legal commitments to providing users with control over the data they store in Personal&#8217;s service.&lt;ref&gt;{{cite web |url=http://www.privacybydesign.ca/index.php/ambassador/personal-com/ |title=Personal.com |website=Privacy by Design |accessdate=15 August 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://www.privacybydesign.ca/content/uploads/2010/03/2011-10-24-Personal.com_.pdf |title=Personal and Privacy by Design |website=Privacy by Design |accessdate=20 August 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://www.privacybydesign.ca/index.php/ambassador/joshua-p-galper/ |title=Joshua P. Galper |website=Privacy by Design |accessdate=20 August 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://www.privacybydesign.ca/index.php/ambassador/shane-green/ |title=Shane Green |website=Privacy by Design |accessdate=20 August 2014}}&lt;/ref&gt; The company received recognition for its user agreement, called the Owner Data Agreement,&lt;ref&gt;{{cite web |url=https://www.personal.com/owner-data-agreement/ |title=Owner Data Agreement |date=7 February 2014 |website=Personal, Inc. |accessdate=8 August 2014}}&lt;/ref&gt; which acted like a reverse license agreement when data was shared between registered parties and emphasized that data ownership resides with the user. [[Doc Searls]] wrote in ''[[The Intention Economy: When Customers Take Charge]]'' that the Owner Data Agreement &#8220;had no precedent and modeled a new legal position, both for vendors and for intermediaries.&#8221;&lt;ref&gt;{{cite book |last=Searls |first=Doc |date=May 1, 2012 |title=The Intention Economy: When Customers Take Charge |publisher=Harvard Business Review Press |page=186 |isbn=978-1422158524 }}&lt;/ref&gt; 
[[Fast Company (magazine)|Fast Company]] called the Data Vault &#8220;a tool that will simplify our lives.&#8221;&lt;ref&gt;{{cite web |url=http://www.fastcompany.com/1836521/personalcom-creates-online-vault-manage-all-your-data |title=PERSONAL.COM CREATES AN ONLINE VAULT TO MANAGE ALL YOUR DATA |last=Boyd |first=E.B. |date=7 May 2012 |website=Fast Company |accessdate=8 August 2014}}&lt;/ref&gt; Personal has been included in case studies by Ctrl-Shift and Forrester regarding Personal Data Stores and Personal Identity Management.&lt;ref&gt;{{cite web |url=https://www.ctrl-shift.co.uk/index.php/research/product/64 |title=Personal Data Stores |website=Ctrl-Shift |accessdate=20 August 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://blog.personal.com/uploads/2011/10/Forrester-Research-personal_identity_management.pdf |last=Khatibloo |first=Fatemeh |last2=Frankland |first2=Dave |last3=Maler |first3=Eve |last4=Smith |first4=Allison |date=30 September 2011 |title=Personal Identity Management |website=Forrester |accessdate=20 August 2014}}&lt;/ref&gt;

In 2011, Personal received the Innovator Spotlight Award at Privacy Identity Innovation Conference (pii2011) and participated in the Technology Showcase at pii2012.&lt;ref&gt;{{cite web |url=http://www.prweb.com/releases/2011/5/prweb8484188.htm |last=Fonseca |first=Natalie |title=Personal and Passtouch Receive Innovator Spotlight Award at Privacy Identity Innovation Conference (pii2011) |website=PRWeb |accessdate=20 August 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=https://www.privacyidentityinnovation.com/pii2012-seattle/pii2012-technology-showcase |title=pii2012 Technology Showcase |website=Privacy Identity Innovation |accessdate=20 August 2014}}&lt;/ref&gt; In 2012, TechHive named Personal as one of the top five apps or web services of [[SXSW]].&lt;ref&gt;{{cite web |url=http://www.techhive.com/article/251744/hot_apps_and_web_services_of_sxsw.html |last=Sullivan |first=Mark |date=13 March 2012 |title=Hot Apps and Web Services of SXSW |accessdate=20 August 2014}}&lt;/ref&gt; Personal won the 2013 Campus Technology Innovators Award with Lone Star College in July 2013.&lt;ref&gt;{{cite web |url=http://campustechnology.com/articles/2013/07/23/2013-innovators-awards.aspx |last=Raths |first=David |last2=Namahoe |first2=Kanoe |last3=Lloyd |first3=Meg |date=23 July 2013 |title=2013 Innovators Awards |website=Campus Technology |accessdate=20 August 2014}}&lt;/ref&gt; Personal was included in a list of Executive Travel Magazine's favorite travel apps for 2013 in its May/June issue.&lt;ref&gt;{{citation |url=http://www.executivetravelmagazine.com/articles/ets-favorite-travel-apps-for-2013 |last=Null |first=Christopher |title=ET's Favorite Travel Apps of 2013 |archiveurl=https://web.archive.org/web/20131023184535/http://www.executivetravelmagazine.com/articles/ets-favorite-travel-apps-for-2013 |archivedate=2013-10-23 |accessdate=20 August 2014}}&lt;/ref&gt;
In 2013, Personal was also included as part of NYU GovLab's Open Data 500 and was named by J. Walter Thompson as one of 100 things to watch for in 2014.&lt;ref&gt;{{cite web |url=http://www.opendata500.com/us/Personal-Inc/ |title=Personal, Inc. |website=Open Data 500 |accessdate=20 August 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://www.jwtintelligence.com/2013/12/100-watch-2014/#axzz2qyBVCrMs |last=Mack |first=Ann |date=26 December 2013 |title=100 Things to Watch in 2014 |website=JWT Intelligence |accessdate=20 August 2014}}&lt;/ref&gt; In 2015, the National Law Journal named Company Chief Policy Officer and General Counsel, Joshua P. Galper, as one of their 50 "Cybersecurity &amp; Privacy Trailblazers."&lt;ref&gt;{{Cite web|url=http://pdfserver.amlaw.com/nlj/flipbook/Cybersecurity_Trailblazers_2015/Cyber_Security_Trailblazers_2015_Web.html#p%253D14%2523p=8|title=Cybersecurity_Trailblazers|website=pdfserver.amlaw.com|access-date=2016-10-03}}&lt;/ref&gt;

In May 2016, Personal Co-Founder and CEO Shane Green announced the launch of TeamData with one of the other co-founders, Tarik Kurspahic, and new board chair [[Eric C. Anderson]]. TeamData focuses on the problem of securing and collaboratively managing data in the workplace, and is based on the technology and platform of Personal.&lt;ref&gt;{{Cite web|url=https://medium.com/@shanegreen/why-personal-com-graduated-to-teamdata-today-f75e0d539ba1#.yrkhukyec|title=Why Personal.com "graduated" to TeamData today|last=Green|first=Shane|date=2016-05-20|access-date=2016-10-03}}&lt;/ref&gt; Onboardly included the new collaborative TeamData solution in its list of "Top 10 apps to keep your team on track" and as part of its Top 50 list of "all time best content marketing tools."&lt;ref&gt;{{Cite web|url=http://onboardly.com/content-marketing/all-time-best-tools-for-content-marketing-teams/|title=All Time Best Tools for Content Marketing Teams via @Onboardly|date=2016-04-07|language=en-US|access-date=2016-10-03}}&lt;/ref&gt;

==References==
{{reflist|2}}

==External links==
*{{Official website}}
**{{URL|www.fillit.com|Fill It homepage}}
*{{ITunes Preview App|493536192|Personal}}
*{{ITunes Preview App|910517122|Fill It}}
*{{Google Play|com.personal.android|Personal}}
*{{Google Play|com.personal.fillit|Fill It}}
*[http://tech.co/tag/personal Personal] collected news and commentary at ''[[Tech Cocktail]]''
*{{Crunchbase|Personal|Personal}}

[[Category:American websites]]
[[Category:Android (operating system) software]]
[[Category:Companies based in Washington, D.C.]]
[[Category:Companies established in 2009]]
[[Category:Data management]]
[[Category:Data security]]
[[Category:Internet companies of the United States]]
[[Category:Internet properties established in 2009]]
[[Category:IOS software]]</text>
      <sha1>adwdknhj13sx1i4rkk0huqyf6zvnxhh</sha1>
    </revision>
  </page>
  <page>
    <title>AnalytiX DS</title>
    <ns>0</ns>
    <id>43846669</id>
    <revision>
      <id>739647310</id>
      <parentid>735226992</parentid>
      <timestamp>2016-09-16T01:25:07Z</timestamp>
      <contributor>
        <username>Dl2000</username>
        <id>917223</id>
      </contributor>
      <minor />
      <comment>en-IN</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7025" xml:space="preserve">{{Use dmy dates|date=September 2016}}
{{Use Indian English|date=September 2016}}
{{Orphan|date=September 2014}}

{{Infobox company
| name             =AnalytiX DS
| logo             =[[File:AnalytiX DS Logo.png]]
| type             =[[Private limited company|Private]]
| location_city    =[[Chantilly, Virginia|Chantilly]], [[Virginia]]
| location_country =[[United States]]
| founder          ={{Unbulleted list|Mike Boggs|}} 
| foundation       =2006 
| area_served      =Worldwide
| key_people       =Madan K. ([[CEO]])&lt;br /&gt;Mike Boggs ([[Chief Technology Officer|CTO]] &amp; Founder)&lt;br/&gt;
Sam Benedict (VP-Sales &amp; Marketing)&lt;br/&gt; John Carter (Director of Professional Services)
| industry         =[[Software Company]]
| services         =IT, [[business consulting]] and automation services
| homepage         ={{URL|www.analytixds.com}}
| intl             =yes
}}
''' AnalytiX Data Services '''  is a software vendor that provides specialized data mapping and ETL conversion tools for [[data integration]], [[data management]], [[enterprise application integration]] and [[big data]] software and services.&lt;ref&gt;{{cite web|title=Mapping Manager, the missing link in moving data around|url=http://www.bloorresearch.com/analysis/analytix-mapping-manager-missing-link-moving-data|publisher=Bloor|accessdate=17 September 2014}}&lt;/ref&gt;  Headquarter's based in Chantilly, Virginia, AnalytiX DS has offices in Dallas, TX and Hyderabad, India and an international network of technical and services partners.&lt;ref&gt;{{cite web|title=AnalytiX DS Partners|url=http://analytixds.com/technology-partners/}}&lt;/ref&gt;

Michael Boggs, the CTO and founder of AnalytiX DS coined the term "Pre ETL Mapping" now it is widely used and accepted synonym for design phase of data integration. 

Mapping Manager is the flagship product of AnalytiX DS, an agile Unified Platform designed to govern and accelerate data integration processes by eliminating manual processes and replacing them with software designed to automate, govern and accelerate manual processes.&lt;ref&gt;{{cite web|title=Mapping Manager|url=http://analytixds.com/amm/}}&lt;/ref&gt;  Several versions of Mapping Manager were released since the release of its first version in 2006. 

AnalytiX DS in April 2016 launched Mapping Manager Version 7.0, a major release version which extends the unified platform for enterprise data mapping, governance and automation.&lt;ref&gt;{{cite web|title=AnalytiX Data Services announces the launch of major release of Mapping Manager Version 7.0|url=http://www.pr.com/press-release/680901|publisher= PR |accessdate= 27 July 2016}}&lt;/ref&gt;The latest release has added plenty of features and revolutionary modules never before seen.

With every new release, out of box features and functionalities were introduced. Later, AnalytiX DS began to add new modules including Release Management, Reference Data Manager, Code Set Manager, CATfX, LiteSpeed Conversion, Code automation Templates for Data Vault, Mapping Manager Big Data Edition, Data Quality Assessment Manager(DQAM), Metadata Management, Data Vault-Code Gen Bundle, and Test Manager, which extends the tools capabilities above and beyond management of the data mapping process.

[[Mapping Manager Big Data Edition]]:&lt;ref&gt;{{cite web|title=Mapping Manager Big Data Edition|url=http://analytixds.com/mapping-manager-bigdata-edition/}}&lt;/ref&gt;  Helps automate the big data mapping process and provides a bridge between structured and unstructured data to meet big data challenges.

[[Release Management]]:&lt;ref&gt;{{cite web|title=AnalytiX DS Release Manager|url=http://analytixds.com/arm/}}&lt;/ref&gt; Helps track the release process approvals, audits and verifications through the approval process.

[[Reference Data Manager]]:&lt;ref&gt;{{cite web|title=AnalytiX DS Reference Data Manager|url=http://analytixds.com/rdm/}}&lt;/ref&gt;  Helps create database like structure to maintain all reference data. 

[[Code Set Manager]]:&lt;ref&gt;{{cite web|title=AnalytiX DS CodeSet Manager|url=http://analytixds.com/code-set-manager/}}&lt;/ref&gt;  Helps drive the organization of user defined reference data and Code Sets across an enterprise.

[[Customizable Code-Automation Framework (CATfX)]]:&lt;ref&gt;{{cite web|title=AnalytiX DS Customizable Code-Automation Framework (CATfX)|url=http://analytixds.com/catfx-code-automation-templates/}}&lt;/ref&gt;  Helps automate manual coding and tasks for ETL integration and data profiling, Testing Automation and more.

[[LiteSpeed Conversion]]:&lt;ref&gt;{{cite web|title=AnalytiX DS LiteSpeed Conversion|url=http://analytixds.com/litespeed-conversion/}}&lt;/ref&gt;  Helps automate the conversion of ETL tool platforms through an automated framework.

[[Code automation Templates for Data Vault]]:&lt;ref&gt;{{cite web|title=AnalytiX DS Code automation Templates for Data Vault|url=http://analytixds.com/data-vault-automation/}}&lt;/ref&gt; Helps automatically generate the Hub ETL code, Link ETL code and the Satellite ETL code through your existing ETL Platform. 

[[Data Quality Assessment Manager(DQAM)]]:&lt;ref&gt;{{cite web|title=AnalytiX DS Data Quality Assessment Manager(DQAM)|url=http://analytixds.com/data-quality-assessment-manager/}}&lt;/ref&gt;  Helps standardize and execute a formal data quality assessment methodology. 

[[Metadata Management]]:&lt;ref&gt;{{cite web|title=AnalytiX DS Metadata Management|url=http://analytixds.com/metadata-management/}}&lt;/ref&gt; Helps metadata in Mapping Manager Unified Platform to be ported into third party metadata environments. 

[[Data Vault-Code Gen Bundle]]:&lt;ref&gt;{{cite web|title=AnalytiX DS Data Vault-Code Gen Bundle|url=http://analytixds.com/data-vault-code-gen-bundle/}}&lt;/ref&gt;  Helps automate the code-generation process for building the Data Vault through Code automation Templates (CAT's). 

[[Test Manager]]:&lt;ref&gt;{{cite web|title=AnalytiX DS Test Manager|url=http://analytixds.com/test-manager/}}&lt;/ref&gt; Helps Test Cases and Test SQL generation to be managed in a purpose built module for testing data mappings and ETL processes. 

Recently, AnalytiX DS expanded its presence in the US with the opening of a new office in Dallas. 

AnalytiX DS has been named to CIO Review's 20 Most PROMISING PRODUCTIVITY TOOLS SOLUTION PROVIDERS 2015.&lt;ref&gt;{{cite web|title=CIO Review's 20 Most PROMISING PRODUCTIVITY TOOLS SOLUTION PROVIDERS 2015|url=http://www.pr.com/press-release/662862|publisher= PR |accessdate= 18 March 2016}}&lt;/ref&gt;AnalytiX DS is a platinum sponsor for WWDVC 2016.&lt;ref&gt;{{cite web|title=AnalytiX Data Services To Platinum-Sponsor for WWDVC 2016|url=http://wwdvc.com/sponsors/}}&lt;/ref&gt;


== References ==
{{reflist}}

==External links==
* {{official website|http://www.analytixds.com/}}
* {{YouTube|u=AnalytiXDS|AnalytiX DS}}
* {{Facebook|AnalytiX.Data.Services|AnalytiX DS}}

[[Category:Software companies of India]]
[[Category:Data management]]
[[Category:Extract, transform, load tools]]
[[Category:Data mapping]]
[[Category:Data warehousing products]]
[[Category:International information technology consulting firms]]
[[Category:Multinational companies headquartered in the United States]]</text>
      <sha1>kdivrxcn5ho7e2mar2z5ore2z5khs8z</sha1>
    </revision>
  </page>
  <page>
    <title>Rtolap</title>
    <ns>0</ns>
    <id>2878165</id>
    <revision>
      <id>735451509</id>
      <parentid>634627779</parentid>
      <timestamp>2016-08-20T20:24:48Z</timestamp>
      <contributor>
        <username>W Nowicki</username>
        <id>9177019</id>
      </contributor>
      <comment>remove uncited promotion</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2014" xml:space="preserve">{{multiple issues|
{{Unreferenced|date=October 2008}}
{{Original research|date=October 2008}}
}}

==RTOLAP - Real Time OLAP==

Whilst many [[OLAP]] Servers like [[Microsoft Analysis Services]] store pre-calculating consolidations and calculated elements to achieve rapid response times. A Real Time OLAP Server will calculate the values on the fly, when they are required. 
The essential characteristic of RTOLAP system is in holding all the data in RAM.

It is a protocol which analyzes fly values when required. It saves every bit of information in RAM. The calculations are executed in a &#8220;right-away&#8221; manner which reduces the setback linked with &#8220;information outburst&#8221; since it only saves information under the RAM size standard.

== Advantages ==
* Since precalculated values aren't stored, the size of a cube in an RTOLAP system is smaller than of an OLAP product which resorts to precalculation. RTOLAP often reduces the problem which may be associated with "Data explosion", by  means of storing less data. 
* RTOLAP essentially performs calculations "just-in-time" by only calculating values when they are needed space can be saved, since in a precalculated system, a great deal of calculations will be stored which may well never be called up.
* Incremental updates are available once they are loaded, and any modifications to data will flow through the system immediately. With RTOLAP when a change is made, everyone sees the result. This isn't a unique characteristic of RTOLAP, since other OLAP systems (e.g. [[SAS Institute]], [[Microsoft Analysis Services]], [[MicroStrategy]]) behave the same way.

== Disadvantages ==
* Since RTOLAP stores the entire cube in RAM, it doesn't scale to the data volumes larger than the RAM size
* Performance of queries can be slower since the values need to be calculated on the fly instead of being accessed from the precalculated storage

[[Category:Data management]]
[[Category:Information technology management]]
[[Category:Online analytical processing]]</text>
      <sha1>aa6miwaxvdmsypkaoh8k4thu2yc0jqa</sha1>
    </revision>
  </page>
  <page>
    <title>Data recovery hardware</title>
    <ns>0</ns>
    <id>46219812</id>
    <revision>
      <id>757508648</id>
      <parentid>754251533</parentid>
      <timestamp>2016-12-31T02:15:00Z</timestamp>
      <contributor>
        <username>Fixuture</username>
        <id>19796795</id>
      </contributor>
      <comment>new key for [[Category:Data recovery]]: "*" using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3808" xml:space="preserve">{{more footnotes|date=March 2015}}

'''Data recovery hardware''' was developed because [[data recovery software]] lacks the ability to deal with all lost or corrupted data files. Often the failures, such as media files with [[bad sectors]], [[firmware]] failures, PCB ([[Printed circuit board]]) failures, hard drive head failures, etc., cannot be fixed.

==Bad sectors==

The two types of bad sectors are "physical" and "logical" bad sectors, or "hard" and "soft" bad sectors.&lt;ref&gt;{{cite web
 | url = http://www.howtogeek.com/173463/bad-sectors-explained-why-hard-drives-get-bad-sectors-and-what-you-can-do-about-it/
 | title = Bad Sectors Explained: Why Hard Drives Get Bad Sectors and What You Can Do About It
 | date = October 9, 2013| accessdate = March 26, 2015
 | author = Chris Hoffman  | website = How-To Geek, LLC
}}&lt;/ref&gt;

When a disk has physical bad sectors, software cannot effectively offer soft reset, hard reset, power reset, error handling, read algorithm auto exchange nor skip sectors. If a disk with bad physical sectors is connected to a [[Personal computer|PC]], the condition would potentially not be detected.

Soft bad sectors can potentially be fixed by either data recovery software or hardware, depending on the damaged condition. Some amount of bad sectors can be skipped using software, while a severely corrupted disk with a large area of bad sectors may potentially only be repaired.&lt;ref&gt;{{cite web
 | url = https://www.winxdvd.com/resource/repair-mp4-file-free.htm
 | title = How to Repair Corrupted MP4 Video File
 | date = July 3, 2015| accessdate = August 24, 2016
 | author = Estrella Garcia  | website = WinXDVD
}}&lt;/ref&gt;  Bad sectors are areas on the hard drive that cannot be read. Even new hardrives sometimes contain bad sectors. Since manufacturers intensely compete to cram more space into disks, systems operate close to the limit of that generation of technology.&lt;ref&gt;{{Cite web|url=https://www.grc.com/sr/faq.htm|title=GRC {{!}} SpinRite 6.0 FAQ - Frequently Asked Questions|website=www.grc.com|access-date=2016-10-20}}&lt;/ref&gt;

==Dead PCB==

When the drive has dead PCB ([[Printed circuit board]]), users need to:
* swap in a new PCB
* put one donor [[Integrated circuit|chip]] and write by chip reader with matching ROM ([[Read-only memory]]) content
* get the dead drive spinning

When the drive has physical head damage, users need to open the drive in [[cleanroom]] environment and find donor heads or other donor components to swap.

==Data recovery hardware types==
*Disk image;
*File extraction hardware;
*Firmware repair hardware;
*ROM chip reader;
*Head and Platter Swap Tools (See [[Hard disk drive platter]]);
*Spindle release hardware;
*Other hardware

== See also ==
{{Portal|Computer security|Computing}}

{{Div col||20em}}
* [[Data recovery]]
* [[Firmware]]
* [[Bad sector]]
* [[Disk image]]
* [[Printed circuit board]]
* [[Cleanroom]]
* [[List of data recovery software]]
* [[Comparison of file systems]]
* [[Computer forensics]]
* [[Continuous data protection]]
* [[Data archaeology]]
* [[Data loss]]
* [[Error detection and correction]]
* [[File carving]]
* [[Hidden file and hidden directory]]
* [[Knowledge extraction]]
* [[Undeletion]]
{{Div col end}}

==Further reading==
* Tanenbaum, A. &amp; Woodhull, A. S. (1997). ''Operating Systems: Design And Implementation,'' 2nd ed. New York: Prentice Hall.
* {{dmoz|Computers/Hardware/Storage/Data_Recovery/|Data recovery}}
* {{cite web|url=https://www.grc.com/sr/faq.htm|title=GRC&amp;nbsp;-&amp;nbsp;SpinRite 6.0 FAQ - Frequently Asked Questions|publisher=}}

==References==
{{Reflist|30em}}

{{DEFAULTSORT:Data Recovery}}
[[Category:Data recovery|*]]
[[Category:Computer data]]
[[Category:Data management]]
[[Category:Transaction processing]]
[[Category:Hard disk software|*]]
[[Category:Backup|Recovery]]</text>
      <sha1>cg6leipv6545xqw7o0x3njop1o20kb8</sha1>
    </revision>
  </page>
  <page>
    <title>Software intelligence</title>
    <ns>0</ns>
    <id>45124802</id>
    <revision>
      <id>718367937</id>
      <parentid>653767789</parentid>
      <timestamp>2016-05-03T03:19:43Z</timestamp>
      <contributor>
        <username>Ljgua124</username>
        <id>2738115</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="780" xml:space="preserve">{{one source|date=January 2015}}
'''Software Intelligence''' ('''SI''') is [[software]] designed to analyze [[source code]] to better understand [[Information technology|Information Technology]] environments. Similarly to [[Business intelligence|Business Intelligence]] (BI), Software Intelligence is a set of software tools and techniques for the [[Data mining|mining of data]] into meaningful and useful information.&lt;ref&gt;{{cite web|last1=Hassan|first1=Ahmed|last2=Xie|first2=Tao|title=Software Intelligence: The Future of Mining Software Engineering Data|url=http://web.engr.illinois.edu/~taoxie/publications/foser10-si.pdf|website=http://web.engr.illinois.edu|accessdate=19 January 2015}}&lt;/ref&gt;

==References==
{{Reflist}}

[[Category:Data management]]
[[Category:Source code]]</text>
      <sha1>ao18cg1oukc7m7frti9mh4afgx7jos8</sha1>
    </revision>
  </page>
  <page>
    <title>COMMIT (SQL)</title>
    <ns>0</ns>
    <id>46362717</id>
    <revision>
      <id>655680694</id>
      <timestamp>2015-04-09T14:08:00Z</timestamp>
      <contributor>
        <username>Skssxf</username>
        <id>16077845</id>
      </contributor>
      <comment>[[WP:AES|&#8592;]]Created page with '{{Unreferenced|date=April 2015}} A &lt;code&gt;COMMIT&lt;/code&gt; statement in [[SQL]] ends a [[database transaction|transaction]] within a relational database management...'</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1092" xml:space="preserve">{{Unreferenced|date=April 2015}}
A &lt;code&gt;COMMIT&lt;/code&gt; statement in [[SQL]] ends a [[database transaction|transaction]] within a [[relational database management system]] (RDBMS) and makes all changes visible to other users. The general format is to issue a &lt;code&gt;[[Begin work (SQL)|BEGIN WORK]]&lt;/code&gt; statement, one or more SQL statements, and then the &lt;code&gt;COMMIT&lt;/code&gt; statement. Alternatively, a &lt;code&gt;[[Rollback (data management)|ROLLBACK]]&lt;/code&gt; statement can be issued, which undoes all the work performed since &lt;code&gt;BEGIN WORK&lt;/code&gt; was issued. A &lt;code&gt;COMMIT&lt;/code&gt; statement will also release any existing [[savepoint]]s that may be in use.

In terms of transactions, the opposite of commit is to discard the tentative changes of a transaction, a [[rollback (data management)|rollback]].

==See also==
* [[Commit (data management)]]
* [[Atomic commit]]
* [[Two-phase commit protocol]]
* [[Three-phase commit protocol]]

{{databases}}

{{DEFAULTSORT:Commit (Data Management)}}
[[Category:Data management]]
[[Category:SQL]]
[[Category:Transaction processing]]

{{comp-sci-stub}}</text>
      <sha1>s6ikhk8l9cz53v6vcsls8on6aoinozu</sha1>
    </revision>
  </page>
  <page>
    <title>Data based decision making</title>
    <ns>0</ns>
    <id>46235825</id>
    <revision>
      <id>748811865</id>
      <parentid>699147434</parentid>
      <timestamp>2016-11-10T14:55:27Z</timestamp>
      <contributor>
        <username>CitationCleanerBot</username>
        <id>15270283</id>
      </contributor>
      <minor />
      <comment>/* General references */clean up, url redundant with jstor, and/or remove accessdate if no url using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8466" xml:space="preserve">{{more footnotes|date=May 2015}}

'''Data based decision making''' or data driven decision making refers to educator&#8217;s  ongoing process of collecting and analyzing different types of data, including demographic, student achievement test, satisfaction, process data to guide decisions towards improvement of educational process. DDDM becomes more important in education since federal and state test-based  accountability policies. [[No Child Left Behind Act]] opens broader opportunities and incentives in using [[data]] by educational organizations by requiring schools and districts to analyze additional components of data, as well as pressing them to increase student test scores. Information makes schools accountable for year by year improvement various student groups. DDDM helps to recognize the problem and who is affected by the problem; therefore, DDDM can find a solution of the problem

==Purpose==

The purpose of DDDM is to help educators, schools, districts, and states to use information they have to actionable knowledge to improve student outcomes. DDDM requires high-quality data and possibly technical assistance; otherwise, data can misinform and lead to unreliable inferences. [[Data management]] techniques can improve teaching and learning in schools. Test scores are used by many principals to identify &#8220;bubble kids&#8221;, students whose results are just below proficiency level in reading and mathematics.&lt;ref name=r1&gt;{{cite journal|last1=Mandinach|first1=Ellen|title=A perfect time for data use|journal=Educational Psychologist|date=April 23, 2012|volume=47|page=2|doi=10.1080/00461520.2012.667064}}&lt;/ref&gt;

==Types of data used in education==

There are 4 major types of data used in education: demographics data, perceptions data, student learning data, and school processes data.&lt;ref name=Bernhardt&gt;{{cite book|last1=Bernhardt|first1=Victoria|title=Data analysis for continuous school improvement|date=2013|publisher=Routledge|location=711 Third Avenue, New York, 10017|isbn=978-1-59667-252-9|pages=27&#8211;80}}&lt;/ref&gt;

1. Demographics data in educational organizations answers the question, "Who are we?". Demographics show the current context of the school and shows the trends. Trends help to predict and plan for the future, along with seeing measures where leaders work towards continuous school improvement. Thorough demographic data explains the structure of school, system, and the leadership. In education demographic data to the next items: number of students in the school, number of students with special needs, number of English learners, age or grade of students in cohorts, socio-economical status of students, attendance rates, [[ethnicity]]/[[race (human classification)|race]]/[[religious beliefs]], graduation rates, dropout rates, experience information of teachers, information about parents of students.&lt;ref name="Bernhardt"/&gt;

2. Perception data tells us what students, staff, and parents think about a school and answers the question, "How do we do business?". School culture, climate, and organizational processes are assessed by perception data. Perception data includes values, beliefs, perceptions, opinions, observations. Perception data is collected mostly questionnaires. Perception data can be differentiate by two groups: 1- staff, 2 - students and parents. Staff are being asked if any changes in instruction or [[curriculum]] need to take place. Student and parent are questioned to report their interests, how difficult it take them to learn, how are they taught and treated.&lt;ref name="Bernhardt"/&gt;

3. Student learning data answers two questions: How are our students doing? and Where are we now? Student learning data requires information from all subject areas, disaggregated by demographic groups, by teachers, by grade level, by cohorts over time, and individual student growth. This type of data helps to address additional help to students who are not proficient, deepening into what they know and what they don't know to become proficient. Student learning data connects with [[curriculum]], [[Teaching|instruction]], and [[Educational assessment|assessment]] in order to improve outcomes. Student learning data can clearly state the effectiveness of a single educator or the entire school. SLD can be gathered by looking at diagnostic tests, formative assessments, performance assessments, standardized tests, non-referenced tests, summative assessments, teacher-assigned tests, and others.&lt;ref name="Bernhardt"/&gt;

4. School processes refer to actions of administrators and teachers to achieve the purpose of the school. Teachers' habits, customs, knowledge, and professionalism are the things leading towards progress inside organizations. School processes data tell us what works, what doesn't, the results of educational process, and answers the question, "What are our processes are?". School processes produce school and class results. There are 4 major types of school processes: 1. instructional processes, 2. Organizational processes, 3. Administrative processes, 4. Continuous school improvement processes.   
&lt;ref name="Bernhardt"/&gt;

==Use in educational organizations==

[[The U.S. Department of Education]] and the [[Institute of Education Sciences]] require to use data and DDDM in past decades to run educational organizations. Hard evidence and the use of data are emphasized to inform decisions. The data in educational organizations means more than analyzing test scores. Educational data movement is considered as a sociotechnical revolution. Educational data systems involve technologies and evidence to explain districts', schools', classrooms' tendencies. DDDM is used to explain complexity of education, support collaboration, creating new designs of teaching. Student performance is central in DDDM. NCLB provided boost in the collection and use of educational information.&lt;ref name=Piety&gt;{{cite book|last1=Piety|first1=Philip|title=Assessing the educational data movement|date=2013|publisher=Teachers college press|location=New york|isbn=978-0-8077-5426-9|pages=1&#8211;20}}&lt;/ref&gt;

For example, in a rural area educators tried to understand why a particular subset of students were struggling academically. Data analysts collected students performance data, medical records, behavioral data, attendance, and other data less qualitative information. After not finding direct correlation between collected data and student outcomes they decided to include transportation data into the research. As result, educators found that students who had longer way from houses to the school were struggling the most. According to the finding administrators modified transportation arrangements to make the way shorter for students as well as installing Internet access in buses so students could concentrate on doing homework. DDDM in this particular case helped to improve student results.&lt;ref name="r1"/&gt;

==Effects on schools==

Effective schools showing outstanding gains in academic measures report that the wide and wise use of data has a positive effect on student achievement and progress. DDDM is suggested to be a main tool to move educational organizations towards school improvement and [[educator effectiveness]]. Data can be used to measure growth over time, program evaluation along with identifying root causes of problems connected to education. Involving school teachers in data inquiry causes more collaborative work from staff. Data provides increasing communication and knowledge which has a positive effect on altering educator attitudes towards groups inside schools which are underperforming 
&lt;ref&gt;{{cite journal|last1=Wayman|first1=Jeffrey|title=Involving teachers in data driven decision making:Using computer data systems to support teacher inquiry and reflection|journal=Journal of education for students placed at risk|date=2005|pages=296&#8211;300}}&lt;/ref&gt;

==Notes==
{{Reflist}}

==General references==
* {{cite journal|last1=Spillane|first1=James P.|title=Data in Practice: Conceptualizing the Data-Based Decision-Making Phenomena|journal=American Journal of Education|date=2012|volume=118|issue=2|pages=113&#8211;141|jstor=10.1086/663283|doi=10.1086/663283}}
* {{cite journal|last1=Reeves|first1=Patricia L.|last2=Burt|first2=Walter L.|title=Challenges in Data-based Decision-making: Voices from Principals|journal=Educational Horizons|date=2006|volume=85|issue=1|pages=65&#8211;71|jstor=42925967}}

[[Category:Data management]]
[[Category:Standards-based education]]</text>
      <sha1>qdox7jvyqh68mqbl09d1er0hnienmwx</sha1>
    </revision>
  </page>
  <page>
    <title>Data lake</title>
    <ns>0</ns>
    <id>46626475</id>
    <revision>
      <id>759623152</id>
      <parentid>759457989</parentid>
      <timestamp>2017-01-12T07:01:28Z</timestamp>
      <contributor>
        <ip>125.18.177.100</ip>
      </contributor>
      <comment>/* Criticism */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5993" xml:space="preserve">{{Use dmy dates|date=May 2016}}
A '''data lake''' is a method of storing [[data]] within a system or repository, in its natural format,&lt;ref&gt;[http://blogs.sas.com/content/datamanagement/2016/11/21/growing-import-big-data-quality/ The growing importance of big data quality]&lt;/ref&gt; that facilitates the collocation of data in various schemata and structural forms, usually object blobs or files. 

== Invention ==
James Dixon, then chief technology officer at [[Pentaho]] coined the term&lt;ref name="woods2011"&gt;{{cite news | title=Big data requires a big architecture |last=Woods |first=Dan |work=Forbes |date=21 July 2011 |department=Tech |url=http://www.forbes.com/sites/ciocentral/2011/07/21/big-data-requires-a-big-new-architecture/ }}&lt;/ref&gt; to contrast it with [[data mart]], which is a smaller repository of interesting attributes extracted from raw data.&lt;ref name="dixon2010"&gt;{{cite web | last=Dixon|first=James|title=Pentaho, Hadoop, and Data Lakes|url=https://jamesdixon.wordpress.com/2010/10/14/pentaho-hadoop-and-data-lakes/|website=James Dixon&#8217;s Blog|publisher=James|accessdate=7 November 2015 |quote=If you think of a datamart as a store of bottled water &#8211; cleansed and packaged and structured for easy consumption &#8211; the data lake is a large body of water in a more natural state. The contents of the data lake stream in from a source to fill the lake, and various users of the lake can come to examine, dive in, or take samples.}}&lt;/ref&gt; He argued that data marts have several inherent problems, and that data lakes are the optimal solution. These problems are often referred to as [[information silo]]ing. [[PricewaterhouseCoopers]] said that data lakes could "put an end to data silos.&lt;ref name="stein2014"&gt;{{cite report | url=http://www.pwc.com/en_US/us/technology-forecast/2014/cloud-computing/assets/pdf/pwc-technology-forecast-data-lakes.pdf |format=pdf |title=Data lakes and the promise of unsiloed data |last2=Morrison |first2=Alan |last=Stein |first=Brian |publisher=PricewaterhouseCooper |series=Technology Forecast: Rethinking integration |year=2014 }}&lt;/ref&gt; In their study on data lakes they noted that enterprises were "starting to extract and place data for analytics into a single, Hadoop-based repository."

==Characteristics==
The idea of data lake is to have a single store of all data in the enterprise ranging from raw data (which implies exact copy of source system data) to transformed data which is used for various tasks including [[Data reporting|reporting]], [[data visualization|visualization]], [[data analytics|analytics]] and [[machine learning]].

The data lake includes structured data from relational databases (rows and columns), semi-structured data (CSV, logs, XML, JSON), unstructured data (emails, documents, PDFs) and even binary data (images, audio, video) thus creating a centralized data store accommodating all forms of data.

== Examples ==

One example of a data lake is the distributed file system [[Apache Hadoop]].

Many companies also use cloud storage services such as [[Amazon S3]].&lt;ref name="tuulos2015"&gt;{{cite web | title=Petabyte-Scale Data Pipelines with Docker, Luigi and Elastic Spot Instances |last=Tuulos |first=Ville |date=22 September 2015 |url=http://tech.adroll.com/blog/data/2015/09/22/data-pipelines-docker.html}}&lt;/ref&gt; There is a gradual academic interest in the concept of data lakes, for instance, [http://www.researchgate.net/publication/283053696_Personal_Data_Lake_With_Data_Gravity_Pull Personal DataLake]&lt;ref&gt;http://ieeexplore.ieee.org/xpl/abstractAuthors.jsp?reload=true&amp;arnumber=7310733&lt;/ref&gt; an ongoing project at Cardiff University to create a new type of data lake which aims at managing big data of individual users by providing a single point of collecting, organizing, and sharing personal data.&lt;ref&gt;http://www.researchgate.net/publication/283053696_Personal_Data_Lake_With_Data_Gravity_Pull&lt;/ref&gt;

The earlier data lake (Hadoop 1.0) had limited capabilities with its batch oriented processing (Map Reduce) and was the only processing paradigm associated with it. Interacting with the data lake meant you had to have expertise in Java with map reduce and higher level tools like Pig &amp; Hive (which by themselves were batch oriented). With the dawn of Hadoop 2.0 and separation of duties with Resource Management taken over by YARN (Yet another resource negotiator), new processing paradigms like Streaming, interactive, on-line have become available via Hadoop and the Data Lake.

== Criticism ==
{{criticism section|date=December 2015}}

In June 2015, David Needle characterized "so-called data lakes" as "one of the more controversial ways to manage [[big data]]".&lt;ref name="needle2015"&gt;{{cite news |  last=Needle |first=David | title=Hadoop Summit: Wrangling Big Data Requires Novel Tools, Techniques |date=10 June 2015 | work=eWeek | url= http://www.eweek.com/enterprise-apps/hadoop-summit-wrangling-big-data-requires-novel-tools-techniques-2.html | department=Enterprise Apps | access-date = 1 November 2015 | quote = Walter Maguire, chief field technologist at HP's Big Data Business Unit, discussed one of the more controversial ways to manage big data, so-called data lakes. }}&lt;/ref&gt; [[PricewaterhouseCoopers]] were also careful to note in their research that not all data lake initiatives are successful. They quote Sean Martin, CTO of [[Cambridge Semantics]],
{{quote|sign=|source=|We see customers creating big data graveyards, dumping everything into HDFS [Hadoop Distributed File System] and hoping to do something with it down the road. But then they just lose track of what&#8217;s there.&lt;ref name="stein2014"/&gt;}}
They advise that "The main challenge is not creating a data lake, but taking advantage of the opportunities it presents."&lt;ref name="stein2014"/&gt; They describe companies that build successful data lakes as gradually maturing their lake as they figure out which data and metadata are important to the organization.

== References ==
&lt;references/&gt;

[[Category:Data management]]</text>
      <sha1>ocq95239jxte2dn83dy0p8rd6eeu08j</sha1>
    </revision>
  </page>
  <page>
    <title>Database administrator</title>
    <ns>0</ns>
    <id>254789</id>
    <revision>
      <id>746401775</id>
      <parentid>742615669</parentid>
      <timestamp>2016-10-27T05:23:33Z</timestamp>
      <contributor>
        <username>Bumm13</username>
        <id>63286</id>
      </contributor>
      <minor />
      <comment>changed "System Monitoring" (monitoring) wikilink to "System monitoring"</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6920" xml:space="preserve">{{Infobox Occupation
|caption= Database Administrator
|official_names= Database administrator, database analyst
|activity_sector=[[Information technology]], [[information system]]s
|competencies= [[Database design|Databases design and implementation]], [[Computer programming|programming]] skills, [[database theory]], [[Computer network|networking]] basics, [[analytical skill]]s, [[critical thinking]]
|formation=At least a [[Academic certificate|certificate]] with experience.
}}

'''Database administrators''' ('''DBAs''') use specialized software to store and organize data.&lt;ref name="BLS-DBA"&gt;{{cite web | url=http://www.bls.gov/ooh/computer-and-information-technology/database-administrators.htm | title=Database Administrators | publisher=Bureau Of Labor Statistics | work=11/04/2015 | accessdate=4 November 2015}}&lt;/ref&gt;

The role may include [[capacity planning]], [[Installation (computer programs)|installation]], [[Computer configuration|configuration]], [[database design]], [[Data migration|migration]], performance monitoring, [[Computer security|security]], [[troubleshooting]], as well as [[backup]] and [[data recovery]].&lt;ref name="techrepublic"&gt;{{cite web | url=http://www.techrepublic.com/blog/the-enterprise-cloud/what-does-a-dba-do-all-day/ | title=What does a DBA do all day? | publisher=techrepublic.com | work=11/04/2015 | accessdate=4 November 2015}}&lt;/ref&gt;

==Skills==
List of skills required to become database administrators are:&lt;ref&gt;{{cite web|last1=Spenik|first1=Mark|last2=Sledge|first2=Orryn|date=2001-03-20|url=http://www.developer.com/db/article.php/718491/What-Is-a-Database-Administrator.htm|title=What is a Database Administrator? (DBA)|publisher=Developer.com|accessdate=2012-02-06|archiveurl=https://web.archive.org/web/20110613101702/http://www.developer.com/db/article.php/718491/What-Is-a-Database-Administrator.htm|archivedate=2011-06-13}}&lt;/ref&gt;&lt;ref&gt;http://www.dba-oracle.com/oracle_tips_dba_job_skills.htm&lt;/ref&gt;&lt;ref&gt;http://www.orafaq.com/wiki/Roles_and_Responsibilities&lt;/ref&gt;
* [[Communication]] skills
* Knowledge of [[database theory]]
* Knowledge of [[database design]]
* Knowledge about the [[Relational database management system|RDBMS]] itself, e.g. [[Microsoft SQL Server]] or [[MySQL]]
* Knowledge of [[SQL|structured query language]] (SQL), e.g. [[SQL/PSM]] or [[Transact-SQL]]
* General [[understanding]] of [[Distributed computing|distributed computing architectures]], e.g. [[Client&#8211;server model]]
* General understanding of [[operating system]], e.g. [[Microsoft Windows|Windows]] or [[Linux]]
* General understanding of [[Computer data storage|storage technologies]] and [[Computer network|networking]]
* General understanding of routine maintenance, recovery, and handling failover of a database

Database administrators benefit from a [[bachelor's degree]] or [[master's degree]] in [[computer science]]. An [[associate degree]] or a [[Academic certificate|certificate]] may be sufficient with work experience.&lt;ref name="study.com"&gt;{{cite web | url=http://study.com/articles/Database_Administrator_Job_Description_and_Requirements.html | title=Database Administrator: Job Description and Requirements | publisher=study.com | work=11/4/2015 | accessdate=4 November 2015}}&lt;/ref&gt;

===Certification===
There are many certifications available for becoming a certified database administrator. Many of these certifications are offered by database vendors themselves. By passing a series of tests and sometimes other requirements, you can earn a database administrator certification. Schools offering Database Administration degrees can also be found.&lt;ref name="learn.org"&gt;{{cite web | url=http://learn.org/articles/How_Do_I_Become_a_Certified_Database_Administrator.html | title=How Do I Become a Certified Database Administrator? | publisher=learn.org | work=learn.org | accessdate=4 November 2015}}&lt;/ref&gt;

For example:
* IBM Certified Advanced Database Administrator - DB2 10.1 for Linux, Unix and Windows&lt;ref name="ibm.com"&gt;{{cite web |url=http://www-03.ibm.com/certify/index.shtml |title=IBM Professional Certification Program |work=ibm.com |publisher=[[IBM]] |accessdate=2014-08-10}}&lt;/ref&gt;
* IBM Certified Database Administrator - DB2 10.1 for Linux, Unix, and Windows&lt;ref name="ibm.com"/&gt;
* Oracle Database 11g Administrator Certified Professional&lt;ref&gt;{{cite web |url=http://education.oracle.com/pls/web_prod-plq-dad/db_pages.getpage?page_id=143&amp;p_org_id=1001&amp;lang=US |title=Oracle Certification Program |work=oracle.com |publisher=[[Oracle Corporation]] |accessdate=2011-06-18}}&lt;/ref&gt;
* Oracle MySQL 5.6 Database Administrator Certified Professional&lt;ref&gt;{{cite web |url=https://education.oracle.com/pls/web_prod-plq-dad/ou_product_category.getPage?p_cat_id=159&amp;p_org_id=15941&amp;lang=US#tabs-3 |title=Oracle Certified Professional, MySQL 5.6 Database Administrator |work=oracle.com |publisher=[[Oracle Corporation]] |accessdate=2016-09-18}}&lt;/ref&gt;
* MCSA SQL Server 2012&lt;ref name=MCSASQL&gt;{{cite web |url=https://www.microsoft.com/en-us/learning/mcsa-sql-certification.aspx |title=MCSA: SQL Server |work=microsoft.com |publisher=[[Microsoft]] |accessdate=2015-11-03}}&lt;/ref&gt;
* MCSE Data Platform Solutions Expert &lt;ref name="microsoftsolutionsexpert"&gt;{{cite web | url=https://www.microsoft.com/en-us/learning/mcse-sql-data-platform.aspx | title=MCSE: Data Platform | publisher=microsoft.com | work=11/4/2015 | accessdate=4 November 2015}}&lt;/ref&gt;

==Duties==
A database administrator's responsibilities can include the following tasks:&lt;ref&gt;{{cite web |url=http://docs.oracle.com/cd/B10501_01/server.920/a96521/dba.htm#852 |title=Oracle DBA Responsibilities |work=[[Oracle Corporation]] |accessdate=2012-02-06}}&lt;/ref&gt;
* [[Installation (computer programs)|Installing]] and [[upgrade|upgrading]] the database server and application tools
* Allocating system storage and [[planning]] future storage requirements for the database system
* Modifying the database structure, as necessary, from information given by application developers
* Enrolling users and maintaining system [[Computer security|security]]
* Ensuring compliance with database vendor [[license|license agreement]]
* Controlling and [[System Monitoring|monitoring]] [[user (computing)|user]] access to the database
* Monitoring and [[Program optimization|optimizing]] the performance of the database
* Planning for [[backup]] and recovery of database information
* Maintaining [[archive]]d data
* Backing up and restoring databases
* Contacting database [[vendor]] for [[technical support]]
* Generating various reports by querying from database as per need

==See also==
* [[Comparison of database tools]]

==References==
{{Reflist}}

==External links==
"Database Administrators"

{{Database}}

{{Use British English|date=June 2012}}
{{Use dmy dates|date=June 2012}}

{{DEFAULTSORT:Database Administrator}}
[[Category:Computer occupations]]
[[Category:Data management]]
[[Category:Database specialists| ]]</text>
      <sha1>l7crakjrfyajfda7e3b2jt2yij4y5ay</sha1>
    </revision>
  </page>
  <page>
    <title>Embedded analytics</title>
    <ns>0</ns>
    <id>47485347</id>
    <revision>
      <id>758183392</id>
      <parentid>752487254</parentid>
      <timestamp>2017-01-03T23:27:26Z</timestamp>
      <contributor>
        <ip>66.201.57.34</ip>
      </contributor>
      <comment>/* Tools */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3545" xml:space="preserve">'''Embedded analytics''' is the technology designed to make [[data analysis]] and [[business intelligence]] more accessible by all kinds of application or user.

==Definition==

According to Gartner analysts Kurt Schlegel, traditional [[business intelligence]] were suffering in 2008 a lack of integration between the data and the business users.&lt;ref&gt;{{cite web
| last = Kelly
| first = Jeff
| title = Gartner Business Intelligence Summit: Embed BI within business processes
| publisher = TechTarget
| accessdate = August 2015
| url = http://searchbusinessanalytics.techtarget.com/news/1507180/Gartner-Business-Intelligence-Summit-Embed-BI-within-business-processes
}}&lt;/ref&gt; This technology intention is to be more pervasive by real-time autonomy and self-service of data visualization or customization, meanwhile decision makers, business users or even customers are doing their own daily workflow and tasks.

==History==

First mentions of the concept were made by Howard Dresner, consultant, author, former Gartner analyst and inventor of the term "business intelligence".&lt;ref&gt;{{cite web
| last = Kelly
| first = Jeff
| title = Gartner Business Intelligence Summit: Embed BI within business processes
| publisher = TechTarget
| accessdate= August 2015
| url = http://searchbusinessanalytics.techtarget.com/news/1507180/Gartner-Business-Intelligence-Summit-Embed-BI-within-business-processes
}}&lt;/ref&gt; Consolidation of [[business intelligence]] "doesn't mean the BI market has reached maturity" &lt;ref&gt;{{cite web
| last = Dresner
| first = Howard 
| title = Howard Dresner predicts the future of business intelligence
| publisher = TechTarget
| accessdate= August 2015
| url = http://searchbusinessanalytics.techtarget.com/podcast/Howard-Dresner-predicts-the-future-of-business-intelligence
}}&lt;/ref&gt; said Howard Dresner while he was working for Hyperion Solutions, a company that Oracle bought in 2007. Oracle started then to use the term "embedded analytics" at their press release for Oracle&#174; Rapid Planning on 2009.&lt;ref&gt;{{cite web
| title = Oracle Announces Oracle&#174; Rapid Planning
| publisher = Oracle
| accessdate= August 2015
| url = http://www.oracle.com/us/corporate/press/040402
}}&lt;/ref&gt; Gartner Group, a company for which Howard Dresner has been working,  finally added the term to their IT Glossary on November 5, 2012. 
&lt;ref&gt;{{cite web
| title = Gartner IT Glossary: Embedded Analytics 
| publisher = Gartner
| accessdate= August 2015
| url = http://www.gartner.com/it-glossary/embedded-analytics 
}}&lt;/ref&gt;
. It was clear this was a mainstream technology when Dresner Advisory Services published the 2014 Embedded Business Intelligence Market Study as part of the Wisdom of Crowds&#174; Series of Research, including 24 vendors.&lt;ref&gt;{{cite web
| title = 2014 Embedded Business Intelligence Market Study Now Available From Dresner Advisory Services 
| publisher = Market Wired
| accessdate= August 2015
| url = http://www.marketwired.com/press-release/2014-embedded-business-intelligence-market-study-now-available-from-dresner-advisory-1962227.htm
}}&lt;/ref&gt;

==Tools==

{{colbegin|2}}

* [[Actuate Corporation|Actuate]]
* [[Dundas Data Visualization]]
* [[GoodData]]
* [[IBM]]
* [[icCube]]
* [[Logi Analytics]]
* [[Pentaho]]
* [[Qlik]]
* [[SAP_SE|SAP]]
* [[SAS_(software)|SAS]]
* [[ServiceNow]]
* [[Tableau Software|Tableau]]
* [[ThoughtSpot]]
* [[TIBCO]]
* [[Sisense]]

{{colend}}

==References==
{{Reflist}}

[[Category:Types of analytics]]
[[Category:Big data|analytics]]
[[Category:Business intelligence]]
[[Category:Data management]]</text>
      <sha1>ihkm9tmn6te8ocqkg0a72kg2pvwz1v1</sha1>
    </revision>
  </page>
  <page>
    <title>Skyline operator</title>
    <ns>0</ns>
    <id>46213978</id>
    <revision>
      <id>748210532</id>
      <parentid>747078399</parentid>
      <timestamp>2016-11-07T00:46:55Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor />
      <comment>/* Implementation */Removed invisible unicode characters + other fixes, replaced: &#8594;   (2) using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3294" xml:space="preserve">The '''Skyline operator''' is used in a query and performs a filtering of results from a database so that it keeps only those objects that are not worse than any other.

This operator is an extension to [[SQL]] proposed by B&#246;rzs&#246;nyi et al.&lt;ref name=borzsony2001skyline&gt;{{cite journal|last1=Borzsonyi|first1=Stephan|last2=Kossmann|first2=Donald|last3=Stocker|first3=Konrad|title=The Skyline Operator|journal=Proceedings 17th International Conference on Data Engineering|date=2001|pages=421&#8211;430|doi=10.1109/ICDE.2001.914855}}&lt;/ref&gt;  A classic example of application of the Skyline operator involves selecting a hotel for a holiday. The user wants the hotel to be both cheap and close to the beach. However, hotels that are close to the beach may also be expensive. In this case, the Skyline operator would only present those hotels that are not worse than any other hotel in both price and distance to the beach.

== Proposed syntax ==

To give an example in SQL: B&#246;rzs&#246;nyi et al.&lt;ref name=borzsony2001skyline/&gt; proposed the following syntax for the Skyline operator:

&lt;source lang="sql"&gt;
SELECT ... FROM ... WHERE ...
GROUP BY ... HAVING ...
SKYLINE OF [DISTINCT] d1 [MIN | MAX | DIFF],
                 ..., dm [MIN | MAX | DIFF]
ORDER BY ...
&lt;/source&gt;
where d&lt;sub&gt;1&lt;/sub&gt;, ... d&lt;sub&gt;m&lt;/sub&gt; denote the dimensions of the Skyline and MIN, MAX and DIFF specify whether the value in that dimension should be minimised, maximised or simply be different.

== Implementation ==
The Skyline operator can be implemented directly in SQL using current SQL constructs, however this has been shown to be very slow.&lt;ref name=borzsony2001skyline/&gt; Other algorithms have been proposed that make use of divide and conquer, indices,&lt;ref name=borzsony2001skyline/&gt; [[MapReduce]]&lt;ref&gt;{{cite journal|last1=Mullesgaard|first1=Kasper|last2=Pedersen|first2=Jens Laurits|last3=Lu|first3=Hua|last4=Zhou|first4=Yongluan|title=Efficient Skyline Computation in MapReduce|journal=Proc. 17th International Conference on Extending Database Technology (EDBT)|date=2014|pages=37&#8211;48|url=http://www.openproceedings.eu/2014/conf/edbt/MullesgaardPLZ14.pdf}}&lt;/ref&gt; and [[General-purpose computing on graphics processing units|general-purpose computing on graphics cards]].&lt;ref&gt;{{cite journal|last1=B&#248;gh|first1=Kenneth S|last2=Assent|first2=Ira|last3=Magnani|first3=Matteo|title=Efficient GPU-based skyline computation|journal=Proceedings of the Ninth International Workshop on Data Management on New Hardware|date=2013|pages=5:1&#8211;5:6|doi=10.1145/2485278.2485283}}&lt;/ref&gt; Skyline queries on data streams (i.e. continuous skyline queries) have been studied in the context of parallel query processing on multicores, owing to their wide diffusion in real-time decision making problems and data streaming analytics.&lt;ref&gt;{{cite journal|last1=De Matteis|first1=Tiziano|last2=Di Girolamo|first2=Salvatore|last3=Mencagli|first3=Gabriele|title=Continuous skyline queries on multicore architectures|journal=Concurrency and Computation: Practice and Experience|date=25 August 2016|volume=28|issue=12|pages=3503&#8211;3522|doi=10.1002/cpe.3866}}&lt;/ref&gt;

==References==
&lt;references /&gt;

[[Category:Data management]]
[[Category:Query languages]]
[[Category:Relational database management systems]]
[[Category:SQL]]


{{database-software-stub}}</text>
      <sha1>26swzixi1cieylt3so6y3uglsmd4rt3</sha1>
    </revision>
  </page>
  <page>
    <title>Document-oriented database</title>
    <ns>0</ns>
    <id>15002414</id>
    <revision>
      <id>762848584</id>
      <parentid>758861200</parentid>
      <timestamp>2017-01-31T01:50:37Z</timestamp>
      <contributor>
        <username>GreenC bot</username>
        <id>27823944</id>
      </contributor>
      <minor />
      <comment>Reformat 1 archive link. [[User:Green Cardamom/WaybackMedic_2.1|Wayback Medic 2.1]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="23726" xml:space="preserve">{{about|the software type|deployed applications of the software type|Full text database}}
{{primary sources|date=May 2012}}

A '''document-oriented database''', or '''document store''', is a [[computer program]] designed for storing, retrieving and managing document-oriented information, also known as [[Semi-structured model|semi-structured data]]. Document-oriented databases are one of the main categories of [[NoSQL]] databases, and the popularity of the term "document-oriented database" has grown&lt;ref&gt;[http://db-engines.com/en/ranking_categories DB-Engines Ranking per database model category]&lt;/ref&gt; with the use of the term NoSQL itself. [[XML database]]s are a subclass of document-oriented databases that are optimized to work with [[XML]] documents. [[Graph databases]] are similar, but add another layer, the ''relationship'', which allows them to link documents for rapid traversal.

Document-oriented databases are inherently a subclass of the [[Key-value database|key-value store]], another NoSQL database concept. The difference lies in the way the data is processed; in a key-value store the data is considered to be inherently opaque to the database, whereas a document-oriented system relies on internal structure in the ''document'' in order to extract [[metadata]] that the database engine uses for further optimization. Although the difference is often moot due to tools in the systems,{{efn|To the point that document-oriented and key-value systems can often be interchanged in operation.}} conceptually the document-store is designed to offer a richer experience with modern programming techniques.

Document databases{{efn|And key-value stores in general.}} contrast strongly with the traditional [[relational database]] (RDB). Relational databases generally store data in separate ''tables'' that are defined by the programmer, and a single object may be spread across several tables. Document databases store all information for a given object in a single instance in the database, and every stored object can be different from every other. This makes mapping objects into the database a simple task, normally eliminating anything similar to an [[object-relational mapping]]. This makes document stores attractive for programming [[web application]]s, which are subject to continual change in place, and where speed of deployment is an important issue.

== Documents ==
The central concept of a document-oriented database is the notion of a ''document''. While each document-oriented database implementation differs on the details of this definition, in general, they all assume documents encapsulate and encode data (or information) in some standard formats or encodings. Encodings in use include [[XML]], [[YAML]], [[JSON]], and [[BSON]], as well as binary forms like PDF and Microsoft Office documents (MS Word, Excel, and so on).

Documents in a document store are roughly equivalent to the programming concept of an object. They are not required to adhere to a standard schema, nor will they have all the same sections, slots, parts, or keys. Generally, programs using objects have many different types of objects, and those objects often have many optional fields. Every object, even those of the same class, can look very different. Document stores are similar in that they allow different types of documents in a single store, allow the fields within them to be optional, and often allow them to be encoded using different encoding systems. For example, the following is a document, encoded in JSON:

&lt;syntaxhighlight lang="javascript"&gt;
{
    FirstName: "Bob", 
    Address: "5 Oak St.", 
    Hobby: "sailing"
}
&lt;/syntaxhighlight&gt;

A second document might be encoded in XML as:
&lt;syntaxhighlight lang="xml"&gt;
  &lt;contact&gt;
    &lt;firstname&gt;Bob&lt;/firstname&gt;
    &lt;lastname&gt;Smith&lt;/lastname&gt;
    &lt;phone type="Cell"&gt;(123) 555-0178&lt;/phone&gt;
    &lt;phone type="Work"&gt;(890) 555-0133&lt;/phone&gt;
    &lt;address&gt;
      &lt;type&gt;Home&lt;/type&gt;
      &lt;street1&gt;123 Back St.&lt;/street1&gt;
      &lt;city&gt;Boys&lt;/city&gt;
      &lt;state&gt;AR&lt;/state&gt;
      &lt;zip&gt;32225&lt;/zip&gt;
      &lt;country&gt;US&lt;/country&gt;
    &lt;/address&gt;
  &lt;/contact&gt;
&lt;/syntaxhighlight&gt;

These two documents share some structural elements with one another, but each also has unique elements. The structure and text and other data inside the document are usually referred to as the document's ''content'' and may be referenced via retrieval or editing methods, (see below). Unlike a relational database where every record contains the same fields, leaving unused fields empty; there are no empty 'fields' in either document (record) in the above example. This approach allows new information to be added to some records without requiring that every other record in the database share the same structure.

Document databases typically provide for additional [[metadata]] to be associated with and stored along with the document content. That metadata may be related to facilities the datastore provides for organizing documents, providing security, or other implementation specific features.

=== CRUD operations ===
The core operations a document-oriented database supports on documents are similar to other databases and while the terminology isn't perfectly standardized, most practitioners will recognize them as [[CRUD]]

* Creation (or insertion)
* Retrieval (or query, search, finds)
* Update (or edit)
* Deletion (or removal)

=== Keys ===
Documents are addressed in the database via a unique ''key'' that represents that document. This key is a simple [[identifier]] (or ID), typically a [[String (computer science)|string]], a [[URI]], or a [[Path (computing)|path]]. The key can be used to retrieve the document from the database. Typically the database retains an [[Database index|index]] on the key to speed up document retrieval, and in some cases the key is required to create or insert the document into the database.

=== Retrieval ===
Another defining characteristic of a document-oriented database is that, beyond the simple key-to-document lookup that can be used to retrieve a document, the database offers an API or query language that allows the user to retrieve documents based on content (or metadata). For example, you may want a query that retrieves all the documents with a certain field set to a certain value.  The set of query APIs or query language features available, as well as the expected performance of the queries, varies significantly from one implementation to another. Likewise, the specific set of indexing options and configuration that are available vary greatly by implementation.

It is here that the document store varies most from the key-value store. In theory, the values in a key-value store are opaque to the store, they are essentially black boxes. They may offer search systems similar to those of a document store, but may have less understanding about the organization of the content. Document stores use the metadata in the document to classify the content, allowing them, for instance, to understand that one series of digits is a phone number, and another is a postal code. This allows them to search on those types of data, for instance, all phone numbers containing 555, which would ignore the zip code 55555.

=== Editing ===
Document databases typically provide some mechanism for updating or editing the content (or other metadata) of a document, either by allowing for replacement of the entire document, or individual structural pieces of the document.

=== Organization ===
Document database implementations offer a variety of ways of organizing documents, including notions of

* Collections: groups of documents, where depending on implementation, a document may be enforced to live inside one collection, or may be allowed to live in multiple collections
* Tags and non-visible metadata: additional data outside the document content 
* Directory hierarchies: groups of documents organized in a tree-like structure, typically based on path or URI

Sometimes these organizational notions vary in how much they are logical vs physical, (e.g. on disk or in memory), representations.

==Relationship to other databases ==

=== Relationship to key-value stores ===

A document-oriented database is a specialized  [[Key-value database|key-value store]], which itself is another NoSQL database category.  In a simple key-value store, the document content is opaque. A document-oriented database provides APIs or a query/update language that exposes the ability to query or update based on the internal structure in the ''document''. This difference may be moot for users that do not need richer query, retrieval, or editing APIs that are typically provided by document databases. Modern key-value stores often include features for working with metadata, blurring the lines between document stores.

=== Relationship to search engines ===
Some search engines (aka [[information retrieval]]) systems like [[Elasticsearch]] provide enough of the core operations on documents to fit the definition of a document-oriented database.

=== Relationship to relational databases ===

{{cleanup|section|reason="Requires cleanup"|date=July 2016}}

In a relational database, data is first categorized into a number of predefined types, and ''tables'' are created to hold individual entries, or ''records'', of each type. The tables define the data within each record's ''fields'', meaning that every record in the table has the same overall form. The administrator also defines the ''relationships'' between the tables, and selects certain fields that they believe will be most commonly used for searching and defines ''indexes'' on them. A key concept in the relational design is that any data that may be repeated is normally placed in its own table, and if these instances are related to each other, a column is selected to group them together, the ''foreign key''. This design is known as ''[[database normalization]]''.&lt;ref&gt;{{cite web |url=https://support.microsoft.com/en-ca/kb/283878 |title=Description of the database normalization basics |website=Microsoft}}&lt;/ref&gt;

For example, an address book application will generally need to store the contact name, an optional image, one or more phone numbers, one or more mailing addresses, and one or more email addresses. In a canonical relational database solution, tables would be created for each of these rows with predefined fields for each bit of data: the CONTACT table might include FIRST_NAME, LAST_NAME and IMAGE columns, while the PHONE_NUMBER table might include COUNTRY_CODE, AREA_CODE, PHONE_NUMBER and TYPE (home, work, etc.). The PHONE_NUMBER table also contains a foreign key column, "CONTACT_ID", which holds the unique ID number assigned to the contact when it was created. In order to recreate the original contact, the database engine uses the foreign keys to look for the related items across the group of tables and reconstruct the original data.

In contrast, in a document-oriented database there may be no internal structure that maps directly onto the concept of a table, and the fields and relationships generally don't exist as predefined concepts. Instead, all of the data for an object is placed in a single document, and stored in the database as a single entry. In the address book example, the document would contain the contact's name, image, and any contact info, all in a single record. That entry is accessed through its key, which allows the database to retrieve and return the document to the application. No additional work is needed to retrieve the related data; all of this is returned in a single object.

A key difference between the document-oriented and relational models is that the data formats are not predefined in the document case. In most cases, any sort of document can be stored in any database, and those documents can change in type and form at any time. If one wishes to add a COUNTRY_FLAG to a CONTACT, this field can be added to new documents as they are inserted, this will have no effect on the database or the existing documents already stored. To aid retrieval of information from the database, document-oriented systems generally allow the administrator to provide ''hints'' to the database to look for certain types of information. These work in a similar fashion to indexes in the relational case. Most also offer the ability to add additional metadata outside of the content of the document itself, for instance, tagging entries as being part of an address book, which allows the programmer to retrieve related types of information, like "all the address book entries". This provides functionality similar to a table, but separates the concept (categories of data) from its physical implementation (tables).

In the classic normalized relational model, objects in the database are represented as separate rows of data with no inherent structure beyond that given to them as they are retrieved. This leads to problems when trying to translate programming objects to and from their associated database rows, a problem known as [[object-relational impedance mismatch]].&lt;ref&gt;{{cite web |url=http://www.agiledata.org/essays/impedanceMismatch.html |title=The Object-Relational Impedance Mismatch |first=Scott |last=Wambler |website=Agile Data}}&lt;/ref&gt; Document stores more closely, or in some cases directly, map programming objects into the store. This eliminates the impedance mismatch problem, and is offered as one of the main advantages of the NoSQL approach.

== Implementations ==
{{main cat|Document-oriented databases}}

{| class="wikitable sortable"
|-
! Name
! Publisher
! License
! Languages supported
! Notes
! [[Representational State Transfer|RESTful]] API
|-
| [[BaseX]]
| BaseX Team
| {{free|[[BSD License]]}}
| [[Java (programming language)|Java]], [[XQuery]]
| Support for XML, JSON and binary formats; client-/server based architecture; concurrent structural and full-text searches and updates.
| {{yes}}
|-
| [[InterSystems Cach&#233;|Cach&#233;]]
| [[InterSystems]] Corporation
| {{proprietary}}
| [[Java (programming language)|Java]], [[C Sharp (programming language)|C#]], [[Node.js]]
| Commonly used in Health, Business and Government applications.
| {{yes}}
|-
| [[Cloudant]]
| Cloudant, Inc.
| {{proprietary}}
| [[Erlang (programming language)|Erlang]], [[Java (programming language)|Java]], [[Scala (programming language)|Scala]], and [[C (programming language)|C]]
| Distributed database service based on [[BigCouch]], the company's [[open source]] fork of the [[Apache Software Foundation|Apache]]-backed [[CouchDB]] project.  Uses JSON model.
| {{yes}}
|-
| [[Clusterpoint|Clusterpoint Database]]
| Clusterpoint Ltd.
| {{proprietary}} with free download
| [[JavaScript]], [[SQL]], [[PHP]], [[.NET]], [[Java (programming language)|Java]], [[Python (programming language)|Python]], [[Node.js]], [[C (programming language)|C]], [[C++]], 
| Distributed document-oriented XML / JSON database platform with [[ACID]]-compliant [[transaction processing|transactions]]; [[high-availability]] [[data replication]] and [[sharding]]; built-in [[full text search]] engine with [[relevance]] [[ranking]]; JS/SQL [[query language]]; [[Geographic information system|GIS]]; Available as pay-per-use [[cloud database|database as a service]] or as an on-premise free software download.&lt;ref&gt;[http://www.clusterpoint.com Document-oriented Database]. Clusterpoint. Retrieved on 2015-10-08.&lt;/ref&gt;
| {{yes}}
|-
| [[Couchbase Server]]
| [[Couchbase, Inc.]]
| {{free|[[Apache License]]}}
| [[C]], [[.NET]], [[Java (programming language)|Java]], [[Python (programming language)|Python]], [[Node.js]], [[PHP]], [[SQL]], [[GoLang]], [[Spring Framework]], [[LINQ]]
|Distributed NoSQL Document Database, JSON model and SQL based Query Language.
| {{yes}}&lt;ref&gt;[http://www.couchbase.com/docs/ Documentation]. Couchbase. Retrieved on 2013-09-18.&lt;/ref&gt;
|-
| [[CouchDB]]
| [[Apache Software Foundation]]
| {{free|[[Apache License]]}}
| Any language that can make HTTP requests
| JSON over REST/HTTP with [[Multi-Version Concurrency Control]] and limited [[ACID]] properties. Uses [[map (higher-order function)|map]] and [[fold (higher-order function)|reduce]] for views and queries.&lt;ref&gt;[http://couchdb.apache.org/docs/overview.html CouchDB Overview] {{webarchive |url=https://web.archive.org/web/20111020074113/http://couchdb.apache.org/docs/overview.html |date=October 20, 2011 }}&lt;/ref&gt;
| {{yes}}&lt;ref&gt;[http://wiki.apache.org/couchdb/HTTP_Document_API CouchDB Document API]&lt;/ref&gt;
|-
| [[CrateIO]]
| CRATE Technology GmbH
| {{free|[[Apache License]]}}
| [[Java (programming language)|Java]]
| Use familiar SQL syntax for real time distributed queries across a cluster. Based on Lucene / Elasticsearch ecosystem with built-in support for binary objects (BLOBs).
| {{yes}}&lt;ref&gt;{{cite web|url=https://crate.io/docs/stable/sql/rest.html |title=Archived copy |accessdate=2015-06-22 |deadurl=yes |archiveurl=https://web.archive.org/web/20150622174526/https://crate.io/docs/stable/sql/rest.html |archivedate=2015-06-22 |df= }}&lt;/ref&gt;
|-
|djondb
|djondb.com
|GNU GPL and Commercial
|C, .Net, Java, Python, NodeJS, PHP.
|Document Store with support to transactions.
|{{No}}
|-
| [[DocumentDB]]
| Microsoft
| {{proprietary}}
| [[.NET]], [[Java (programming language)|Java]], [[Python (programming language)|Python]], [[Node.js]], [[JavaScript]], [[SQL]]
| Platform-as-a-Service offering, part of the [[Microsoft Azure]] platform.
| {{yes}}
|-
|[[Elasticsearch]]
|[[Shay Banon]]
|{{free|[[Apache License]]}}
|[[Java (programming language)|Java]]
|[[JSON]], Search engine.
|{{yes}}
|-
| [[eXist]]
| eXist
| {{free|[[LGPL]]}}
| [[XQuery]], [[Java (programming language)|Java]]
| XML over REST/HTTP, WebDAV, Lucene Fulltext search, binary data support, validation, versioning, clustering, triggers, URL rewriting, collections, ACLS, XQuery Update
| {{yes}}&lt;ref&gt;[http://exist-db.org eXist-db Open Source Native XML Database]. Exist-db.org. Retrieved on 2013-09-18.&lt;/ref&gt;
|-
| [[HyperDex]]
| hyperdex.org
| {{free|[[BSD License]]}}
| [[C (programming language)|C]], [[C++]], [[Go (programming language)|Go]], [[Node.js]], [[Python (programming language)|Python]], [[Ruby (programming language)|Ruby]] 
| Support for [[JSON]] and binary documents.
| {{no}}
|-
| [[Informix]]
| IBM
| Proprietary, with no-cost editions&lt;ref&gt;http://www.ibm.com/developerworks/data/library/techarticle/dm-0801doe/&lt;/ref&gt;
| Various (Compatible with MongoDB API)
| RDBMS with JSON, replication, sharding and ACID compliance.
| {{yes}}
|-
|[[Apache Jackrabbit|Jackrabbit]]
|Apache Foundation
|{{free|[[Apache License]]}}
|[[Java (programming language)|Java]]
|[[Java Content Repository]] implementation
|{{dunno}}
|-
| [[Lotus Notes]] ([[IBM Lotus Domino]])
| IBM
| {{proprietary}}
| [[LotusScript]], [[Java (programming language)|Java]], Lotus @Formula
| [[MultiValue]]
| {{yes}}
|-
| [[MarkLogic]]
| MarkLogic Corporation
| Free Developer license or Commercial&lt;ref&gt;http://developer.marklogic.com/licensing&lt;/ref&gt;
| [[REST]], [[Java (programming language)|Java]], [[JavaScript]], [[Node.js]], [[XQuery]], [[SPARQL]], [[XSLT]], [[C++]]
| Distributed document-oriented database for JSON, XML, and [[Resource Description Framework|RDF triples]]. Built-in [[Full text search]], [[ACID]] transactions, [[High availability]] and [[Disaster recovery]], certified security.
| {{yes}}
|-
| [[MongoDB]]
| MongoDB, Inc
| {{free|[[Affero General Public License|GNU AGPL v3.0]] for the DBMS, [[Apache 2 License]] for the client drivers}}&lt;ref&gt;[http://www.mongodb.org/about/licensing/ MongoDB Licensing]&lt;/ref&gt;
| [[C (programming language)|C]], [[C++]], [[C Sharp (programming language)|C#]], [[Java (programming language)|Java]], [[Perl]], [[PHP]], [[Python (programming language)|Python]], [[Node.js]], [[Ruby (programming language)|Ruby]], [[Scala (programming language)|Scala]] &lt;ref&gt;[http://docs.mongodb.org/ecosystem/drivers/community-supported-drivers/ Additional 30+ community MongoDB supported drivers]&lt;/ref&gt;
| Document database with replication and sharding, [[BSON]] store (binary format [[JSON]]).
| {{yes}}&lt;ref&gt;[http://www.mongodb.org/display/DOCS/Http+Interface#HttpInterface-RESTInterfaces MongoDB REST Interfaces]&lt;/ref&gt;
|-
| [[MUMPS]] Database
| {{dunno}}
| [[proprietary software|Proprietary]] and [[Affero General Public License|Affero GPL]]&lt;ref&gt;[http://sourceforge.net/projects/fis-gtm/ GTM MUMPS FOSS on SourceForge]&lt;/ref&gt;
| [[MUMPS]]
| Commonly used in health applications.
| {{dunno}}
|-
| [[ObjectDatabase++]]
| Ekky Software
| {{proprietary}}
| [[C++]], [[C Sharp (programming language)|C#]], [[TScript]]
| Binary Native C++ class structures
| {{dunno}}
|-
| [[OrientDB]]
| Orient Technologies
| {{free|[[Apache License]]}}
| [[Java (programming language)|Java]]
| JSON over HTTP, SQL support, [[ACID]] transactions
| {{yes}}
|-
| [[PostgreSQL]]
| PostgreSQL
| {{free | [http://www.postgresql.org/about/licence/ PostgreSQL Free License]}}
| [[C (programming language)|C]]
| HStore, JSON store (9.2+), JSON function (9.3+), HStore2 (9.4+), JSONB (9.4+)
|{{no}}
|-
| [[Qizx]]
| [[Qualcomm]]
| [[Commercial software|Commercial]]
| [[REST]], [[Java (programming language)|Java]], [[XQuery]], [[XSLT]], [[C (programming language)|C]], [[C++]], [[Python (programming language)|Python]]
| Distributed document-oriented [[XML database]] with integrated [[full text search]]; support for [[JSON]], text, and binaries.
|{{yes}}
|-
| [[RethinkDB]]
| {{dunno}}
| {{free|[[Affero General Public License|GNU AGPL]] for the DBMS, [[Apache 2 License]] for the client drivers}}
| [[C++]], [[Python (programming language)|Python]], [[JavaScript]], [[Ruby (programming language)|Ruby]], [[Java (programming language)|Java]]
| Distributed document-oriented [[JSON]] database with replication and sharding.
|{{no}}
|-
| [[Rocket U2]]
| Rocket Software
| {{proprietary}}
| {{dunno}}
| UniData, UniVerse
|{{yes}} (Beta)
|-
| [[Sedna (database)|Sedna]]
|sedna.org
|{{free|[[Apache License]]}}
|[[C++]], [[XQuery]]
|[[XML database]]
|{{no}}
|-
| [[SimpleDB]]
| Amazon
| Proprietary online service
|[[Erlang (programming language)|Erlang]]
|
|{{dunno}}
|-
| [[Solr]]
| Apache
|{{free|[[Apache License]]}}
|[[Java (programming language)|Java]]
|Search engine
|{{yes}}
|-
| [[TokuMX]]
|Tokutek
|{{free|[[GNU Affero General Public License]]}}
|[[C++]], [[C Sharp (programming language)|C#]], [[Go (Programming language)|Go]]
|[[MongoDB]] with [[Fractal tree index|Fractal Tree indexing]]
|{{dunno}}
|-
| [[Virtuoso Universal Server|OpenLink Virtuoso]]
| [[OpenLink Software]]
|GPLv2[1] and proprietary
|[[C++]], [[C Sharp (programming language)|C#]], [[Java (programming language)|Java]], [[SPARQL]]
|[[Middleware]] and [[database engine]] hybrid
|{{yes}}
|}

===XML database implementations===
{{Further information|XML database}}
Most XML databases are document-oriented databases.

== See also ==
* [[Database theory]]
* [[Data hierarchy]]
* [[Full text search]]
* [[In-memory database]]
* [[Internet Message Access Protocol]] (IMAP)
* [[Machine-Readable Documents]]
* [[NoSQL]]
* [[Object database]]
* [[Online database]]
* [[Real time database]]
* [[Relational database]]

==Notes==
{{notelist}}

==References==
{{Reflist}}

==Further reading==
* Assaf Arkin. (2007, September 20). [https://web.archive.org/web/20080327222152/http://blog.labnotes.org:80/2007/09/20/read-consistency-dumb-databases-smart-services/ Read Consistency: Dumb Databases, Smart Services.]
{{refend}}

==External links==
* [http://db-engines.com/en/ranking/document+store DB-Engines Ranking of Document Stores] by popularity, updated monthly

{{Database models}}
{{Databases}}

[[Category:Document-oriented databases| ]]
[[Category:Data management]]
[[Category:Database management systems]]
[[Category:Types of databases]]</text>
      <sha1>5cvv6wmu0da2d7pial41r5x9m3nu43a</sha1>
    </revision>
  </page>
  <page>
    <title>Sedona Canada Principles</title>
    <ns>0</ns>
    <id>48536375</id>
    <revision>
      <id>734979237</id>
      <parentid>702608210</parentid>
      <timestamp>2016-08-17T22:37:10Z</timestamp>
      <contributor>
        <username>Gvcormac</username>
        <id>3113450</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8926" xml:space="preserve">{{orphan|date=January 2016}}
&lt;!-- Don't mess with this line! --&gt;&lt;!-- Write your article below this line --&gt;
The '''Sedona Canada Principles''' are a set of authoritative guidelines published by The Sedona Conference &#174; to aid members of the Canadian legal community involved in the identification, collection, preservation, review and production of [[electronically stored information]] (ESI).  The principles were drafted by a small group of lawyers, judges and technologists called the Sedona Working Group 7 or ''Sedona Canada''.  Sedona Canada is an offshoot of The Sedona Conference &#174; which is an American &#8220;non-profit&#8230;research and educational institute dedicated to the advanced study of law and policy in the areas of antitrust law, complex litigation, and intellectual property rights.&#8221;&lt;ref&gt;{{cite web|url=https://thesedonaconference.org/|title=The Sedona Conference&#174; - "Moving the law forward in a reasoned and just way."|work=thesedonaconference.org}}&lt;/ref&gt;

==Background==
[[Civil procedure in Canada]] is jurisdictional with each province following its own rules of civil procedure.&lt;ref&gt;{{cite web|url=https://en.wikibooks.org/wiki/Canadian_Civil_Procedure/Rules_by_Province|title=Canadian Civil Procedure/Rules by Province|work=wikibooks.org}}&lt;/ref&gt; However, each province must address the fact that due to the advancement of technology the discovery process enshrined in the rules of civil procedure can be potentially derailed due to the sheer volume of [[electronically stored information]] (ESI). &lt;ref name="mccarthy.ca"&gt;{{cite web|url=http://www.mccarthy.ca/article_detail.aspx?id=4068|title=McCarthy T&#233;trault - Taming the Beast of Electronic Discovery with Sedona Canada Principles - Article Detail|work=mccarthy.ca}}&lt;/ref&gt; When dealing with litigation matters that involve [[electronically stored information]] (ESI), the discovery process is commonly called [[electronic discovery|e-discovery]].  The problems associated with [[electronic discovery|e-discovery]] in Canada led to the creation of the Sedona Canada Principles. &lt;ref name="mccarthy.ca"/&gt; Rule 29.1.03(4) of the [[wikibooks:Ontario Rules of Civil Procedure]] specifically refers to the Sedona Canada Principles in referencing Principles re Electronic Discovery although it has been reported that this rule has been largely ignored in practice.&lt;ref name="canlii.org"&gt;{{cite web|url=http://www.canlii.org/en/on/onsc/doc/2010/2010onsc3670/2010onsc3670.html|title=CanLII - 2010 ONSC 3670 (CanLII)|work=canlii.org}}&lt;/ref&gt;

==Summary==
The Sedona Canada Principles largely refer to the processes found in the Electronic Discovery Reference Model.&lt;ref&gt;{{cite web|url=http://www.edrm.net/resources/edrm-stages-explained|title=EDRM Stages|work=edrm.net}}&lt;/ref&gt;

The principles urge proportionality due to the potentially enormous volumes of documents that may be discoverable when dealing with ESI.  They also encourage [[good faith]] in the document preservation stage and regular meetings between parties to discuss the scope of the litigation.  Parties are urged to be aware of the potential costs involved in producing relevant ESI but are advised that only reasonably accessible ESI need be produced.  The principles stipulate that parties should not be required to search for or collect deleted material unless there is an agreement or court order related to those terms.  The use of electronic tools and processes such as data sampling and web harvesting are acceptable practices.  Parties are encouraged to agree early in the litigation process on production format required for the exchange of relevant documents as part of the discovery process (native files, [[pdf]], [[Tagged Image File Format|tiff]], [[metadata]] requirements etc).  Agreements or direction should be sought, if necessary, with respect to [[wikt:privilege|privilege]] or other confidential information related to production of electronic documents and data.  Parties should be aware that legal precedents can be formed as a result of [[e-discovery]] practices and sanctions can be considered for a party&#8217;s failure to meet their discovery obligations unless it can be demonstrated that the failure was not intentional.  All parties must bear the &#8220;reasonable&#8221; costs associated with [[e-discovery]] but other arrangements can be agreed upon by the parties or by court order.&lt;ref&gt;{{cite web|url=https://www.canlii.org/en/commentary/sedonacanada/principles_en.html|title=CanLII - The Sedona Canada Principles Addressing Electronic Discovery (Jan. 2008)|work=canlii.org}}&lt;/ref&gt;

==Caselaw==

In ''Warman v. National Post Company'' proportionality was at issue in a case where the plaintiff was suing the defendant for libel.  A motion was brought by the defendant to have the plaintiff provide a mirror image of his hard drive in an effort to prove an internet article was indeed authored by the plaintiff.  Issues of proportionality and the work of the Sedona Conference and Sedona Canada Principles were factored in to the decision to grant the defendant only limited access to the hard drive.&lt;ref name="canlii.org"/&gt;

In ''Innovative Health Group Inc. v. Calgary Health Region'' the plaintiff&#8217;s legal obligation to produce imaged hard drives is in question.  Justice Conrad refers to the advice of Sedona Canada on proportionality and problems associated with time and expense related to the difficulties associated with electronically stored information.&lt;ref&gt;{{cite web|url=http://www.canlii.org/en/ab/abca/doc/2008/2008abca219/2008abca219.html|title=CanLII - 2008 ABCA 219 (CanLII)|work=canlii.org}}&lt;/ref&gt;

In ''York University v. Michael Markicevic'' Justice Brown specifically refers to the need for the parties to agree upon a formal e-discovery plan to be drafted in consultation with Sedona Canada Principles.&lt;ref name="canlii.org1"&gt;{{cite web|url=http://www.canlii.org/en/on/onsc/doc/2013/2013onsc378/2013onsc378.html|title=CanLII - 2013 ONSC 378 (CanLII)|work=canlii.org}}&lt;/ref&gt;

In ''Friends of Lansdowne v. Ottawa'' Master MacLeod refers to the need for Sedona Canada principles and states &#8220;This is particularly true in the current information age when e-mail is ubiquitous and multiple copies or variants of messages may be held on various kinds of data storage devices including individual hard drives, e-mail and Blackberry servers.  Even documents that ultimately exist in paper form normally begin their life on computers and negotiations frequently involve exchanges of electronic drafts.  To find every scrap of paper and every electronic trace of relevant information has become a nightmarish task that threatens to render any kind of litigation extravagantly expensive.&#8221;&lt;ref name="canlii.org1"/&gt;

==Criticism==

Critics of the Sedona Canada Principles believe they should address [[system integrity]] and that the true history of any file preserved cannot be identified without proof of the integrity of the electronic record systems management it comes from.&lt;ref&gt;{{cite web|url=http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2530515|title=The Sedona Canada Principles are Very Inadequate on Records Management and for Electronic Discovery|work=ssrn.com}}&lt;/ref&gt;

Other criticism is more directed to the Sedona Canada working group and complaints that it is insular and irrelevant&lt;ref&gt;{{cite web|url=http://www.canadianlawyermag.com/5078/Sedona-Canada-is-alive-and-well.html|title=Sedona Canada is alive and well|author=Colin Campbell and James Swanson|work=canadianlawyermag.com}}&lt;/ref&gt;

==External links==

[https://www.canlii.org/en/commentary/sedonacanada/principles_en.html/The Sedona Canada Principles]

[http://www.canadianlawyermag.com/5078/Sedona-Canada-is-alive-and-well.html/ Sedona Canada is alive and well]

[https://www.highbeam.com/doc/1G1-181488000.html/ Taming the beast of eDiscovery with Sedona Canada Principles]

[https://www.highbeam.com/doc/1G1-400332555.html/ 2014 eDiscovery Year in Review includes Sedona Canada Principles]

[http://www.canadianlawyermag.com/legalfeeds/469/ontario-judge-slams-dark-ages-court-system.html/Ontario Courts discuss Sedona Canada Principles]

[http://www.canadianlawyermag.com/3988/What-is-predictive-coding-and-can-it-help-me.html/ Sedona Canada Principles and predictive coding]

[http://www.canadianlawyermag.com/5019/Alternative-routes.html/ Document review using Sedona Canada Principles]


==References==

{{reflist}}
&lt;!-- After listing your sources please cite them using inline citations and place them after the information they cite. Please see http://en.wikipedia.org/wiki/Wikipedia:REFB for instructions on how to add citations. --&gt;
*
*
*
*

&lt;!-- STOP! Be warned that by using this process instead of Articles for Creation, this article is subject to scrutiny. As an article in "mainspace", it will be DELETED if there are problems, not just declined. If you wish to use AfC, please return to the Wizard and continue from there. --&gt;



[[Category:Data management]]</text>
      <sha1>dkfg9hov9ztvim4tmf8zuybe6yp4cin</sha1>
    </revision>
  </page>
  <page>
    <title>Data philanthropy</title>
    <ns>0</ns>
    <id>49882988</id>
    <revision>
      <id>744370663</id>
      <parentid>740717604</parentid>
      <timestamp>2016-10-14T19:39:43Z</timestamp>
      <contributor>
        <ip>38.140.131.42</ip>
      </contributor>
      <comment>removed errant closing bracket</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="15600" xml:space="preserve">{{Orphan|date=March 2016}}

'''Data philanthropy''' describes a form of collaboration in which private sector companies share data for public benefit.&lt;ref name="Pawelke"&gt;Pawelke, A. and Tatevossian, A. (2013, May 8) [http://www.unglobalpulse.org/data-philanthropy-where-are-we-now Data philanthropy: where are we now?] United Nations Global Pulse.&lt;/ref&gt; There are multiple uses of data philanthropy being explored from humanitarian, corporate, human rights, and academic use. Since the introduction of this term the [[United Nations Global Pulse]] has began pushing for a global &#8220;data philanthropy movement.&#8221;&lt;ref name= "Coren"&gt;Coren, M. (2011, December 9) [http://www.fastcoexist.com/1678963/data-philanthropy-open-data-for-world-changing-solutions Data Philanthropy: Open data for world-changing solutions] Fast Company.&lt;/ref&gt;

== Definition==
A large amount of data collected from the Internet comes from [[user-generated content]]. This includes blogs, posts on social networks, and information submitted in forms. Besides user-generated data, corporations are also currently [[data mining]] data from consumers in order to understand customers, identify new markets, and make investment decisions. Kirkpatrick the Director at United Nations Global Pulse labels this data &#8220;massive passive data&#8221; or &#8220;data exhaust.&#8221;&lt;ref name="Kirkpatrick"&gt;Kirkpatrick, R. (2011, September 20). [http://www.forbes.com/sites/oreillymedia/2011/09/20/data-philanthropy-is-good-for-business/ Data philanthropy is good for business] Forbes.&lt;/ref&gt; Data philanthropy is the idea that something positive can come from this overload of data. Data philanthropy is defined as the private sector sharing this data in ways that the public can benefit.&lt;ref name="Pawelke" /&gt; The term philanthropy helps to emphasis that [[data sharing]] is a positive act and that the shared data is a public good.&lt;ref name="Kirkpatrick" /&gt;

== Challenges ==
A challenge that comes with sharing data is the [[Internet privacy]] of the user whose data is being used. Mathematical techniques ([[differential privacy]] and space time boxes) have been introduced in order to make personal data accessible, while providing the users providing such data with anonymity. But even if these algorithms work there is always the possibility and fear of re-identification.&lt;ref name="Pawelke" /&gt;
 
The other challenge is convincing corporations to share their data. The big data corporations collect provides them with market competitiveness. They are able to infer meaning regarding [[consumer behavior]]. The fear is that by sharing all their information, they may lose their competitive edge.&lt;ref name="Pawelke" /&gt;

== Sharing strategies ==
The goal of data philanthropy is to create a global data commons where companies, governments, and individuals can contribute anonymous, aggregated datasets.&lt;ref name="Coren" /&gt; The United Nations Global Pulse offers four different tactics that companies can use to share their data that preserve consumer anonymity.  These include:&lt;ref name="Pawelke" /&gt;
# Share aggregated and derived data sets for analysis under nondisclosure agreements (NDA)
# Allow researchers to analyze data within the private company&#8217;s own network, under NDA
# Real-Time Data Commons: data pooled and aggregated between multiple companies of the same industry to protect competitiveness
# Public/Private Alerting Network: companies mine data behind their own firewalls and share indicators

By providing these four tactics United Nations Global Pulse hopes to provide initiative and options for companies to share their data with the public.

== Digital disease detection ==
Data philanthropy has led to advancements in the field of health and wellness. By using data gathered from social media, cell phones, and other communication modes health researchers have been able to track the spread of diseases.&lt;ref name="Schmidt"&gt;Schmidt, C. (2012). [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3261963/ Trending Now: Using Social Media to Predict and Track Disease Outbreaks.] Environ Health Perspect, 120(1), A30&#8211;a33-A30&#8211;a33.&lt;/ref&gt;

In the United States [[HealthMap]], a freely available website and mobile app software is using data philanthropy related tactics to track the outbreak of diseases. HealthMap analyzes data from publicly available media sources such as news websites, government alerts, and social media sites like Twitter for outbreaks of various illnesses around the world.&lt;ref name="Schmidt" /&gt;&lt;ref name="Reddy"&gt;Reddy, E. (2015, July 14). [https://blog.twitter.com/2015/twitter-data-public-health Using Twitter data to study the world's health] Twitter.&lt;/ref&gt;  The creators of HealthMap have another website, Flu Near You, which allows users to report their own health status on a weekly basis. Traditional flu surveillance can take up to 2 weeks to confirm outbreaks.&lt;ref name= "Schmidt" /&gt; Doctors must wait for virological test to confirm the outbreak before reporting it to the Centers for Disease Control. This form of data philanthropy allows for up to date information regarding various health concerns by using publicly available information gathered from news outlets, government alerts, and social media sites. It is the data gathered on social media sites, where users are not aware their data is being mined that leads to HealthMap and Flue Near You being considered data philanthropy.&lt;ref name="Schmidt" /&gt; 
 
The [[Centers for Disease Control and Prevention]] collaborated with Google and launched [[Google Flu Trends]] in 2008, a website that tracks flu-related searches and user location to track the spread of the flu. Users can visit the website to compare the amount of flu-related search activity against the reported numbers of flu outbreaks on a graphic map. The difficulty with this method of tracking is that Google searched are sometimes performed due to curiosity rather than because an individual is suffering from the flu. According to Ashley Fowlkes, an epidemiologist in the CDC Influenza division, &#8220;the Google Flu Trends system tries to account for that type of media bias by modeling search terms over time to see which ones remain stable.&#8221;&lt;ref name="Schmidt" /&gt; Google Flu Trends is not longer publishing current flu estimates on the public website. Visitors to the site can still view and download previous estimates. Current data can be shared with verified researchers.&lt;ref name="O'Connor'"&gt;O'Connor, F. (2015, August 20). [http://www.pcworld.com/article/2974153/websites/google-flu-trends-calls-out-sick-indefinitely.html Google Flu Trends calls out sick, indefinitely] PC World.&lt;/ref&gt;
 
A study by Harvard School of Public Health (HSPH) released in the October 12, 2012 issues of the journal Science discussed how phone data helped curb the spread of malaria in Kenya. The researchers mapped phone calls and texts made by 14,816,521 Kenyan mobile phone subscribers.&lt;ref name= "Datz"&gt;Datz, T. (2012, October 11). [http://www.hsph.harvard.edu/news/press-releases/cell-phone-data-malaria/ Using cell phone data to curb the spread of malaria.] Harvard Chan.&lt;/ref&gt; When individuals left their primary living location the destination and length of journey was calculated. This data was then compared to a 2009 malaria prevalence map to estimate the disease&#8217;s commonness in each location.  Combining all this information the researchers can estimate the probability of an individual carrying malaria and map the movement of the disease. This research, a result of data philanthropy, can be used to track the spread of similar diseases.&lt;ref name="Datz" /&gt;

==Application in various fields==
Through data philanthropy &#8216;[[big data]]&#8217; corporations such as [[social networking sites]], telecommunication companies, [[search engines]] amongst others, collect and make user generated information available to a data sharing system. This also permits institutions to give back to a beneficial cause. With the onset of [[technological]] advancements, sharing data on a global scale and an in-depth analysis of these data structures could alter the reaction towards certain occurrences, be it [[natural disaster]]s, [[epidemics]], worldwide [[economic]] problems and many other events. Some analyst have argued&lt;ref name="Forbes"&gt;[http://www.forbes.com/sites/oreillymedia/2011/09/20/data-philanthropy-is-good-for-business Data Philanthropy is Good for Business], by Robert Kirkpatrick, Forbes, 2011-09-20&lt;/ref&gt; that this aggregated Information is beneficial for the common good and can lead to developments in [[research]] and [[data]] production in a range of varied fields.&lt;ref name="Forbes"/&gt;

===Humanitarian aid===
Calling patterns of [[mobile phone]] users can determine the [[socioeconomic]] standings of the populace which can be used to deduce &#8220;its access to housing, education, healthcare, and basic services such as water and electricity&#8221;.&lt;ref name="Forbes"/&gt; Researchers from Columbia University and Karolinska Institute utilize information from [[mobile phone]] providers, in order to assist in the dispersal of resources by deducing the movement of those displaced by natural disasters. Big data can also provide information on looming disasters and can assist relief organizations in rapid response and locating displaced individuals. By analyzing certain patterns within this &#8216;big data&#8217;, could successively transform the response to destructive occurrences like natural disasters, [[outbreak]]s of diseases and global economic distress, by employing real-time information to achieve a comprehension of the welfare of individuals. Corporations utilize digital services, such as human sensor systems to detect and solve impending problems within [[communities]]. This is a strategy implemented by the private sector in order to protect its citizens by anonymously dispersing customer information to the public sector, whilst also ensuring the protection of their privacy.&lt;ref name="Forbes"/&gt;

===Impoverished areas===
[[Poverty]] still remains a worldwide issue with over 2.5 billion people&lt;ref name="Smart Data Collective"&gt;[http://www.smartdatacollective.com/rick-delgado/200566/lifting-how-big-data-can-help-eliminate-poverty Lifting Up: How Big Data Can Help Eliminate Poverty], by Rick Delgado, Smart Data Collection , 2014-05-23&lt;/ref&gt; currently impoverished. Accumulating accurate data has been a complex issue but developments in [[technology]] and utilising 'big data',&lt;ref name="Smart Data Collective" /&gt; is one solution for improving this situation. Statistics indicate the widespread use of mobile phones, even within impoverished communities. This availability could prove vital in gathering data on populations living in poverty. Additional data can be collected through [[Internet access]], social media, utility payments and [[governmental]] statistics. Data-driven activities can lead to the cumulation of &#8216;big data&#8217;, which in turn can assist international non-governmental organization in documenting and evaluating the needs of underprivileged populations. Through data philanthropy, [[NGO]]&#8217;s can distribute information whilst cooperating with governments and private companies.&lt;ref name="Smart Data Collective" /&gt;

===Corporate===
Data philanthropy incorporates aspects of social philanthropy by permitting  [[corporations]] to create profound impacts through the act of giving back by dispersing proprietary datasets.&lt;ref name="Irevolution"&gt;[http://irevolution.net/2012/06/04/big-data-philanthropy-for-humanitarian-response/Big Data Philanthropy for Humanitarian Response], by Irevolution, 2012-07-04&lt;/ref&gt; The [[public sector]], is faced with an unequal and limited access to the frequency of data and they also produce, collect and preserve information, which has proven to be an essential asset. Company&#8217;s track and analyze users online activities, so as to gain more insight into their needs in relation to new products and services.&lt;ref&gt;[https://hbr.org/2014/07/sharing-data-is-a-form-of-corporate-philanthropy/Sharing Data Is a Form of Corporate Philanthropy], by Matt Stempeck,Harvard Business Review 2014-07-24&lt;/ref&gt;
These companies view the welfare of the population as a vital key to the expansion and progression of businesses by using their data to place a spotlight on the plight of global citizens.&lt;ref name="Forbes" /&gt;[[Expert]]s in the private sector contend the importance of merging various data streams such as retail, mobile phone and social media data to create necessary solutions to handle global issues. Despite the inevitable risk of sharing private information, it works in a beneficial manner and serves the interest of the public.&lt;ref&gt;[https://hbr.org/2013/03/a-new-type-of-philanthropy-don&amp;cm_sp=Article-_-Links-_-Top%20of%20Page%20Recirculation A New Type of Philanthropy: Donating Data], by Robert Kirkpatrick,Harvard Business Review 2013-03-21&lt;/ref&gt; The digital revolution causes an extensive production of &#8216;big data&#8217; that is user-generated and available on the web. Corporations accumulate information on customer preferences through the digital services they utilize and products they purchase, in order to gain a clear insight on their clientele and future market opportunities.&lt;ref name="Forbes" /&gt; However the rights of individuals concerning privacy and ownership of data are a controversial issue as governments and other institutions can use this collective data for other unethical purposes. Companies monitor and probe consumer online activities in order to better comprehend and develop tailored needs for their clientele and in turn increase their profits.&lt;ref name="Jim Fruchterman"&gt;[https://hbr.org/2013/03/big-data-means-more-than-big-p Big Data Means More Than Big Profits], by Jim Fruchterman, Harvard Business Review, 2013-03-19&lt;/ref&gt;

===Academia===
Data philanthropy plays an important role in [[academia]]. Researchers encounter countless obstacles whilst attempting to access data. This data is available to a limited number of researchers with sole access to restricted resources who are authorized to utilize this information; like social media streams enabling them to produce more [[knowledge]] and develop new studies. For example, Twitter markets access to its real-time APIs at exorbitant prices, which often surpasses the budgets of most researchers. 'Data Grants&#8217;&lt;ref name="Jim Fruchterman" /&gt; is a trial program created by Twitter that provides a selective number of academics and researchers with access to real-time databases in order to garner more knowledge. They apply to gain entry into vast data downloads, on specific topics.&lt;ref name="Jim Fruchterman" /&gt;

===Human rights===
Data philanthropy aids the human rights movement, by assisting in the dispersal of evidence for truth commissions and war crimes tribunals. Proponents of human rights accumulate data on abuse occurring within states, which is then used for scientific analysis and propels awareness and action. For example, non-profit organizations compile data from Human Rights monitors in war zones in order to assist the UN High Commissioner for Human Rights. It uncovers inconsistencies in the number of casualties of war, which in turn leads to international attention and exerts influence on discussions relating to global policy.&lt;ref name="Jim Fruchterman" /&gt;

==See also==
* [[Big Data]]

==References==
&lt;references /&gt;

== External links ==
* [http://www.unglobalpulse.org/data-philanthropy-where-are-we-now Data Philanthropy, where are we now?] in UN Global Pulse blog by Adreas Pawelke and Anoush Rima Tatevossian (2013-05-08).

[[Category:Big data| ]]
[[Category:Data management]]</text>
      <sha1>sl69so4qa9x7qxyff179krazz1xi9mi</sha1>
    </revision>
  </page>
  <page>
    <title>SQL injection</title>
    <ns>0</ns>
    <id>526999</id>
    <revision>
      <id>761518788</id>
      <parentid>761154159</parentid>
      <timestamp>2017-01-23T11:29:22Z</timestamp>
      <contributor>
        <username>Ahmedschuh</username>
        <id>22760832</id>
      </contributor>
      <comment>Provided extra resources where you can find additional detailed technical information about the many different variants of the SQL Injection vulnerability.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="40510" xml:space="preserve">{{Use mdy dates|date=February 2012}}
[[File:KD SQLIA Classification 2010.png|thumb|alt=Classification of SQL injection attack vectors in 2010|A classification of SQL injection attacking vector as of 2010.]]

'''SQL injection ''' is a [[code injection]] technique, used  to [[Attack (computing)|attack]] data-driven applications, in which nefarious [[SQL]] statements are inserted into an entry field for execution (e.g. to dump the database contents to the attacker).&lt;ref&gt;{{cite web | url = http://technet.microsoft.com/en-us/library/ms161953%28v=SQL.105%29.aspx | title = SQL Injection | accessdate = 2013-08-04 | author = Microsoft | quote = SQL injection is an attack in which malicious code is inserted into strings that are later passed to an instance of SQL Server for parsing and execution. Any procedure that constructs SQL statements should be reviewed for injection vulnerabilities because SQL Server will execute all syntactically valid queries that it receives. Even parameterized data can be manipulated by a skilled and determined attacker.}}&lt;/ref&gt; SQL injection must exploit a [[security vulnerability]] in an application's software, for example, when user input is either incorrectly filtered for [[string literal]] [[escape sequence|escape characters]] embedded in SQL statements or user input is not [[Strongly-typed programming language|strongly typed]] and unexpectedly executed. SQL injection is mostly known as an attack [[Vector (malware)|vector]] for websites but can be used to attack any type of SQL database.

SQL injection attacks allow attackers to spoof identity, tamper with existing data, cause repudiation issues such as voiding transactions or changing balances, allow the complete disclosure of all data on the system, destroy the data or make it otherwise unavailable, and become administrators of the database server.

In a 2012 study, it was observed that the average web application received 4 attack campaigns per month, and retailers received twice as many attacks as other industries.&lt;ref&gt;{{cite web | url = http://www.imperva.com/docs/HII_Web_Application_Attack_Report_Ed4.pdf | title = Imperva Web Application Attack Report | accessdate = 2013-08-04 | author = Imperva | date = July 2012 | format = PDF | quote = Retailers suffer 2x as many SQL injection attacks as other industries. / While most web applications receive 4 or more web attack campaigns per month, some websites are constantly under attack. / One observed website was under attack 176 out of 180 days, or 98% of the time.}}&lt;/ref&gt;

==History==

The first public discussions of SQL injection started appearing around 1998;&lt;ref&gt;{{cite web |title= How Was SQL Injection Discovered? The researcher once known as Rain Forrest Puppy explains how he discovered the first SQL injection more than 15 years ago. |author= Sean Michael Kerner  |date= November 25, 2013 |url= http://www.esecurityplanet.com/network-security/how-was-sql-injection-discovered.html }}&lt;/ref&gt; for example, a 1998 article in [[Phrack Magazine]].&lt;ref&gt;{{cite journal |title= NT Web Technology Vulnerabilities |author= Jeff Forristal (signing as rain.forest.puppy) |journal= [[Phrack Magazine]] |volume= 8 |issue= 54 (article 8) |date= Dec 25, 1998 |url= http://www.phrack.com/issues.html?issue=54&amp;id=8#article }}&lt;/ref&gt;

==Form==
SQL injection (SQLI) is considered one of the top 10 web application vulnerabilities of 2007 and 2010 by the [[OWASP|Open Web Application Security Project]].&lt;ref&gt;{{cite web|url=https://www.owasp.org/index.php/Category:OWASP_Top_Ten_Project |title=Category:OWASP Top Ten Project |publisher=OWASP |accessdate=2011-06-03}}&lt;/ref&gt; In 2013, SQLI was rated the number one attack on the OWASP top ten.&lt;ref&gt;{{cite web|url=https://www.owasp.org/index.php/Top_10_2013-Top_10 |title=Category:OWASP Top Ten Project |publisher=OWASP |accessdate=2013-08-13}}&lt;/ref&gt; There are four main sub-classes of SQL injection:
* Classic SQLI
* Blind or Inference SQL injection
* [[Database management system]]-specific SQLI
* Compounded SQLI

:* SQL injection + insufficient authentication&lt;ref&gt;{{cite web|url=http://www.xiom.com/whid-2007-60 |title=WHID 2007-60: The blog of a Cambridge University security team hacked |publisher=Xiom |accessdate=2011-06-03}}&lt;/ref&gt;
:* SQL injection + [[DDoS]] attacks&lt;ref&gt;{{cite web|url=http://www.xiom.com/content/whid-2009-1-gaza-conflict-cyber-war |title=WHID 2009-1: Gaza conflict cyber war |publisher=Xiom |accessdate=2011-06-03}}&lt;/ref&gt;
:* SQL injection + [[DNS hijacking]]&lt;ref&gt;[http://www.xiom.com/whid-list/DNS%20Hijacking ] {{webarchive |url=https://web.archive.org/web/20090618125914/http://www.xiom.com/whid-list/DNS%20Hijacking |date=June 18, 2009 }}&lt;/ref&gt;
:* SQL injection + [[Cross-site scripting|XSS]]&lt;ref&gt;{{cite web|url=http://www.darkreading.com/security/management/showArticle.jhtml?articleID=211201482 |title=Third Wave of Web Attacks Not the Last |publisher=Dark Reading |accessdate=2012-07-29}}&lt;/ref&gt;

The [[Storm Worm]] is one representation of Compounded SQLI.&lt;ref&gt;{{cite web|last=Danchev |first=Dancho |url=http://ddanchev.blogspot.com/2007/01/social-engineering-and-malware.html|title=Mind Streams of Information Security Knowledge: Social Engineering and Malware |publisher=Ddanchev.blogspot.com |date=2007-01-23 |accessdate=2011-06-03}}&lt;/ref&gt;

This classification represents the state of SQLI, respecting its evolution until 2010&#8212;further refinement is underway.&lt;ref&gt;{{cite web|last=Deltchev|first=Krassen|title=New Web 2.0 Attacks|url=http://www.nds.ruhr-uni-bochum.de/teaching/theses/Web20/|work=B.Sc. Thesis|publisher=Ruhr-University Bochum|accessdate=February 18, 2010}}&lt;/ref&gt;

==Technical implementations==

===Incorrectly filtered escape characters===
This form of SQL injection occurs when user input is not filtered for [[escape character]]s and is then passed into an SQL statement. This results in the potential manipulation of the statements performed on the database by the end-user of the application.

The following line of code illustrates this vulnerability:

 statement = "&lt;source lang="sql" enclose="none"&gt;SELECT * FROM users WHERE name = '&lt;/source&gt;" + userName + "&lt;source lang="sql" enclose="none"&gt;';&lt;/source&gt;"

This SQL code is designed to pull up the records of the specified username from its table of users. However, if the "userName" variable is crafted in a specific way by a malicious user, the SQL statement may do more than the code author intended. For example, setting the "userName" variable as:

&lt;pre&gt;' OR '1'='1&lt;/pre&gt;
or using comments to even block the rest of the query (there are three types of SQL comments&lt;ref&gt;{{citation |title= IBM Informix Guide to SQL: Syntax. Overview of SQL Syntax &amp;gt; How to Enter SQL Comments |publisher= IBM |url= http://publib.boulder.ibm.com/infocenter/idshelp/v10/index.jsp?topic=/com.ibm.sqls.doc/sqls36.htm }}&lt;/ref&gt;). All three lines have a space at the end:
&lt;pre&gt;' OR '1'='1' --
' OR '1'='1' ({
' OR '1'='1' /* &lt;/pre&gt;
.
renders one of the following SQL statements by the parent language:

&lt;source lang="sql"&gt;SELECT * FROM users WHERE name = '' OR '1'='1';&lt;/source&gt;
&lt;source lang="sql"&gt;SELECT * FROM users WHERE name = '' OR '1'='1' -- ';&lt;/source&gt;

If this code were to be used in an authentication procedure then this example could be used to force the selection of every data field (*) from ''all'' users rather than from one specific user name as the coder intended,  because the evaluation of '1'='1' is always true ([[short-circuit evaluation]]).

The following value of "userName" in the statement below would cause the deletion of the "users" table as well as the selection of all data from the "userinfo" table (in essence revealing the information of every user), using an [[API]] that allows multiple statements:

 a';&lt;source lang="sql" enclose="none"&gt;DROP TABLE users; SELECT * FROM userinfo WHERE 't' = 't'&lt;/source&gt;

This input renders the final SQL statement as follows and specified:

&lt;source lang="sql"&gt;SELECT * FROM users WHERE name = 'a';DROP TABLE users; SELECT * FROM userinfo WHERE 't' = 't';&lt;/source&gt;

While most SQL server implementations allow multiple statements to be executed with one call in this way, some SQL APIs such as [[PHP]]'s &lt;code&gt;mysql_query()&lt;/code&gt; function do not allow this for security reasons. This prevents attackers from injecting entirely separate queries, but doesn't stop them from modifying queries.

===Incorrect type handling===
This form of SQL injection occurs when a '''user-supplied''' field is not [[strongly typed]] or is not checked for [[data type|type]] constraints. This could take place when a numeric field is to be used in a SQL statement, but the programmer makes no checks to validate that the user supplied input is numeric. For example:
 statement := "&lt;source lang="sql" enclose="none"&gt;SELECT * FROM userinfo WHERE id = &lt;/source&gt;" + a_variable + ";"

It is clear from this statement that the author intended a_variable to be a number correlating to the "id" field. However, if it is in fact a [[String (computer science)|string]] then the [[end-user]] may manipulate the statement as they choose, thereby bypassing the need for escape characters. For example, setting a_variable to

&lt;pre&gt;1;DROP TABLE users&lt;/pre&gt;

will drop (delete) the "users" table from the database, since the SQL becomes:

&lt;source lang="sql"&gt;SELECT * FROM userinfo WHERE id=1; DROP TABLE users;&lt;/source&gt;

===Blind SQL injection===
Blind SQL Injection is used when a web application is vulnerable to an SQL injection but the results of the injection are not visible to the attacker. The page with the vulnerability may not be one that displays data but will display differently depending on the results of a logical statement injected into the legitimate SQL statement called for that page.
This type of attack has traditionally been considered time-intensive because a new statement needed to be crafted for each bit recovered, and depending on its structure, the attack may consist of many unsuccessful requests. Recent advancements have allowed each request to recover multiple bits, with no unsuccessful requests, allowing for more consistent and efficient extraction. &lt;ref&gt;{{cite web | url = http://howto.hackallthethings.com/2016/07/extracting-multiple-bits-per-request.html | title = Extracting Multiple Bits Per Request From Full-blind SQL Injection Vulnerabilities | publisher = Hack All The Things | accessdate = July 8, 2016 |archiveurl = https://web.archive.org/web/20160708190141/http://howto.hackallthethings.com/2016/07/extracting-multiple-bits-per-request.html |archivedate = July 8, 2016}}&lt;/ref&gt; There are several tools that can automate these attacks once the location of the vulnerability and the target information has been established.&lt;ref&gt;{{cite web | url = http://www.justinclarke.com/archives/2006/03/sqlbrute.html | title = Using SQLBrute to brute force data from a blind SQL injection point | publisher = Justin Clarke | accessdate = October 18, 2008 |archiveurl = http://web.archive.org/web/20080614203711/http://www.justinclarke.com/archives/2006/03/sqlbrute.html &lt;!-- Bot retrieved archive --&gt; |archivedate = June 14, 2008}}&lt;/ref&gt;

====Conditional responses====
One type of blind SQL injection forces the database to evaluate a logical statement on an ordinary application screen. As an example, a book review website uses a [[query string]] to determine which book review to display. So the [[URL]] &lt;code&gt;&lt;nowiki&gt;http://books.example.com/showReview.php?ID=5&lt;/nowiki&gt;&lt;/code&gt; would cause the server to run the query
&lt;source lang="sql"&gt;SELECT * FROM bookreviews WHERE ID = 'Value(ID)';&lt;/source&gt;
from which it would populate the review page with data from the review with [[Identifier|ID]] 5, stored in the [[Table (database)|table]] bookreviews. The query happens completely on the server; the user does not know the names of the database, table, or fields, nor does the user know the query string. The user only sees that the above URL returns a book review. A [[Hacker (computer security)|hacker]] can load the URLs &lt;code&gt;&lt;source lang="sql" enclose="none"&gt;http://books.example.com/showReview.php?ID=5 OR 1=1&lt;/source&gt;&lt;/code&gt; and &lt;code&gt;&lt;source lang="sql" enclose="none"&gt;http://books.example.com/showReview.php?ID=5 AND 1=2&lt;/source&gt;&lt;/code&gt;, which may result in queries
&lt;source lang="sql"&gt;SELECT * FROM bookreviews WHERE ID = '5' OR '1'='1';
SELECT * FROM bookreviews WHERE ID = '5' AND '1'='2';&lt;/source&gt;
respectively. If the original review loads with the "1=1" URL and a blank or error page is returned from the "1=2" URL, and the returned page has not been created to alert the user the input is invalid, or in other words, has been caught by an input test script, the site is likely vulnerable to a SQL injection attack as the query will likely have passed through successfully in both cases. The hacker may proceed with this query string designed to reveal the version number of [[MySQL]] running on the server: &lt;code&gt;&lt;source lang="mysql" enclose="none"&gt;http://books.example.com/showReview.php?ID=5 AND substring(@@version, 1, INSTR(@@version, '.') - 1)=4&lt;/source&gt;&lt;/code&gt;, which would show the book review on a server running MySQL 4 and a blank or error page otherwise. The hacker can continue to use code within query strings to glean more information from the server until another avenue of attack is discovered or his or her goals are achieved.&lt;ref&gt;{{cite web|url=http://forum.intern0t.org/web-hacking-war-games/818-blind-sql-injection.html|title=Blind SQL Injection tutorial|author=macd3v|accessdate=6 December 2012}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=TDSS botnet: full disclosure|url=http://nobunkum.ru/analytics/en-tdss-botnet|accessdate=6 December 2012|author=Andrey Rassokhin|author2=Dmitry Oleksyuk }}&lt;/ref&gt;

===Second order SQL injection===
Second order SQL injection occurs when submitted values contain malicious commands that are stored rather than executed immediately.  In some cases, the application may correctly encode an SQL statement and store it as valid SQL.  Then, another part of that application without controls to protect against SQL injection might execute that stored SQL statement.  This attack requires more knowledge of how submitted values are later used.  Automated web application security scanners would not easily detect this type of SQL injection and may need to be manually instructed where to check for evidence that it is being attempted.

==Mitigation==
An SQL injection is a well known attack and easily prevented by simple measures. After an apparent SQL injection attack on [[TalkTalk Group|Talktalk]] in 2015, the BBC reported that security experts were stunned that such a large company would be vulnerable to it.&lt;ref&gt;{{Cite web|title = Questions for TalkTalk - BBC News|url = http://www.bbc.com/news/technology-34636308|website = BBC News|accessdate = 2015-10-26|language = en}}&lt;/ref&gt;

===Parameterized statements===
{{Main article|Prepared statement}}
With most development platforms, parameterized statements that work with parameters can be used  (sometimes called placeholders or [[bind variable]]s) instead of embedding user input in the statement. A placeholder can only store a value of the given type and not an arbitrary SQL fragment. Hence the SQL injection would simply be treated as a strange (and probably invalid) parameter value.

In many cases, the SQL statement is fixed, and each parameter is a [[Scalar (computing)|scalar]], not a [[Table (database)|table]]. The user input is then assigned (bound) to a parameter.&lt;ref&gt;{{cite web|title=SQL Injection Prevention Cheat Sheet|url=https://www.owasp.org/index.php/SQL_Injection_Prevention_Cheat_Sheet|publisher=Open Web Application Security Project|accessdate=3 March 2012}}&lt;/ref&gt;

====Enforcement at the coding level====
Using [[object-relational mapping]] libraries avoids the need to write SQL code. The ORM library in effect will generate parameterized SQL statements from object-oriented code.

===Escaping===
A straightforward, though error-prone way to prevent injections is to escape characters that have a special meaning in SQL. The manual for an SQL DBMS explains which characters have a special meaning, which allows creating a comprehensive [[Blacklist (computing)|blacklist]] of characters that need translation. For instance, every occurrence of a single quote (&lt;code&gt;'&lt;/code&gt;) in a parameter must be replaced by two single quotes (&lt;code&gt;&lt;nowiki&gt;''&lt;/nowiki&gt;&lt;/code&gt;) to form a valid SQL string literal. For example, in [[PHP]] it is usual to escape parameters using the function &lt;code&gt;mysqli_real_escape_string();&lt;/code&gt; before sending the SQL query:
&lt;source lang="php"&gt;
$mysqli = new mysqli('hostname', 'db_username', 'db_password', 'db_name');
$query = sprintf("SELECT * FROM `Users` WHERE UserName='%s' AND Password='%s'",
                  $mysqli-&gt;real_escape_string($username),
                  $mysqli-&gt;real_escape_string($password));
$mysqli-&gt;query($query);
&lt;/source&gt;

This function prepends backslashes to the following characters: \x00, \n, \r, \, ', " and \x1a.
This function is normally used to make data safe before sending a query to [[MySQL]].&lt;ref&gt;{{cite web|url=http://in2.php.net/manual/en/mysqli.real-escape-string.php|title=mysqli-&gt;real_escape_string - PHP Manual|publisher=PHP.net}}&lt;/ref&gt;&lt;br /&gt; There are other functions for many database types in PHP such as pg_escape_string() for [[PostgreSQL]]. The function &lt;code&gt;addslashes(string $str)&lt;/code&gt; works for escaping characters, and is used especially for querying on databases that do not have escaping functions in PHP.  It returns a string with backslashes before characters that need to be quoted in database queries, etc. These characters are single quote ('), double quote ("), backslash (\) and NUL (the NULL byte).&lt;ref&gt;{{cite web|url=http://pl2.php.net/manual/en/function.addslashes.php|title=Addslashes - PHP Manual|publisher=PHP.net}}&lt;/ref&gt;&lt;br /&gt;
Routinely passing escaped strings to SQL is error prone because it is easy to forget to escape a given string. Creating a transparent layer to secure the input can reduce this error-proneness, if not entirely eliminate it.&lt;ref&gt;{{cite web|url=http://www.xarg.org/2010/11/transparent-query-layer-for-mysql/|title=Transparent query layer for MySQL|publisher=Robert Eisele|date=November 8, 2010}}&lt;/ref&gt;

===Pattern check===
Integer, float or boolean,string parameters can be checked if their value is valid representation for the given type. Strings that must follow some strict pattern (date, UUID, alphanumeric only, etc.) can be checked if they match this pattern.

===Database permissions===
Limiting the permissions on the database logon used by the web application to only what is needed may help reduce the effectiveness of any SQL injection attacks that exploit any bugs in the web application.

For example, on [[Microsoft SQL Server]], a database logon could be restricted from selecting on some of the system tables which would limit exploits that try to insert JavaScript into all the text columns in the database.
&lt;source lang="tsql"&gt;
deny select on sys.sysobjects to webdatabaselogon;
deny select on sys.objects to webdatabaselogon;
deny select on sys.tables to webdatabaselogon;
deny select on sys.views to webdatabaselogon;
deny select on sys.packages to webdatabaselogon;
&lt;/source&gt;

==Examples==
* In February 2002, Jeremiah Jacks discovered that Guess.com was vulnerable to an SQL injection attack, permitting anyone able to construct a properly-crafted URL to pull down 200,000+ names, credit card numbers and expiration dates in the site's customer database.&lt;ref&gt;{{cite web|url=http://www.securityfocus.com/news/346|title=Guesswork Plagues Web Hole Reporting|publisher=[[SecurityFocus]]|date=March 6, 2002}}&lt;/ref&gt;
* On November 1, 2005, a teenaged hacker used SQL injection to break into the site of a [[Taiwan]]ese information security magazine from the Tech Target group and steal customers' information.&lt;ref&gt;{{cite web|url=http://www.xiom.com/whid-2005-46|title=WHID 2005-46: Teen uses SQL injection to break to a security magazine web site|publisher=[[Web Application Security Consortium]]|date=November 1, 2005|accessdate=December 1, 2009}}&lt;/ref&gt;
* On January 13, 2006, [[Russia]]n computer criminals broke into a [[Government of Rhode Island|Rhode Island government]] website and allegedly stole credit card data from individuals who have done business online with state agencies.&lt;ref&gt;{{cite web|url=http://www.xiom.com/whid-2006-3|title=WHID 2006-3: Russian hackers broke into a RI GOV website|publisher=[[Web Application Security Consortium]]|date=January 13, 2006|accessdate=May 16, 2008}}&lt;/ref&gt;
* On March 29, 2006, a hacker discovered an SQL injection flaw in an official [[Government of India|Indian government]]'s [[Tourism in India|tourism]] site.&lt;ref&gt;{{cite web|url=http://www.xiom.com/whid-2006-27|title=WHID 2006-27: SQL Injection in incredibleindia.org|publisher=[[Web Application Security Consortium]]|date=March 29, 2006|accessdate=March 12, 2010}}&lt;/ref&gt;
* On June 29, 2007, a computer criminal defaced the [[Microsoft]] UK website using SQL injection.&lt;ref&gt;{{cite web|url=http://www.cgisecurity.net/2007/06/hacker-defaces.html|title=Hacker Defaces Microsoft U.K. Web Page|publisher=cgisecurity.net|author=Robert|date=June 29, 2007|accessdate=May 16, 2008}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://rcpmag.com/news/article.aspx?editorialsid=8762|title=Hacker Defaces Microsoft UK Web Page|publisher=Redmond Channel Partner Online|author=Keith Ward|date=June 29, 2007|accessdate=May 16, 2008}}&lt;/ref&gt; UK website ''[[The Register]]'' quoted a Microsoft [[spokesperson]] acknowledging the problem.
* On September 19, 2007 and January 26, 2009 the Turkish hacker group "m0sted" used SQL injection to exploit Microsoft's SQL Server to hack web servers belonging to [[McAlester Army Ammunition Plant]] and the [[United States Army Corps of Engineers|US Army Corps of Engineers]] respectively.&lt;ref&gt;{{cite web|url=http://www.informationweek.com/architecture/anti-us-hackers-infiltrate-army-servers/d/d-id/1079964|publisher=[[Information Week]]|title=Anti-U.S. Hackers Infiltrate Army Servers|date=May 29, 2009|accessdate=December 17, 2016}}&lt;/ref&gt;
* In January 2008, tens of thousands of PCs were infected by an automated SQL injection attack that exploited a vulnerability in application code that uses [[Microsoft SQL Server]] as the database store.&lt;ref name="chinesefarm" /&gt;
* In July 2008, [[Kaspersky Lab|Kaspersky]]'s [[Malaysia]]n site was hacked by the "m0sted" hacker group using SQL injection.
* On April 13, 2008, the [[Sex offender registries in the United States|Sexual and Violent Offender Registry]] of [[Oklahoma]] shut down its website for "[[routine maintenance]]" after being informed that 10,597 [[Social Security number]]s belonging to [[sex offender]]s had been downloaded via an SQL injection attack&lt;ref&gt;{{cite web|url=http://thedailywtf.com/Articles/Oklahoma-Leaks-Tens-of-Thousands-of-Social-Security-Numbers,-Other-Sensitive-Data.aspx|publisher=[[The Daily WTF]]|title=Oklahoma Leaks Tens of Thousands of Social Security Numbers, Other Sensitive Data|author=Alex Papadimoulis|date=April 15, 2008|accessdate=May 16, 2008}}&lt;/ref&gt;
* In May 2008, a [[server farm]] inside [[China]] used automated queries to [[Google Search|Google's search engine]] to identify [[Microsoft SQL Server|SQL server]] websites which were vulnerable to the attack of an automated SQL injection tool.&lt;ref name="chinesefarm"&gt;{{cite web | url = http://www.pcworld.com/businesscenter/article/146048/mass_sql_injection_attack_targets_chinese_web_sites.html | title = Mass SQL Injection Attack Targets Chinese Web Sites | author = Sumner Lemon, IDG News Service | publisher = [[PC World (magazine)|PCWorld]] | date = May 19, 2008 | accessdate = May 27, 2008 }}&lt;/ref&gt;&lt;ref name="attackspecifics"&gt;{{cite web | url = http://www.bloombit.com/Articles/2008/05/ASCII-Encoded-Binary-String-Automated-SQL-Injection.aspx | title = ASCII Encoded/Binary String Automated SQL Injection Attack |author=Michael Zino| date = May 1, 2008 }}&lt;/ref&gt;
* In 2008, at least April through August, a sweep of attacks began exploiting the SQL injection vulnerabilities of Microsoft's [[Internet Information Services|IIS web server]] and [[Microsoft SQL Server|SQL Server database server]]. The attack does not require guessing the name of a table or column, and corrupts all text columns in all tables in a single request.&lt;ref name="broad_inject_specifics"&gt;{{cite web | url = http://hackademix.net/2008/04/26/mass-attack-faq/ | title = Mass Attack FAQ |author=Giorgio Maone| date = April 26, 2008 }}&lt;/ref&gt;  A HTML string that references a [[malware]] [[JavaScript]] file is appended to each value. When that database value is later displayed to a website visitor, the script attempts several approaches at gaining control over a visitor's system. The number of exploited web pages is estimated at 500,000.&lt;ref name="broad_inject_numbers"&gt;{{cite web | url = http://www.computerworld.com/article/2535473/security0/huge-web-hack-attack-infects-500-000-pages.html | title = Huge Web hack attack infects 500,000 pages |author=Gregg Keizer| date = April 25, 2008 |accessdate=October 16, 2015}}&lt;/ref&gt;
* On August 17, 2009, the [[United States Department of Justice]] charged an American citizen, [[Albert Gonzalez]], and two unnamed Russians with the theft of 130 million credit card numbers using an SQL injection attack. In reportedly "the biggest case of [[identity theft]] in American history", the man stole cards from a number of corporate victims after researching their [[Payment processor|payment processing system]]s. Among the companies hit were credit card processor [[Heartland Payment Systems]], convenience store chain [[7-Eleven|7&amp;#8209;Eleven]], and supermarket chain [[Hannaford Brothers]].&lt;ref&gt;{{cite news |url=http://news.bbc.co.uk/2/hi/americas/8206305.stm |title=US man 'stole 130m card numbers' |publisher=BBC |date=August 17, 2009 |accessdate=August 17, 2009}}&lt;/ref&gt;
* In December 2009, an attacker breached a [[RockYou]] plaintext database containing the [[Encryption|unencrypted]] usernames and passwords of about 32&amp;nbsp;million users using an SQL injection attack.&lt;ref&gt;{{cite news | url=http://www.nytimes.com/external/readwriteweb/2009/12/16/16readwriteweb-rockyou-hacker-30-of-sites-store-plain-text-13200.html | title = RockYou Hacker - 30% of Sites Store Plain Text Passwords | work=New York Times | first=Jolie | last=O'Dell | date=December 16, 2009 | accessdate=May 23, 2010}}&lt;/ref&gt;
*On July 2010, a South American security researcher who goes by the [[User (computing)|handle]] "Ch&amp;nbsp;Russo" obtained sensitive user information from popular [[BitTorrent]] site [[The Pirate Bay]]. He gained access to the site's administrative control panel and exploited a SQL injection vulnerability that enabled him to collect user account information, including [[IP address]]es, [[MD5]] [[Cryptographic hash function|password hashes]] and records of which torrents individual users have uploaded.&lt;ref&gt;{{cite news |url=http://krebsonsecurity.com/2010/07/pirate-bay-hack-exposes-user-booty/ | title= The pirate bay attack | date=July 7, 2010 }}&lt;/ref&gt;
*From July 24 to 26, 2010, attackers from [[Japan]] and [[China]] used an SQL injection to gain access to customers' credit card data from Neo Beat, an [[Osaka]]-based company that runs a large online supermarket site. The attack also affected seven business partners including supermarket chains Izumiya Co, Maruetsu Inc, and Ryukyu Jusco Co. The theft of data affected a reported 12,191 customers. As of August 14, 2010 it was reported that there have been more than 300 cases of credit card information being used by third parties to purchase goods and services in China.
* On September 19 during the [[Swedish general election, 2010|2010 Swedish general election]] a voter attempted a code injection by hand writing SQL commands as part of a [[Write-in candidate|write&amp;#8209;in]] vote.&lt;ref&gt;{{cite web|url=http://alicebobandmallory.com/articles/2010/09/23/did-little-bobby-tables-migrate-to-sweden |title=Did Little Bobby Tables migrate to Sweden? |publisher=Alicebobandmallory.com |accessdate=2011-06-03}}&lt;/ref&gt;
* On November 8, 2010 the British [[Royal Navy]] website was compromised by a Romanian hacker named TinKode using SQL injection.&lt;ref&gt;[http://www.bbc.co.uk/news/technology-11711478 Royal Navy website attacked by Romanian hacker] ''BBC News'', 8-11-10, Accessed November 2010&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://news.sky.com/skynews/Home/World-News/Stuxnet-Worm-Virus-Targeted-At-Irans-Nuclear-Plant-Is-In-Hands-Of-Bad-Guys-Sky-News-Sources-Say/Article/201011415827544 |title=Super Virus A Target For Cyber Terrorists
|author=Sam Kiley |date=November 25, 2010 |accessdate=November 25, 2010}}&lt;/ref&gt;
* On February 5, 2011 [[HBGary]], a technology security firm, was broken into by [[LulzSec]] using a SQL injection in their CMS-driven website&lt;ref&gt;{{cite web|url=http://www.par-anoia.net/We_Are_Anonymous_Inside_the_Hacker_World_of_LulzSe.pdf|title=We Are Anonymous: Inside the Hacker World of LulzSec|publisher=Little, Brown and Company}}&lt;/ref&gt;
* On March 27, 2011, [http://www.mysql.com mysql.com], the official homepage for [[MySQL]], was compromised by a hacker using SQL blind injection&lt;ref&gt;{{cite web|url=http://blog.sucuri.net/2011/03/mysql-com-compromised.html|title=MySQL.com compromised|publisher=[[sucuri]]}}&lt;/ref&gt;
* On April 11, 2011, [[Barracuda Networks]] was compromised using an SQL injection flaw. [[Email address]]es and usernames of employees were among the information obtained.&lt;ref&gt;{{cite web|url=http://www.networkworld.com/news/2011/041211-hacker-breaks-into-barracuda-networks.html?hpg1=bn |title=Hacker breaks into Barracuda Networks database}}&lt;/ref&gt;
*Over a period of 4&amp;nbsp;hours on April 27, 2011, an automated SQL injection attack occurred on [[Broadband Reports]] website that was able to extract 8% of the username/password pairs: 8,000 random accounts of the 9,000 active and 90,000 old or inactive accounts.&lt;ref name="DSLReports"&gt;{{cite web|url=http://www.dslreports.com/forum/r25793356- |title=site user password intrusion info |publisher=Dslreports.com |accessdate=2011-06-03}}&lt;/ref&gt;&lt;ref name="Cnet News"&gt;{{cite news|url=http://news.cnet.com/8301-27080_3-20058471-245.html|title=DSLReports says member information stolen|publisher=Cnet News|date=2011-04-28|accessdate=2011-04-29}}&lt;/ref&gt;&lt;ref name="The Tech Herald"&gt;{{cite news|url=http://www.thetechherald.com/article.php/201117/7127/DSLReports-com-breach-exposed-more-than-100-000-accounts|title=DSLReports.com breach exposed more than 100,000 accounts|publisher=The Tech Herald|date=2011-04-29|accessdate=2011-04-29}}&lt;/ref&gt;
*On June 1, 2011, "[[hacktivist]]s" of the group [[LulzSec]] were accused of using SQLI to steal [[coupon]]s, download keys, and passwords that were stored in plaintext on [[Sony]]'s website, accessing the personal information of a million users.&lt;ref&gt;{{citation |title= LulzSec hacks Sony Pictures, reveals 1m passwords unguarded | date= June 2, 2011 |work= electronista.com |url= http://www.electronista.com/articles/11/06/02/lulz.security.hits.sony.again.in.security.message/ }}&lt;/ref&gt;&lt;ref&gt;{{citation |title= LulzSec Hacker Arrested, Group Leaks Sony Database|author=Ridge Shan | date= June 6, 2011 |work= The Epoch Times |url=http://www.theepochtimes.com/n2/technology/lulzsec-member-arrested-group-leaks-sony-database-57296.html}}&lt;/ref&gt;
* In June 2011, [[PBS]] was hacked, mostly likely through use of SQL injection; the full process used by hackers to execute SQL injections was described in this [http://blog.imperva.com/2011/05/pbs-breached-how-hackers-probably-did-it.html Imperva] blog.&lt;ref name="PBS Breached - How Hackers Probably Did It"&gt;{{cite news|url=http://blog.imperva.com/2011/05/pbs-breached-how-hackers-probably-did-it.html|title=Imperva.com: PBS Hacked - How Hackers Probably Did It|accessdate=2011-07-01}}&lt;/ref&gt;
* In May 2012, the website for ''[[Wurm Online]]'', a [[massively multiplayer online game]], was shut down from an SQL injection while the site was being updated.&lt;ref&gt;{{cite web|url=http://wurmonline.tumblr.com/post/22835329693/wurm-online-restructuring |title=Wurm Online is Restructuring |date=May 11, 2012}}&lt;/ref&gt;
* [[2012 Yahoo! Voices hack|In July 2012]] a hacker group was reported to have stolen 450,000 login credentials from [[Yahoo!]]. The logins were stored in [[plain text]] and were allegedly taken from a Yahoo [[subdomain]], [[Yahoo! Voices]]. The group breached Yahoo's security by using a "[[Set operations (SQL)#UNION operator|union]]-based SQL injection technique".&lt;ref&gt;Chenda Ngak. [http://www.cbsnews.com/8301-501465_162-57470956-501465/yahoo-reportedly-hacked-is-your-account-safe/ "Yahoo reportedly hacked: Is your account safe?"], CBS News. July 12, 2012. Retrieved July 16, 2012.&lt;/ref&gt;&lt;ref&gt;http://www.zdnet.com/450000-user-passwords-leaked-in-yahoo-breach-7000000772/&lt;/ref&gt;
* On October 1, 2012, a hacker group called "Team GhostShell" published the personal records of students, faculty, employees, and alumni from 53 universities including [[Harvard]], [[Princeton University|Princeton]], [[Stanford]], [[Cornell]], [[Johns Hopkins University|Johns Hopkins]], and the [[University of Zurich]] on [[Pastebin|pastebin.com]]. The hackers claimed that they were trying to "raise awareness towards the changes made in today&#8217;s education", bemoaning changing education laws in Europe and increases in [[College tuition in the United States|tuition in the United States]].&lt;ref&gt;{{cite news|last=Perlroth|first=Nicole|title=Hackers Breach 53 Universities and Dump Thousands of Personal Records Online|url=http://bits.blogs.nytimes.com/2012/10/03/hackers-breach-53-universities-dump-thousands-of-personal-records-online/|newspaper=New York Times|date=3 October 2012}}&lt;/ref&gt;
* In February 2013, a group of Maldivian hackers, hacked the website "UN-Maldives" using SQL Injection.
* On June 27, 2013, hacker group "[[RedHack]]" breached Istanbul Administration Site.&lt;ref&gt;{{Cite news | title=RedHack Breaches Istanbul Administration Site, Hackers Claim to Have Erased Debts | url=http://news.softpedia.com/news/RedHack-Breaches-Istanbul-Administration-Site-Hackers-Claim-to-Have-Erased-Debts-364000.shtml}}&lt;/ref&gt;  They claimed that, they&#8217;ve been able to erase people's debts to water, gas, Internet, electricity, and telephone companies. Additionally, they published admin user name and password for other citizens to log in and clear their debts early morning. They announced the news from Twitter.&lt;ref&gt;{{Cite news | title=Redhack tweet about their achievement | url=http://twitter.com/RedHack_EN/statuses/350461821456613376 }}&lt;/ref&gt;
* On November 4, 2013, hacktivist group "RaptorSwag" allegedly compromised 71 Chinese government databases using an SQL injection attack on the Chinese Chamber of International Commerce. The leaked data was posted publicly in cooperation with [[Anonymous (group)|Anonymous]].&lt;ref&gt;http://news.softpedia.com/news/Hackers-Leak-Data-Allegedly-Stolen-from-Chinese-Chamber-of-Commerce-Website-396936.shtml&lt;/ref&gt;
* On February 2, 2014, AVS TV had 40,000 accounts leaked by a hacking group called @deletesec &lt;ref&gt;http://www.maurihackers.info/2014/02/40000-avs-tv-accounts-leaked.html&lt;/ref&gt;
* On February 21, 2014, United Nations Internet Governance Forum had 3,215 account details leaked.&lt;ref&gt;http://www.batblue.com/united-nations-internet-governance-forum-breached/&lt;/ref&gt;
* On February 21, 2014, Hackers of a group called @deletesec hacked Spirol International after allegedly threatening to have the hackers arrested for reporting the security vulnerability. 70,000 user details were exposed over this conflict.&lt;ref&gt;http://news.softpedia.com/news/Details-of-70-000-Users-Leaked-by-Hackers-From-Systems-of-SPIROL-International-428669.shtml&lt;/ref&gt;
* On March 7, 2014, officials at Johns Hopkins University publicly announced that their Biomedical Engineering Servers had become victim to an SQL injection attack carried out by an Anonymous hacker named "Hooky" and aligned with hacktivist group "RaptorSwag". The hackers compromised personal details of 878 students and staff, posting a [http://pastebin.com/UG4fYnby press release] and the leaked data on the internet.&lt;ref&gt;http://articles.baltimoresun.com/2014-03-07/news/bs-md-hopkins-servers-hacked-20140306_1_engineering-students-identity-theft-server&lt;/ref&gt;
* In August 2014, [[Milwaukee]]-based computer security company Hold Security disclosed that it uncovered [[2014 Russian hacker password theft|a theft of confidential information]] from nearly 420,000 websites through SQL injections.&lt;ref&gt;Damon Poeter. [http://www.pcmag.com/article2/0,2817,2462057,00.asp 'Close-Knit' Russian Hacker Gang Hoards 1.2 Billion ID Creds], ''PC Magazine'', August 5, 2014&lt;/ref&gt; ''[[The New York Times]]'' confirmed this finding by hiring a security expert to check the claim.&lt;ref&gt;Nicole Perlroth. [http://www.nytimes.com/2014/08/06/technology/russian-gang-said-to-amass-more-than-a-billion-stolen-internet-credentials.html?_r=0 Russian Gang Amasses Over a Billion Internet Passwords], ''The New York Times'', August 5, 2014.&lt;/ref&gt;
* In October 2015, an SQL injection attack was used to steal the personal details of 156,959 customers from British telecommunications company [[TalkTalk Group|Talk Talk's]] servers, exploiting a vulnerability in a legacy web portal&lt;ref&gt;https://ico.org.uk/about-the-ico/news-and-events/news-and-blogs/2016/10/talktalk-gets-record-400-000-fine-for-failing-to-prevent-october-2015-attack/&lt;/ref&gt;

==In popular culture==
* Unauthorized login to web sites by means of SQL injection forms the basis of one of the subplots in [[J.K. Rowling]]'s novel ''[[The Casual Vacancy]]'', published in 2012.
* An ''[[xkcd]]'' cartoon involved a character "Robert'); DROP TABLE students;--" named to carry out a SQL injection. As a result of this cartoon, SQL injection is sometimes informally referred to as 'Bobby Tables'.&lt;ref&gt;{{cite web|last=Munroe|first=Randall|title=XKCD: Exploits Of A Mom|url=http://xkcd.com/327/|accessdate=26 February 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Bobby Tables: A guide to preventing SQL injection|url=http://bobby-tables.com/|accessdate=6 October 2013}}&lt;/ref&gt;
* In 2014, an individual in Poland legally renamed his business to ''&lt;nowiki&gt;Dariusz Jakubowski x'; DROP TABLE users; SELECT '1&lt;/nowiki&gt;'' in an attempt to disrupt operation of spammers&#8217; [[Web scraping|harvesting bots]].&lt;ref&gt;{{cite web|title=Jego firma ma w nazwie SQL injection. Nie zazdro&#347;cimy tym, kt&#243;rzy b&#281;d&#261; go fakturowali ;)|website=Niebezpiecznik|language=pl|date=11 September 2014|url=http://niebezpiecznik.pl/post/jego-firma-ma-w-nazwie-sql-injection-nie-zazdroscimy-tym-ktorzy-beda-go-fakturowali/|accessdate=26 September 2014}}&lt;/ref&gt;
* The 2015 game [[Hacknet]] has a hacking program called SQL_MemCorrupt. It is described as injecting a table entry that causes a corruption error in a SQL database, then queries said table, causing a SQL database crash and core dump.

==See also==
{{Portal|Software Testing}}
* [[Code injection]]
* [[Cross-site scripting]]
* [[Metasploit Project]]
* [[OWASP]] Open Web Application Security Project
* [[SGML entity]]
* [[Uncontrolled format string]]
* [[w3af]]
* [[Web application security]]

==References==
{{Reflist|30em}}

==External links==
* [http://www.techyfreaks.com/2012/05/manual-sql-injection-tutorial.html Manual Sql Injection Tutorial] By The Ajay Devgan
* [http://www.websec.ca/kb/sql_injection SQL Injection Knowledge Base], by Websec.
* [http://www.sqlinjectionwiki.com/ SQL Injection Wiki]
* [http://projects.webappsec.org/SQL-Injection WASC Threat Classification - SQL Injection Entry], by the Web Application Security Consortium.
* [https://docs.google.com/leaf?id=0BykNNUTb95yzYTRjMjNjMWEtODBmNS00YzgwLTlmMGYtNWZmODI2MTNmZWYw&amp;sort=name&amp;layout=list&amp;num=50 Why SQL Injection Won't Go Away], by Stuart Thomas.
* [http://www.owasp.org/index.php/SQL_Injection_Prevention_Cheat_Sheet SQL Injection Prevention Cheat Sheet], by OWASP.
* [http://sqlmap.org/ sqlmap: automatic SQL injection and database takeover tool]
* [http://go.microsoft.com/?linkid=9707610 SDL Quick security references on SQL injection] by Bala Neerumalla.
* [http://arstechnica.com/information-technology/2016/10/how-security-flaws-work-sql-injection/ How security flaws work: SQL injection]
* [https://www.netsparker.com/blog/web-security/sql-injection-cheat-sheet/ SQL Injection Cheat Sheet] by Netsparker

[[Category:Data management]]
[[Category:Injection exploits]]
[[Category:SQL]]
[[Category:Articles with example SQL code]]
[[Category:Computer security exploits]]</text>
      <sha1>h4mo7gk9524i9sty0vt114zqopvhskg</sha1>
    </revision>
  </page>
  <page>
    <title>Cambridge Semantics</title>
    <ns>0</ns>
    <id>50373790</id>
    <revision>
      <id>761892340</id>
      <parentid>749270743</parentid>
      <timestamp>2017-01-25T12:13:27Z</timestamp>
      <contributor>
        <username>Mitch Ames</username>
        <id>6326132</id>
      </contributor>
      <comment>Remove supercategory of existing category per [[WP:SUBCAT]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="10385" xml:space="preserve">{{Infobox company
|name             = Cambridge Semantics
|logo             =
|logo_size        =200
|type             = [[Private company|Private]]
|genre            =
|fate             =
|predecessor      =
|successor        =
|foundation       = 2007
|founder          = Sean Martin&lt;br&gt;Lee Feigenbaum&lt;br&gt;Simon Martin&lt;br&gt;Emmett Eldred
|defunct          =
|location_city    = [[Boston, MA]]
|location_country = [[United States]]
|locations        = (2) [[Boston, MA]] &amp; [[San Diego, CA]]
|area_served      =
|key_people       =  Chuck Pieper (CEO)&lt;br&gt;Alok Prasad (President)
|industry         =  [[Computer Software]]
|products         =
|production       =
|services         =
|revenue          = 
|operating_income =
|net_income       =
|aum              =
|assets           =
|equity           =
|slogan           = The Smart Data Company
|owner            =
|num_employees    =
|parent           =
|divisions        =
|subsid           =
|homepage         =  {{URL|CambridgeSemantics.com}} 
|footnotes        =
|intl             =
}}

'''Cambridge Semantics''' is a privately held company headquartered in [[Boston, Massachusetts]] with a West Coast office in [[San Diego, California]]. The company develops and sells a suite of smart data products for Data Management, Data Discovery and Enterprise Analytics.

==History==

Cambridge Semantics was founded in 2007 by Sean Martin, Lee Feigenbaum, Simon Martin, Rouben Meschian and Emmett Eldred who all previously worked at [[IBM]]'s Advanced Technology Internet Group.&lt;ref&gt;{{cite web|last1=Lynch|first1=Brendan|website=[[Boston Business Journal]]|title=Ex-IBMers aim at better search tech|url=http://www.bizjournals.com/boston/blog/mass-high-tech/2008/03/ex-ibmers-aim-at-better-search-tech.html|accessdate=27 April 2016}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last1=Resende|first1=Patricia|title=With explosion of big data comes big growth for Cambridge Semantics|url=http://www.bizjournals.com/boston/blog/techflash/2015/02/with-explosion-of-big-data-comes-big-growth-for.html|website=[[Boston Business Journal]]|accessdate=27 April 2016}}&lt;/ref&gt;

In 2012, Cambridge Semantics appointed Chuck Pieper as Chief Executive Officer. Prior to joining Cambridge Semantics, Pieper was Vice Chairman of Alternative Investments and Managing Director of [[Credit Suisse]] within the Asset Management Division.&lt;ref&gt;{{cite web|last1=Seiffert|first1=Don|title=Chuck Pieper named CEO at Cambridge Semantics|url=http://www.bizjournals.com/boston/blog/techflash/2012/12/chuck-pieper-named-ceo-at-cambridge.html|website=[[Boston Business Journal]]|accessdate=27 April 2016}}&lt;/ref&gt;

In 2015, Cambridge Semantics formed an alliance with [[MarkLogic]].&lt;ref&gt;{{cite web|title=Cambridge Semantics and MarkLogic Partner to Advance Semantic-Driven Data Management|url=http://www.dbta.com/Editorial/News-Flashes/Cambridge-Semantics-and-MarkLogic-Partner-to-Advance-Semantic-Driven-Data-Management-106569.aspx|website=Dbta.com|accessdate=27 April 2016|language=en-US|date=24 September 2015}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=MarkLogic, Cambridge Semantics partner for NoSQL|url=http://www.kmworld.com/Articles/News/News/MarkLogic-Cambridge-Semantics-partner-for-NoSQL-106568.aspx|website=[[KMWorld|KMWorld Magazine]]|accessdate=27 April 2016|language=en-US|date=24 September 2015}}&lt;/ref&gt;

In January 2016, Cambridge Semantics acquired SPARQL City and its [[graph database]] [[intellectual property]].&lt;ref&gt;{{cite web|last1=Leopold|first1=George|title=Cambridge Semantics Buys Graph Database Specialist|url=http://www.datanami.com/2016/01/14/cambridge-semantics-buys-graph-database-specialist/|website=Datanami|accessdate=27 April 2016|date=14 January 2016}}&lt;/ref&gt;

==Products==
* Anzo Smart Data Platform is a platform for building unified information solutions based on a set of open data standards implemented using [[Semantic Web |Semantic Web Technologies]].&lt;ref&gt;{{cite web|last1=Bertolucci|first1=Jeff|title=Big Data + Semantic Web: Love At First Terabyte? - InformationWeek|url=http://www.informationweek.com/big-data/big-data-analytics/big-data-+-semantic-web-love-at-first-terabyte/d/d-id/1107520?|website=[[InformationWeek]]|accessdate=28 April 2016}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last1=Shacklett|first1=Mary|title=A start to solving the enterprise data usage problem - TechRepublic|url=http://www.techrepublic.com/article/a-start-to-solving-the-enterprise-data-usage-problem/|website=[[TechRepublic]]|accessdate=28 April 2016}}&lt;/ref&gt; It allows IT departments and their business users to quickly and flexibly access all of their diverse data for breakthrough insights.&lt;ref&gt;{{cite web|last1=Lawson|first1=Loraine|title=Cambridge Semantics Offers New Integration Tool|url=http://www.itbusinessedge.com/blogs/integration/cambridge-semantics-offers-new-integration-tool.html|website=IT Business Edge|accessdate=27 April 2016}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Cambridge Semantics Launches Anzo Smart Data Integration|url=http://www.econtentmag.com/Articles/News/News-Item/Cambridge-Semantics-Launches-Anzo-Smart-Data-Integration-98007.htm|website=EContent Magazine|accessdate=27 April 2016|language=en-US|date=3 July 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=The time for Smart Data has finally arrived: Cambridge Semantics Inc.|url=http://thesiliconreview.com/magazines/the-time-for-smart-data-has-finally-arrived-cambridge-semantics-inc/|website=The Silicon Review|accessdate=27 April 2016|language=en-US}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last1=Kutz|first1=Erin|title=Cambridge Semantics, Looking to Put Microsoft Excel "On Steroids," Brings Intelligent Data Sorting to Non-Techies|url=http://www.xconomy.com/boston/2010/07/08/cambridge-semantics-looking-to-put-microsoft-excel-on-steroids-brings-intelligent-data-sorting-to-non-techies/|website=[[Xconomy]]|accessdate=27 April 2016|language=en-US|date=8 July 2010}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last1=McNamara|first1=Paul|title=Book of Odds opening eyes to new probabilities|url=http://www.networkworld.com/article/2231870/data-center/book-of-odds-opening-eyes-to-new-probabilities.html|website=[[Network World]]|accessdate=28 April 2016}}&lt;/ref&gt;
* Anzo Smart Data Manager
* Anzo Graph Query Engine
* Anzo Smart Data Lake

==Awards and recognition==

* Cambridge Semantics named [[Software and Information Industry Association|SIIA]] [[CODiE Award]] 2016 finalist.&lt;ref&gt;{{cite web|title=2016 Finalists|url=https://www.siia.net/codie/2016-Finalists|website=Siia.net|accessdate=27 April 2016}}&lt;/ref&gt;
* Cambridge Semantics named [[KMWorld]]&#8217;s 2016 &#8216;100 Companies That Matter in Knowledge Management&#8217;&lt;ref&gt;{{cite web|title=KMWorld 100 COMPANIES That Matter in Knowledge Management|url=http://www.kmworld.com/Articles/Editorial/Features/KMWorld-100-COMPANIES-That-Matter-in-Knowledge-Management-109344.aspx|website=[[KMWorld|KMWorld Magazine]]|accessdate=27 April 2016|language=en-US|date=1 March 2016}}&lt;/ref&gt; and [[KMWorld]] Trend-Setting Products of 2015.&lt;ref&gt;{{cite web|last1=McKellar|first1=Hugh|title=KMWorld Trend-Setting Products of 2015|url=http://www.kmworld.com/Articles/Editorial/Features/KMWorld-Trend-Setting-Products-of-2015-105783.aspx|website=[[KMWorld|KMWorld Magazine]]|accessdate=27 April 2016|language=en-US|date=1 September 2015}}&lt;/ref&gt;
* Cambridge Semantics named  2016 Bio-IT World Best of Show People's Choice Award Contenders&lt;ref&gt;{{cite web|title=2016 Bio-IT World Best of Show People's Choice Award Contenders|url=http://www.bio-itworld.com/2016/3/29/2016-best-of-show-peoples-choice-award-contenders.asp|website=Bio-IT World|accessdate=27 April 2016}}&lt;/ref&gt; and 2015 Bio-IT best of show finalist.&lt;ref&gt;{{cite web|title=Bio-IT World Recognizes 2015 Best of Show Winners|url=http://www.bio-itworld.com/2015/4/27/bio-it-world-recognizes-2015-best-of-show-winners.html|website=Bio-IT World|accessdate=27 April 2016}}&lt;/ref&gt;
* CIO Review Recognizes Cambridge Semantics as 2015 Top 20 Tech Solution Provider for [[Pharmaceutical industry|Pharma and Life Sciences Industry]].&lt;ref&gt;{{cite web|title=20  Most Promising Pharma and Life Sciences Tech Solution Providers  20 15|url=http://pharma-life-sciences.cioreview.com/vendors/2015/20special1|website=CIOReview|accessdate=27 April 2016}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Cambridge Semantics:Smart Data Management and Advanced Analytics for Pharma and Life Sciences|url=http://pharma-life-sciences.cioreview.com/vendor/2015/cambridge_semantics|website=CIOReview|accessdate=27 April 2016}}&lt;/ref&gt;
* Anzo Insider Trading Investigation and Surveillance named 2015 [[CODiE Award]] finalist.&lt;ref&gt;{{cite web|title=Finalists - 2015 SIIA CODiE Awards|url=https://www.siia.net/archive/codies/2015/finalists.asp|website=Siia.net|accessdate=27 April 2016}}&lt;/ref&gt;
* Cambridge Semantics Selected as Finalist for 2014 [[MIT Sloan]] CIO Symposium's Innovation Showcase.&lt;ref&gt;{{cite web|title=Lead Your Digital Enterprise Mit Sloan Cio|url=http://www.mitcio.com/wp-content/uploads/2015/12/mitcio_2014.pdf|accessdate=27 April 2016}}&lt;/ref&gt;
* Cambridge Semantics named [[Software and Information Industry Association|SIIA]] [[CODiE Award]] 2014 finalist.&lt;ref&gt;{{cite web|title=Finalists - 2014 SIIA CODiE Awards |url=http://archive.siia.net/codies/2014/finalist_detail.asp?ID=3 |website=Siia.net |accessdate=27 April 2016 }}{{dead link|date=November 2016 |bot=InternetArchiveBot |fix-attempted=yes }}&lt;/ref&gt;
* Cambridge Semantics Win 2013 [[Software and Information Industry Association|SIIA]] [[CODiE Award]] for best business intelligence and analytics solution.&lt;ref&gt;{{cite web|title=2013 CODiE Award Winners|url=http://www.siia.net/archive/codies/2015/pw_2013.asp|website=Siia.net|accessdate=27 April 2016}}&lt;/ref&gt;
* Cambridge Semantics wins [[KMWorld]] 2012 Promise Award.&lt;ref&gt;{{cite web|title=KMWorld Promise Award Winner|url=http://www.kmworld.com/Articles/Editorial/Features/KMWorld-2012-Promise-and-Reality-award-winners-and-finalists-85829.aspx|website=KMWorld Magazine|accessdate=27 April 2016|language=en-US|date=30 October 2012}}&lt;/ref&gt;
* Cambridge Semantics wins Best of Show at 2012 Bio-IT World Conference.&lt;ref&gt;{{cite web|title=2012 Best of Show Winners|url=http://www.bio-itworld.com/2012/04/26/2012-best-of-show-winners.html|website=Bio-IT World|accessdate=27 April 2016}}&lt;/ref&gt;

==References==
{{reflist|2}}

==External links==
* [https://www.cambridgesemantics.com/ Official website]

[[Category:Software companies based in Massachusetts]]
[[Category:Companies established in 2007]]
[[Category:Data management]]</text>
      <sha1>7xdigwvbfim1lue2b35sanurscm8ql9</sha1>
    </revision>
  </page>
  <page>
    <title>Database normalization</title>
    <ns>0</ns>
    <id>8640</id>
    <revision>
      <id>762261030</id>
      <parentid>759999494</parentid>
      <timestamp>2017-01-27T18:03:37Z</timestamp>
      <contributor>
        <ip>91.146.162.142</ip>
      </contributor>
      <comment>A list is better than prose.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="13286" xml:space="preserve">'''Database normalization''', or simply '''normalization''', is the process of organizing the [[column (database)|columns]] (attributes) and [[table (database)|tables]] (relations) of a [[relational database]] to reduce [[data redundancy]] and improve data integrity.

Normalization involves arranging attributes in tables based on [[Dependency theory (database theory)|dependencies]] between attributes, ensuring that the dependencies are properly enforced by database integrity constraints. Normalization is accomplished through applying some formal rules either by a process of synthesis or decomposition. Synthesis creates a normalized database design based on a known set of dependencies. Decomposition takes an existing (insufficiently normalized) database design and improves it based on the known set of dependencies.

[[Edgar F. Codd]], the inventor of the [[relational model]] (RM), introduced the concept of normalization and what we now know as the [[First normal form]] (1NF) in 1970.&lt;ref name="Codd1970"&gt;{{cite journal|first=E. F.|last=Codd|authorlink=E.F. Codd|title=A Relational Model of Data for Large Shared Data Banks|journal=[[Communications of the ACM]]|volume=13|issue=6|date=June 1970|pages=377&#8211;387|url=http://www.acm.org/classics/nov95/toc.html | doi = 10.1145/362384.362685}}&lt;/ref&gt; Codd went on to define the [[Second normal form]] (2NF) and [[Third normal form]] (3NF) in 1971,&lt;ref name="Codd, E.F 1971"&gt;Codd, E.F. "Further Normalization of the Data Base Relational Model". (Presented at Courant Computer Science Symposia Series 6, "Data Base Systems", New York City, May 24&#8211;25, 1971.) IBM Research Report RJ909 (August 31, 1971). Republished in Randall J. Rustin (ed.), ''Data Base Systems: Courant Computer Science Symposia Series 6''. Prentice-Hall, 1972.&lt;/ref&gt; and Codd and [[Raymond F. Boyce]] defined the Boyce-Codd Normal Form ([[Boyce&#8211;Codd normal form|BCNF]]) in 1974.&lt;ref name="CoddBCNF"&gt;Codd, E. F. "Recent Investigations into Relational Data Base Systems". IBM Research Report RJ1385 (April 23, 1974). Republished in ''Proc. 1974 Congress'' (Stockholm, Sweden, 1974). , N.Y.: North-Holland (1974).&lt;/ref&gt; Informally, a relational database table is often described as "normalized" if it meets Third Normal Form.&lt;ref name="DateIntroDBSys"&gt;C.J. Date.  ''An Introduction to Database Systems''. Addison-Wesley (1999), p. 290&lt;/ref&gt;  Most 3NF tables are free of insertion, update, and deletion anomalies.


==Objectives==
A basic objective of the [[first normal form]] defined by Codd in 1970 was to permit data to be queried and manipulated using a "universal data sub-language" grounded in [[first-order logic]].&lt;ref&gt;"The adoption of a relational model of data ... permits the development of a universal data sub-language based on an applied predicate calculus. A first-order predicate calculus suffices if the collection of relations is in first normal form. Such a language would provide a yardstick of linguistic power for all other proposed data languages, and would itself be a strong candidate for embedding (with appropriate syntactic modification) in a variety of host Ianguages (programming, command- or problem-oriented)."  Codd, [http://www.acm.org/classics/nov95/toc.html "A Relational Model of Data for Large Shared Data Banks"], p. 381&lt;/ref&gt; ([[SQL]] is an example of such a data sub-language, albeit one that Codd regarded as seriously flawed.)&lt;ref&gt;Codd, E.F.  Chapter 23, "Serious Flaws in SQL", in ''The Relational Model for Database Management: Version 2''. Addison-Wesley (1990), pp. 371&#8211;389&lt;/ref&gt;

The objectives of normalization beyond 1NF (First Normal Form) were stated as follows by Codd:

{{Quotation|
# To free the collection of relations from undesirable insertion, update and deletion dependencies;
# To reduce the need for restructuring the collection of relations, as new types of data are introduced, and thus increase the life span of application programs;
# To make the relational model more informative to users;
# To make the collection of relations neutral to the query statistics, where these statistics are liable to change as time goes by.
|E.F. Codd|"Further Normalization of the Data Base Relational Model"&lt;ref&gt;Codd, E.F. "Further Normalization of the Data Base Relational Model", p. 34&lt;/ref&gt;}}

The sections below give details of each of these objectives.

===Free the database of modification anomalies===
[[File:Update anomaly.svg|280px|thumb|right|An '''update anomaly'''. Employee 519 is shown as having different addresses on different records.]]
[[File:Insertion anomaly.svg|280px|thumb|right|An '''insertion anomaly'''. Until the new faculty member, Dr. Newsome, is assigned to teach at least one course, his details cannot be recorded.]]
[[File:Deletion anomaly.svg|280px|thumb|right|A '''deletion anomaly'''. All information about Dr. Giddens is lost if he temporarily ceases to be assigned to any courses.]]
When an attempt is made to modify (update, insert into, or delete from) a table, undesired side-effects may arise in tables that have not been sufficiently normalized. An insufficiently normalized table might have one or more of the following characteristics:

* The same information can be expressed  on multiple rows; therefore updates to the table may result in logical inconsistencies. For example, each record in an "Employees' Skills" table might contain an Employee ID, Employee Address, and Skill; thus a change of address for a particular employee will potentially need to be applied to multiple records (one for each skill). If the update is not carried through successfully&#8212;if, that is, the employee's address is updated on some records but not others&#8212;then the table is left in an inconsistent state. Specifically, the table provides conflicting answers to the question of what this particular employee's address is. This phenomenon is known as an '''update anomaly'''.
* There are circumstances in which certain facts cannot be recorded at all. For example, each record in a "Faculty and Their Courses" table might contain a Faculty ID, Faculty Name, Faculty Hire Date, and Course Code&#8212;thus we can record the details of any faculty member who teaches at least one course, but we cannot record the details of a newly hired faculty member who has not yet been assigned to teach any courses except by setting the Course Code to null. This phenomenon is known as an '''insertion anomaly'''.
* Under certain circumstances, deletion of data representing certain facts necessitates deletion of data representing completely different facts. The "Faculty and Their Courses" table described in the previous example suffers from this type of anomaly, for if a faculty member temporarily ceases to be assigned to any courses, we must delete the last of the records on which that faculty member appears, effectively also deleting the faculty member, unless we set the Course Code to null in the record itself.  This phenomenon is known as a '''deletion anomaly'''.

===Minimize redesign when extending the database structure===
When a fully normalized database structure is extended to allow it to accommodate new types of data, the pre-existing aspects of the database structure can remain largely or entirely unchanged. As a result, applications interacting with the database are minimally affected.

Normalized tables, and the relationship between one normalized table and another, mirror real-world concepts and their interrelationships.

===Example===
Querying and manipulating the data within a data structure that is not normalized, such as the following non-1NF representation of customers, credit card transactions, involves more complexity than is really necessary:

{| class="wikitable"
! Customer !! Cust. ID !! Transactions
|-
| Jones || 1
|| 
{| class="wikitable"
! Tr. ID !! Date !! Amount
|-
| 12890
| 14-Oct-2003
| &amp;minus;87
|-
| 12904
| 15-Oct-2003
| &amp;minus;50
|}
|-
| Wilkins || 2
|| 
{| class="wikitable"
! Tr. ID !! Date !! Amount
|-
| 12898
| 14-Oct-2003
| &amp;minus;21
|}
|-
| Stevens || 3
|| 
{| class="wikitable"
! Tr. ID !! Date !! Amount
|-
| 12907
| 15-Oct-2003
| &amp;minus;18
|-
| 14920
| 20-Nov-2003
| &amp;minus;70
|-
| 15003
| 27-Nov-2003
| &amp;minus;60
|}
|}
&lt;br&gt;
To each customer corresponds a ''repeating group'' of transactions.  The automated evaluation of any query relating to customers' transactions therefore would broadly involve two stages:
# Unpacking one or more customers' groups of transactions allowing the individual transactions in a group to be examined, and
# Deriving a query result based on the results of the first stage

For example, in order to find out the monetary sum of all transactions that occurred in October 2003 for all customers, the system would have to know that it must first unpack the ''Transactions'' group of each customer, then sum the ''Amounts'' of all transactions thus obtained where the ''Date'' of the transaction falls in October 2003.

One of Codd's important insights was that this structural complexity could always be removed completely, leading to much greater power and flexibility in the way queries could be formulated (by [[user (computing)|users]] and [[application software|applications]]) and evaluated (by the [[database management system|DBMS]]).  The normalized equivalent of the structure above would look like this:

{| class="wikitable"
|-
! Customer !! Cust. ID
|-
| Jones || 1
|-
| Wilkins || 2
|-
| Stevens || 3
|}

{| class="wikitable"
|-
! Cust. ID !! Tr. ID !! Date !! Amount
|-
| 1 || 12890 || 14-Oct-2003 || &amp;minus;87
|-
| 1 || 12904 || 15-Oct-2003 || &amp;minus;50
|-
| 2 || 12898 || 14-Oct-2003 || &amp;minus;21
|-
| 3 || 12907 || 15-Oct-2003 || &amp;minus;18
|-
| 3 || 14920 || 20-Nov-2003 || &amp;minus;70
|-
| 3 || 15003 || 27-Nov-2003 || &amp;minus;60
|}

In the modified structure, the keys are {Customer} and {Cust. ID} in the first table, {Cust. ID, Tr ID} in the second table.

Now each row represents an individual credit card transaction, and the DBMS can obtain the answer of interest, simply by finding all rows with a Date falling in October, and summing their Amounts.  The data structure places all of the values on an equal footing, exposing each to the DBMS directly, so each can potentially participate directly in queries; whereas in the previous situation some values were embedded in lower-level structures that had to be handled specially.  Accordingly, the normalized design lends itself to general-purpose query processing, whereas the unnormalized design does not. The normalized version also allows the user to change the customer name in one place and guards against errors that arise if the customer name is misspelled on some records.

==List of Normal Forms==
* UNF - "[[Denormalization|Unnormalized]] Form"
* [[First normal form|1NF - First Normal Form]]
* [[Second normal form|2NF - Second Normal Form]]
* [[Third normal form|3NF - Third Normal Form]]
* [[Elementary Key Normal Form|EKNF - Elementary Key Normal Form]]
* [[Boyce&#8211;Codd normal form|BCNF - Boyce&#8211;Codd Normal Form]]
* [[Fourth normal form|4NF - Fourth Normal Form]]
* [http://researcher.watson.ibm.com/researcher/files/us-fagin/icdt12.pdf ETNF - Essential Tuple Normal Form]
* [[Fifth normal form|5NF - Fifth Normal Form]]
* [[Sixth normal form|6NF - Sixth Normal Form]]
* [[Domain/key normal form|DKNF - Domain/Key Normal Form]]

==See also==
*[[Refactoring]]

==Notes and references==
{{reflist|2}}
{{refbegin}}
{{refend}}

==Further reading==
* Date, C. J. (1999), ''[http://www.aw-bc.com/catalog/academic/product/0,1144,0321197844,00.html  An Introduction to Database Systems]'' (8th ed.). Addison-Wesley Longman. ISBN 0-321-19784-4.
* Kent, W. (1983) ''[http://www.bkent.net/Doc/simple5.htm A Simple Guide to Five Normal Forms in Relational Database Theory]'', Communications of the ACM, vol. 26, pp.&amp;nbsp;120&#8211;125
* H.-J. Schek, P. Pistor Data Structures for an Integrated Data Base Management and Information Retrieval System

==External links==
* [http://databases.about.com/od/specificproducts/a/normalization.htm Database Normalization Basics] by Mike Chapple (About.com)
* [http://www.databasejournal.com/sqletc/article.php/1428511 Database Normalization Intro], [http://www.databasejournal.com/sqletc/article.php/26861_1474411_1 Part 2]
* [http://mikehillyer.com/articles/an-introduction-to-database-normalization/ An Introduction to Database Normalization] by Mike Hillyer.
* [http://phlonx.com/resources/nf3/ A tutorial on the first 3 normal forms] by Fred Coulson
* [http://www.dbnormalization.com/ DB Normalization Examples]
* [http://support.microsoft.com/kb/283878 Description of the database normalization basics] by Microsoft
* [http://www.barrywise.com/2008/01/database-normalization-and-design-techniques/ Database Normalization and Design Techniques] by Barry Wise, recommended reading for the Harvard MIS.
* [http://www.bkent.net/Doc/simple5.htm A Simple Guide to Five Normal Forms in Relational Database Theory]
* [http://beginnersbook.com/2015/05/normalization-in-dbms/ Normalization in DBMS by Chaitanya (beginnersbook.com)]

{{Database normalization}}
{{Database}}
{{Databases}}

{{DEFAULTSORT:Database Normalization}}
[[Category:Database normalization| ]]
[[Category:Database constraints]]
[[Category:Data management]]
[[Category:Data modeling]]
[[Category:Relational algebra]]</text>
      <sha1>7rmmnbjsmcu6l0xscyvpuotw16rnwa3</sha1>
    </revision>
  </page>
  <page>
    <title>Information repository</title>
    <ns>0</ns>
    <id>13255720</id>
    <revision>
      <id>732160430</id>
      <parentid>720231226</parentid>
      <timestamp>2016-07-30T01:30:19Z</timestamp>
      <contributor>
        <username>Ushkin N</username>
        <id>28390915</id>
      </contributor>
      <comment>where to place data is not a property of [[:Category:Computer storage]] but [[:Category:Data management]] decision; also this article lacks direct references, I don't see clear link to [[Computer storage]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3101" xml:space="preserve">{{other uses|Knowledge base}}

An '''information repository''' is an easy way to deploy a secondary tier of [[data storage device|data storage]] that can comprise multiple, networked data storage technologies running on diverse [[operating system]]s, where data that no longer needs to be in primary storage is protected, classified according to captured [[metadata]], processed, de-duplicated, and then purged, automatically, based on data service level objectives and requirements. In information repositories, data storage resources are virtualized as composite storage sets and operate as a [[Federation (information technology)|federated]] environment.

Information repositories were developed to mitigate problems arising from [[data proliferation]] and eliminate the need for separately deployed data storage solutions because of the concurrent deployment of diverse storage technologies running diverse operating systems. They feature centralized management for all deployed data storage resources. They are self-contained, support heterogeneous storage resources, support resource management to add, maintain, recycle, and terminate media, track of off-line media, and operate autonomously.

==Automated data management==
Since one of the main reasons for the implementation of an Information repository is to reduce the maintenance workload placed on IT staff by traditional data storage systems, information repositories are automated. Automation is accomplished via polices that can process data based on time, events, data age, and data content. Policies manage the following:
*File system space management
*Irrelevant data elimination (mp3, games, etc.)
*Secondary storage resource management
Data is processed according to media type, [[Storage virtualization|storage pool]], and [[data storage device|storage technology]].

Because information repositories are intended to reduce IT staff workload, they are designed to be easy to deploy and offer configuration flexibility, virtually limitless extensibility, redundancy, and reliable failover.

==Data recovery==
Information repositories feature robust, client based data search and recovery capabilities that, based on permissions, enable end users to search the information repository, view information repository contents, including data on off-line media, and recover individual files or multiple files to either their original [[Computer network|network]] computer or another network computer.

==References==
*NGDC Conference: Understand advanced IT infrastructures, Protecting Information: Benefits of a Federated Information Repository as a Secondary Storage Tier. http://www.networkworld.com/ngdc/ 
*SNIA Enterprise Information World 2007 Conference: Benefits of a Federated Information Repository as a Secondary Storage Tier. http://www.enterpriseinformationworld.com/abstracts/benefits_federated_info.htm

{{DEFAULTSORT:Information Repository}}
[[Category:Information technology management|*]]
[[Category:Content management systems|*]]
[[Category:Data management]]
[[Category:Data security]]
[[Category:Records management]]</text>
      <sha1>gc66rq9l3howzlhfvhm8n9jqsnxwc8s</sha1>
    </revision>
  </page>
  <page>
    <title>Machine-Readable Documents</title>
    <ns>0</ns>
    <id>51558108</id>
    <revision>
      <id>749697700</id>
      <parentid>748852842</parentid>
      <timestamp>2016-11-15T18:51:02Z</timestamp>
      <contributor>
        <username>Owen Ambur</username>
        <id>3035628</id>
      </contributor>
      <comment>Added reference to usability versus human-readability.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="10853" xml:space="preserve">'''Machine-readable documents''' are [[document]]s whose content can be readily processed by [[computer]]s.  Such documents are distinguished from [[machine-readable data]] by virtue of having sufficient structure to provide the necessary context to support the business processes for which they are created.  [[Data]] without [[context (language use)]] is meaningless and lacks the four essential [http://www.archives.gov/records-mgmt/policy/managing-web-records.html#1.0 characteristics] of trustworthy [[business record]]s specified in [[ISO 15489 Information and documentation -- Records management]]: 
* Reliability
* Authenticity
* Integrity
* [[Usability]]
The vast bulk of information is [[unstructured data]] and, from a business perspective, that means it is "immature", i.e., Level 1 (chaotic) of the [[Capability Maturity Model]].  Such immaturity fosters inefficiency, diminishes quality, and limits effectiveness.  Unstructured information is also ill-suited for [[records management]] functions, provides inadequate [[evidence]] for legal purposes, drives up the cost of [[discovery (law)]] in [[litigation]], and makes access and usage needlessly cumbersome in routine, ongoing [[business process]]es.

There are at least four aspects to machine-readability:  
* First, words or phrases should be discretely delineated (tagged) so that computer software and/or hardware logic can be applied to them as individual conceptual elements.  
* Second, the semantics of each element should be specified so that computers can help human beings achieve a common understanding of their meanings and potential usages.  
* Third, if the relationships among the individual elements are also specified, computers can automatically apply inferences to them, thereby further relieving human beings of the burden of trying to understand them, particularly for purposes of inquiry, discovery, and analysis. 
* Fourth, if the structures of the documents in which the elements occur are also specified, human understanding is further enhanced and the data becomes more reliable for legal and business-quality purposes.

As early as 1981, the U.S. [[Government Accountability Office]] (GAO) began reporting on the problem of inadequate record-keeping practices in the U.S. federal government.&lt;ref&gt;{{cite web
 | url=http://www.gao.gov/products/PLRD-81-2
 | title=FEDERAL RECORDS MANAGEMENT: A History of Neglect
 | work=gao.gov
 | date=1981-02-24
 | accessdate=2016-09-08}}
&lt;/ref&gt;  Such deficiencies are not unique to government and advances in information technology mean that most information is now "born digital" and thus potentially far more easily managed by automated means.&lt;ref&gt;{{cite web
 | url=http://www.oclc.org/content/dam/research/activities/hiddencollections/borndigital.pdf
 | title=Defining "Born Digital": An Essay by Ricky Erway, OCLC Research
 | work=oclc.org
 | date=2010-11-30
 | accessdate=2016-09-08}}
&lt;/ref&gt;  However, in testimony to Congress in 2010, GAO highlighted problems with managing electronic records, and as recently as 2015, GAO has continued to report inadequacies in the performance of Executive Branch agencies in meeting records management requirements.&lt;ref&gt;{{cite web
 | url=http://www.gao.gov/new.items/d10838t.pdf
 | title=INFORMATION MANAGEMENT: The Challenges of Managing Electronic Records, Statement of Valerie C. Melvin, Director, Information Management and Human Capital Issues 
 | work=gao.gov
 | date=2010-06-17
 | accessdate=2016-09-08}}
&lt;/ref&gt;
&lt;ref&gt;{{cite web
 | url=http://www.gao.gov/products/GAO-15-339
 | title=INFORMATION MANAGEMENT: Additional Actions Are Needed to Meet Requirements of the Managing Government Records Directive
 | work=gao.gov
 | date=2015-05-14
 | accessdate=2016-09-08}}
&lt;/ref&gt;  Moreover, more than two decades after a major and formerly highly respected auditing firm, [[Arthur Andersen]], met its demise due to a records destruction scandal, record-keeping practices became a central issue in the 2016 Presidential election.

On January 4, 2011, President Obama signed H.R. 2142, the [[Government Performance and Results Act]] (GPRA) Modernization Act of 2010 (GPRAMA), into law as P.L. 111-352. Section 10 of GPRAMA requires U.S. federal agencies to publish their strategic and performance plans and reports in searchable, machine-readable format.&lt;ref&gt;{{cite web
 | url=http://xml.fido.gov/stratml/references/PL111-532StratML.htm#SEC10
 | title=GPRAMA SEC. 10. FORMAT OF PERFORMANCE PLANS AND REPORTS.
 | work=congress.gov
 | date=2011-01-04
 | accessdate=2016-09-08}}
&lt;/ref&gt;
Additionally, in 2013, he issued [[Executive Order]] 13642, Making Open and Machine Readable the New Default for Government Information in general.&lt;ref&gt;{{cite web
 | url=http://xml.fido.gov/stratml/carmel/EOOMRDwStyle.xml
 | title=Executive Order 13642 in open, standard, machine-readable Strategy Markup Language format
 | work=whitehouse.gov
 | date=2013-05-09
 | accessdate=2016-09-08}}
&lt;/ref&gt;
On July 28, 2016, the [[Office of Management and Budget]] (OMB) followed up by including in the revised issuance of Circular A-130 direction for agencies to use [http://xml.fido.gov/stratml/carmel/iso/A130wStyle.xml#_3f7d15f0-5799-11e6-8d37-8523b3fa12e0 open, machine-readable formats] and to publish "[http://xml.fido.gov/stratml/carmel/iso/A130wStyle.xml#_3f7d449e-5799-11e6-8d37-8523b3fa12e0 public information online in a manner that promotes analysis and reuse for the widest possible range of purposes]", meaning that the information is both publicly accessible and machine-readable.

In support of such policy direction, technological advancement is enabling more efficient and effective management and use of machine-readable electronic records.  [[Document-oriented database]]s have been developed for storing, retrieving, and managing document-oriented information, also known as semi-structured data.  Extensible Markup Language ([[XML]]) is a World Wide Web Consortium ([[W3C]]) [[World Wide Web Consortium#W3C recommendation .28REC.29|Recommendation]] setting forth rules for encoding documents in a format that is both [[human-readable]] and machine-readable.  Many [[XML editor]] tools have been developed and most, if not all major information technology applications support XML to greater or lesser degrees.  The fact that XML itself is an open, standard, machine-readable format makes it relatively easy for application developers to do so.

The W3C's accompanying XML Schema ([[XSD]]) Recommendation specifies how to formally describe the elements in an XML document.  With respect to the specification of XML schemas, the [[Organization for the Advancement of Structured Information Standards]] (OASIS) is a leading [[standards-developing organization]]. [[JSON#JSON Schema|JSON Schema]] was proposed by the [[Internet Engineering Task Force]] (IETF) but was allowed to expire in 2013 and thus is less mature and a riskier alternative to XSD, the most recent version of which was approved by the W3C in 2012.

The W3C's Extensible Stylesheet Language ([[XSL]]) family of languages provides for the transformation and rendering of XML documents for human-readable presentation.  Machine-readable documents can be automatically rendered in human-readable format but documents formatted primarily for attractiveness of presentation cannot easily be processed by computers to support [[usability]] by human beings.  

The [[Portable Document Format]] (PDF) is a file format used to present documents in a manner independent of application software, hardware, and operating systems. Each PDF file encapsulates a complete description of the presentation of the document, including the text, fonts, graphics, and other information needed to display it.  [[PDF/A]] is an ISO-standardized version of the PDF specialized for use in the archiving and long-term preservation of electronic documents.  PDF/A-3 allows embedding of other file formats, including XML, into PDF/A conforming documents, thus potentially providing the best of both human- and machine-readability.  The W3C's [[XSL-FO]] (XSL Formatting Objects) markup language is commonly used to generate PDF files

[[Metadata]], data about data, can be used to organize electronic resources, provide digital identification, and support the archiving and preservation of resources.  In well-structured, machine-readable electronic records, the content can be [[Repurposing|repurposed]] as both data and metadata.  In the context of electronic record-keeping systems, the terms "management" and "metadata" are virtually synonymous.  Given proper metadata, records management functions can be automated, thereby reducing the risk of [[spoliation of evidence]] and other fraudulent manipulations of records.  Moreover, such records can be used to automate the process of [[audit]]ing data maintained in [[database]]s, thereby reducing the risk of single points of failure associated with the [[Machiavellianism#In the workplace|Machiavellian]] concept of a [[single source of truth]].

[[Blockchain (database)]] is a new technology for maintaining continuously-growing lists of records secured from tampering and revision.  A key feature is that every node in a decentralized system has a copy of the blockchain so there is no [[single point of failure]] subject to manipulation and [[fraud]].

==See also==

* [[Budapest Declaration on Machine Readable Travel Documents]]
* [[Comparison of XML editors]]
* [[Integrity]] and particularly [[Data integrity]]
* [[Linked data]]
* [[Machine-readable passport]]
* [[Open data]]
* [[Data reliability|Reliability]], particularly [[Reliability (statistics)]], [[Data reliability]], [[Reliability (computer networking)]], and [[Reliability (research methods)]]
* [[Strategy Markup Language]] (StratML)
* [[Structured document]]
* [[Tag (metadata)]]
* [[Universal Business Language]] (UBL)
* [[XBRL]] (eXtensible Business Reporting Language)

==References==
{{reflist}}

==External  links==
* [http://xml.fido.gov/stratml/carmel/M-13-13wStyle.xml#_78e85ef4-b91c-11e2-bf2b-79d279ad226c OMB M-13-13], Open Data Policy: Managing Information as an Asset, which requires agencies to use open, machine-readable, data format standards
* [http://ambur.net/CaponeConsultancyMethod.pdf Driving a Stake in the Heart of the Capone Consultancy Method of Records Management: Best Practices for Correcting Non-Records Non-Policy Nonsense], March 9, 2015
* The U.S. Code, which includes [http://uscode.house.gov/search.xhtml?searchString=machine-readable&amp;pageNumber=1&amp;itemsPerPage=100&amp;sortField=CODE_ORDER&amp;action=search&amp;q=bWFjaGluZS1yZWFkYWJsZQ%3D%3D%7C%3A%3A%3A%3A%3A%3A%3A%3Afalse%3A%7C%3A%3A%3A%3A%3A%3A%3A%3Afalse%3A%7Cfalse%7C%5B%3A%3A%3A%3A%3A%3A%3A%3Afalse%3A%5D%7C%5B%3A%5D 51 references] to the term "machine-readable" as of September 10, 2016

[[Category:Data management]]
[[Category:Records management]]</text>
      <sha1>p22a7jz3oplavgfrg5sfu88i5kwl8nq</sha1>
    </revision>
  </page>
  <page>
    <title>Data exhaust</title>
    <ns>0</ns>
    <id>51905821</id>
    <revision>
      <id>746686060</id>
      <parentid>744239518</parentid>
      <timestamp>2016-10-28T22:30:58Z</timestamp>
      <contributor>
        <username>Pegship</username>
        <id>355698</id>
      </contributor>
      <minor />
      <comment>stub sort</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1470" xml:space="preserve">{{Multiple issues|
{{Orphan|date=October 2016}}
{{refimprove|date=October 2016}}
}}

'''Data exhaust''' refers to the trail of [[data]] left by the activities of an [[Internet]] user during his/her online activity. An enormous amount of often raw data are created. These data (which can take the form of [[Cookie (computing)|cookies]], temporary files, [[log file]]s etc.) can help to improve the online experience (for example through customized content). But they can also compromise privacy, as they offer a valuable insight into the user&#8217;s habits. It can be used to improve tracking trends and studying data exhaust also improves the user interface and the layout design. &lt;ref name=techtarget&gt;{{cite web|url=http://whatis.techtarget.com/definition/data-exhaust|title=What is data exhaust? - Definition from WhatIs.com|publisher=}}&lt;/ref&gt;

Unlike primary content, these data are not purposefully created by the user, who is often unaware of their very existence. A bank for example would consider as [[primary data]] information concerning the sums and parties of a transaction, whilst [[secondary data]] might include the percentage of transactions carried out at a [[cash machine]] instead of a real bank.&lt;ref&gt;{{cite web|url=http://www.pcworld.com/article/3069507/5-things-you-need-to-know-about-data-exhaust.html|title=5 things you need to know about data exhaust|publisher=}}&lt;/ref&gt;

==References==
&lt;references /&gt;


[[Category:Data management]]

{{internet-stub}}</text>
      <sha1>j2kbfib0qmgr5gos9m9cplevmpe7bfg</sha1>
    </revision>
  </page>
  <page>
    <title>Object storage</title>
    <ns>0</ns>
    <id>40572678</id>
    <revision>
      <id>762727634</id>
      <parentid>760637203</parentid>
      <timestamp>2017-01-30T11:43:33Z</timestamp>
      <contributor>
        <ip>89.145.186.214</ip>
      </contributor>
      <comment>/* External links */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="33593" xml:space="preserve">'''Object storage''' (also known as '''object-based storage'''&lt;ref&gt;{{cite journal|last=Mesnier|first=Mike|author2=Gregory R. Ganger |author3=Erik Riedel |title=Object-Based Storage|journal=IEEE Communications Magazine|date=August 2003|pages=84&#8211;90|url=http://www.storagevisions.com/White%20Papers/MesnierIEEE03.pdf|accessdate=27 October 2013|doi=10.1109/mcom.2003.1222722 }}&lt;/ref&gt;) is a storage architecture that manages data as objects, as opposed to other storage architectures like [[file systems]] which manage data as a file hierarchy and [[block storage]] which manages data as blocks within sectors and tracks.&lt;ref&gt;{{cite web|last=Porter De Leon|first=Yadin|author2=Tony Piscopo|title=Object Storage versus Block Storage: Understanding the Technology Differences|url=http://www.druva.com/blog/object-storage-versus-block-storage-understanding-technology-differences/|publisher=Druva.com|accessdate=19 January 2015}}&lt;/ref&gt; Each object typically includes the data itself, a variable amount of [[metadata]], and a globally unique identifier. Object storage can be implemented at multiple levels, including the device level (object storage device), the system level, and the interface level. In each case, object storage seeks to enable capabilities not addressed by other storage architectures, like interfaces that can be directly programmable by the application, a namespace that can span multiple instances of physical hardware, and data management functions like data replication and data distribution at object-level granularity.

Object storage systems allow relatively inexpensive, scalable and self-healing retention of massive amounts of [[unstructured data]]. Object storage is used for diverse purposes such as storing photos on [[Facebook]], songs on [[Spotify]], or files in online collaboration services, such as [[Dropbox (service)|Dropbox]].&lt;ref&gt;{{cite web|authors=Chandrasekaran, Arun, Dayley, Alan|title=Critical Capabilities for Object Storage|publisher=Gartner Research|date=11 February 2014|url=http://www.gartner.com/technology/reprints.do?id=1-1R78PJ9&amp;ct=140226&amp;st=sb}}&lt;/ref&gt;

==History==

===Origins===
In 1995, new research by Garth Gibson, ''et al.'' on [[Network Attached Secure Disks]] first promoted the concept of splitting less common operations, like namespace manipulations, from common operations, like reads and writes, to optimize the performance and scale of both.&lt;ref name="NASD"&gt;{{cite web|title=File Server Scaling with Network-Attached Secure Disks|url=http://www.pdl.cmu.edu/ftp/NASD/Sigmetrics97.pdf|publisher=Proceedings of the ACM International Conference on Measurement and Modeling of Computer Systems (Sigmetrics &#8216;97)|accessdate=27 October 2013|author=Garth A. Gibson |author2=Nagle D. |author3=Amiri K. |author4=Chan F. |author5=Feinberg E. |author6=Gobioff H. |author7=Lee C. |author8=Ozceri B. |author9=Riedel E. |author10=Rochberg D. |author11=Zelenka J.}}&lt;/ref&gt;  In the same year, 1995, a Belgium company - FilePool - was established to build the basis for archiving functions by using those and own concepts. Object storage was proposed  at [[Carnegie Mellon University|Carnegie Mellon University's]] Parallel Data Lab as a research project in 1996 .&lt;ref&gt;{{cite web|last1=Factor|first1=Michael|last2=Meth|first2=K.|last3=Naor|first3=D.|last4=Rodeh|first4=O.|last5=Satran |first5=J.|title=Object Storage: The Future Building Block for Storage Systems|url=http://webhdd.ru/library/files/PositionOSD.pdf|publisher=IBM Haifa Research Labs|accessdate=26 September 2013}}&lt;/ref&gt;   Another key concept was abstracting the writes and reads of data to more flexible data containers (objects). Fine grained access control through object storage architecture&lt;ref&gt;{{cite web|title=Security for Network Attached Storage Devices (CMU-CS-97-185)|url=http://repository.cmu.edu/cgi/viewcontent.cgi?article=1147&amp;context=pdl|publisher=Parallel Data Laboratory|accessdate=7 November 2013|author=Gobioff, Howard|author2=Gibson, Garth A. |author3= Tygar, Doug |date=1 October 1997}}&lt;/ref&gt;  was further described by one of the NASD team, Howard Gobioff, who later was one of the inventors of the [[Google File System]].&lt;ref&gt;{{cite web|title=The Google File System|url=http://research.google.com/archive/gfs-sosp2003.pdf|publisher=Google|accessdate=7 November 2013|author=Sanjay Ghemawat |author2=Howard Gobioff |author3=Shun-Tak Leung|date=October 2003}}&lt;/ref&gt;  Other related work includes the [[Coda (file system)|Coda]] filesystem project at [[Carnegie Mellon]], which started in 1987, and spawned the [[Lustre (file system)|Lustre file system]].&lt;ref name="Lustre"&gt;{{cite web|last=Braam|first=Peter|title=Lustre: The intergalactic &#64257;le system|url=http://ols.fedoraproject.org/OLS/Reprints-2002/braam-reprint.pdf|accessdate=17 September 2013}}&lt;/ref&gt; There is also the OceanStore project at UC Berkeley,&lt;ref&gt;{{cite web|title=OceanStore|url=http://oceanstore.cs.berkeley.edu/|accessdate=18 September 2013}}&lt;/ref&gt; which started in 1999.&lt;ref&gt;{{cite journal|last1=Kubiatowicz|first1=John|last2=Bindel|first2=D.|last3=Chen|first3=Y.|last4=Czerwinski|first4=S.|last5=Eaton|first5=P.|last6=Geels|first6=D.|last7=Gummadi|first7=R.|last8=Rhea|first8=S.|last9=Weatherspoon|first9=H.|last10=Weimer |first10=W.|last11=Wells|first11=C.|last12=Zhao|first12=B.|title=OceanStore: An Architecture for Global-Scale Persistent Storage|journal=Proceedings of the Ninth international Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS 2000)|date=November 2000|url=http://oceanstore.cs.berkeley.edu/publications/papers/pdf/asplos00.pdf|accessdate=18 September 2013}}&lt;/ref&gt;

One of the earliest and best-known object storage products, EMC's Centera, debuted in 2002.&lt;ref&gt;{{cite news|title=EMC Unveils Low-Cost Data-Storage Product|url=http://articles.latimes.com/2002/apr/30/business/fi-techbriefs30.3|accessdate=17 September 2013|newspaper=LA Times|date=April 30, 2002}}&lt;/ref&gt; [[Content-addressable storage|Centera's technology]]  has been developed at Filepool and the company had been acquired  by EMC&#178; in 2002.

===Development===
Overall industry investment in object storage technology has been sustained for over a decade. From 1999 to 2013, there has been at least $300 million of venture financing related to object storage, including vendors like Amplidata, Bycast, Cleversafe, Cloudian, Nirvanix, and Scality.&lt;ref&gt;{{cite web|last=Leung|first=Leo|title=After 10 years, object storage investment continues and begins to bear significant fruit|url=http://blog.oxygencloud.com/2013/09/16/after-10-years-object-storage-investment-continues-and-begins-to-bear-significant-fruit/|accessdate=17 September 2013|date=16 September 2013}}&lt;/ref&gt; This doesn't include millions of dollars of private engineering from systems vendors like DataDirect Networks (WOS), [http://www.emc.com/en-us/storage/ecs/index.htm#collapse=&amp;tab14=0 Dell EMC Elastic Cloud Storage], Centera, [[Atmos]], HDS (HCP), HP ([[HP OpenStack]]), IBM, NetApp (StorageGRID), Redhat GlusterFS and [http://www.keepertech.com Keeper Technology] ([http://www.keepertech.com/products/keepersafe/ keeperSAFE]), cloud services vendors like Amazon ([[AWS S3]]), Microsoft ([[Microsoft Azure]]) and Google ([[Google Cloud Storage]]), or the many man years of open source development at [[Lustre (file system)|Lustre]], OpenStack ([[OpenStack#Object Storage .28Swift.29|Swift]]), MogileFS, [[Ceph (file system)|Ceph]], [[Skylable SX (object storage)|Skylable SX]] and OpenIO.&lt;ref name="Mellor"&gt;{{cite web|last=Mellor|first=Chris (Dec. 2, 2015)|title=Openio's objective is opening up object storage space|url=http://www.theregister.co.uk/2015/12/02/openio_object_storage_upstart/}}&lt;/ref&gt;&lt;ref name="Nicolas"&gt;{{cite web|last=Nicolas|first=Philippe (Oct. 2, 2015)|title=OpenIO, ready to take off|url=http://filestorage.blogspot.fr/2015/10/openio-ready-to-take-off.html/}}&lt;/ref&gt;&lt;ref name="Raffo"&gt;{{cite web|last=Raffo|first=Dave (May 20, 2016)|title=OpenIO joins object storage cloud scrum|url=http://searchcloudstorage.techtarget.com/news/450296765/OpenIO-joins-object-storage-cloud-scrum/}}&lt;/ref&gt;&lt;ref name="Maleval"&gt;{{cite web|last=Maleval|first=Jean-Jacques (Apr. 25, 2016)|title=Start-Up Profile: OpenIO|url=http://www.storagenewsletter.com/rubriques/start-ups/start-up-profile-openio/}}&lt;/ref&gt;

A great article written by Philippe Nicolas illustrating products' timeline was published in July 2016 on The Register with all players, pioneers, mergers and acquisitions and of course genesis with CAS included.&lt;ref&gt;{{cite web|last=Nicolas|first=Philippe (July 15, 2016)|title=The History Boys: Object storage ... from the beginning|url=http://www.theregister.co.uk/2016/07/15/the_history_boys_cas_and_object_storage_map/}}&lt;/ref&gt;

==Architecture==
[[File:High level object storage architecture.png|thumb]]

===Abstraction of storage===
One of the design principles of object storage is to abstract some of the lower layers of storage away from the administrators and applications. Thus, data is exposed and managed as objects instead of files or blocks. Objects contain additional descriptive properties which can be used for better indexing or management. Administrators do not have to perform lower level storage functions like constructing and managing [[Logical unit number|logical volumes]] to utilize disk capacity or setting [[RAID]] levels to deal with disk failure.

Object storage also allows the addressing and identification of individual objects by more than just file name and file path. Object storage adds a unique identifier within a bucket, or across the entire system, to support much larger namespaces and eliminate name collisions.

=== Inclusion of rich custom metadata within the object ===
Object storage explicitly separates file metadata from data to support additional capabilities:
As opposed to fixed metadata in file systems (filename, creation date, type, etc.), object storage provides for full function, custom, object-level metadata in order to:
* Capture application-specific or user-specific information for better indexing purposes
* Support data management policies (e.g. a policy to drive object movement from one storage tier to another)
* Centralize management of storage across many individual nodes and clusters
* Optimize metadata storage (e.g. encapsulated, database or key value storage) and caching/indexing (when authoritative metadata is encapsulated with the metadata inside the object) independently from the data storage (e.g. unstructured binary storage)

Additionally, in some object-based file system implementations:
* The file system clients only contact metadata servers once when the file is opened and then get content directly via object storage servers (vs. block-based file systems which would require constant metadata access)
* Data objects can be configured on a per-file basis to allow adaptive stripe width, even across multiple object storage servers, supporting optimizations in bandwidth and I/O

'''Object-based storage devices''' ('''OSD''') as well as some software implementations (e.g., Caringo Swarm) manage metadata and data at the storage device level:
* Instead of providing a block-oriented interface that reads and writes fixed sized blocks of data, data is organized into flexible-sized data containers, called objects
* Each object has both data (an uninterpreted sequence of bytes) and metadata (an extensible set of attributes describing the object); physically encapsulating both together benefits recoverability.
* The command interface includes commands to create and delete objects, write bytes and read bytes to and from individual objects, and to set and get attributes on objects
* Security mechanisms provide per-object and per-command access control

===Programmatic data management===
Object storage provides programmatic interfaces to allow applications to manipulate data. At the base level, this includes [[CRUD]] functions for basic read, write and delete operations. Some object storage implementations go further, supporting additional functionality like object versioning, object replication, and movement of objects between different tiers and types of storage. Most API implementations are [[Representational state transfer|ReST]]-based, allowing the use of many standard [[HTTP]] calls.

==Implementation==

===Object-based storage devices===
Object storage at the protocol and device layer was proposed 20 years ago and approved for the [[SCSI]] command set nearly 10 years ago as "Object-based Storage Device Commands" (OSD),&lt;ref&gt;{{cite web|last=Riedel|first=Erik|title=Object Storage and Applications|url=https://www.usenix.org/legacy/event/lsf07/tech/riedel.pdf|accessdate=3 November 2013|author2=Sami Iren |date=February 2007}}&lt;/ref&gt; but has not been productized until the development of the Seagate Kinetic Open Storage platform.&lt;ref&gt;{{cite web|title=The Seagate Kinetic Open Storage Vision|url=http://www.seagate.com/tech-insights/kinetic-vision-how-seagate-new-developer-tools-meets-the-needs-of-cloud-storage-platforms-master-ti/|publisher=Seagate|accessdate=3 November 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite news|last=Gallagher|first=Sean|title=Seagate introduces a new drive interface: Ethernet|url=http://arstechnica.com/information-technology/2013/10/seagate-introduces-a-new-drive-interface-ethernet/|accessdate=3 November 2013|newspaper=Arstechnica.com|date=27 October 2013}}&lt;/ref&gt;  The [[SCSI]] command set for Object Storage Devices was developed by a working group of the [[Storage Networking Industry Association]] (SNIA) for the T10 committee of the [[International Committee for Information Technology Standards]] (INCITS).&lt;ref&gt;{{cite web|last=Corbet|first=Jonathan|title=Linux and object storage devices|url=https://lwn.net/Articles/305740/|accessdate=8 November 2013|newspaper=LWN.net|date=4 November 2008}}&lt;/ref&gt;  T10 is responsible for all SCSI standards.

===Object-based file systems===
Some distributed file systems use an object-based architecture, where file metadata is stored in metadata servers and file data is stored in object storage servers. File system client software interacts with the distinct servers, and abstracts them to present a full file system to users and applications. [[IBM General Parallel File System|IBM Spectrum Scale (also known as GPFS)]], [http://www.emc.com/en-us/storage/ecs/index.htm#collapse=&amp;tab14=0 Dell EMC Elastic Cloud Storage], [[Ceph (software)|Ceph]], [[XtreemFS]], and [[Lustre (file system)|Lustre]] are examples of this type of object storage.

===Archive storage===
Some early incarnations of object storage were used for archiving, as implementations were optimized for data services like immutability, not performance. [[Content-addressable storage|EMC Centera]] and Hitachi HCP (formerly known as HCAP) are two commonly cited object storage products for archiving. Another example is Quantum Lattus Object Storage Platform.

===Cloud storage===
The vast majority of cloud storage available in the market leverages an object storage architecture. Two notable examples are [[AWS S3|Amazon Web Services S3]], which debuted in 2005, and [[Rackspace]] Files (whose code was released as [[OpenStack#Swift|OpenStack Swift]]). Other major cloud storage services include Microsoft Azure, Google Cloud Storage, Alibaba Cloud OSS, Oracle Elastic Storage Service and DreamHost based on Ceph.

==="Captive" object storage===
Some large internet companies developed their own software when object storage products were not commercially available or use cases were very specific. Facebook famously invented their own object storage software, code-named Haystack, to address their particular massive scale photo management needs efficiently.&lt;ref name="haystack"&gt;{{cite web|last=Vajgel|first=Peter|title=Needle in a haystack: efficient storage of billions of photos|url=https://www.facebook.com/note.php?note_id=76191543919|accessdate=17 September 2013}}&lt;/ref&gt;

===Hybrid storage===
A few object storage systems, such as [[Ceph (software)|Ceph]], [[GlusterFS]], [[Cloudian]],&lt;ref name="Primesberger"&gt;{{cite web|last=Primesberger|first=Chris (27 October 2016)|title=Cloudian Raises $41 Million VC for Hybrid Cloud Object Storage|url=http://www.eweek.com/storage/cloudian-raises-41-million-vc-for-hybrid-cloud-object-storage.html}}&lt;/ref&gt; and [[Scality]] support Unified File and Object (UFO) storage, allowing some clients to store objects on a storage system while simultaneously other clients store files on the same storage system. While "hybrid storage" is not a widely accepted term for this concept, interoperable interfaces to the same set of data is becoming available in some object storage products.

===Virtual object storage===
In addition to object storage systems that own the managed files, some systems provide an object abstraction on top of one or more traditional filesystem based solutions. These solutions do not own the underlaying raw storage, but instead actively mirror the filesystem changes and replicate them in their own object catalog, alongside any metadata that can be automatically extracted from the files. Users can then contribute additional metadata through the virtual object storage APIs. A global namespace and replication capabilities both inside and across filesystems are typically supported.

Notable examples in this category are [[Nirvana (software)|Nirvana]], and its open-source cousin iRODS.

Most products in this category have recently extended their capabilities to support other Object Store solutions as well.

===Object storage systems===
More general purpose object storage systems came to market around 2008. Lured by the incredible growth of "captive" storage systems within web applications like Yahoo Mail and the early success of cloud storage, object storage systems promised the scale and capabilities of cloud storage, with the ability to deploy the system within an enterprise, or at an aspiring cloud storage service provider. Notable examples of object storage systems include [[EMC Atmos]], [[OpenStack#Object Storage (Swift)|OpenStack Swift]], [[Scality|Scality RING]], Caringo Swarm&lt;ref&gt;{{cite web|last=Nicolas|first=Philippe (Sept. 21, 2009)|title=Caringo FileFly, back to the future|url=http://continuousdataprotection.blogspot.fr/2015/09/caringo-filefly-back-to-future.html}}&lt;/ref&gt; (formerly CAStor), [[Cloudian]],&lt;ref name="Primesberger"/&gt; and OpenIO.&lt;ref name="Mellor"/&gt;

==Market adoption==
[[File:Titan supercomputer at the Oak Ridge National Laboratory.jpg|thumb|The Titan supercomputer at Oak Ridge National Laboratory]]
One of the first object storage products, Lustre, is used in 70% of the Top 100 supercomputers and ~50% of the [[Top 500]].&lt;ref&gt;{{cite web|last=Dilger|first=Andreas|title=Lustre Future Development|url=http://storageconference.org/2012/Presentations/M04.Dilger.pdf|publisher=IEEE MSST|accessdate=27 October 2013}}&lt;/ref&gt; As of June 16, 2013, this includes 7 of the top 10, including the current fastest system on the list - China's Tianhe-2 and the second fastest, the [[Titan (supercomputer)|Titan supercomputer]] at [[Oak Ridge National Laboratory]] (pictured on the right).&lt;ref&gt;{{cite web|title=Datadirect Networks to build world's fastest storage system for Titan, the world's most powerful supercomputer|url=http://www.multivu.com/mnr/60497-datadirect-networks-titan-supercomputer-storage-system-ornl|accessdate=27 October 2013}}&lt;/ref&gt;

Object storage systems had good adoption in the early 2000s as an archive platform, particularly in the wake of compliance laws like [[Sarbanes-Oxley]]. After five years in the market, EMC's Centera product claimed over 3,500 customers and 150 [[petabytes]] shipped by 2007.&lt;ref&gt;{{cite web|title=EMC Marks Five Years of EMC Centera Innovation and Market Leadership|url=http://www.emc.com/about/news/press/us/2007/04182007-5028.htm|publisher=EMC|accessdate=3 November 2013|date=18 April 2007}}&lt;/ref&gt; Hitachi's HCP product also claims many [[petabyte]]-scale customers.&lt;ref&gt;{{cite web|title=Hitachi Content Platform Supports Multiple Petabytes, Billions of Objects|url=http://www.techvalidate.com/portals/hitachi-content-platform-customers-with-more-than-1pb-of-data-stored|publisher=Techvalidate.com|accessdate=19 September 2013}}&lt;/ref&gt; Newer object storage systems have also gotten some traction, particularly around very large custom applications like eBay's auction site, where EMC Atmos is used to manage over 500 million objects a day.&lt;ref&gt;{{cite news|last=Robb|first=Drew|title=EMC World Continues Focus on Big Data, Cloud and Flash|url=http://www.infostor.com/backup-and_recovery/cloud-storage/emc-world-continues-focus-on-big-data-cloud-and-flash-.html|accessdate=19 September 2013|newspaper=Infostor|date=11 May 2011}}&lt;/ref&gt; As of March 3, 2014, EMC claims to have sold over 1.5 exabytes of Atmos storage.&lt;ref&gt;{{cite web|last=Hamilton|first=George|title=In it for the Long Run: EMC's Object Storage Leadership|url=http://www.rethinkstorage.com/in-it-for-the-long-run-emcs-object-storage-leadership#.UyEzj9yllFI|accessdate=15 March 2014}}&lt;/ref&gt; On July 1, 2014, [[Los Alamos National Lab]] chose the [[Scality|Scality RING]] as the basis for a 500 petabyte storage environment, which would be among the largest ever.&lt;ref&gt;{{cite news|last1=Mellor|first1=Chris|title=Los Alamos National Laboratory likes it, puts Scality's RING on it|url=http://www.theregister.co.uk/2014/07/01/scalitys_ring_goes_faster/|accessdate=26 January 2015|publisher=The Register|date=1 July 2014}}&lt;/ref&gt;

"Captive" object storage systems like Facebook's Haystack have scaled impressively. In April 2009, Haystack was managing 60 billion photos and 1.5 petabytes of storage, adding 220 million photos and 25 terabytes a week.&lt;ref name="haystack" /&gt;&lt;ref&gt;{{cite web|last=Nicolas|first=Philippe (Sept. 13, 2009)|title=Haystack chez Facebook|url=http://filestorage.blogspot.com/2009/09/haystack-chez-facebook.html}}&lt;/ref&gt; Facebook more recently stated that they were adding 350 million photos a day and were storing 240 billion photos.&lt;ref&gt;{{cite news|last=Miller|first=Rich|title=Facebook Builds Exabyte Data Centers for Cold Storage|url=http://www.datacenterknowledge.com/archives/2013/01/18/facebook-builds-new-data-centers-for-cold-storage/|accessdate=6 November 2013|newspaper=Datacenterknowledge.com|date=13 January 2013}}&lt;/ref&gt; This could equal as much as 357 petabytes.&lt;ref&gt;{{cite web|last=Leung|first=Leo|title=How much data does x store?|url=http://techexpectations.org/2014/05/17/how-much-data-does-x-store/|publisher=Techexpectations.org|accessdate=23 May 2014|date=17 May 2014}}&lt;/ref&gt;

Cloud storage has become pervasive as many new web and mobile applications choose it as a common way to store [[binary data]].&lt;ref&gt;{{cite web|last=Leung|first=Leo|title=Object storage already dominates our days (we just didn&#8217;t notice)|url=http://blog.oxygencloud.com/2012/01/11/object-storage-already-dominates/|accessdate=27 October 2013|date=January 11, 2012}}&lt;/ref&gt;  As the storage backend to many popular applications like [[Smugmug]] and [[Dropbox (service)|Dropbox]], AWS S3 has grown to massive scale, citing over 2 trillion objects stored in April 2013.&lt;ref&gt;{{cite news|last=Harris|first=Derrick|title=Amazon S3 goes exponential, now stores 2 trillion objects|url=http://gigaom.com/2013/04/18/amazon-s3-goes-exponential-now-stores-2-trillion-objects/|accessdate=17 September 2013|newspaper=Gigaom|date=18 April 2013}}&lt;/ref&gt; Two months later, Microsoft claimed that they stored even more objects in Azure at 8.5 trillion.&lt;ref&gt;{{cite news|last=Wilhelm|first=Alex|title=Microsoft: Azure powers 299M Skype users, 50M Office Web Apps users, stores 8.5T objects|url=http://thenextweb.com/microsoft/2013/06/27/microsoft-our-cloud-powers-hundreds-of-millions/|accessdate=18 September 2013|newspaper=thenextweb.com|date=27 June 2013}}&lt;/ref&gt; By April 2014, Azure claimed over 20 trillion objects stored.&lt;ref&gt;{{cite news|last1=Nelson|first1=Fritz|title=Microsoft Azure's 44 New Enhancements, 20 Trillion Objects|url=http://www.tomsitpro.com/articles/microsoft-azure-paas-iaas-cloud-computing,1-1841.html|accessdate=3 September 2014|publisher=Tom's IT Pro|date=4 April 2014}}&lt;/ref&gt; Windows Azure Storage manages Blobs (user files), Tables (structured storage), and Queues (message delivery) and counts them all as objects.&lt;ref&gt;{{cite web|last=Calder|first=Brad|title=Windows Azure Storage: A Highly Available Cloud Storage Service with Strong Consistency|url=http://sigops.org/sosp/sosp11/current/2011-Cascais/printable/11-calder.pdf|publisher=Microsoft|accessdate=6 November 2013|location=23rd ACM Symposium on Operating Systems Principles (SOSP)}}&lt;/ref&gt;

==Market analysis==
[[International Data Corporation|IDC]] has begun to assess the object-based storage market annually using its MarketScape methodology. IDC describes the MarketScape as: "...a quantitative and qualitative assessment of the characteristics that assess a vendor's current and future success in the said market or market segment and provide a measure of their ascendancy to become a Leader or maintain a leadership. IDC MarketScape assessments are particularly helpful in emerging markets that are often fragmented, have several players, and lack clear leaders."&lt;ref&gt;{{cite web|last1=Nadkarni|first1=Ashish|title=IDC MarketScape: Worldwide Object-Based Storage 2013 Vendor Assessment|url=http://www.idc.com/getdoc.jsp?containerId=244081|website=http://www.idc.com|publisher=IDC|accessdate=26 January 2015}}&lt;/ref&gt;

In 2013, IDC rated [[Cleversafe]], [[Scality]], [[DataDirect Networks]], [[Amplidata]], and [[EMC Corporation|EMC]] as leaders.&lt;ref&gt;{{cite news|last1=Mellor|first1=Chris|title=IDC's explicit snapshot: Everyone who's anyone in object storage: In 3D|url=http://www.theregister.co.uk/2013/11/27/idcs_objectscape_pretty_as_a_picture/|accessdate=26 January 2015|publisher=The Register|date=27 November 2013}}&lt;/ref&gt; In 2014, it rated [[Scality]], [[Cleversafe]], [[DataDirect Networks]], [[Hitachi Data Systems]], [[Amplidata]], [[EMC Corporation|EMC]], and [[Cloudian]]&lt;ref&gt;{{cite web|last=Nicolas|first=Philippe (Sept. 14, 2015)|title=Cloudian shakes the object storage market|url=http://filestorage.blogspot.fr/2015/09/cloudian-shakes-object-storage-market.html}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Mellor|first=Chris (June. 21, 2016)|title=Cloudian clobbers car drivers with targeted ads|url=http://www.theregister.co.uk/2016/06/21/cloudian_could_clobber_car_drives_with_targeted_ads/}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Nicolas|first=Philippe (June. 22, 2016)|title=Cloudian is the real S3 leader|url=http://filestorage.blogspot.fr/2016/06/cloudian-is-real-s3-leader.html}}&lt;/ref&gt; as leaders.&lt;ref&gt;{{cite news|last1=Mellor|first1=Chris|title=IDC: Who's HOT and who's NOT (in object storage) in 2014|url=http://www.theregister.co.uk/2015/01/06/idc_shows_emcs_object_presence_shrinking/|accessdate=26 January 2015|publisher=The Register|date=6 January 2015}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Mellor|first=Chris (Nov. 24, 2015)|title=We pick storage brains: Has object storage endgame started?|url=http://www.channelregister.co.uk/2015/11/24/object_storage_endgame/}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Nicolas|first=Philippe (Oct. 19, 2015)|title=Red alert for Object Storage vendors|url=http://filestorage.blogspot.com/2015/10/red-alert-for-object-storage-vendors.html}}&lt;/ref&gt;

==Standards==

===Object-based storage device standards===

====OSD version 1====
In the first version of the OSD standard,&lt;ref&gt;{{cite web|title=INCITS 400-2004|url=http://www.techstreet.com/cgi-bin/detail?product_id=1204555|publisher=InterNational Committee for Information Technology Standards|accessdate=8 November 2013}}&lt;/ref&gt; objects are specified with a 64-bit partition ID and a 64-bit object ID. Partitions are created and deleted within an OSD, and objects are created and deleted within partitions. There are no fixed sizes associated with partitions or objects; they are allowed to grow subject to physical size limitations of the device or logical quota constraints on a partition.

An extensible set of attributes describe objects. Some attributes are implemented directly by the OSD, such as the number of bytes in an object and the modify time of an object. There is a special policy tag attribute that is part of the security mechanism. Other attributes are uninterpreted by the OSD. These are set on objects by the higher-level storage systems that use the OSD for persistent storage. For example, attributes might be used to classify objects, or to capture relationships among different objects stored on different OSDs.

A list command returns a list of identifiers for objects within a partition, optionally filtered by matches against their attribute values. A list command can also return selected attributes of the listed objects.

Read and write commands can be combined, or piggy-backed, with commands to get and set attributes. This ability reduces the number of times a high-level storage system has to cross the interface to the OSD, which can improve overall efficiency.

====OSD version 2====
A second generation of the SCSI command set, "Object-Based Storage Devices - 2" (OSD-2) added support for snapshots, collections of objects, and improved error handling.&lt;ref&gt;{{cite web|title=INCITS 458-2011|url=http://www.techstreet.com/products/1801667|publisher=InterNational Committee for Information Technology Standards|accessdate=8 November 2013|date=15 March 2011}}&lt;/ref&gt;

A [[snapshot (computer storage)|snapshot]] is a point in time copy of all the objects in a partition into a new partition. The OSD can implement a space-efficient copy using [[copy-on-write]] techniques so that the two partitions share objects that are unchanged between the snapshots, or the OSD might physically copy the data to the new partition. The standard defines clones, which are writeable, and snapshots, which are read-only.

A collection is a special kind of object that contains the identifiers of other objects. There are operations to add and delete from collections, and there are operations to get or set attributes for all the objects in a collection. Collections are also used for error reporting.  If an object becomes damaged by the occurrence of a media defect (i.e., a bad spot on the disk) or by a software error within the OSD implementation, its identifier is put into a special error collection. The higher-level storage system that uses the OSD can query this collection and take corrective action as necessary.

==Differences between Key-Value and Object Stores==
{{Disputed|date=December 2015}}
Let&#8217;s first clarify what a key/value store and an object store are. Using the traditional block storage interface, one has a series of fixed size blocks which are numbered starting at 0. Data must be that exact fixed size and can be stored in a particular block which is identified by its logical block number (LBN). Later, one can retrieve that block of data by specifying its unique LBN.

With a key/value store, data is identified by a key rather than a LBN. A key might be "cat" or "olive" or "42". It can be an arbitrary sequence of bytes of arbitrary length. Data (called a value in this parlance) does not need to be a fixed size and also can be an arbitrary sequence of bytes of arbitrary length. One stores data by presenting the key and data (value) to the data store and can later retrieve the data by presenting the key. You&#8217;ve seen this concept before in programming languages. Python calls them dictionaries, Perl calls them hashes, Java and C++ call them maps, etc. Several data stores also implement key/value stores such as Memcached, Redis and CouchDB.

Object stores are similar to key/value stores except that the key must be a positive integer like a LBN. However, unlike a LBN, the key can be any positive integer; it does not have to map to an existing logical block number. In practice, it is usually limited to 64 bits. More like a key/value store than the traditional block storage interface, data is not limited to a fixed size block but may be an arbitrary size. Object stores also allow one to associate a limited set of attributes with each piece of data. The key, value and set of attributes is referred to as an object. To add more confusion, sometimes key/value stores are loosely referred to as object stores but technically there is a difference.&lt;ref&gt;http://blog.gigaspaces.com/were-flash-keyvalue-and-object-stores-made-for-each-other-guest-post-by-johann-george-sandisk/&lt;/ref&gt;

==See also==
*[[Cloud storage]]
*[[Clustered file system]]
*[[Object access method]]

==References==
{{Reflist|2}}
&lt;!--- After listing your sources please cite them using inline citations and place them after the information they cite. Please see http://en.wikipedia.org/wiki/Wikipedia:REFB for instructions on how to add citations. ---&gt;
*
*
*
*

==External links==
*[http://docs.aws.amazon.com/AmazonS3/latest/API/Welcome.html AWS S3 API Documentation]
*[https://developers.google.com/storage/ Google Cloud Storage API Documentation]
*[http://docs.openstack.org/developer/swift/ Openstack Swift API Documentation]
*[https://developers.seagate.com/display/KV/Kinetic+Open+Storage+Documentation+Wiki Seagate Kinetic Open Storage Documentation]
*[http://msdn.microsoft.com/en-us/library/windowsazure/dd179355.aspx Windows Azure Storage API Documentation]
*[https://nkolayofis.com a Saas solution in Turkey]
*[https://quictransfer.com a Cloud storage]


[[Category:Data management]]
[[Category:Data management software]]
[[Category:Computer file systems]]
[[Category:Computer data storage]]
[[Category:Network file systems]]
[[Category:Cloud storage]]</text>
      <sha1>rxwbacfda537a0wadiv4b51b09a5tyq</sha1>
    </revision>
  </page>
  <page>
    <title>Wiping</title>
    <ns>0</ns>
    <id>89314</id>
    <revision>
      <id>763082313</id>
      <parentid>759995595</parentid>
      <timestamp>2017-02-01T06:12:15Z</timestamp>
      <contributor>
        <ip>180.191.150.1</ip>
      </contributor>
      <comment>/* Philippines */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="60260" xml:space="preserve">{{About|the broadcasting practice||Wipe (disambiguation)}}
{{Multiple issues|
{{refimprove|date=February 2007}}
{{original research|date=September 2009}}
{{globalize|date=September 2009}}
}}

'''Wiping''', also known as '''junking''', is a colloquial term for action taken by [[radio]] and [[television]] production and broadcasting companies, in which old [[audiotape]]s, [[videotape]]s, and [[telerecording]]s ([[kinescope]]s), are [[List of lost television broadcasts|erased, reused, or destroyed]]. Although the practice was once very common, especially in the 1960s and 1970s, wiping is now practiced much less frequently. Older video and audio formats took up much more storage space than modern digital video or audio files, making their retention more costly, thus increasing the incentive of discarding existing broadcast material to recover storage space for newer programmes.

The advent of domestic audiovisual playback technology (e.g., videocassette and [[DVD]]) has made wiping less beneficial, with broadcasters and production houses realizing both the economic and cultural value of keeping archived material for both rebroadcast and potential profits through release on [[home video]].

==Australia==
Australian broadcasters did not gain access to videotape-recording technology until the early 1960s, and as a result nearly all programmes prior to that were broadcast live-to-air. Very little programming survives from the earliest years of Australian TV (1956&#8211;1960), as [[kinescope]] recording to film was expensive and most of what was recorded in this way has since been lost or destroyed. Some early programmes have survived, however; for example, ATN-7, a Sydney station, prerecorded (via kinescopes) some of their 1950s output such as ''[[Autumn Affair]]'' (1958&#8211;1959), ''[[The Pressure Pak Show]]'' (1957&#8211;1958) and ''[[Leave It to the Girls (Australian TV series)|Leave it to the Girls]]'' (1957&#8211;1958); some of these kinescopes have survived and are now held by the [[National Film and Sound Archive]],&lt;ref&gt;{{cite web| url = http://colsearch.nfsa.gov.au/nfsa/search/summary/summary.w3p;adv=;group=;groupequals=;page=0;parentid=;query=autumn%20affair%20Media%3A%22TELEVISION%22;querytype=;resCount=200|title = NSFA, Autumn Affair}}&lt;/ref&gt;&lt;ref&gt;{{cite web| url = http://colsearch.nfsa.gov.au/nfsa/search/summary/summary.w3p;adv=;group=;groupequals=;page=0;parentid=;query=leave%20it%20to%20the%20girls%20Media%3A%22TELEVISION%22;querytype=;resCount=10|title = NSFA, Leave it to the Girls}}&lt;/ref&gt;&lt;ref&gt;{{cite web| url = http://colsearch.nfsa.gov.au/nfsa/search/summary/summary.w3p;adv=yes;group=;groupequals=;page=0;parentid=;query=pressure%20pak%20show%20Years%3A%3E%3D1957%20Years%3A%3C%3D1958%20Media%3A%22TELEVISION%22;querytype=;resCount=20|title = NSFA, Pressure Pak Show&#176;}}&lt;/ref&gt; with soap opera ''Autumn Affair'' surviving near-intact, likely one of the earliest Australian series for which this is the case.

===ABC===

The [[Australian Broadcasting Corporation]] (ABC) erased much of its early output. Much of the videotaped ABC programme material from the 1960s and early 1970s was erased as part of an economy policy instituted in the late 1970s in which old programme tapes were surrendered for bulk erasure and reuse. This policy particularly targeted older programmes recorded in black-and-white, leading to the loss of many recordings made before 1975, when Australian television converted to colour. The ABC continued erasing older television output into the early 1980s.

Programmes known to have been lost include most studio segments from the 1960s current affairs shows ''[[This Day Tonight]]'' and ''Monday Conference'', hundreds of episodes of the long-running rural serial ''[[Bellbird (TV series)|Bellbird]]'', all but a handful of episodes of the early-1970s drama series ''[[Certain Women (television series)|Certain Women]]'', an early-1970s miniseries of dramatizations based on [[Norman Lindsay]]'s novels, and nearly all of the first 18 months of the weekly pop-music show ''[[Countdown (Australian TV series)|Countdown]]''.

===Network Ten===

Many episodes of popular Australian commercial TV series are also lost. In the 1970s, [[Network Ten]] had an official policy to reuse tapes; hence, many tapes of ''[[Young Talent Time]]'' and ''[[Number 96 (TV series)|Number 96]]'' were wiped. To this day, Network Ten still only keeps some of its programming.{{Citation needed|date=November 2008}} Other notable losses from the Ten archive include hundreds of episodes of the Melbourne-based pop music shows commissioned and broadcast by ATV-0 Melbourne in the 1960s and early 1970s&#8212;''[[The Go!! Show]]'' (1964&#8211;1967), ''Kommotion'' (1964&#8211;1967), ''Uptight'' (1968&#8211;70), and the ''Happening 70s'' series (1970&#8211;1972).

===Nine Network===

The [[Nine Network]] discarded copies of some of their programs, including the popular [[GTV-9]] series ''[[In Melbourne Tonight]]'' hosted by [[Graham Kennedy]]. Though it ran five nights a week from 1957 to 1970, fewer than 100 episodes are known to survive, and many of the surviving episodes are edited prints made for rebroadcast across Australia. Early episodes of ''[[Hey Hey It's Saturday]]'' do not exist because the programme was broadcast live and did not begin videotape recordings until a number of years later.

==Brazil==
From 1968&#8211;1969, [[Rede Tupi|TV Tupi]] produced new episodes of the soap opera ''[[Beto Rockfeller]]'' by recording over previous episodes; as a result, few episodes survive. After the closure of TV Tupi in 1980 the 536 tapes at its S&#227;o Paulo studios were simply left to deteriorate until they were recovered in 1985 and subsequently restored by [[TV Cultura]] in 1989. Only two TV Tupi O&amp;Os are known to have any preserved videotapes; TV Itacolomi's archives are now owned by the unrelated [[TV Alterosa]], affiliated with [[Sistema Brasileiro de Televis&#227;o|SBT]], whereas the few remaining tapes belonging to TV Piratini are stored privately in a museum in Porto Alegre, albeit in a deteriorated state.

[[Rede Record]] also lost much footage from the 1960s due to wiping, fires, and deterioration; most of the [[M&#250;sica popular brasileira|MPB]] music festivals no longer exist, and the sitcom ''[[:pt:Fam&#237;lia Trapo|Fam&#237;lia Trapo]]'' has only one surviving episode, featuring [[Pel&#233;]]. Until 1997 Rede Record had no policy on archiving videotapes, since then at least 600 videotapes that were previously believed to be lost have been recovered.

[[Rede Globo]] lost the first 35 broadcasts of both ''[[Fant&#225;stico]]'' and ''[[Jornal Nacional]]'', in addition to many segments of their other soap operas, as a result of wiping, and also due to three fires that occurred in 1969, 1971 and 1976, where an estimated 920 to 1500 tapes were destroyed.

Most of [[Rede Excelsior]]'s output was damaged in a fire in 1969; however, in the late 1990s about 100 tapes of Rede Excelsior programming were discovered and these tapes were subsequently donated to the [[Cinemateca Brasileira]] in 2001.

==Canada==
The [[Canadian Broadcasting Corporation]] never practiced wiping, and maintains a complete archive of all programming that was recorded.&lt;ref&gt;{{cite web|url=http://archives.cbc.ca/info/archives/archives_en_04.asp?IDLan=1|title=CBC Archives|date=10 April 2013|publisher=}}&lt;/ref&gt;

The [[CTV Television Network]] has admitted to wiping many programmes during the 1970s. Because of [[Canadian content]] requirements, the need for Canadian-produced programming led to more preservation of the shows they produced, and even very poorly received programmes (such as the infamous ''[[The Trouble with Tracy]]'') were saved and rerun for several years after their cancellation. Furthermore, Canadian rebroadcasts have been a source of some broadcasts that are otherwise lost in the United States and the United Kingdom.

==Japan==
Some TV stations in Japan practiced wiping, this example included the [[Doraemon (1973 anime)|first anime]] adaption of ''[[Doraemon]]''.

==Philippines==
Episodes from 1979 to 1982 of the longest running noontime show, ''[[Eat Bulaga!]]'', have been lost.

Another example of the wiping of TV archives in the Philippines was when martial law was declared, soldiers raided the [[ABS-CBN|ABS-CBN Broadcast Center]] and placed it under military control. As a result, ABS-CBN's pre-martial law archives, dating from 1953 to 1972, were lost.

==Mexico==
Due to its multiple studio facilities, namely its Chapultepec and [[Televisa San Angel|San Angel studios]], [[Televisa]] preserved most of its scripted series for broadcast years after the preserved programs had ended their original runs.  Some Televisa programs, however, were lost not due to wiping, but due to the [[1985 Mexico City earthquake]] that destroyed part of the network's archive. However, smaller channels, such as [[XEIPN-TV]] and [[XHDF-TV]], did not began to preserve their recorded broadcasts until the early 1980s. [[Monterrey]]'s [[Multimedios Televisi&#243;n]] keeps most of its programming, though some special historical programming dealing with [[XHAW-TDT|its flagship station]]'s history clearly shows that some footage has been either donated by viewers recorded from its original broadcast, or uses footage of its programming recorded by fans and uploaded to [[YouTube]].

==United Kingdom==

===BBC===
The [[BBC]], the United Kingdom's first [[public service broadcaster]], had no policy on archiving until 1978.&lt;ref&gt;{{cite web|url=http://cuttingsarchive.org.uk/missing/mis_overv.htm |title=Cuttings Archive: The Missing Episodes - Overview |publisher=Cuttings Archive |accessdate=2008-11-23 |deadurl=yes |archiveurl=https://web.archive.org/web/20080725012437/http://www.cuttingsarchive.org.uk/missing/mis_overv.htm |archivedate=July 25, 2008 }}&lt;/ref&gt; Much of the corporation's output between the 1930s and 1980s has been lost. Rationales behind this policy include:

====Technological====
The BBC's [[television]] service dates back to 1936 and was originally a nearly live-only medium. The hours of transmission were very limited and the bulk of the programming was transmitted either live from the studio, or from [[Outside broadcasting|outside broadcast (OB)]] units; film was a minor contributor to the output. When the first television broadcasts were made, there were two competing systems in use. The EMI electronic system (using [[405 lines]]) competed with the Baird 240-line [[mechanical television]] system. Baird adopted an intermediate film technique where the live material was filmed using a standard film camera mounted on a large cabinet which contained a rapid processing unit and an early [[flying spot scanner]] to produce the video output for transmission. The pioneer broadcasts were not, however, preserved on this intermediate film as the nitrate (celluloid) stock was scanned while still wet from the fixer bath and never washed to remove the fixer chemicals. Consequently, the film decomposed very soon after transmission; nothing is known to have survived.
No studio or OB programmes from 1936 to 1939 or 1946 to 1947 have survived because there was no means of preserving them. Historical 'firsts' from this era; the world's earliest television crime drama ''[[Telecrime]]'' (1938&#8211;39 and 1946) or ''[[Pinwright's Progress]]'' (1946&#8211;47, the world's first regular [[situation comedy]]), only remain visually as a handful of still photographs.

The earliest recording method for television was [[Kinescope|telerecording]], which involved recording the image from a special television monitor onto film with a modified film camera. Early examples made by this method include the first two episodes of ''[[The Quatermass Experiment]]'' (1953), transmitted live while simultaneously telerecorded. The visual quality of the second episode's recording was considered so poor&#8212;a fly entered the gap between the camera and monitor at one point&#8212;that the remainder of the series was not recorded.

Although [[Quadruplex videotape]] recording technology was utilised in the UK from 1958, this system was expensive and complex; recorded programmes were often erased after broadcast. The vast majority of live programmes were never recorded at all. Videotape was not initially thought to be a permanent archivable medium &#8211; its high cost and the potential reuse of the tapes led to the transfer of programme material to film via [[Kinescope|telerecording]] whenever sales of overseas screening rights were possible or preservation deemed worthwhile. The recycling of videotapes, coupled with savings made on the storage of the bulky 2" tapes,&lt;ref&gt;By 1973, about 20,000 hours of recorded material was stored on videotape at the BBC weighing about 400,000 lbs in total. See ''BBC Engineering'', No.95, September 1973, London: BBC Publications, p.3&lt;/ref&gt; enabled the BBC to keep costs down.

====Cultural====
Drama and entertainment output was studio-based and followed the tradition of live [[theatre]]. Conventional filmmaking was only gradually introduced from the 1960s. ''The Sunday Night Play'' (a major event in the 1950s) was performed live in the studio. On Thursday, because telerecording was of insufficient broadcast quality, another live performance followed, the artists returning to perform the play again.

Today, most programmes are pre-recorded and it is relatively inexpensive to preserve programming for posterity; even so, the [[BBC Charter]] makes no mention of any obligation to retain all of them.

====Rights====
All television programmes have copyright and other rights issues associated with them. For some genres of programmes&#8212;such as drama and entertainment&#8212;the actors, writers, and musicians involved in a production all have underlying rights. In the past, these rights were defended rigorously&#8212;permission could even be denied by a contributor for the repeat or re-use of a programme. Talent [[Trade union|unions]] were highly suspicious of the threat to new work if programmes were repeated; indeed, before 1955 [[Equity (trade union)|Equity]] insisted that any telerecording made (of a repeat performance) could only "be viewed privately" on BBC premises and not transmitted.

====Colour television====
The introduction of colour television in the United Kingdom from 1967 meant that broadcasters felt there was even less value in retaining monochrome recordings. Such tapes could not be re-used for colour production, so they were disposed of to create space for the new colour tapes in the archives, which were quickly filling up. The increased cost of colour [[2 inch Quadruplex videotape]]&#8212;approximately &#163;1000 per tape at today's prices&#8212;meant that companies still often re-used the tapes for efficiency. Negative attitudes to a programme's value also persisted. For these reasons, many programmes survive only as monochrome film recordings, if at all.

Some colour productions were telerecorded onto monochrome film for export to countries which did not yet have colour television. In some cases, early colour programmes only survive in this form.

====Significant wiped programmes====
High-profile examples of programme losses include many early episodes of [[Doctor Who]] (97), ''[[The Wednesday Play]]'', most of the seminal comedy series ''[[Not Only But Also]]'', all of the 1950s televised [[Francis Durbridge]] serials (further, the first two serials were never recorded), the vast majority of the BBC's [[Apollo 11]] [[British television Apollo 11 coverage|Moon landing studio coverage]], all but one of the 39 episodes of ''[[The First Lady (TV series)|The First Lady]]'',&lt;ref&gt;{{cite web|url=http://www.lostshows.com/default.aspx?programme=ec5863f7-6843-4552-acda-07e19396fdae|title=Lost UK TV Shows Search Engine|author=Simon Coward, Invisible Technology Ltd|publisher=}}&lt;/ref&gt; and all 147 episodes of the [[soap opera]] ''[[United!]]''. There are many gaps in many long-running BBC series (''[[Dixon of Dock Green]]'', ''[[Hancock's Half Hour]]'', ''[[Sykes]]'', ''[[Out of the Unknown]]'', and ''[[Z-Cars]]''). [[The Beatles]]' only live appearance on ''[[Top Of The Pops]]'' in 1966, performing the single "[[Paperback Writer]]" is believed to have been wiped clean in a clear-out in the 1970s.

The first acting appearance of musician [[Bob Dylan]], in a 1963 play entitled ''[[The Madhouse on Castle Street]]'', was erased in 1968.&lt;ref&gt;{{cite news|url = http://www.offthetelly.co.uk/reviews/2005/arenadylan.htm|title = Arena: Dylan in the Madhouse|date = 2005-09-28|last = Worthington|first = TJ|work = OFF THE TELLY}}&lt;/ref&gt;

There is lost material in all genres &amp;mdash; as late as 1993, a large number of videotaped children's programmes from the 1970s and 1980s were irretrievably wiped by Adam Lee of the [[BBC]] [[archives]] on the assumption that they were of "no use", without consulting the BBC children's department itself.&lt;ref&gt;{{cite news |url=http://www.offthetelly.co.uk/oldott/www.offthetelly.co.uk/index8e01.html?page_id=781 |title=Of Finger Mice and Mr. Men - The Story of Watch with Mother Part Eleven: Andy is Waving Goodbye |last=Worthington |first=TJ |date = November 2006|work=Off the Telly}}&lt;/ref&gt;

====Other lost material====
Virtually the entire runs of the corporation's pre-1970s soap operas have been lost. In the 1950s and 1960s, the [[BBC]] soap operas ''[[The Appleyards]]'', ''[[The Grove Family]]'', ''[[Compact (soap opera)|Compact]]'', ''[[The Newcomers (TV series)|The Newcomers]]'', ''[[199 Park Lane]]'', and ''[[United!]]'' produced approximately 1200 episodes altogether.

There are no episodes of either  ''United!'' or ''199 Park Lane'' in the archives, while only one episode of ''The Appleyards'', three episodes of ''The Grove Family'', and four episodes each of ''Compact'' and ''The Newcomers'' are known to exist.

Also vulnerable to the corporation's wiping policy were programmes that only lasted for one season. ''[[Abigail and Roger]]'', ''[[The Airbase]]'', ''[[As Good Cooks Go]]'', the 1960 adaptation of ''[[The Citadel (novel)|The Citadel]]'', the 1956 adaptation of ''[[David Copperfield (novel)|David Copperfield]]'', ''[[The Dark Island]]'', ''[[The Gnomes of Dulwich]]'', ''[[Hurricane]]'', ''[[For Richer...For Poorer]]'', ''[[Hereward the Wake]]'', ''The Naked Lady'', ''Night Train To Surbiton'', ''Outbreak of Murder'', ''Where do I Sit?'', and ''Witch Hunt'' have all been wiped with no footage surviving while four out of seven episodes of the paranormal anthology series ''[[Dead of Night (TV series)|Dead of Night]]'' were wiped.

An edition of ''[[Hugh and I]]'' ("Chinese Crackers"), starring [[Hugh Lloyd]], [[Terry Scott]], [[John Le Mesurier]] and [[David Jason]] was located by [[Kaleidoscope Publishing]] in 2010 in the archives of [[UCLA]], and brought to general public attention in February 2011.

Early episodes of the pop music-chart show ''[[Top of the Pops]]'' were wiped or never recorded while they were being transmitted live, including the only in-studio appearance by [[The Beatles]]. Clips of [[the Beatles]] miming "[[Can't Buy Me Love]]" and "[[You Can't Do That]]" on an episode from 25 March 1964 were found online by missing episode hunter Ray Langstone in 2015. The last lost edition dates from 8 September 1977. There are only four complete ''TOTP'' episodes surviving from the 1960s, while many otherwise-missing episodes survive only as fragments. Only two episodes still exist of ''[[The Sandie Shaw Supplement]]'' (a music-variety show hosted by the singer), recorded in 1967.

====Finding missing BBC programmes====
Since the establishment of an archival policy for television in 1978, BBC archivists and others over the years have used various contacts in the UK and abroad to try to track down missing programmes. For example, all [[BBC Worldwide]] customers&#8212;broadcasters around the world&#8212;who had bought programmes from the corporation were contacted to see if they still had copies which could be returned; ''Doctor Who'' is a prime example of how this method recovered episodes that the corporation did not hold itself. At the turn of the 21st century, the BBC established its [[BBC Archives#Archive Treasure Hunt|Archive Treasure Hunt]], a public appeal to recover lost productions, which has had some successes.&lt;ref name="BBCTH"&gt;{{cite web|url=http://www.bbc.co.uk/cult/treasurehunt/about/listoffinds.shtml |title=BBC Online - Cult - Treasure Hunt - List of Finds |publisher=Bbc.co.uk |date= |accessdate=30 July 2010}}&lt;/ref&gt;

The BBC also has close contacts with the [[National Film and Television Archive]], which is part of the [[British Film Institute]] and its "Missing Believed Wiped" event which was first held in 1993 and is part of a campaign to locate lost items from British television's past. There is also a network of collectors who, if they find any programmes missing from the BBC archives, will contact the corporation with information&#8212;or sometimes even the actual footage. Some examples of programmes recovered for the archives are ''[[Doctor Who]]'', ''[[Steptoe and Son]]'', ''[[Dad's Army]]'', ''[[Letter from America]]'',&lt;ref&gt;[http://www.bbc.co.uk/informationandarchives/archivenews/2014/letters_from_america_rediscovered.html Letter from America rediscovered], bbc.co.uk, 28 March 2014&lt;/ref&gt; ''[[The Likely Lads]]'', and ''[[Play for Today]]''.

For many years the [[television pilot|pilot]] episode of ''[[Are You Being Served?]]'' survived only in black and white, appearing in this form on the 2003 DVD release of the show. In 2009, a colour version was [[colour recovery|reconstructed]] when it was realised that the black and white film reel had actually recorded sufficient colour information as a [[dot crawl]] pattern to allow [[colour recovery]].

===ITV===
The BBC was not alone in this practice &#8211; the commercial companies that formed its main rival [[ITV (TV network)|ITV]] also wiped videotapes and destroyed [[telerecording]]s, leaving gaps in their archive holdings. The state of the archives varies greatly between the different companies; [[Granada Television]] holds a large number of its older black-and-white programmes, the company having an unofficial policy of retaining as much of its broadcast material (albeit by telerecording) as possible despite financial hardship in its early years. This includes the entirety of the soap opera ''[[Coronation Street]]'' which is now held at the [[Yorkshire Television]] archive, which itself possesses largely intact archives, although some early colour shows from the late 1960s and the early 1970s such as the entire output of the drama ''Castle Haven'', the first two series of ''[[Sez Les]]'' and the children's variety show ''[[Junior Showtime]]'' are missing and believed wiped. The former ITV company [[Thames Television]] also has a significant library.

These cases tend to be the exception, however; the former nature of the ITV network, in which private independent companies were awarded licences to serve geographical areas for a set period of time, meant that when companies lost their licences their archives were often sold to third parties and became fragmented&#8212;and/or risked being destroyed, as ownership and [[copyright]] remained with the production companies rather than with the network. The archive of networked programmes made by [[Southern Television]], for example, is now owned by the otherwise-unconnected Australian media company [[Southern Star Group]] but Southern's regional output is in the hands of [[ITV plc]]. The few surviving tapes of [[Associated-Rediffusion]] belong to many different organisations as the majority of Associated-Rediffusion's tapes were recorded in [[monochrome]] and therefore deemed of no use upon the arrival of colour broadcasting; as such they were disposed of by London successor [[Thames Television]]), although in recent years there have been occasional discoveries such as a 1959 episode of ''[[Double Your Money]]'' and the remaining missing episode of ''[[Around the World with Orson Welles]]'', found by Ray Langstone in 2011. Many master tapes belonging to [[Associated Television|ATV]] have since deteriorated due to bad storage and are unsuitable for broadcasting. In particular, the ATV version of the popular soap ''[[Crossroads (soap opera)|Crossroads]]'' is missing 2,850 episodes of its original 3,555. Also often largely lost are quiz shows; few editions exist of the 1970s version of ''Celebrity Squares'' with [[Bob Monkhouse]], or Southern's children's quiz ''[[Runaround (game show)|Runaround]]''.{{cn|date=December 2016}}

Further, responsibility for archive preservation was left to individual companies. For example, ITV has no record of its live coverage of [[Apollo 11|the 1969 Moon landings]] after the station responsible for providing the coverage, [[London Weekend Television]], wiped the tapes. Of the 96 [[United Kingdom|British]] inserts to the 1980s franchised [[United Kingdom|Anglo]]-[[United States|American]]-[[Canada|Canadian]] children's show ''[[Fraggle Rock]]'', only 12 are known to exist as the library of the British producer ([[Television South|TVS]]) has been sold and subsequently split up.

In recent years, the trend of preserving material has started to change. The archives of [[Westward Television]] and [[Television South West]] are now held in trust for the public as the South West Film and Television Archive, whilst changes in legislation mean that ITV companies which lose their franchises must donate archives to the [[British Film Institute]]. However, the change of ITV from a federal structure to one centralised company means that changes of regional companies in the future seems highly unlikely.

Most material from the 1960s also only survive as telerecordings. Some early episodes are also believed to be damaged or in poor quality, whereas much of the output of other broadcasters &#8211; such as many early episodes of ''[[The Avengers (TV series)|The Avengers]]'' which were shot in the electronic studio rather than on film, produced by [[Associated British Corporation]] &#8211; have been destroyed.

No copies of ''The Adventures of [[Francie and Josie|Francie &amp; Josie]]'' exist, as most of [[Scottish Television]]'s early shows were destroyed in a fire in late 1969 (although some sources state 1973). ''The Adventures Of Francie &amp; Josie'' was made from 1961 to 1965 by STV.

===Recovery of missing programmes===
Since the BBC library was first audited in 1978, missing programmes or extracts on either film or tape are often found in unexpected places. An appeal to broadcasters in other countries who had shown missing programmes (notably [[Australia]], [[New Zealand]], [[Canada]], and [[Africa]]n nations such as [[Nigeria]]) produced "missing" episodes from the archives of those television companies. Episodes have also been returned to broadcasters by private film collectors who had acquired 16mm film copies from various sources.{{cn|date=December 2016}}
* Two Series 1 episodes of ''[[The Avengers (TV series)|The Avengers]]'' (an [[Associated British Corporation]] production) which were thought to be missing were recovered from the [[UCLA]] Film &amp; Television Archive in the United States.
* It emerged in September 2010 that more than 60 recordings of BBC and ITV drama productions originally sent for broadcast in the United States by the [[Public Broadcasting Service|PBS]] station [[WNET]] (which serves [[New York City]] and [[New Jersey]]) had been found at the [[Library of Congress]].&lt;ref&gt;Vanessa Thorpe [https://www.theguardian.com/tv-and-radio/2010/sep/12/lost-tapes-classic-british-television "Lost tapes of classic British television found in the US"], ''The Observer'', 15 September 2010&lt;/ref&gt;
* The BBC [[sitcom]] ''[[Steptoe and Son]]'' is completely intact, although approximately half of the colour episodes only exist in monochrome; this was after copies of episodes thought to be lost were recovered in the late 1990s from early non-broadcast standard video recordings made for writers [[Ray Galton]] and [[Alan Simpson (scriptwriter)|Alan Simpson]] by BBC technicians.
* A few audio recordings of ''[[Til Death Us Do Part]]'' have been recovered, as well as an extract of the pilot and two episodes from series three.

Copies of several compilations from the British 1960s comedy ''[[At Last The 1948 Show]]'', held by many to be a forerunner of ''[[Monty Python's Flying Circus]]'', were discovered in the archives of the Swedish broadcaster [[Sveriges Television|SVT]], to whom the producers [[Associated Rediffusion|Rediffusion London]] had sold them upon the companies' loss of its broadcasting licence. The master tapes, along with much of Rediffusion's programming, were wiped or disposed of by London successor Thames Television. Their recovery enabled the reconstruction of otherwise missing original editions of the programme, meaning most of the series exists in visual form.

Off-air home audio recordings of various television programmes have also been recovered, at least preserving the soundtracks to otherwise missing shows, and some of these (particularly from ''[[Doctor Who]]'') have been released on [[CD]] by the BBC following restoration and the addition of narration to describe purely visual elements. [[Tele-snaps]], a commercial service of off-screen shots of programmes often purchased by [[actor]]s and [[television director]]s to keep a record of their work in the days before [[videocassette recorder]]s, have also been recovered for many lost programmes.

===Preservation of the current archive===
Advances in technology have resulted in old programmes being transferred to new digital media, where they can be restored or (if they are damaged or otherwise cannot be restored) kept from decaying further. In the United Kingdom, the archives of both the BBC and those available of ITV, along with other channels, are being switched from cumbersome [[Quadruplex videotape|2-inch quadruplex videotape]] to digital format. This is an extensive and expensive process and one that will take many years to complete.

Live broadcasts in Britain are still not necessarily kept, and wiping of material has not ceased. According to writer and broadcaster [[Matthew Sweet (writer)|Matthew Sweet]], there are "big gaps in the record of children's television of the Nineties."&lt;ref&gt;Matthew Sweet [http://www.telegraph.co.uk/culture/tvandradio/10492487/Searching-for-televisions-missing-gems-Doctor-Who-Woody-Allen-Ridley-Scott-and-Dennis-Potter.html "Searching for television's missing gems: Doctor Who, Woody Allen, Ridley Scott and Dennis Potter"], telegraph.co.uk, 4 December 2013&lt;/ref&gt;

==United States==
In the [[United States]], the major broadcast networks also engaged in the practice of wiping recordings until the late 1970s. Many episodes were erased, especially daytime and late-night programming such as daytime [[soap opera]]s and [[game show]]s. The daytime shows, almost all of them having been taped, were erased because it was believed at the time that nobody wanted to see them after their first broadcast. The success of [[cable television]] networks devoted to reruns of these genres proved that this was not the case, as the large number of episodes that were required for a daily program made even a short-run game show an ideal candidate for [[broadcast syndication|syndication]]. By this time, however, the damage had already been done.

===Preservation by institutions such as museums===

Some museums and other [[cultural institution]]s such as the Paley Center for Media have taken steps to discover and preserve (see, e.g., "[[Paley Center for Media#Archives]]") old recordings previously thought to have been wiped or discarded, lost, or misfiled.

===Hosting sequences===
Hosting sequences on videotape, nearly always featuring celebrities, were sometimes made for telecasts of family films, notably for the first nine telecasts of MGM's ''[[The Wizard of Oz (1939 film)|The Wizard of Oz]]''. It is not known if those made for ''Oz'' survived since they have not been seen since 1967. One hosting sequence from that era that does survive is the one [[Eddie Albert]]  made for the 1965 CBS telecast of ''[[The Nutcracker]]'', starring [[Edward Villella]], [[Patricia McBride]], and [[Melissa Hayden (dancer)|Melissa Hayden]]. It has even been included on the DVD release of the program.&lt;ref&gt;{{cite web|url=http://www.wbshop.com/product/nutcracker+the+1965+tv+sp+1000179869.do?from=Search|title=Nutcracker, The (1965 TV SP) (MOD)|work=www.WBShop.com}}&lt;/ref&gt;

===Ernie Kovacs===
Many of [[Ernie Kovacs]]'s videotaped network programs were also wiped. During different times as comedian, writer, and performer Kovacs had programs on all four major television networks ([[American Broadcasting Company|ABC]], [[CBS]], [[DuMont Television Network|DuMont]], and [[NBC]]). After Kovacs's death, the networks wiped many programs. Kovacs's widow [[Edie Adams]] obtained as many programs and episodes as she could find, donating them to [[UCLA]]'s [[Special Collections]].

===Soap operas===
Though most soap operas transitioned from live broadcast to videotaping their shows during the 1960s, it was still common practice to wipe and reuse the tapes. This practice was due to the high cost of videotape at the time. While soap operas began routinely saving their episodes between 1976 and 1979, several soaps have saved recordings of most or all their episodes. ''[[Days of Our Lives]]''  has recordings of all its episodes; its first two episodes exist on their original master tapes, and were aired by [[SOAPnet]] in 2005. ''[[The Young and the Restless]]'',  ''[[Dark Shadows]]''  and ''[[Ryan's Hope]]'' saved most of their episodes, despite the fact that they debuted during the 1960s and 1970s, before retaining tapes became common practice. Episodes of ''[[The Doctors (soap opera)|The Doctors]]'' began to be saved no later than December 4, 1967; this is where reruns of the series began when picked up by [[Retro Television Network]] in September 2014. Episodes of other soaps broadcast during the 1950s to 1970s do exist in different forms and have been showcased in various places online.

[[Procter and Gamble]] started saving their shows around 1979. Very few pre-1979 color episodes of the Procter and Gamble-sponsored shows survive, with most extant episodes preserved as monochrome kinescopes. Exceptions include two episodes of ''[[The Guiding Light]]'' (1973 and 1977), which have been released on DVD. ''[[As the World Turns]]'' and ''[[The Edge of Night]]'' aired live until 1975, the year ''The Edge of Night'' moved to [[American Broadcasting Company|ABC]] and ''As the World Turns'' expanded from a 30-minute broadcast to one hour. Both shows began taping episodes in preparation for the move of ''The Edge of Night'' to ABC. ''The Edge of Night''&lt;nowiki&gt;'s&lt;/nowiki&gt; ABC debut is believed to have survived. Overall, the number of surviving monochrome episodes recorded on kinescope outnumber color episodes for these programs.

[[Agnes Nixon]] initially produced her series ''[[One Life to Live]]'' and ''[[All My Children]]'' through her own production company, Creative Horizons, Inc., and kept a complete archive of monochrome kinescopes until ABC bought the shows from her in 1975. When the network wanted to expand ''All My Children'' from 30 minutes to a full hour in the late 1970s, Nixon agreed on the condition that the network would begin saving the episodes. ABC complied, and full hour broadcasts began on April 25, 1977. However, a fire destroyed the vast majority of the early-1970s kinescopes, leaving only a few random episodes from each season.

Virtually all episodes of ''[[General Hospital]]'', from its premiere in April 1963 through August 1970, are archived at [[UCLA]]. The [[UCLA Film &amp; Television Archive]] holds a large number of daytime television airings that were spared from the wiping practice.  Also archived there are handfuls of episodes of each soap opera that was on the air from 1971 and 1973, including ''[[A World Apart (TV series)|A World Apart]]'', ''[[Where the Heart Is (1969 TV series)|Where the Heart Is]]'', and ''[[Return to Peyton Place (TV series)|Return to Peyton Place]]''.

===DuMont programs===
It is believed that virtually the entire archive of the [[DuMont Television Network]], covering its whole history from 1946 to 1956, was disposed of during the 1970s by a "successor" broadcaster (presumably [[Metromedia]], the holder of DuMont's assets), who dumped all of the kinescopes/videotapes into the [[East River]] to make room for other tapes (believed to be ABC's) at a New York City warehouse.&lt;ref name="LoC"&gt;{{cite web|last = Adams|first = Edie|authorlink = Edie Adams|title = Television/Video Preservation Study: Los Angeles Public Hearing|work = National Film Preservation Board| publisher = Library of Congress|date = March 1996|url = http://www.loc.gov/film/hrng96la.html|accessdate = 2008-05-09}}&lt;/ref&gt; Further, a large number of DuMont's kinescopes were destroyed in about 1958 for their silver content.

Of the over 20,000 shows carried by DuMont in its ten-year existence, [[List of surviving DuMont Television Network broadcasts|approximately 350 or so episodes of DuMont programming are known to exist today]], less than two percent of its total output. The remainder were either never recorded (e.g., ''[[NFL on DuMont]]'') or were dumped in the earlier purges.

===''The Tonight Show''===
{{See also|The Tonight Show Starring Johnny Carson}}
Almost all of ''[[The Tonight Show]]'' with [[Jack Paar]] and the first ten years hosted by his successor [[Johnny Carson]] were taped over by the network, with Carson's blessing, under the assumption that the broadcasts were of no real value.&lt;ref&gt;[http://www.washingtonpost.com/entertainment/tv/carson-on-tcm-shows-why-johnny-was-the-king/2013/07/03/3c25d1ce-e264-11e2-aef3-339619eab080_story.html Carson on TCM shows why Johnny was the king]. ''The Washington Post''. Retrieved July 9, 2013.&lt;/ref&gt; This is part of the reason why Carson's late 1960s shows had poorer picture quality{{Citation needed|date=November 2010}} compared to his competitor [[Dick Cavett]] on [[American Broadcasting Company|ABC]]; [[NBC]] was using the ''Tonight Show'' tapes repeatedly. Another reason for their poorer quality is that many of the 1960s ''Tonight Show'' episodes only survived in the kinescope format. (Cavett's ABC shows were also taped over by his network in favor of other shows produced at ABC's studios in New York.)

===Early sporting events===
{{See also|List of World Series broadcasters|List of Super Bowl broadcasters|NFL on CBS|NFL on NBC}}

Many early sporting events, such as the [[World Series]] and the first two [[Super Bowl]]s, were also lost, though a nearly intact recording of the first Super Bowl was found in 2005.

====[[National Football League]]====
''[[Super Bowl I]]'' was aired by both [[CBS]] and [[NBC]] (the only Super Bowl to be aired by two networks), but neither network felt the need to preserve the game long-term; CBS saved the telecast for a few months and reran it as filler programming at least once before wiping it. A color videotape containing the first, second and fourth quarters of the telecast from [[WYOU]] (the CBS affiliate for [[Scranton, Pennsylvania]]) was found in 2005 and is in the process of being restored.&lt;ref&gt;Fybush, Scott (2011-02-07). [http://www.fybush.com/NERW/2011/110207/nerw.html Will New York Outlaw Pirate Radio?]. ''NorthEast Radio Watch''. Retrieved 2011-02-07.&lt;/ref&gt; On January 15, 2016, the [[NFL Network]] reaired the first Super Bowl, featuring audio from [[NBC Radio]] and most of the TV network broadcast and newly discovered [[NFL Films]] footage of the game. ''[[Super Bowl II]]'' was aired exclusively by CBS and was believed to have been erased, but it was later found that the entire telecast fully exists and rests in the vaults of [[NFL Films]].&lt;ref name="foo"&gt;{{cite web
 | url        = http://www.marketwatch.com/story/the-hunt-for-tvs-lost-baseball-treasures-2010-10-27?pagenumber=2
 | title      = The hunt for TV&#8217;s lost baseball treasures
 | author     = David B. Wilkerson
 | date       = October 27, 2010
 | work       =
 | publisher  = [[Wall Street Journal]] Marketwatch
 | accessdate = November 26, 2012
}}&lt;/ref&gt;  Though the telecast of ''[[Super Bowl III]]'' exists in full color, only half of the ''[[Super Bowl IV]]'' broadcast does (the rest was preserved by Canadian television in black-and-white). The first three quarters of ''[[Super Bowl V]]'' broadcast by NBC Los Angeles affiliate [[KNBC]] exist, but the fourth quarter is missing, though the [[Mike Curtis (American football)|Mike Curtis]] interception and [[Jim O'Brien (American football)|Jim O'Brien]] game-winning field goal were recovered via news highlights from [[CBC Television|CBC]]. ''[[Super Bowl VI]]'' also exists in its entirety. It was not until ''[[Super Bowl VII]]'' that a continuous archive was established.&lt;ref name="foo" /&gt;

Similarly, all of the telecasts of the [[NFL Championship Game]]s prior to the Super Bowl are believed to have been lost, with all surviving footage of those games coming from separately produced film. The status of most regular season and playoff games from the early years of television up to the immediate years following the 1970 [[AFL&#8211;NFL merger]] are also unknown. Among the footage that has survived include at least some of NBC's coverage from the 1972 AFC Divisional Playoff game between the [[1972 Pittsburgh Steelers season|Pittsburgh Steelers]] and [[1972 Oakland Raiders season|Oakland Raiders]] that featured the [[Immaculate Reception]], as well as the inaugural telecast of ''[[Monday Night Football]]'' between the [[1970 Cleveland Browns season|Cleveland Browns]] and the [[1970 New York Jets season|New York Jets]], though several ''Monday Night Football'' games in the ensuing seasons were lost. A [[1974 NFL season|1974 game]] that featured [[John Lennon]] being interviewed by [[Howard Cosell]] in the booth only survived due to a [[home video]] recording of the game; the game itself was wiped by ABC. CBS kept coverage of a 1978 [[Eagles&#8211;Giants rivalry|matchup]] between the [[1978 New York Giants season|New York Giants]] and [[1978 Philadelphia Eagles season|Philadelphia Eagles]] that would feature the now-infamous [[Miracle at the Meadowlands]], although the existence of many 1978 games on CBS by private collectors shows that the networks by that point started keeping recordings of regular season games. There are rare exceptions of CBS games from [[1977 NFL season|1977]] back, but by [[1978 NFL season|1978]] the library of most teams is almost fully complete. NBC is another story.

The NFL had its own filmmakers, [[NFL Films]], filming the game with its own equipment. Thus, preserving the telecasts on tape was not seen as a priority by the networks when another source was available &#8211; though the sportscasters' play-by-play comments, as a result, were lost.

====World Series telecasts====
All telecasts of World Series games starting in [[1975 World Series|1975]] ([[1975 Cincinnati Reds season|Reds]]&#8211;[[1975 Boston Red Sox season|Red Sox]]) are known to exist in full.&lt;ref name="Surviving World Series Telecasts"&gt;{{cite web|url=http://www.dbsforums.com/vbulletin/showthread.php?t=78232|title=www.dbsforums.com|publisher=}}&lt;/ref&gt; Follows is the known footage of World Series telecasts prior to 1975:
* [[1952 World Series|1952]] ([[1952 New York Yankees season|Yankees]]&#8211;[[1952 Brooklyn Dodgers season|Dodgers]]) &#8211; Games 6&#8211;7 are intact.
* [[1955 World Series|1955]] ([[1955 New York Yankees season|Yankees]]&#8211;[[1955 Brooklyn Dodgers season|Dodgers]]) &#8211; Only the first half of Game 5 is known to exist.
* [[1956 World Series|1956]] ([[1956 New York Yankees season|Yankees]]&#8211;[[1956 Brooklyn Dodgers season|Dodgers]]) &#8211; Only the last three innings of Game 2 are known to exist. Game 3 is intact minus the second and third inning. Game 5 ([[Don Larsen]]'s [[perfect game]]) is intact minus the first inning, and was aired on January 1, 2009 during the [[MLB Network]]'s first broadcast day.
* [[1957 World Series|1957]] ([[1957 New York Yankees season|Yankees]]&#8211;[[1957 Milwaukee Braves season|Braves]]) &#8211; Game 1 is intact by way of a print from the United States [[American Forces Network|Armed Forces Radio and Television Service]].&lt;ref&gt;https://www.youtube.com/watch?v=72Eo0pIka4o&lt;/ref&gt; Game 3 is intact, minus a snip of [[Tony Kubek]]'s second home run in the top 7th inning. Games 6 (most of the first six innings) and 7 reportedly exist as well.
* [[1960 World Series|1960]] ([[1960 New York Yankees season|Yankees]]&#8211;[[1960 Pittsburgh Pirates season|Pirates]]) &#8211; Game 7 (with [[Bill Mazeroski]]'s series-clinching walk-off home run) was found intact on [[kinescope]] in December 2009 in the wine cellar of Pirates' part-owner [[Bing Crosby]], who had the game recorded at his own expense. MLB Network aired it in December 2010.&lt;ref&gt;{{cite news|url = http://www.nytimes.com/2010/09/24/sports/baseball/24crosby.html?_r=1&amp;src=mv|title = In Bing Crosby's Wine Cellar, Vintage Baseball|first = Richard|last = Sandomir|authorlink = Richard Sandomir|publisher = ''[[The New York Times]]''|date = 2010-09-23|accessdate = 2010-09-25}}&lt;/ref&gt;
* [[1961 World Series|1961]] ([[1961 New York Yankees season|Yankees]]&#8211;[[1961 Cincinnati Reds season|Reds]]) &#8211; Half-hour segments of Games 3 (the first two innings), 4 (the 4th and 5th innings), and 5 (open and top of the 1st inning) are known to exist.
* [[1963 World Series|1963]] ([[1963 New York Yankees season|Yankees]]&#8211;[[1963 Los Angeles Dodgers season|Dodgers]]) &#8211; Game 3 is intact.
* [[1965 World Series|1965]] ([[1965 Minnesota Twins season|Twins]]&#8211;[[1965 Los Angeles Dodgers season|Dodgers]]) &#8211; All seven games were preserved by the [[CBC Television|CBC]] on [[kinescope]].
* [[1968 World Series|1968]] ([[1968 Detroit Tigers season|Tigers]]&#8211;[[1968 St. Louis Cardinals season|Cardinals]]) &#8211; All seven games were preserved by the [[CBC Television|CBC]] on [[kinescope]].
** It is likely the 1965 and 1968 Series were preserved by the CBC due to the Twins' and Tigers' proximity to Canada; the country would not get its own MLB team until the [[Montreal Expos]] began play in 1969.
* [[1969 World Series|1969]] ([[1969 Baltimore Orioles season|Orioles]]&#8211;[[1969 New York Mets season|Mets]]) &#8211; Games 1&#8211;2 were preserved by the [[CBC Television|CBC]] on [[kinescope]], while Games 3&#8211;5 exist on their original color videotape from "truck feeds".
* [[1970 World Series|1970]] ([[1970 Baltimore Orioles season|Orioles]]&#8211;[[1970 Cincinnati Reds season|Reds]]) &#8211; Games 1&#8211;4 were preserved by the [[CBC Television|CBC]] on [[kinescope]], while Game 5 exists on its original color videotape from the "truck feed".
* [[1971 World Series|1971]] ([[1971 Baltimore Orioles season|Orioles]]&#8211;[[1971 Pittsburgh Pirates season|Pirates]]) &#8211; Games 1&#8211;2 and 6&#8211;7 are intact, while Games 3&#8211;5 only partially exist and Game 4 (the first World Series night game) is near-complete.
* [[1972 World Series|1972]] ([[1972 Oakland Athletics season|A's]]&#8211;[[1972 Cincinnati Reds season|Reds]]) &#8211; Game 4 is intact, along with nearly all of Game 5 and a fair chunk of Game 2. Fragments exist for Games 1, 3, and 6, while Game 7 is missing.
* [[1973 World Series|1973]] ([[1972 Oakland Athletics season|A's]]&#8211;[[1972 New York Mets season|Mets]]) &#8211; Game 1 is intact, Game 2 is missing the last inning and a half (including both [[Mike Andrews]] plays), Game 3 is complete minus the last inning, Game 4 is intact from the pregame show to the top of the 4th inning, and Game 5 only has the last two innings. About 30 minutes of excerpts from Game 6 survive, while Game 7 cuts off with one out at the top of the 9th inning.
** While the last inning and a half of Game 2 is missing from the Major League Baseball/[[Major League Baseball on NBC|NBC]] copy, the Andrews plays (totaling about 60 seconds of coverage) survived because after the World Series, NBC put together a 20-minute presentation tape narrated by [[Curt Gowdy]] to submit to the [[Peabody Awards]] in order to get consideration for an award for their coverage by the committee; the tape includes the two Andrews plays with Gowdy and [[Tony Kubek]]'s calls and analysis of them. The presentation tape is held by the Peabody vault, creating a case where "reconstructing" a game in an incomplete format would require going to two different outlets.
* [[1974 World Series|1974]] ([[1974 Oakland Athletics season|A's]]&#8211;[[1974 Los Angeles Dodgers season|Dodgers]]) &#8211; Games 1&#8211;4 are complete. Game 5 is near intact, but the bottom of the 9th inning is missing and only exists on the original radio broadcast.

====League Championship Series telecasts====
For the League Championship Series telecasts spanning from 1969 to 1975, only Game 2 of the [[1972 American League Championship Series]] ([[1972 Oakland Athletics season|Oakland]]&#8211;[[1972 Detroit Tigers season|Detroit]]) is known to exist;&lt;ref name="Surviving World Series Telecasts"/&gt; however, the copy on the trade circuit is missing the [[Bert Campaneris]]&#8211;[[Lerrin LaGrow]] brawl.

There are some instances where the only brief glimpse of telecast footage of an early LCS game can be seen in a surviving newscast from that night.
* Clips of Game 5 of the [[1972 National League Championship Series]] featuring the then-[[Cincinnati Reds]] announcer [[Al Michaels]] calling the two crucial plays of the game, the game-tying home run by [[Johnny Bench]] and wild pitch bringing home [[George Foster (baseball)|George Foster]] with the series-clinching run, are available.
* The last out of the [[1973 National League Championship Series]] as described by [[Jim Simpson (sportscaster)|Jim Simpson]] was played on that night's ''[[NBC Nightly News]]'', but other than that the entire game is gone.
* On the day the [[1969 New York Mets season|New York Mets]] and [[1969 Baltimore Orioles season|Baltimore Orioles]] wrapped up their respective League Championship Series in 1969, a feature story on the ''[[CBS Evening News]]'' showed telecast clips of the [[1969 American League Championship Series|ALCS]] game (albeit with no original sound). This is all that likely remains of anything from that third game of the [[1969 Baltimore Orioles season|Orioles]]&#8211;[[1969 Minnesota Twins season|Twins]] series.

While all telecasts of World Series games starting with [[1972 World Series|1975]] are accounted for and exist, the LCS is still a spotty situation through the late 1970s:
* [[1976 American League Championship Series|1976 ALCS]] &#8211; Game 5 is intact, from the [[American Broadcasting Company|ABC]] vault.
* [[1976 National League Championship Series|1976 NLCS]] &#8211; Game 3 is intact, albeit an off-air recording taped in the [[KATU|Portland market]]. Apparently, this copy is the only extant version because the ABC vault copy has no sound.
* [[1977 National League Championship Series|1977 NLCS]] &#8211; Game 3 is intact, from the [[1977 Philadelphia Phillies season|Philadelphia Phillies]]' local [[KYW-TV|NBC affiliate]]. A copy is held by Major League Baseball, who also appears to have Game 4 as well.
* [[1977 American League Championship Series|1977 ALCS]] &#8211; Game 5 is intact, with both the [[WPIX-TV|WPIX]] and [[Major League Baseball on NBC|NBC]] versions existing through off-air recordings.
** Clips of these games may be seen in highlight shows such as ''[[Yankeeography]]''. It is believed that incomplete tapes of the ALCS exist. It is possible these games are not shown in part because the audio quality is poor. A common method of getting around such deficiencies would be to overlay a radio telecast or narration by a player or commentator where gaps exist.
* [[1978 American League Championship Series|1978 ALCS]] &#8211; All four games ([[Major League Baseball on ABC|ABC]] version) are intact via off-air recordings.
* [[1978 National League Championship Series|1978 NLCS]] &#8211; Game 4 is intact, again from off-air recordings.

====NBA Finals====
{{see also|List of NBA Finals broadcasters}}
* [[1963 NBA Finals|1963]]: [[Boston Celtics|Celtics]]&#8211;[[Los Angeles Lakers|Lakers]] &#8211; Game 6 is intact.
* [[1969 NBA Finals|1969]]: Celtics&#8211;Lakers &#8211; only the entire 4th quarter of Game 7 exists.
* [[1970 NBA Finals|1970]]: Lakers&#8211;[[New York Knicks|Knicks]] &#8211; Game 7 is intact.
* [[1971 NBA Finals|1971]]: [[Milwaukee Bucks|Bucks]]&#8211;[[Washington Bullets|Bullets]] &#8211; only nearly all of the second half of game 4 exists.
* [[1972 NBA Finals|1972]]: Knicks&#8211;Lakers &#8211; Game 5 is intact with the exception of the last 3&#8211;4 minutes of the game
* [[1973 NBA Finals|1973]]: Knicks&#8211;Lakers &#8211; Games 1&#8211;4 are missing, while the entire Game 5 wasn't found until 2013 and some of which was shown in the ''[[30 for 30]]'' documentary ''When The Garden Was Eden''.
* [[1974 NBA Finals|1974]]: Bucks&#8211;Celtics &#8211; only the 4th quarter and 2 overtime of Game 6 and the 4th quarter of Game 7 exist.*[1975 NBA Finals -.Bullets-Warriors game ..1,2,&amp;3 intact
* [[1976]]: Suns-Celtics - Games 5 &amp; 6 are intact.

===Wiped programs===

====Early live shows====
Many programs in the early days of television were live broadcasts that are lost because they were not recorded. Most prime-time programs that were preserved used the [[kinescope|kinescope recording]] process, which involved filming the live broadcast from a television screen using a motion-picture camera (videotape, for recording programs, was not perfected until the late 1950s and was not widely used until the late 1960s). This was also a common practice for broadcasting live TV shows to the [[West Coast of the United States|west coast]], as performers often performed a show back-to-back, but never back-to-back-to-back.

Daytime programs, however, were generally not kinescoped for preservation (although many were temporarily kinescoped for later broadcast, episodes recorded in this way were often junked). Many local station and network newscasts were prone to wiping.

====News====
Some early news programs, such as ''[[Camel News Caravan]]'', are largely lost. Moving images of [[Walter Cronkite]] reading the news in his studio every night for six years are gone with the exception of his coverage of the [[Cuban Missile Crisis]] in 1962 and the [[JFK assassination]] in 1963. Studio shots of [[Peter Jennings]] inside his [[American Broadcasting Company|ABC]] studio during his first year there (1965) are also gone.

[[Vanderbilt University]] has kept all evening national news telecasts since Monday, August 5, 1968.

As of 1997, CBS had saved 1,000,000 videotapes of news reports, broadcasts, stock footage, and outtakes according to a report that year from the [[National Film Preservation Board]]. The same report added, "Television stations still erase and recycle their video cassettes", referring to local news programs.&lt;ref&gt;{{cite web
 | url        = http://www.loc.gov/film/tvstudy.html
 | title      = Television/Video Preservation Study: Volume 1: Report
 | author     = [[Librarian of Congress]]
 | date       = October 1997
 | work       =
 | publisher  = [[Library of Congress]]
 | accessdate = 26 November 2012
}}&lt;/ref&gt; Many local stations contract with outside companies for archiving news coverage.

====Situation comedy====
Little of the first [[sitcom]], ''[[Mary Kay and Johnny|The Mary Kay and Johnny Show]]'', remains today. It was initially live and not recorded, but later in its run kinescopes were made for rebroadcasting. Fragments of episodes and one complete installment are known to exist.

====Game shows====
[[Game show]]s, more than any other genre, were prone to wiping. Many games between 1941 and 1980 had insignificantly-short runs (some measured in a span of weeks or even days) that the networks felt it unnecessary to keep them for posterity, whereas recycling the tapes would be more profitable and less of an effort than attempting to sell the series in reruns, in an era before [[cable television]].

[[Mark Goodson]]&#8211;[[Bill Todman]] Productions (and to a lesser extent, [[Barry &amp; Enright Productions|Barry-Enright Productions]] and [[Chuck Barris Productions]]) and to an even lesser extent [[Heatter-Quigley Productions]] had the foresight to preserve many of their games for later reruns; for years, these shows dominated the [[Game Show Network]] (GSN) line-up and now make up a major portion of [[Buzzr TV]]'s lineup.

Most other game shows from that era were not so fortunate.  All of the [[Bob Stewart (television)|Bob Stewart]] (except ''[[Pyramid (game show)|Pyramid]]''), [[Heatter-Quigley Productions|Heatter&#8211;Quigley]] except for ''[[PDQ (game show)|PDQ]]'' which aired in syndication as well as many episodes of ''[[Hollywood Squares]]'', [[Stefan Hatos-Monty Hall Productions|Hatos&#8211;Hall]] (except for a large portion of ''[[Let's Make a Deal]]''), and pre-1980 [[Merv Griffin]] productions have been destroyed, with the exception of a few rare pilots and "cast aside" episodes. The few remaining episodes have therefore become collectors' items, and an active trading circuit exists among collectors.

NBC and ABC continued the wiping process well into the 1970s; while ABC ceased in early 1978, NBC continued to wipe some shows into 1980, leaving much of their daytime game show content lost forever. CBS abandoned the wiping process by September 1972, largely as a result of their collaboration with Goodson-Todman; as a result, even the network's shorter-lived games (such as ''[[Spin-Off (game show)|Spin-Off]]'') still exist in their entirety. Incidentally, all three networks ended their wiping practices during the time [[Fred Silverman]] led their respective networks.

While it remained in business, DuMont wished to keep its programs as intact as possible. However, the network ceased to exist in 1956 and its archive was destroyed in the 1970s. The corporate successor to DuMont, [[Fox Broadcasting Company|Fox]], not only has never aired any daytime programming (other than its [[Fox Kids]] block from 1990 to 2001) but debuted in 1986, well beyond the wiping era.

====Award shows====
Several award shows from the 1950s and 1960s, such as the [[Academy Awards]] and the [[Emmy Awards]], only survive in kinescope format. From [[29th Academy Awards|1957]] to [[37th Academy Awards|1965]], the Academy Awards were taped in black and white, but only survive in kinescope format for overseas distribution, especially for the European TV audiences, which used another system ([[576i|625 lines]] as opposed to [[480i|525 lines]]), as the tapes used for late broadcasting were reused. All of the taped broadcasts of the Oscars from [[38th Academy Awards|1966]] (the first to be broadcast in color) remain intact.

==See also==
{{portal|Television}}
* [[Doctor Who missing episodes|''Doctor Who'' missing episodes]]
* [[British television Apollo 11 coverage]]
* [[Missing Believed Wiped]]
* [[Kinescope]]
* [[Lost film]]
* [[List of lost television broadcasts]]
* [[Film preservation]]
* [[List of surviving DuMont Television Network broadcasts]]

==Footnotes==
{{Reflist|2}}

==References==
*{{cite book |last= Fiddy |first= Dick |authorlink= |coauthors= |title= Missing, Believed Wiped: Searching for the Lost Treasures of British Television |year= 2002 |publisher= [[British Film Institute]] |location= London |isbn= 978-0-85170-866-9 }}

==External links==
*[http://www.bbc.co.uk/cult/treasurehunt/ Full Details of the BBC's treasure Hunt]
*[http://www.lostshows.com/default.aspx? Lost Shows (UK) search engine], Kaleidoscope website
*[http://www.missing-episodes.com/ British TV Missing Episodes Index]
*[http://www.wipednews.com/ Wiped News.Com - A news and features website devoted to missing TV, Film &amp; Radio]
*[http://www.marketwatch.com/story/story/print?guid=E880D4C8-E078-11DF-B7D4-002128049AD6 The hunt for TV&#8217;s lost baseball treasures]
*[http://www.tvobscurities.com/lost/lostormissing/ Television Obscurities &gt;&gt; Television &#8212; Lost or Missing]

{{Major League Baseball on national television}}
{{National Basketball Association on television}}
{{National Football League on television and radio}}

[[Category:Television terminology]]
[[Category:Data management]]
[[Category:Television preservation]]</text>
      <sha1>rm817yipa9nz5pzyy9z4q3nny87dhvk</sha1>
    </revision>
  </page>
  <page>
    <title>Data definition specification</title>
    <ns>0</ns>
    <id>37621028</id>
    <revision>
      <id>751623264</id>
      <parentid>692592795</parentid>
      <timestamp>2016-11-26T21:46:05Z</timestamp>
      <contributor>
        <username>Marcocapelle</username>
        <id>14965160</id>
      </contributor>
      <comment>removed [[Category:Data analysis]]; added [[Category:Data management]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8869" xml:space="preserve">{{technical|date=December 2012}}
In [[computing]], a '''data definition specification''' (DDS) is a guideline to ensure comprehensive and consistent data definition. It represents the attributes required to quantify data definition. A comprehensive data definition specification encompasses enterprise data, the hierarchy of [[data management]], prescribed guidance enforcement and criteria to determine compliance.

==Overview==
A data definition specification may be developed for any organization or specialized field, improving the quality of its products through consistency and transparency. It eliminates redundancy (since all contributing areas are referencing the same specification) and provides standardization, making it easier and more efficient to create, modify, verify, analyze and share information across the enterprise.&lt;ref&gt;Gouin, Deborah. &amp; Corcoran, Charmane K. (2008).  Developing the MSU Enterprise Data Definition Standard.  Michigan State University Web site:  http://eis.msu.edu/uploads/---University%20EIS%20Working%20Committee%20Meetings/05%20August%202008/Enterprise%20Data%20Definition%20Standard%20Presentation082708.pdf&lt;/ref&gt;
   
To understand how a data definition specification works in an enterprise, we must look at the elements of a DDS. Writing data definitions, defining business terms (or rules) in the context of a particular environment, provides structure for an organization&#8217;s [[data architecture]]. In developing these definitions, the words used must be traceable to clearly defined data.

A data definition specification may be used in the following activities to provide consistency and clarity between  departments supporting the activity:&lt;ref name="datagovernance"&gt;Thomas, Gwen. (2008). Writing Enterprise-Quality Data Definitions: Tips for Creating Terms and Definitions. Data Governance Institute Web site: http://www.datagovernance.com/dgi_wp_writing_enterprise-quality_data_definitions.pdf&lt;/ref&gt;
*  [[Business intelligence]]
*  [[Business process modeling]]
*  Business rules management
*  [[Data analysis]] and [[Data modeling|modeling]]
*  [[Information architecture]]
*  [[Metadata modeling]]
*  Report generation

== Criteria ==
A data definition specification requires data definitions to be:
* ''Atomic'' &#8211; singular, describing only one concept. Commonly used and ambiguous terms should be defined.&lt;ref name="datagovernance" /&gt; While a term refers to one concept, several words may be used in a term:
:*File &#8211; A concept identifiable with one word
:*File extension &#8211; A concept identifiable with more than one word 
* ''Traceable'' &#8211; Mapped to a specific data element. In business, a term may be traced to an entity (for example, a customer) or an attribute (such as a customer's name). A term may be a value in a [[data set]] (such as gender), or designate the data set itself. Traceability indicates relationships in the [[data hierarchy]].  
* ''Consistent'' - Used in a standard [[Syntax (programming languages)|syntax]]; if used in a specific context, the context is noted
* ''Accurate'' - Precise, correct and unambiguous, stating what the term is and is not&lt;ref&gt;International Organization for Standardization JTC1/SC32 Committee. (2004) ISO 11179-4. http://standards.iso.org/ittf/PubliclyAvailableStandards/index.html.&lt;/ref&gt;
* ''Clear'' - Readily understood by the reader
* ''Complete'' - With the term, its description and contextual references
* ''Concise'' - To avoid circular references

== Applications ==

=== Enterprise data ===
A data definition specification was produced by the [[Open Mobile Alliance]] to document charging data.&lt;ref&gt;{{cite web|url=http://www.openmobilealliance.org/Technical/release_program/docs/Charging_Data/V1_0-20110201-A/OMA-DDS-Charging_Data-V1_0-20110201-A.pdf|date=1 February 2011|website=Open Mobile Alliance|pages=6, 35|format=PDF|title=Charging Data|archiveurl=https://web.archive.org/web/20131006172727/http://technical.openmobilealliance.org/Technical/release_program/docs/Charging_Data/V1_0-20110201-A/OMA-DDS-Charging_Data-V1_0-20110201-A.pdf|archivedate=6 October 2013|accessdate=12 March 2014}}&lt;/ref&gt; The document, the centralized catalog of data elements defined for interfaces, specifies the mapping of these data elements to protocol fields in the interfaces. Created for the exchange of financial data, Market Data Definition Language (MDDL) is an [[XML]] specification designed 
{{quote|to enable the interchange of information necessary to account, to analyze, and to trade financial instruments of the world's markets. It defines an XML-based interchange format and common data dictionary on the fields needed to describe: (1) financial instruments, (2) corporate events affecting value and tradability, and (3) market-related, economic and industrial indicators. The principal function of MDDL is to allow entities to exchange market data by standardizing formats and definitions. MDDL provides a common format for market data so that it can be efficiently passed from one processing system to another and provides a common understanding of market data content by standardizing terminology and by normalizing the relationships of various data elements to one another&amp;nbsp;... From the user perspective, the goal of MDDL is to enable users to integrate data from multiple sources by standardizing both the input feeds used for data warehousing (i.e., define what's being provided by vendors) and the output methods by which client applications request the data (i.e., ensure compatibility on how to get data in and out of applications)."&lt;ref&gt;{{cite web|title=Market Data Definition Language (MDDL)|date=December 26, 2002|website=Cover Pages |url=http://xml.coverpages.org/mddl.html|archiveurl=https://web.archive.org/web/20131214075132/http://xml.coverpages.org/mddl.html|archivedate=December 14, 2013|accessdate=March 12, 2014}}&lt;/ref&gt;}}

=== Clinical submissions ===
The [[Clinical Data Interchange Standards Consortium]], a global, multidisciplinary, non-profit organization, has established standards to support the acquisition, exchange, submission and archiving of clinical research data and metadata. CDISC standards are vendor-neutral, platform-independent and freely available from the CDISC website. The Case Report Tabulation Data Definition Specification (define.xml) draft version 2.0, the oldest data definition specification, is part of the evolution from the 1999 FDA electronic submission (eSub) guidance and electronic Common Technical Document (eCTD) documents specifying that a document describing the content and structure of included data be included in a submission. Define.xml was developed to automate the review process by generating a machine-readable data-definition document. Define.xml has standardized submissions to the [[Food and Drug Administration]], reducing review times from over two years to several months.&lt;ref&gt;{{cite web|title=Define-XML|year=2012|website=Clinical Data Interchange Standards Consortium|url=http://www.cdisc.org/define-xml|archiveurl=https://web.archive.org/web/20131004232219/http://www.cdisc.org/define-xml|archivedate=October 4, 2013|accessdate=March 12, 2014}}&lt;/ref&gt;

=== Archival data ===
A data definition specification is the foundation of [[metadata]] for [[scientific data archiving]]. The [[Metadata Encoding and Transmission Standard]] (METS) uses one principle of a DDS: consistent use of key terms to catalog digital objects for global use. The METS schema is a flexible mechanism for encoding descriptive, administrative and structural metadata for a [[digital library]] object and expressing complex links between metadata, and can provide a useful standard for the exchange of digital-library objects between repositories.&lt;ref&gt;Metadata Encoding &amp; Transmission Standard (METS) Web site from The Library of Congress- Standards http://www.loc.gov/standards/mets/&lt;/ref&gt;

A similar effort is underway to preserve complex data associated with video-game archiving. Preserving Virtual Worlds attempted to address archival-format deficiencies, citing the lack of suitable documentation for interactive fiction and games at the [[bit]] level: specifically, the absence of "representation information" needed to map raw bits into higher-level data constructs.&lt;ref&gt;&#8220;Meta Data Schema Development&#8221; (2008) [http://pvw.illinois.edu/pvw/?page_id=25 Preserving Virtual Worlds website]&lt;/ref&gt; Preserving Virtual Worlds 2 is a research project expanding on initial efforts in this field.&lt;ref&gt;Preserving Virtual Worlds 2, Researching best practices for videogame preservation. (2012). http://pvw.illinois.edu/pvw2/&lt;/ref&gt;

== See also ==
* [[Clinical Data Interchange Standards Consortium]] (CDISC)
* [[Data governance]]
* [[ISO/IEC 11179]]
* [[Metadata Encoding and Transmission Standard]] (METS)
* [[OASIS (organization)|OASIS]]

==References==
{{reflist}}

[[Category:Data management]]</text>
      <sha1>rltmg2obyyo7wa5muus9ocixkcptmip</sha1>
    </revision>
  </page>
  <page>
    <title>5 Ways of Conceptualizing Data</title>
    <ns>0</ns>
    <id>52060631</id>
    <revision>
      <id>758411216</id>
      <parentid>752813742</parentid>
      <timestamp>2017-01-05T07:31:05Z</timestamp>
      <contributor>
        <username>Melcous</username>
        <id>20472590</id>
      </contributor>
      <minor />
      <comment>fix spelling, publically --&gt; publicly</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9513" xml:space="preserve">{{multiple issues|
{{essay-like|date=December 2016}}
{{notability|date=December 2016}}
{{original research|date=December 2016}}
}}
{{Orphan|date=December 2016}}

[[Data]] can be viewed as a measurement of numbers, and characters that are set in a way to understand a certain subject. However, there are many different ways to view data; such as conceptualizing data. These are five ways of conceptualizing data. They all have positive and negative points to each technique. Although they are different, they all bring up questions and concerns with data collection and what happens with the information afterward. Another concern is what is the goal with the data that has been collected depending on the category. The five ways of conceptualizing data are technically, ethically, politically and economically, spatially/ temporal, and philosophically. Typically viewed by critical data scholars, they have all of these ways of viewing data because it is important to see the different ways that data can be viewed and to see if there may be any bias. Not only is it important to see if there is any bias, however, it is also important to understand what all the data will mean in the bigger picture. The way that this is normally done is by understanding raw data, then placing them into categories that will help with the better understanding and creating new knowledge.

==Technically==
Technically viewing data concerns the knowledge about the quality of data, if it is reliable, if it is authentic, if it is valid. It is also about knowing how the data is structured, shared, processed, and analyzed.&lt;ref&gt;(Kitchin, p.12)&lt;/ref&gt; There are views about the concerns around data such as the representativeness, how it is uncertain, the reliability of it, the chances of any errors, the likelihood of any bias, and around the measuring of the research design and the execution of it.&lt;ref&gt;(Kitchin, p.13)&lt;/ref&gt; There are also questions around if this form of scientific technique is going to bring the data that is wanted and needed.&lt;ref&gt;(Kitchin, p.13)&lt;/ref&gt; Other reliability concerns go with this technical view about data such as Quixotic reliability, Diachronic reliability, and Synchronic reliability. Quixotic reliability concern is where there is one observation method which produces unvarying measurements.&lt;ref&gt;(Kitchin, p.13-14)&lt;/ref&gt; Diachronic reliability is the stability of an observation through time. Lastly, Synchronic reliability is the similarity of observations within the same time period.&lt;ref&gt;(Kitchin,p.14)&lt;/ref&gt; With it being technology, there are many different ways that errors could arise, such as, missing data, mistakes, misunderstaning's, bias&#8217;, and uncertainty.&lt;ref&gt;(Kitchin, p.14)&lt;/ref&gt;

==Ethically==
The [[ethics|ethical]] view of data is more about the idea of why the data is generated, and what use the data is going to be placed in. There are concerns around how the data will be shared, protected, traded, and to how they are employed&#8221;.&lt;ref&gt;(Kitchin, p.15)&lt;/ref&gt; This also deals with the issue of sensitivity. Some data is low when it comes to sensitivity, such as the traffic. However, some are a lot higher, such as speaking to survivors of crime.&lt;ref&gt;(Kitchin, p. 15)&lt;/ref&gt; With the sensitivity scale, there comes privacy issues, how someone may be treated, and the issue of human rights.&lt;ref&gt;(Kitchin, p. 15)&lt;/ref&gt; It is helpful to know that some companies have a data protection act and have privacy laws.&lt;ref&gt;(Kitchin, p.15)&lt;/ref&gt; Other components that add to the category of Ethics are the question of equality, fairness, justice, honesty, respect, entitlements, rights, and care of the information that is provided and towards those that give the information.&lt;ref&gt;(Kitchin, p.14)&lt;/ref&gt; The honesty, respect and the care of the information can also be misinformed to the subject that is giving the data willingly. Causing ethical concerns for how long the information will be kept, or what the information will be used for. This is an ethical concern in the exchange of the subject and the researcher.&lt;ref&gt;(Jacob)&lt;/ref&gt;

==Politically and economically==
[[Politics|politically]] and [[economic]]ally viewed data is seen to how the data could be viewed or theorized as public goods, intellectual property, political capital, and how they are traded and how they are regulated.&lt;ref&gt;(Kitchin, p.16)&lt;/ref&gt; Economically there are many decisions when funding data researching, as well as investing in data researching. Data could be used to manage goals and raise the profits and values to those that invest in it.&lt;ref&gt;(Kitchin, p.15)&lt;/ref&gt; Such as the multi-billion-dollar data marketplace, where many companies are trading and using that data to help themselves make a profit. It is positively effecting due to the production of knowledge.&lt;ref&gt;(Kitchin, p. 15)&lt;/ref&gt; The more that the company knows about what the people want, and how to market to them, the more that they may profit and gain off of the data, due to them giving what the people want. However, there still is the political side to this. Although the data can make a profit and is economically great, there is also the competition which want to influence opinions and make the data terrain greater.&lt;ref&gt;(Kitchin, p.16)&lt;/ref&gt; It is also political because the difference between publicly good data, which is shared with anyone that can have access to it, is much different than business data. This is because business data is wanting to keep the data that they have found and use it to their advantage, such as the &#8220;production of knowledge.&#8221; &lt;ref&gt;(Kitchin, p.16)&lt;/ref&gt; The publicly good data is free to anyone that wants to view it, which would not be helpful in any way to any business strategies or marketing.&lt;ref&gt;(Kitchin, p. 16)&lt;/ref&gt;

==Spatially/temporal==
Spatial and [[temporal]] views data around technical, ethical, political, and economic [[Regime]] with the production of the data.&lt;ref&gt;(Kitchin, p.17)&lt;/ref&gt; The way that the terms spatial and temporal can be viewed is around how the data is developed and changed across time and space. Although, depending on the time and where this data is being collected, the process, the analysis, the storage of some information, yet not of others will be different, just due to a time frame and area will be different than others because of the different history that has happened and the different geographical locations. As noted the process of taking in data changes over time, however, they are never sudden changes. These changes happen slowly over time due to different laws that come in place around how data is handled or protected, the different forms of organizing, the improvements around administration, if any new technology has formed, when the methods of data sorting have changed, along with the methods of sorting the data, the geographical statistics that vary and the new techniques of statistics.&lt;ref&gt;(Kitchin, p.17)&lt;/ref&gt; Not only does the geographical location change how the assemblage of data is taken is, but it can also be different depending on the person due to how they manage the data, or how they produce it.&lt;ref&gt;(Kitchin, p.17)&lt;/ref&gt;

Looking over data temporally can bring forth either questions or patterns depending on what the data is about. An example of this is looking at graphs that have time in them. They present inclines and declines in a pattern about the data over time.&lt;ref&gt;(Whitney)&lt;/ref&gt; Spatial data, on the other hand, looks more towards the geographical sense in the data. The information that is gathered could be about the location, the size or the shape of a particular object. A system that uses spatial data is [[GIS]] (Geographical Information System) &lt;ref&gt;(Rouse)&lt;/ref&gt;

==Philosophical==
[[Philosophy]] brings forth views around the areas of epistemology and ontology. In this view of data, there is no interpretations, opinions, importance, or relevance of the data that has been found and processed. The data is simply measured for what it is. Which brings forth to how it is viewed. The data that is viewed philosophically is also viewed in an objective way which means that the data is fixed in some way to prove a specific point. Although the data may be truthful, how the data was provided and how it is placed makes the difference. The data is also viewed in a realist view such as how things truly are. No information is changed, everything is the way that it is and is seen for that.&lt;ref&gt;(Kitchin, p.17-19)&lt;/ref&gt; This view also brings up issues around property rights.&lt;ref&gt;(Liu, p.61)&lt;/ref&gt; Who would own what and who can have the right to take things.

==References==
{{Reflist|20em}}

===Works cited===
{{refbegin}}
* {{cite book |last=Kitchin |first=Rob |year=2014 |title=The data revolution: Big data, open data, data infrastructures &amp; their consequences |place=London |publisher=Sage |chapter=Conceptualising data |pp=1&#8211;26 |chapter-url=http://www.uk.sagepub.com/upm-data/63923_Kitchin_CH1.pdf |format=pdf}}
* Metcalf, Jacob, Emily F. Keller, and danah boyd. 2016. &#8220;Perspectives on Big Data, Ethics, and Society.&#8221; Council for Big Data, Ethics, and Society.
* Rouse, Margaret. (2013). "What Is Spatial Data? - Definition from WhatIs.com." ''SearchSQLServer''. TechTarget.
* Liu, Hong. (2016). "Philosophical Reflections on Data. " Philosophical Reflections on Data. Science Direct.
* Whitney, Hunter. (2014). "It's About Time." It's About Time: Visualizing Temporal Data to Reveal Patterns and Stories | UX Magazine. UX Magazine.
{{refend}}



[[Category:Data management]]</text>
      <sha1>a2dcdx408ri5n5l3tpmy8lc2y6rotgz</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Computer logging</title>
    <ns>14</ns>
    <id>52734920</id>
    <revision>
      <id>757516453</id>
      <parentid>757515801</parentid>
      <timestamp>2016-12-31T03:16:34Z</timestamp>
      <contributor>
        <username>Fixuture</username>
        <id>19796795</id>
      </contributor>
      <comment>+[[Category:Computing]]; +[[Category:Data management]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="51" xml:space="preserve">[[Category:Computing]]
[[Category:Data management]]</text>
      <sha1>bib0r2low7hskfj7ca4yi0kb8irmsur</sha1>
    </revision>
  </page>
  <page>
    <title>Data profiling</title>
    <ns>0</ns>
    <id>794330</id>
    <revision>
      <id>761897546</id>
      <parentid>761897543</parentid>
      <timestamp>2017-01-25T13:12:33Z</timestamp>
      <contributor>
        <username>ClueBot NG</username>
        <id>13286072</id>
      </contributor>
      <minor />
      <comment>Reverting possible vandalism by [[Special:Contribs/Musicbydma|Musicbydma]] to version by Widr. [[WP:CBFP|Report False Positive?]] Thanks, [[WP:CBNG|ClueBot NG]]. (2909915) (Bot)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7321" xml:space="preserve">{{multiple issues|
{{Expert needed|Mathematics|ex2=Science|talk=Section title goes here|reason=it needs additional citations|date=August 2016}}
{{refimprove article|date=August 2010}}
{{copy edit|for=Incorrect citation formatting|date=August 2016}}
}}

'''Data profiling''' is the process of examining data available from an existing information source (e.g. a [[database]] or a [[computer file|file]]) and collecting [[descriptive statistics|statistics]] or informative summaries about that data.&lt;ref name="Johnson2009"&gt;[Theodore Johnson (2009), "Data Profiling", in Encyclopedia of Database Systems, Springer, Heidelberg]&lt;/ref&gt; The purpose of these statistics may be to:
# Find out whether existing data can easily be used for other purposes
# Improve the ability to search data by [[tag (metadata)|tagging]] it with [[Index term|keywords]], descriptions, or assigning it to a category
# Give [[Software metric|metrics]] on [[data quality]], including whether the data conforms to particular standards or patterns
# Assess the risk involved in [[data integration|integrating data]] in new applications, including the challenges of [[Join (SQL)|join]]s
# Discover [[metadata]] of the source database, including value patterns and [[frequency distribution|distributions]], [[candidate key|key candidates]], [[inclusion dependency|foreign-key candidates]], and [[functional dependency|functional dependencies]]
# Assess whether known metadata accurately describes the actual values in the source database
# Understanding data challenges early in any data intensive project, so that late project surprises are avoided. Finding data problems late in the project can lead to delays and cost overruns.
# Have an enterprise view of all data, for uses such as [[master data management]], where key data is needed, or [[data governance]] for improving data quality.

== Introduction ==

Data profiling refers to the analysis of information for use in a [[data warehouse]] in order to clarify the structure, content, relationships, and derivation rules of the data.&lt;ref name="Kimball2008"&gt;[Ralph Kimball et al. (2008), &#8220;The Data Warehouse Lifecycle Toolkit&#8221;, Second Edition, Wiley Publishing, Inc., ISBN 9780470149775], (p. 297) (p. 376)&lt;/ref&gt; Profiling helps to not only understand anomalies and assess data quality, but also to discover, register, and assess enterprise metadata.&lt;ref name="Loshin2009"&gt;[David Loshin (2009), &#8220;Master Data Management&#8221;, Morgan Kaufmann Publishers, ISBN 9780123742254], (pp. 94&#8211;96)&lt;/ref&gt;&lt;ref name="Loshin2003"&gt;[David Loshin (2003), &#8220;Business Intelligence: The Savvy Manager&#8217;s Guide, Getting Onboard with Emerging IT&#8221;, Morgan Kaufmann Publishers, ISBN 9781558609167], (pp. 110&#8211;111)]&lt;/ref&gt; The result of the analysis is used to determine the suitability of the candidate source systems, usually giving the basis for an early go/no-go decision, and also to identify problems for later solution design.&lt;ref name="Kimball2008"/&gt;

== How Data Profiling is Conducted ==

Data profiling utilizes methods of descriptive statistics such as minimum, maximum, mean, mode, percentile, standard deviation, frequency, variation, aggregates such as count and sum, and additional metadata information obtained during data profiling such as data type, length, discrete values, uniqueness, occurrence of null values, typical string patterns, and abstract type recognition.&lt;ref name="Loshin2009"/&gt;&lt;ref name="Rahm2000"&gt;[Erhard Rahm and Hong Hai Do (2000), &#8220;Data Cleaning: Problems and Current Approaches&#8221; in &#8220;Bulletin of the Technical Committee on Data Engineering&#8221;, IEEE Computer Society, Vol. 23, No. 4, December 2000]&lt;/ref&gt;&lt;ref name="Singh2010"&gt;[Ranjit Singh, Dr Kawaljeet Singh et al. (2010), &#8220;A Descriptive Classification of Causes of Data Quality Problems in Data Warehousing&#8221;, IJCSI International Journal of Computer Science Issue, Vol. 7, Issue 3, No. 2, May 2010]&lt;/ref&gt;
The metadata can then be used to discover problems such as illegal values, misspellings, missing values, varying value representation, and duplicates.

Different analyses are performed for different structural levels. E.g. single columns could be profiled individually to get an understanding of frequency distribution of different values, type, and use of each column. Embedded value dependencies can be exposed in a cross-columns analysis. Finally, overlapping value sets possibly representing foreign key relationships between entities can be explored in an inter-table analysis.&lt;ref name="Loshin2009"/&gt;

Normally, purpose-built tools are used for data profiling to ease the process.&lt;ref name="Kimball2008"/&gt;&lt;ref name="Loshin2009"/&gt;&lt;ref name="Rahm2000"/&gt;&lt;ref name="Singh2010"/&gt;&lt;ref name="Kimball2004"&gt;"[Ralph Kimball (2004), &#8220;Kimball Design Tip #59: Surprising Value of Data Profiling&#8221;, Kimball Group, Number 59, September 14, 2004, (www.rkimball.com/html/designtipsPDF/ KimballDT59 SurprisingValue.pdf)]&lt;/ref&gt;&lt;ref name="Olson2003"&gt;[Jack E. Olson (2003), &#8220;Data Quality: The Accuracy dimension&#8221;, Morgan Kaufmann Publishers], (pp. 140&#8211;142)&lt;/ref&gt; The computation complexity increases when going from single column, to single table, to cross-table structural profiling. Therefore, performance is an evaluation criterion for profiling tools.&lt;ref name="Loshin2003"/&gt;

== When Data Profiling is Conducted ==

According to Kimball,&lt;ref name="Kimball2008"/&gt; data profiling is performed several times and with varying intensity throughout the data warehouse developing process. A light profiling assessment should be undertaken immediately after candidate source systems have been identified and DW/BI business requirements have been satisfied. The purpose of this initial analysis is to clarify at an early stage if the correct data is available at the appropriate detail level and that anomalies can be handled subsequently. If this is not the case the project may be terminated.&lt;ref name="Kimball2008"/&gt;

Addition, more in-depth profiling is done prior to the dimensional modeling process in order assess what is required to convert data into a dimensional model. Detailed profiling extends into the ETL system design process in order to determine the appropriate data to extract and which filters to apply to the data set.&lt;ref name="Kimball2008"/&gt;

Additionally, data may be conducted in the data warehouse development process after data has been loaded into staging, the data marts, etc. Conducting data at these stages helps ensure that data cleaning and transformations have been done correctly and in compliance of requirements.

==Benefits==

The benefits of data profiling are to improve data quality, shorten the implementation cycle of major projects, and improve users' understanding of data.&lt;ref name="Olson2003"/&gt; Discovering business knowledge embedded in data itself is one of the significant benefits derived from data profiling.&lt;ref name="Loshin2003"/&gt; Data profiling is one of the most effective technologies for improving data accuracy in corporate databases.&lt;ref name="Olson2003"/&gt;

==See also==
* [[Data quality]]
* [[Data governance]]
* [[Master data management]]
* [[Database normalization]]
* [[Data visualization]]
* [[Analysis paralysis]]

==References==
{{Reflist}}

{{DEFAULTSORT:Data Profiling}}
[[Category:Data management]]
[[Category:Data quality]]
[[Category:Data analysis]]</text>
      <sha1>oldd1ucprzuvn79vz2a4o24vilevw3o</sha1>
    </revision>
  </page>
  <page>
    <title>DataverseNL</title>
    <ns>0</ns>
    <id>52809243</id>
    <revision>
      <id>760750767</id>
      <parentid>760750633</parentid>
      <timestamp>2017-01-18T21:36:09Z</timestamp>
      <contributor>
        <username>Vtyiisg</username>
        <id>18408392</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3893" xml:space="preserve">'''DataverseNL''' is data management service built on top of [[Dataverse]] data repository and jointly offered by participating institutions to the research community of the Netherlands. It's developed by the Dataverse Team at the Institute for Quantitative Social Science ([[IQSS]]) at [[Harvard University]] and [[Data Archiving and Networked Services]] (DANS) and housed and maintained by DANS team.

With DataverseNL, researchers and lecturers can store, share and register research data online, both during research and for up to ten years afterwards. Dataverse platform is used worldwide. In the Netherlands, DataverseNL was installed at the Universiteit Utrecht in 2010, after which it developed into a shared service of over other institutions. The data management is in the hands of the institutions; DANS has been managing the network since May 2014.

As of January 2017, DataverseNL offers access to more than 300 published studies.

== Persistent identifier ==
At DataverseNL, each deposited dataset receives its own persistent identifier that allows information to remain accessible in the long term, even if its location changes. [[DANS]] has developed this functionality for Dataverse repository to use handle system for its persistent identifiers.

== Open access ==
Besides storing data, DataverseNL allows you to share them with other scientists. Researchers themselves determine who gets access to which materials and what their access rights are (user, contributor or curator). Researchers may choose any license for their data, including [[CC0]] (CC Zero Waiver) or [[ODBL]] (Open Database License).

== How to use DataverseNL ==
'''Depositing data'''&lt;br /&gt;
'''1''' Check to see if your university or institution is participating in DataverseNL.&lt;br /&gt;
'''2''' Prepare your data: select the relevant data files and check for any privacy-sensitive aspects. &lt;br /&gt;
'''3''' Log in at https://dataverse.nl. New users must first sign up for an account.&lt;br /&gt;
'''4''' Upload your data to studies within your dataverse.&lt;br /&gt;
'''5''' Describe your data and determine who gets access. &lt;br /&gt;
'''6''' Share the data by publishing them or allowing others access to your dataverse or studies.&lt;br /&gt;

'''Downloading data'''&lt;br /&gt;
'''1''' Search or browse, and if necessary refine your search results, until you have found the dataset you are looking for at https://dataverse.nl. &lt;br /&gt;
'''2''' Look at the metadata to determine if the dataset meets your requirements. &lt;br /&gt;
'''3''' Depending on the access category, you can go to the Data &amp; Analysis tab and download the data. &lt;br /&gt;
'''4''' Check the &#8216;Data citation&#8217; header to see the correct method for citing the data. &lt;br /&gt;

== Participating institutions ==
DataverseNL is a shared service provided by the participating institutions and DANS. DANS performs back office tasks, including server and software maintenance and administrative support.
The participating institutions are responsible for managing the deposited data.

At the moment there are following participating institutions:
* [[4TU]] Data lab
*[[ 4TU.Centre for Research Data]]
* [[Tilburg University]]
* [[TiU]]
* [[Universiteit Utrecht]]
* [[Vrije Universiteit Amsterdam]]
* [[Maastricht University]]
* [[Protestantse Theologische Universiteit]]
* [[Avans Hogeschool]]
* [[Nederlands Instituut voor Ecologie]]
* [[Rijksuniversiteit Groningen]]
* [[Erasmus University Rotterdam]]
* [[Hogeschool Windesheim]]

==External links==
*[https://www.dataverse.nl The DataverseNL Repository (Netherlands)]
*[https://www.dans.knaw.nl Data Archiving and Networked Services]
*[http://dataverse.org/ The Dataverse Project]

[[Category:Data management]]
[[Category:Open science]]
[[Category:Open data]]
[[Category:Open-access archives]]
[[Category:Open access (publishing)]]
[[Category:Academic publishing]]
[[Category:Data publishing]]
[[Category:Scholarly communication]]</text>
      <sha1>b24iwr7uvs9bxll40ln2xyi68n3o2v5</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Audio storage</title>
    <ns>14</ns>
    <id>754789</id>
    <revision>
      <id>547353504</id>
      <parentid>531605223</parentid>
      <timestamp>2013-03-28T00:08:39Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor />
      <comment>[[User:Addbot|Bot:]] Migrating 17 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q8275551]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="229" xml:space="preserve">{{Commons category|Audio storage media}}
{{Cat main|Sound recording and reproduction}}

[[Category:Storage media]]
[[Category:Electronic documents]]
[[Category:Sound recording technology]]
[[Category:Sound production technology]]</text>
      <sha1>9gvkbkbt5f86x1559ielb35u2d9rhjv</sha1>
    </revision>
  </page>
  <page>
    <title>Information capture</title>
    <ns>0</ns>
    <id>3547364</id>
    <revision>
      <id>727453358</id>
      <parentid>674118710</parentid>
      <timestamp>2016-06-29T02:45:30Z</timestamp>
      <contributor>
        <username>I dream of horses</username>
        <id>9676078</id>
      </contributor>
      <minor />
      <comment>clean up using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="513" xml:space="preserve">{{refimprove|date=June 2016}}
'''Information capture''' is the process of collecting paper [[documents]], [[form (document)|form]]s and e-documents, transforming them into accurate, retrievable, [[Digital data|digital]] information, and delivering the information into business applications and [[databases]] for immediate action.&lt;ref&gt;http://www.emc.com/collateral/advertorial/aiim-advertorial.pdf&lt;/ref&gt;

==See also==
* [[Ibml]]

==References==
&lt;references/&gt;

[[Category:Electronic documents]]


{{database-stub}}</text>
      <sha1>scw4p1kmvdm36r8epblg3g8ss5i6yru</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Wave</title>
    <ns>0</ns>
    <id>22992426</id>
    <revision>
      <id>759216965</id>
      <parentid>755996669</parentid>
      <timestamp>2017-01-09T22:05:30Z</timestamp>
      <contributor>
        <username>Sheridan</username>
        <id>33042</id>
      </contributor>
      <comment>/* End of development of original Google Wave under Google in 2010 */ took out superfluous words of title</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="27456" xml:space="preserve">{{Infobox software
|name                       = Apache Wave
|logo                       = Apache Wave logo.png
|screenshot                 = Google Wave.png
|caption                    = Google Wave, the previous incarnation of Apache Wave
|collapsible                = 
|author                     = [[Google]]
|developer                  = [[Apache Software Foundation]], Google
|released                   = {{start date|2009|5|27}}
|latest release version     =
|latest release date        = &lt;!-- {{Start date and age|YYYY|MM|DD}} --&gt;
|latest preview version     =
|latest preview date        = &lt;!-- {{Start date and age|YYYY|MM|DD}} --&gt;
|frequently updated         =
|programming language       = [[Java (programming language)|Java]]&lt;ref&gt;{{cite web |url=http://www.lextrait.com/Vincent/implementations.html |title=The Programming Languages Beacon, v10.0 |first=Vincent |last =Lextrait |date=January 2010 |accessdate=14 March 2010}}&lt;/ref&gt;
|operating system           =
|platform                   = [[Web application]]
|size                       =
|language                   =
|status                     =
|genre                      = [[Collaborative real-time editor]]
|license                    = [[Apache License]]
|website                    = {{URL|incubator.apache.org/wave/}}
|repo                       = {{URL|https://git-wip-us.apache.org/repos/asf/incubator-wave.git}}
}}
'''Apache Wave''' is a software framework for [[Collaborative real-time editor|real-time collaborative editing]] online. [[Google]] originally developed it as '''Google Wave'''.&lt;ref&gt;{{cite web|url=http://wave.google.com/about.html
|title=Google Wave Overview|author=Google Inc.
|quote=[A] new web application for real-time communication and collaboration.
|year=2009|accessdate=May 2010| archiveurl= https://web.archive.org/web/20100427183005/http://wave.google.com/about.html| archivedate= 27 April 2010 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt;
It was announced at the [[Google I/O]] conference on May 27, 2009.&lt;ref&gt;[[TechCrunch]] (May 28, 2009):
[http://www.techcrunch.com/2009/05/28/google-wave-drips-with-ambition-can-it-fulfill-googles-grand-web-vision/ Google Wave Drips With Ambition.  A New Communication Platform For A New Web.]&lt;/ref&gt;&lt;ref name="iokeynote"&gt;{{cite web
|url=https://www.youtube.com/watch?v=v_UyVmITiYQ
|title=I/O Conference Google Wave Keynote|author=Google Inc.}}&lt;/ref&gt;

Wave is a [[web application|web-based]] [[computing platform]] and [[communications protocol]] designed to merge key features of [[Media (communication)|communications media]] such as [[email]], [[instant messaging]], [[wiki]]s, and [[Social networking service|social networking]].&lt;ref name="aboutgw"&gt;{{cite web
|url=http://wave.google.com/help/wave/about.html|title=About Google Wave|author=Google Inc.}}&lt;/ref&gt; Communications using the system can be [[Synchronization|synchronous]] or [[Asynchronous communication#Electronically mediated communication|asynchronous]]. Software extensions provide contextual [[spell checker|spelling and grammar checking]], [[machine translation|automated language translation]]&lt;ref name="iokeynote" /&gt; and other features.&lt;ref name="gwdevblog"&gt;{{cite web|url=http://googlewavedev.blogspot.com/2009/05/introducing-google-wave-apis-what-can.html|title=Google Wave Developer Blog|publisher=Google}}&lt;/ref&gt;

Initially released only to developers, a preview release of Google Wave was extended to 100,000 users in September 2009, each allowed to invite additional users. Google accepted most requests submitted starting November 29, 2009, soon after the September extended release of the technical preview. On May 19, 2010, it was released to the general public.&lt;ref&gt;Shankland, Stephen. (2010-05-19) [http://news.cnet.com/8301-30685_3-20005394-264.html Google Wave: Now open to the public | Deep Tech &#8211; CNET News]. News.cnet.com. Retrieved on 2010-12-14.&lt;/ref&gt;

On August 4, 2010, Google announced the suspension of stand-alone Wave development and the intent of maintaining the web site at least for the remainder of the year,&lt;ref&gt;[http://googleblog.blogspot.com/2010/08/update-on-google-wave.html Official Google Blog: Update on Google Wave]. Googleblog.blogspot.com (2010-04-08). Retrieved on 2010-12-14.&lt;/ref&gt; and on November 22, 2011, announced that existing Waves would become read-only in January 2012 and all Waves would be deleted in April 2012.&lt;ref&gt;{{cite web|url=http://googleblog.blogspot.com/2011/11/more-spring-cleaning-out-of-season.html |title=Official Blog: More spring cleaning out of season |publisher=Googleblog.blogspot.com |date=2011-11-22 |accessdate=2013-06-15}}&lt;/ref&gt; Development was handed over to the [[Apache Software Foundation]] which started to develop a server-based product called '''Wave in a Box'''.&lt;ref&gt;Meyer, David. (2010-09-03) [http://www.zdnet.co.uk/news/application-development/2010/09/03/google-puts-open-source-wave-in-a-box-40089999/ Google puts open-source Wave in a 'box' | Application Development | ZDNet UK]. Zdnet.co.uk. Retrieved on 2010-12-14.&lt;/ref&gt;&lt;ref&gt;[http://www.idg.se/2.1085/1.355483/google-wave-inte-ute-ur-leken Google Wave inte ute ur leken]. IDG.se. Retrieved on 2010-12-14.&lt;/ref&gt;&lt;ref&gt;Murphy, David. (1970-01-01) [http://www.pcmag.com/article2/0,2817,2368730,00.asp Google Spins Wave Into 'Wave in a Box' for Third-Party Use | News &amp; Opinion]. PCMag.com. Retrieved on 2010-12-14.&lt;/ref&gt;

==History==
[[File:Googlewave.svg|thumb|upright|The original logo while owned by Google]]

===Origin of name===
The science fiction television series ''[[Firefly (TV series)|Firefly]]'' provided the inspiration for the project's name.&lt;ref name=itnewsau&gt;{{cite news
|first=Nate
|last=Cochrane
|url=http://www.itnews.com.au/News/104396,opinion-googles-wave-drowns-the-bling-in-microsofts-bing.aspx
|title=Opinion: Google's wave drowns the bling in Microsoft's Bing
|agency=IT News Australia
|date=2009-05-29
|accessdate=2009-06-03
| archiveurl= https://web.archive.org/web/20090603041903/http://www.itnews.com.au/News/104396,opinion-googles-wave-drowns-the-bling-in-microsofts-bing.aspx| archivedate= 3 June 2009 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt; In the series, a ''wave'' is an electronic communication, often consisting of a [[video call]] or video message.&lt;ref name=itnewsau/&gt;  During the developer preview, a number of references were made to the series, such as [[Lars Rasmussen (Software Developer)|Lars Rasmussen]] replying to a message with "shiny", a word used in the series to mean ''cool'' or ''good'', and the crash message of Wave being a popular quotation from the series: "Curse your sudden but inevitable betrayal!"&lt;ref name="iokeynote" /&gt;&lt;ref&gt;Originally said by [[List of characters in the Firefly universe#Hoban Washburne|Wash]] at 6:36, in ''[[Serenity (Firefly episode)|Serenity]]''; [[Firefly (TV series)|Firefly]]: The Complete Series (Blu-ray), 2008, 20th Century Fox.&lt;/ref&gt; Another common error message, "Everything's shiny, Cap'n. Not to fret!" is a quote from [[Kaylee Frye]] in the 2005 motion-picture ''Firefly'' continuation, ''[[Serenity (film)|Serenity]]'', and it is matched with a sign declaring that "This wave is experiencing some turbulence and might explode. If you don't want to explode..." which is another reference to the opening of the film.

During an event in [[Amsterdam]], [[Netherlands]],&lt;ref name="nextweb"&gt;{{cite news | first = Ralf | last = Rottmann | url = http://thenextweb.com/appetite/2009/10/30/breaking-google-wave-opened-federation-today-host/ | title = Google Wave to be opened for federation today! | agency = The Next Web | date = October 30, 2009}}&lt;/ref&gt; it became apparent that the 60-strong team that was currently working on Wave in [[Sydney, Australia]] use  [[Joss Whedon]]-related references to describe, among others, the sandbox version of Wave called ''[[Dollhouse (TV series)|Dollhouse]]'' after the TV-series by ''Firefly'' producer Joss Whedon, which was aired on Fox in the U.S. The development of external extensions is codenamed "Serenity", after the spaceship used in ''Firefly'' and ''Serenity''.

===Open source===
Google released most of the source code as [[open source software]],&lt;ref name="iokeynote"/&gt; allowing the public to develop its features through extensions.&lt;ref name="iokeynote" /&gt;  Google allowed third parties to build their own Wave services (be it private or commercial) because it wanted the [[Google Wave Federation Protocol|Wave protocol]] to replace the [[e-mail]] protocol.&lt;ref name="iokeynote"/&gt;&lt;ref name="gwarchitecture" /&gt;&lt;ref name="wpcsmodel" /&gt; Initially, Google was the only Wave service provider, but it was hoped that other service providers would launch their own Wave services, possibly designing their own unique web-based clients as is common with many email service providers.  The possibility also existed for native Wave clients to be made, as demonstrated with their [[command-line interface|CLI]]-based console client.&lt;ref name="osreleasenext"&gt;{{cite web|url=https://groups.google.com/group/wave-protocol/browse_thread/thread/618ff4e9ef477e80?pli=1|title=Google Wave Federation Protocol and Open Source Updates|publisher=Google}}&lt;/ref&gt;

Google released initial open-source components of Wave:&lt;ref name="osrelease1"&gt;{{cite web|url=http://googlewavedev.blogspot.com/2009/07/google-wave-federation-protocol-and.html|title=Google Wave Federation Protocol and Open Source Updates|publisher=Google}}&lt;/ref&gt;
# the [[operational transformation]] (OT) code,
# the underlying wave model, and
# a basic client/server prototype that uses the wave protocol

In addition, Google provided some detail about later phases of the open-source release:&lt;ref name="osreleasenext" /&gt;
# wave model code that is a simplified version of Google's production code and is tied to the OT code; this code will evolve into the shared code base that Google will use and expects that others will too
# a testing and verification suite for people who want to do their own implementation (for example, for porting the code to other languages)

===Reception===
{{Wikinews|Google to discontinue social networking application Google Wave}}
During the initial launch of Google Wave, invitations were widely sought by users and were sold on auction sites.&lt;ref&gt;[http://mashable.com/2009/09/30/google-wave-invite/ Google Wave Invite Selling for $70 on eBay]&lt;/ref&gt;&lt;!--  However, people were confused as to how to use it.&lt;ref&gt;[http://www.csmonitor.com/Innovation/Horizons/2009/1124/so-youve-got-google-wave-now-what Christian Science Monitor: So you've got Google wave, now what?]&lt;/ref&gt; Google Wave was called an "over-hyped disappointment for the first generation of users"&lt;ref&gt;[http://www.linux-mag.com/id/7653 Linux Mag 2009 Review]&lt;/ref&gt; with "dismal usability"&lt;ref&gt;[http://themilwaukeeseo.com/2009/12/14/the-google-wave-failure/ Google Wave Failure on Milwaukee SEO]&lt;/ref&gt; that "humans may not be able to comprehend."&lt;ref name="EngagetWave"&gt;[http://www.engadget.com/2009/10/27/google-wave-to-have-its-own-app-store/ Google Wave to get its own App Store (Engadget)]&lt;/ref&gt; --&gt;
Those who received invitations and decided to test Google Wave could not communicate with their contacts on their regular email accounts. The initial spread of Wave was very restricted.

===End of development of ''Google Wave''===
Google Wave initially received positive press coverage for its design&lt;ref&gt;[http://news.bbc.co.uk/1/hi/technology/8282687.stm B.B.C. report introducing Google Wave in September 2009]&lt;/ref&gt; and potential uses.&lt;ref name="EngagetWave"&gt;[http://www.engadget.com/2009/10/27/google-wave-to-have-its-own-app-store/ Google Wave to get its own App Store (Engadget)]&lt;/ref&gt;&lt;ref&gt;[http://asia.cnet.com/blogs/tokyo-shift/post.htm?id=63015591 CNET Predictions for 2010]&lt;/ref&gt; On August 4, 2010, Google announced Wave would no longer be developed as a stand-alone product due to a lack of interest.&lt;ref name="ZDNet on GW's death"&gt;[http://www.zdnet.com/blog/google/how-will-google-wave-be-reincarnated/2344 ZDNet on GW's death]&lt;/ref&gt; Google's statement surprised many in the industry and user community.

Google later clarified the Wave service would be available until [[Google Docs]] was capable of accessing saved waves.&lt;ref&gt;{{cite web|url=http://www.google.com/support/wave/bin/answer.py?answer=1083134 |title=Status of Google Wave - Google Help |publisher=Google.com |date= |accessdate=2013-06-15}}&lt;/ref&gt;

Response to the news of the end of development came from Wave users in the form of a website.&lt;ref&gt;[http://www.webpronews.com/topnews/2010/08/09/save-google-wave-site-forms '"Save Google Wave" Site Forms']&lt;/ref&gt; Since their announcement in early August, the website has recorded over 49,000 supporter registrations urging Google Wave's continuation.&lt;ref&gt;[http://www.savegooglewave.com Save Google Wave!]. Retrieved on 2011-05-14.&lt;/ref&gt;

In retrospect, the lack of success of Google Wave was attributed among other things to its complicated user interface resulting in a product that merged features of email, instant messengers and wikis but ultimately failed to do anything significantly better than the existing solutions.&lt;ref&gt;[http://arstechnica.com/software/news/2010/08/google-wave-why-we-didnt-use-it.ars Google Wave: why we didn't use it], [[Ars Technica]]&lt;/ref&gt;

Chris Dawson of online technology magazine [[Zdnet]] discussed inconsistencies in the reasoning of Google in deciding to end support for Wave,&lt;ref name="ZDNet on GW's death"/&gt; mentioning its "deep involvement" in developing social media networks, to which many of Wave's capabilities are ideally suited. Perhaps Google Wave was ended to clear the stage for their new social network [[Google+]] that tried to compete with Facebook but has not gained the reach as a similar comprehensive social networking site.&lt;ref&gt;[http://www.utalkmarketing.com/pages/Article.aspx?ArticleID=21768&amp;Title=Can_Google+_really_challenge_Facebook_and_be_an_asset_to_brands "Can Google+ really challenge Facebook and be an asset to brands?" utalkmarketing.com]&lt;/ref&gt;

===Apache Wave===
Google Wave was accepted by the [[Apache Software Foundation]]'s Incubator program under the project name Apache Wave. The Google Wave Developer blog was updated with news of the change on December 6, 2010.&lt;ref&gt;North, Alex. (2010-12-06) [http://googlewavedev.blogspot.com/2010/12/introducing-apache-wave.html Google Wave Developer Blog: Introducing Apache Wave]. Googlewavedev.blogspot.com. Retrieved on 2010-12-14.&lt;/ref&gt; A Wave Proposal page with details on the project's goals was created on the Apache Foundation's Incubator Wiki.&lt;ref&gt;[http://wiki.apache.org/incubator/WaveProposal WaveProposal &#8211; Incubator Wiki]. Wiki.apache.org (2010-11-24). Retrieved on 2010-12-14.&lt;/ref&gt;

====Wave in a Box====
[[File:Wave in a Box logo.png|thumb|75px|The logo for Wave in a Box]]
Wave in a Box is the current server implementation of Apache Wave.  Currently, there are not any demo servers available.&lt;ref name=demo_servers&gt;{{cite web|title=Wave in a Box demo servers|url=http://incubator.apache.org/wave/demo-servers.html|publisher=Apache Software Foundation|accessdate=10 October 2012}}&lt;/ref&gt;

==Features==
Google Wave was a new [[Internet]] [[communications]] platform. It was written in [[Java (programming language)|Java]] using [[OpenJDK]] and its web interface used the [[Google Web Toolkit]]. Google Wave works like previous messaging systems such as [[email]] and [[Usenet]], but instead of sending a message along with its entire thread of previous messages, or requiring all responses to be stored in each user's inbox for context, message documents (referred to as ''waves'') that contain complete threads of multimedia messages (blips) are perpetually stored on a central server. Waves are shared with collaborators who can be added or removed from the wave at any point during a wave's existence.

Waves, described by Google as "''equal parts conversation and document''", are hosted [[XML]] documents that allow seamless and low latency concurrent modifications.&lt;ref name="ot"&gt;[http://www.waveprotocol.org/whitepapers/operational-transform Google Wave Operational Transformation &#8211; Google Wave Federation Protocol]. Waveprotocol.org. Retrieved on 2010-12-14.&lt;/ref&gt; Any participant of a wave can reply anywhere within the message, edit any part of the wave, and add participants at any point in the process. Each edit/reply is a blip and users can reply to individual blips within waves. Recipients are notified of changes/replies in all waves in which they are active and, upon opening a wave, may review those changes in chronological order. In addition, waves are live. All replies/edits are visible in real-time, letter-by-letter, as they are typed by the other collaborators. Multiple participants may edit a single wave simultaneously in Google Wave. Thus, waves can function not only as e-mails and [[threaded discussion|threaded conversations]] but also as an [[instant messaging]] service when many participants are online at the same time. A wave may repeatedly shift roles between e-mail and instant messaging depending on the number of users editing it concurrently. The ability to show messages as they are typed can be disabled, similar to conventional instant messaging.&lt;ref name="aboutgw"/&gt;

The ability to modify a wave at any location lets users create collaborative documents, [[collaborative editing|edited]] in a manner akin to [[wiki]]s. Waves can easily link to other waves. In many respects, it is a more advanced forum.&lt;ref&gt;[http://variableghz.com/2009/10/google-wave-review/ Google Wave Review]. VariableGHz (2009-10-13). Retrieved on 2010-12-14.&lt;/ref&gt; It can be read and known to exist by only one person, or by two or more and can also be public, available for reading ''and'' writing to everyone on the Wave.

The history of each wave is stored within it. Collaborators may use a playback feature to observe the order in which it was edited, blips that were added, and who was responsible for what in the wave.&lt;ref name="aboutgw"/&gt;&lt;ref name="gwdevblog"/&gt; The history may also be searched by a user to view and/or modify specific changes, such as specific kinds of changes or messages from a single user.&lt;ref name="iokeynote" /&gt;

==Extension programming interface==
{{anchor|Google Wave extensions}}
Google Wave is extensible through an [[application programming interface]] (API). It provides extensions in the form of ''Gadgets'' and ''Robots'', and is embeddable by dropping interactive windows into a given wave on external sites, such as [[blog]] sites.&lt;ref name="iokeynote" /&gt;&lt;ref name="codepage" /&gt;

The last version of robots API is 2.0.&lt;ref name="robotsapiv2"&gt;{{cite web|url=http://googlewavedev.blogspot.com/2010/03/introducing-robots-api-v2-rise-of.html|title=Introducing Robots API v2: The Rise of Active Robots|publisher=Google}}&lt;/ref&gt;

Google Wave also supports extension installers, which bundle back-end elements (robots and gadgets) and front-end user interface elements into an integrated package. Users may install extensions directly within the Wave client using an extension installer.

===Extensions===
Google Wave extensions are [[Plug-in (computing)|add-ins]] that may be installed on Google Wave to enhance its functionality. They may be [[Internet bot]]s (robots) to automate common tasks, or gadgets to extend or change user interaction features, e.g., posting blips on [[microblog]] feeds or providing RSVP recording mechanisms.&lt;ref name="iokeynote" /&gt;&lt;ref name="aboutgw" /&gt;&lt;ref name="codepage"&gt;{{cite web|url=https://code.google.com/apis/wave/|title=Google Wave API &#8211; Google Code|publisher=Google}}&lt;/ref&gt;

Over 150 Google Wave extensions have been developed either in the form of Gadgets or Robots.&lt;ref&gt;[http://wave-samples-gallery.appspot.com/ Google Wave Samples Gallery]. Wave-samples-gallery.appspot.com. Retrieved on 2010-12-14.&lt;/ref&gt;

====Robots====
A robot is an automated participant on a wave. It can read the contents of a wave in which it participates, modify its contents, add or remove participants, and create new blips or new waves. Robots perform actions in response to events. For example, a robot might publish the contents of a wave to a public [[blog]] site and update the wave with user comments.

Robots may be added as participants to the Wave itself. In theory, a robot can be added anywhere a human participant can be involved.

====Gadgets====
Gadget extensions are applications that run within the wave, and to which all participants have access. Robots and Gadgets can be used together, but they generally serve different purposes. A gadget is an application users could participate with, many of which are built on Google&#8217;s [[OpenSocial]] platform. A good comparison would be iGoogle gadgets or Facebook applications.

The gadget is triggered based on the user action. They can be best described as applications installed on a mobile phone. For example, a wave might include a [[sudoku]] gadget that lets the wave participants compete to see who can solve the puzzle first.

Gadgets may be added to individual waves and all the participants share and interact with the gadget.

==Federation protocol==
{{Main article|Google Wave Federation Protocol}}
Google Wave provides [[Federation (information technology)|federation]] using an extension of [[XMPP|Extensible Messaging and Presence Protocol]] (XMPP), the [[open standard|open]] [[Google Wave Federation Protocol|Wave Federation Protocol]]. Being an open protocol, anyone can use it to build a custom Wave system and become a wave provider.&lt;ref&gt;{{cite web|url=http://www.waveprotocol.org/|title=Google Wave Federation Protocol|publisher=Google}}&lt;/ref&gt;  The use of an open protocol is intended to parallel the openness and ease of adoption of the [[e-mail]] protocol and, like e-mail, allow communication regardless of provider. Google hoped that waves would replace e-mail as the dominant form of Internet communication.&lt;ref name="iokeynote"/&gt;&lt;ref name="gwarchitecture"&gt;[http://www.waveprotocol.org/whitepapers/google-wave-architecture Google Wave Federation Architecture &#8211; Google Wave Federation Protocol]. Waveprotocol.org. Retrieved on 2010-12-14.&lt;/ref&gt;&lt;ref name="wpcsmodel"&gt;[http://www.waveprotocol.org/whitepapers/internal-client-server-protocol Google Wave Client-Server Protocol &#8211; Google Wave Federation Protocol]. Waveprotocol.org. Retrieved on 2010-12-14.&lt;/ref&gt;  In this way, Google intended to be only one of many wave providers&lt;ref name="iokeynote"/&gt;&lt;ref name="gwarchitecture" /&gt;&lt;ref name="wpcsmodel" /&gt;  and to also be used as a supplement to [[e-mail]], [[instant messaging]], [[FTP]], etc.

A key feature of the protocol is that waves are stored on the service provider's servers instead of being sent between users. Waves are federated; copies of waves and wavelets are distributed by the wave provider of the originating user to the providers of all other participants in a particular wave or wavelet so all participants have immediate access to up-to-date content. The originating wave server is responsible for hosting, processing, and concurrency control of waves.&lt;ref name="gwarchitecture" /&gt;&lt;ref name="wpcsmodel" /&gt;  The protocol allows private reply wavelets within parent waves, where other participants have no access or knowledge of them.&lt;ref name="gwarchitecture" /&gt;&lt;ref name="wpcsmodel" /&gt;

Security for the communications is provided via [[Transport Layer Security]] authentication, and encrypted connections and waves/wavelets are identified uniquely by a service provider's [[domain name]] and ID strings. User-data is not federated, that is, not shared with other wave providers.

===Adoption of Wave Protocol and Wave Federation Protocol===
Besides Apache Wave itself, there are other open-source variants of servers and clients with different percentage of Wave Federation and Wave Protocol support. Wave has been adopted for corporate applications by Novell for [[Novell Pulse]],&lt;ref&gt;[http://www.novell.com/products/pulse/ Novell Vibe cloud service]. Novell.com. Retrieved on 2010-12-14.&lt;/ref&gt; or by [[SAP AG|SAP]] for Cloudave,&lt;ref&gt;Elliott, Timo. (2009-10-19) [http://www.cloudave.com/link/sap-gravity-prototype-business-collaboration-using-google-wave SAP's Gravity Prototype: Business Collaboration Using Google Wave]. Cloudave.com. Retrieved on 2010-12-14.&lt;/ref&gt; and community projects such as PyOfWave or [[Kune (software)|Kune]].

====Compatible third-party servers====
The following servers are compatible with the Google Wave protocol:
* '''[[Kune (software)|Kune]]'''&lt;ref&gt;{{cite web|title=Kune Homepage|url=http://kune.ourproject.org|accessdate=22 April 2012}}&lt;/ref&gt; is a free/open source platform for social networking, collaborative work and web publishing, focusing on work groups and organizations rather than in individuals. It provides lists, tasks, documents, galleries, etc., while using waves underneath. It focuses on [[free culture movement|free culture]] and [[social movements]] needs.
* '''[[Novell Vibe]]''', formerly known as Novell Pulse&lt;ref&gt;[http://www.novell.com/promo/vibe.html Novell Vibe]. Novell.com (2009-12-31). Retrieved on 2010-12-14.&lt;/ref&gt;
* '''Rizzoma'''&lt;ref&gt;{{cite web|title=Rizzoma Homepage|url=http://rizzoma.com|accessdate=9 May 2012}}&lt;/ref&gt; is a platform for collaborative work in real time. It allows communication within a certain context permitting a chat to instantly become a document where topics of a discussion organized into branches of mind-map diagram and minor details are collapsed to avoid distraction. The user is able to sign in using a Google or Facebook account and choose whether your topics are private or public.
* '''[[SAP StreamWork]]''' is a collaboration decision making service.&lt;ref&gt;Williams, Alex. (2010-05-17) [http://www.readwriteweb.com/cloud/2010/05/sap-streamworks-integrates-wit.php SAP StreamWork Integrates With Google Wave &#8211; ReadWriteCloud]. Readwriteweb.com. Retrieved on 2010-12-14.&lt;/ref&gt;&lt;ref&gt;[http://www.sapstreamwork.com/how-it-works/ How It Works | SAP&#174; StreamWork&#8482;]. Sapstreamwork.com. Retrieved on 2010-12-14.&lt;/ref&gt;

==See also==
{{Portal|Software}}
* [[Microsoft Sharepoint Workspace]]
* [[Real-time text]]
* [[Opera Unite]]

==References==
{{Reflist|colwidth=30em}}

==External links==
{{Commons category|Google Wave}}
* [http://incubator.apache.org/wave/ Apache Wave]
* [http://incubator.apache.org/wave/demo-servers.html Wave in a Box]
* [http://wave.google.com/ Google Wave]
* [https://code.google.com/apis/wave/ Google Wave API]
* [http://googlewavedev.blogspot.com/ Google Wave Developer Blog]
* [https://www.youtube.com/watch?v=v_UyVmITiYQ Full Video of the Developer Preview at Google IO on ]
* [https://www.youtube.com/watch?v=p6pgxLaDdQw Google Wave overview video]
* [http://www.waveprotocol.org/ Google Wave Federation Protocol]

{{Google Inc.}}
{{Apache}}

[[Category:Discontinued Google services|Wave]]
[[Category:Web applications]]
[[Category:Computing platforms]]
[[Category:Electronic documents]]
[[Category:Instant messaging]]
[[Category:Online chat]]
[[Category:Social information processing]]
[[Category:Groupware]]
[[Category:Wikis]]
[[Category:Internet protocols]]
[[Category:Internet Protocol based network software]]
[[Category:Self-organization]]
[[Category:Blogging]]
[[Category:Collaborative real-time editors]]
[[Category:2009 software]]
[[Category:2010 disestablishments]]
[[Category:Discontinued software]]
[[Category:Discontinued Google software]]
[[Category:Software using the Apache license]]
[[Category:Social networking services]]
[[Category:Apache Software Foundation|Wave]]</text>
      <sha1>1jmppoa6x7mlpfo6ztelmzjwcz5fi5g</sha1>
    </revision>
  </page>
  <page>
    <title>Bibcode</title>
    <ns>0</ns>
    <id>14092434</id>
    <revision>
      <id>751428378</id>
      <parentid>751427529</parentid>
      <timestamp>2016-11-25T16:39:07Z</timestamp>
      <contributor>
        <username>Modest Genius</username>
        <id>593712</id>
      </contributor>
      <comment>/* top */ +infobox (not much in it yet)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5685" xml:space="preserve">{{selfref|For the Wikipedia template to link to bibcoded articles, see [[Template:bibcode]]}}
{{Infobox identifier
| name          = Bibcode
| image         = 
| image_size    = 
| image_caption = 
| image_alt     = 
| image_border  = 
| full_name     = Bibliographic code
| acronym       = 
| number        = 
| start_date    = 1990s&lt;!-- {{Start date|YYYY|MM|DD|df=y}} --&gt;
| organisation  = 
| digits        = 19
| check_digit   = none
| example       = 1924MNRAS..84..308E
| website       = &lt;!-- {{URL|example.org}} --&gt;
}}
The '''bibcode''' (also known as the "refcode") is a compact identifier used by several [[astronomy|astronomical]] data systems to uniquely specify literature references.

== Adoption ==
The Bibliographic Reference Code (REFCODE) was originally developed to be used in [[SIMBAD]] and the [[NASA/IPAC Extragalactic Database]] (NED), but it became a de facto standard and is now used more widely, for example, by the NASA [[Astrophysics Data System]] who coined and prefer the term "bibcode".&lt;ref name=a&gt;{{ cite book| url=http://cdsweb.u-strasbg.fr/simbad/refcode/refcode-paper.html| chapter= NED and SIMBAD Conventions for Bibliographic Reference Coding| title=Information &amp; On-Line Data in Astronomy|editor= Daniel Egret|editor2= Miguel A. Albrecht|publisher= Kluwer Academic Publishers|date=1995|isbn =0-7923-3659-3|author= M. Schmitz|author2= G. Helou|author3= P. Dubois|author4= C. LaGue|author5= B.F. Madore|author6= H. G. Corwin Jr.|author7= S. Lesteven|last-author-amp= yes|accessdate= 2011-06-22| archiveurl= https://web.archive.org/web/20110607153038/http://cdsweb.u-strasbg.fr/simbad/refcode/refcode-paper.html| archivedate= 7 June 2011 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt;&lt;ref name=b&gt;{{cite web|url=http://adsabs.harvard.edu/abs_doc/help_pages/data.html|title= The ADS Data, help page|publisher= NASA ADS |accessdate=November 5, 2007| archiveurl= https://web.archive.org/web/20071014195855/http://adsabs.harvard.edu/abs_doc/help_pages/data.html| archivedate= 14 October 2007 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt;

== Format ==
The code has a fixed length of 19 characters and has the form
&lt;center&gt;&lt;tt&gt;YYYYJJJJJVVVVMPPPPA&lt;/tt&gt;&lt;/center&gt;
where &lt;tt&gt;YYYY&lt;/tt&gt; is the four-digit year of the reference and &lt;tt&gt;JJJJJ&lt;/tt&gt; is a code indicating where the reference was published. In the case of a journal reference, &lt;tt&gt;VVVV&lt;/tt&gt; is the volume number, &lt;tt&gt;M&lt;/tt&gt; indicates the section of the journal where the reference was published (e.g., &lt;tt&gt;L&lt;/tt&gt; for a letters section), &lt;tt&gt;PPPP&lt;/tt&gt; gives the starting page number, and &lt;tt&gt;A&lt;/tt&gt; is the first letter of the last name of the first author. Periods (&lt;tt&gt;.&lt;/tt&gt;) are used to fill unused fields and to pad fields out to their fixed length if too short; padding is done on the right for the publication code and on the left for the volume number and page number.&lt;ref name=a /&gt;&lt;ref name=b /&gt; Page numbers greater than 9999 are continued in the &lt;tt&gt;M&lt;/tt&gt; column. The 6-digit article ID numbers (in lieu of page numbers) used by the Physical Review publications since the late 1990s are treated as follows: The first two digits of the article ID, corresponding to the issue number, are converted to a lower-case letter (01 = a etc.) and inserted into column &lt;tt&gt;M&lt;/tt&gt;. The remaining four digits are used in the page field.&lt;ref name=b /&gt;

== Examples ==
Some examples of the code are:
{| class="wikitable"
|-
! Bibcode
! Reference
|-
| &lt;tt&gt;[http://adsabs.harvard.edu/abs/1974AJ.....79..819H 1974AJ.....79..819H]&lt;/tt&gt;
| {{cite journal
   |last=Heintz |first=W. D.
   |date=1974
   |title=Astrometric study of four visual binaries
   |journal=[[The Astronomical Journal]]
   |volume=79 |pages=819&#8211;825
   |doi=10.1086/111614
   |bibcode = 1974AJ.....79..819H }}
|-
| &lt;tt&gt;[http://adsabs.harvard.edu/abs/1924MNRAS..84..308E 1924MNRAS..84..308E]&lt;/tt&gt;
| {{cite journal
   |last=Eddington |first=A. S.
   |date=1924
   |title=On the relation between the masses and luminosities of the stars
   |journal=[[Monthly Notices of the Royal Astronomical Society]]
   |volume=84 |issue=5
   |pages=308&#8211;332
   |bibcode = 1924MNRAS..84..308E 
   | doi = 10.1093/mnras/84.5.308 }}
|-
| &lt;tt&gt;[http://adsabs.harvard.edu/abs/1970ApJ...161L..77K 1970ApJ...161L..77K]&lt;/tt&gt;
| {{cite journal
   |last1=Kemp |first1=J. C.
   |last2=Swedlund |first2=J. B.
   |last3=Landstreet |first3=J. D.
   |last4=Angel |first4=J. R. P.
   |date=1970
   |title=Discovery of circularly polarized light from a white dwarf
   |journal=[[The Astrophysical Journal Letters]]
   |volume=161 |pages=L77&#8211;L79
   |doi=10.1086/180574
 |bibcode = 1970ApJ...161L..77K }}
|-
| &lt;tt&gt;[http://adsabs.harvard.edu/abs/2004PhRvL..93o0801M 2004PhRvL..93o0801M]&lt;/tt&gt;
| {{cite journal
   |last1=Mukherjee |first1=M.
   |last2=Kellerbauer |first2=A.
   |last3=Beck |first3=D.
   |last4=Blaum |first4=K.
   |last5=Bollen |first5=G.
   |last6=Carrel |first6=F.
   |last7=Delahaye |first7=P.
   |last8=Dilling |first8=J.
   |last9=George |first9=S.
   |last10=Gu&#233;naut |first10=C.
   |last11=Herfurth |first11=F.
   |last12=Herlert |first12=A.
   |last13=Kluge |first13=H.-J.
   |last14=K&#246;ster |first14=U.
   |last15=Lunney |first15=D.
   |last16=Schwarz |first16=S.
   |last17=Schweikhard |first17=L.
   |last18=Yazidjian |first18=C.
   |display-authors=3
   |date=2004
   |title=The Mass of &lt;sup&gt;22&lt;/sup&gt;Mg
   |journal=[[Physical Review Letters]]
   |volume=93 |issue=15
   |pages=150801
   |doi=10.1103/PhysRevLett.93.150801
 |bibcode = 2004PhRvL..93o0801M }}
|}

== See also ==
* [[Digital object identifier]]

== References ==
{{Reflist|30em}}

[[Category:Index (publishing)]]
[[Category:Identifiers]]
[[Category:Electronic documents]]
[[Category:Computational astronomy]]</text>
      <sha1>jctf6r37v2iuskuw25yuofb8nwe35lk</sha1>
    </revision>
  </page>
  <page>
    <title>Teleadministration</title>
    <ns>0</ns>
    <id>47533003</id>
    <revision>
      <id>740039241</id>
      <parentid>721262395</parentid>
      <timestamp>2016-09-18T18:01:14Z</timestamp>
      <contributor>
        <username>Danielem1976</username>
        <id>26012251</id>
      </contributor>
      <comment>Removed template: now there are links.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="22505" xml:space="preserve">'''Teleadministration''' is based on the concept that documents in electronic format have legal value. Administrative informatics is not new, but for many years it was merely Information Technology applied to legal documents, that is, the reproduction of paper-based legal documents into electronic file systems. Instead, Teleadministration turns this approach into its head. It is based on research conducted in 1978, the year when, at a conference promoted by the [[Court of Cassation (Italy)|Court of Cassation]], [[:it:Giovanni Duni|Giovanni Duni]] launched the then-futuristic idea that an electronic document could have legal value.&lt;ref&gt;1. Duni, G., L'utilizzabilit&#224; delle tecniche elettroniche nell'emanazione degli atti e nei procedimenti amministrativi. Spunto per una teoria dell'atto amministrativo emanato nella forma elettronica, in "Rivista amm. della Repubblica italiana", 1978, pag.407 ss.&lt;/ref&gt; 1978 was also the year in which the first research on digital signatures ([[RSA (cryptosystem)|RSA]])&lt;ref&gt;2. Rivest, Shamir e [[Leonard Adleman|Adleman]], A method for obtaining digital signature and public key cryptosystems, in Communications of the ACM, vol. 21, febbraio 1978, 120-126. This research referred to the asymmetric encryption technology (Diffie and Hellman, New directions in Cryptography, in IEEE Transaction on Information Theory, November 1976, 644 ss. Diffie and Hellman&#8217;s research was disseminated in Italy by  Gardner, &#8220;Un nuovo tipo di cifrario che richiederebbe milioni di anni per essere decifrato&#8221;, in Le Scienze, December 1977, 126 ss.), who added the regulation for issuing the keys and the public certification process associated to them.&lt;/ref&gt; was published in the United States, yet it would take more than twenty-five years for jurists and mathematicians to start working together.&lt;ref&gt;3. The first application of the research by Rivest, Shamir and Adleman was the 1995 Utah Code, &#167; from 46-3-101 to 46-3-504 (Enacted by l. 1995, ch. 61). The Utah code was analysed in the brilliant dissertation written by Francesca Flora, Evoluzione della informatica nel sistema di governo degli Stati Uniti d&#8217;America (Cagliari, dept. Of Politiacl Science, November 1996). For application at the federal level one had to wait until 1998: US Senate, S. 1594, Digital Signature and Electronic Authentication Law (SEAL) of 1998. &#8212; US House of Representatives, H.R. 3472, Digital Signature and Electronic Authentication Law (SEAL) of 1998.&lt;/ref&gt;

For many years, and even before 1978, IT helped [[Public Administration]] but kept a &#8220;safe distance&#8221;, assuming that the &#8216;sacred nature&#8217; of the Law demanded the use of pen and paper. Information Technology merely managed and filed copies of legal documents: it was known as &#8220;parallel IT&#8221;,&lt;ref&gt;4. Duni, G., Amministrazione digitale, Voce della Enciclopedia del diritto, Annali, I, Milano 2007, p. 13-49.&lt;/ref&gt; since it was an accessory to the activity with formal value, the one based on pen and paper.

Thus, the logical, legal and material premise of Teleadministration is the conferment of legal value to IT documents.

== Origins and terminology  ==
In Italy, the linguistic expression &lt;ref&gt;5. [http://www.treccani.it/enciclopedia/teleamministrazione_(Lessico-del-XXI-Secolo) Teleamministrazione, Lessico del XXI secolo], Treccani&lt;/ref&gt; teleamministrazione was first used in 1991 at the Roman &#8216;La Sapienza&#8217; university, during a conference organised by the Court of Cassation,&lt;ref&gt;6. Duni, G., Il progetto nazionale di teleamministrazione pubblica, in &#8220;L&#8217;informatica giuridica e il Ced della Corte di Cassazione&#8221;, proceedings of the conference held at the Univ. of Rome &#8220;La Sapienza&#8221;, 27-29 Nov. 1991, Milan 1992, p. 87 ss.&lt;/ref&gt; in which it was said that: &#171;the new system of administrative information technology is called &#8220;teleadministration&#8221; because all the work of the [[Public Administration]] will be carried out through devices, that could also be computers, linked to the central server through a network.&#187; Teleadministration was indeed considered a type of teleworking.&lt;ref&gt;7. Applicazioni della multimedialit&#224; nella P.A.: teleamministrazione e telelavoro&#8221;, in Funzione Pubblica, special issue &#8220;I convegni di FORUM P.A. &#8217;96&#8221;, volume I, p. 105.&lt;/ref&gt;

With Teleadministration, amministrative procedures become electronic administrative procedures and, more specifically, those that are initiated by a party realize the electronic One Stop Shop.

== The fundamentals of teleadministration ==
In the decades from 1970 to 1990, the [[Supreme Court of Cassation (Italy)|Court of Cassation]] was at the core of research on the relationship between IT and Law, organising international conferences every five years on the topic. The 1993 international conference featured the fundamentals of teleadministration, providing the details of the administrative systems behind the One Stop Shop concept:&lt;ref&gt;8. Duni, G., presentation at the 5th International Congress at the Court of Cassation on &#8220;IT and Legal Activity&#8221; Rome, 3&#8211;7 May 1993, I.P.Z.S. - Libreria dello Stato, 1994, II, p. 381 ss.&lt;/ref&gt;
# A citizen presents his/her claim to one administration, which then manages the entire procedure.
# A single &#8220;administrative file&#8221; is created, no matter how many different administrations may be involved.
# For both internal and external stages in an administrative procedure, a "warning signal" is sent telematically to the relevant office where the next stage is due, the employee in that office then becomes responsible for that phase of the procedure.
# Any information concerning records already held by public administration is accessed telematically, without involving the citizen. 
# The (electronic) signature identifies the identity of the operator through sophisticated techniques.
# The original of an administrative act is electronic and is therefore always available telematically to any administration that may need it. 
# The presence of an increasing amount of on-line data will necessitate greater use of automatic data processing in decision-making.
# Saving data on multiple memory locations will guarantee the safekeeping of the acts. 
# Statistical data will be available in real time and under multiple profiles, with great benefits for top level decision-making.
# Private citizens can obtain paper copies of the electronic acts.
It should be noted that the 5th fundamental mentions electronic rather than digital signatures.  This is because, in the jurists&#8217; domain, digital signatures were not yet known. The generic reference to the electronic signature is however valid, and its general nature is actually suitable for the rules contained six years later in the Directive 1999/93/EC.
As we will see, the Directive refers in particular to all procedures initiated by private citizens, but the system remains valid for all procedures initiated by the administration offices as well.

== Acknowledgement of the principles in current law ==
The legally accepted form of acts and documents evolved according to the following stages:
# Acts only exist in paper form
# Acts in electronic format are a possible option
# Electronic format is compulsory, safe for a few exceptions
In Italy, Phase 2 was launched by Art 15, para, 2,  Law N. 59 of 15 March 1997, (the so-called &#8216;Bassanini 1&#8217; law: it established the legal value of electronic documents, while regulations would establish the authentication criteria). The EC intervened later with its Directive 1999/93/EC of the [[European Parliament]] and the [[Council of the European Union|Council]] of 13 December 1999 (Transposed by Law Decree N.10 of 23 January 2002), which imposes an obligation on Member States to give legal value to documents with digital signatures (not directly named as such, but all their features are described in the directive). It also establishes that electronic documents should not be rejected a priori, hence opening to a range of different solutions to establish the authorship of a document (the so-called &#8216;weak signatures&#8217;).

The 1993 Directive was revoked and absorbed (for reasons of legal certainty and clarity) by Regulation 910/2014 of the European Parliament and Council of 23 July 2014 (also known as eIDAS regulation) in the OJEU 28 August 2014, which did not renege on the principle of also accepting the so-called &#8216;weak signatures&#8217;.
In Italy, The move to Phase 3 was established by Art.40 of Legislative sl. Decree N.82 of 7 March 2007, Code of Digital Administration (CAD), entitled &#8220;Creation of electronic documents&#8221;, which states: &#8220;Public Administrations make the original copy of their documents with electronic means, according to the provisions of the present Code and the technical specifications of Article 71&#8221;. Exceptions are extremely rare: Comma 3 states: by means of appropriate regulations&#8230;. , proposed by the delegated Ministers for Public Functions, Innovation and Technology and the Minister for Cultural Heritage and Activities, the categories of administrative documents that can be created on paper in original are identified, having regard to the special historical and archive value they will have by nature&#8221; (think, for example, of the resignation of a President of the Republic).

Unfortunately, national administrations are ignoring this provision, and today it is only private companies that are no longer allowed paper-based communication with public administrations (Art. N. 5 bis of the CAD and D.P.C.M. 22 July 2011); compulsory electronic invoicing was added on 31 March 2015 by Law N. 44 of 24 December 2007, Art. N. 1, para 209-214 implemented by Ministerial  Decree N.55 of 3 April 2013, further clarified by Ministerial circular N.1 of 9 March 2015.
The modernisation of procedures was also touched by Presidential Decree N. 447 of 20 October 1998 (creation of the One Stop Shop, but only for production activities, and paper based), while interest for a telematic procedure only began with Legislative Decree N. 82 of 7 March 2005, CAD, which is not as relevant in its first version but was later modified by several interventions, particularly Legislative Decree N. 335 of 10 December 2013.

European sources are also essential. The EC, and later the European Union, have undertaken a wide range of actions on e-government: one of the most important was the launch of the IDABC programme (and financing) for Interoperable Delivery of European eGovernment Services to public Administrations, Business and Citizens, via Decision 2004/387/EC of the European Parliament and Council of 21 April 2004. However, the ultimate acknowledgement of the principles of teleadministration, with the telematic One Stop Shop, is contained in the Directive 2006/123/EC of the European Parliament and Council of 12 December 2006, on the Internal market for services, which provides for Member States to set up an electronic One Stop Shop in the wide field of administrative procedures.

== Teleadministration and the &#8216;star&#8217; procedure ==
This article is not meant to argue the great effect that teleadministration has on the efficiency of administrative activity, as we assume that the reader is fully aware that, once paper based documents are abandoned, the real-time flow of documents greatly improves time management and responsibility of the single offices/ operators, while direct online access improves transparency. Rather, this paragraph wants to emphasize how teleadministration promotes maximum usage of the &#8220;star procedure&#8221;, known and researched in Germany as &#8220;Sternverfahren&#8221;. This procedure, an alternative to the sequential procedure, which has by nature longer head times, in the paper-based world would require making several copies of the administrative file (which can be extremely voluminous) for each office and each administration that needs to express an opinion or issue an authorisation. With the One Stop Shop, the administration initiating the process is charged with this task. Electronic files clearly provide evident benefits for these procedures, since all involved administrations can directly and simultaneously access the file, view the part they need to evaluate and add their opinion or authorisation directly, using a star-shaped scheme.

== Assessing the actual acceptance of teleadministration in current law and real life ==
As a scientific proposition, teleadministration sketched the system of telematic administrative procedures well ahead of the law, and particularly the electronic One Stop Shop concept. Both concepts are based on the dematerialisation of documents and on telematic administrative work.

The concept of documents&#8217; dematerialisation,&lt;ref&gt;9. According to some commentators &#8220;dematerialization&#8221; is not the appropriate term for documents that are created in electronic form, but rather for those that are created in paper form and are later converted in digital format. Though conceptually correct, this observation ignores the fact that the expression now is generally understood to mean any &#8220;form that does without the material presence of paper, from the creation of the document&#8221;&lt;/ref&gt; in existence since 1978 as a scientific notion,&lt;ref&gt;10. See note. 1.&lt;/ref&gt; was first embraced in Italy (Law N. 59 of 15 March 1997, Art. 15, para 2) and later by the E.C. in Directive 1999/93/EC.

Once the principle that an electronic document can have legal value was accepted, it was possible to deal with its management within a telematic procedure.
As mentioned, configuring this procedure within the rules of teleadministration is today accepted in both European and Italian laws. European laws also provide quite a detailed description of the electronic One Stop Shop, with rules that fit nicely within the scientific rules of teleadministration; their main limitation is that they were specifically designed for the free circulation of services within Europe, and hence for the procedures these require. It is the above-mentioned 1996/123/EC Directive, whose Art.6 establishes the One Stop Shop and Art. 8 provides that it should be managed &#8220;remotely and electronically&#8221;, leaving further details to the Commission. And indeed the Commission, with its Decision of 16 October 2009, provided a number of measures to facilitate the use of electronic procedures through the &#8220;One Stop Shop&#8221; under Directive 2006/123/EC. These sources are clear and they apply to a wide-ranging sector: the problem is that any sector or procedure that is not related to the supply of services within the Union is not regulated, and Member States are therefore able to carry on with old-fashioned paper-based procedures.

In light of this limitation, a group of illustrious European Law academics, coordinated by Giovanni Duni, has drafted the most effective text for a Directive providing a universal system of telematic administrative procedure.&lt;ref&gt;11. The outcome of this collective research effort is the text for a draft Directive, that can be found on the CNR ITTIG journal  Informatica e diritto, Vol. XXI, 2012, N. 2, pp. 113-129: The telematic procedures in the European Union. Introducing a draft Directive, as well as  on line, with an Italian and an English language version of the draft on the site [http://www.teleamministrazione.it www.teleamministrazione.it]&lt;/ref&gt;

Italian sources are based on the Code of Digital Administration, the above-mentioned Law Decree N.82 of 7 March 2005 in its current version, following several modifications, which (if correctly interpreted and implemented) should make it compulsory for all public administrations to use teleadministration, thus making the telematic administrative procedure the default procedural method. Art. 14, the key provision, establishes that the proceeding administration creates an electronic file, to which all involved administrations can and should have access, and feed it with the acts of their competence. Private citizens can also access it under Law 241/90
Thus the electronic file is the technical and organizational specification of the telematic administrative procedure, as it is clear that its creation is an operative stage of the procedure and not simply a new filing system for the archives.

Art. 10 of the CAD appears at first to be at odds with this interpretation, since it establishes that the One Stop Shop for productive activities provides its services electronically but leaves doubts about the possibility that the back office activities could be still paper-based. However, if Art. 10 and Art. 41 are interpreted together, the only possible conclusion is that the former is a clarification of front office activities, but all the administrative activity is based on the general rule of electronic files, and therefore on the teleadministration and the One Stop Shop. 
Compared to European and Italian law, reality is somewhat behind. Eight years after Directive 1996/123/CE, there would be grounds for an infraction procedure against Italy. But since Italy enjoys the company of several other non-compliant Member States, they are all &#8216;safe&#8217; for the time being.

Though the letter of the CAD may be not be respected, it seems very unlikely that this may determine the invalidity or nullity of the acts for violation of Art. 41(electronic file) or Art. 40 (statutory requirement of digital signature), because in front of a claim of this nature, the administrative judge would apply Artt. 21 septies and 21 octies of Law 241/90. The claimant should demonstrate that the use of the electronic format and electronic file wold have led to a different outcome.

== Legal sources ==

=== US Law ===
1995 Utah Code, paras 46-3-101 to 46-3-504 (Enacted by Law 1995, Ch. 61). US Senate, S. 1594, Digital Signature and Electronic Authentication Law (SEAL) of 1998. &#8212; US House of Representatives, H.R. 3472, Digital Signature and Electronic Authentication Law (SEAL) of 1998

=== EU Law ===
*European Parliament and Council Directive 1999/93/EC of 13 December 1999, revoked and absorbed by European Parliament and Council Regulation 910/2014 of 23 July 2014, (known as the eIDAS regulation) in OJEU 28 August 2014. 
*European Parliament and Council Decision 2004/387/EC of 21 April 2004 on the interoperable delivery of European eGovernment services
*European Parliament and Council Directive 2006/123/EC of 12 December 2006, on internal market services, providing for Member States to establish the electronic One Stop Shop in this vast field of administrative procedures. 
*Decision of 16 October 2009, establishing measures to facilitate electronic procedures through the &#171;One Stop Shop&#187; under Directive 2006/123/CE 
The telematic procedures in the European Union. Introducing a draft Directive, research coordinated by Duni, G., in CNR ITTIG Informatica e diritto, Vol. XXI, 2012, n. 2, pp.&amp;nbsp;113&#8211;129 and in www.teleamministrazione.it.

=== Italian Law ===
*l. 15 March 1997, N. 59, art. 15, para, 2. 
*D. lg. 23 January 2002, N. 10. 
*D.P.R. 20 October 1998, N. 447 
*D. legils. 7 March 2005, N. 82, codice dell'amministrazione digitale (CAD) 
*D.P.C.M. 22 July 2011 
*D. Legisl 10 December 2013, N. 335

== Bibliography ==
*Contaldo, A., La teleamministrazione con reti transnazionali europee come strumento per l'integrazione delle Pubbliche Amministrazioni dei paesi dell'Unione Europea, in Riv. trim. diritto amministrativo, I, 2004, p.&amp;nbsp;95 and later 
*Diffie and Hellman, New directions in Cryptography, in IEEE Transaction on Information Theory, November 1976, 644 ss 
*Duni, G., L'utilizzabilit&#224; delle tecniche elettroniche nell'emanazione degli atti e nei procedimenti amministrativi. Spunto per una teoria dell'atto amministrativo emanato nella forma elettronica, in "Rivista amm. della Repubblica italiana", 1978, pag.407 ss. &#8212; Il progetto nazionale di teleamministrazione pubblica, in &#8220;L&#8217;informatica giuridica e il Ced della Corte di Cassazione&#8221;, proceedings of the conference held at Univ. of Rome &#8220;La Sapienza&#8221;, 27-29 Nov. 1991, Milan 1992, p.&amp;nbsp;87 ss. &#8212; La teleamministrazione: una &#8220;scommessa&#8221; per il futuro del Paese, presentation at the 5th International Congress at the Court of Cassation on &#8220;IT and Legal Activity&#8221; Rome, 3&#8211;7 May 1993, I.P.Z.S. - Libreria dello Stato, 1994, II, p.&amp;nbsp;381 ss. &#8212; Amministrazione digitale, Item under the Enciclopedia del diritto, Annali, I, Milan 2007, p.&amp;nbsp;13-49 &#8212; L&#8217;amministrazione digitale. Il diritto amministrativo nell&#8217;evoluzione telematica, Giuffr&#232; 2008. 
*Flora, F., Evoluzione della informatica nel sistema di governo degli Stati Uniti d&#8217;America. Dissertation, Cagliari, Dept. Of Political Science, November 1996. 
*Gagliotti, A., Teleamministrazione e concorsi pubblici, in Giustizia amministrativa n. 3/2003, http://www.giustamm.it/ago1/articoli/gaglioti_teleamministrazione.htm#_ednref5 
*Gardner, Un nuovo tipo di cifrario che richiederebbe milioni di anni per essere decifrato, in Le Scienze, December 1977, 126 ss. 
*Masucci, Informatica pubblica, in Dizionario di diritto pubblico directed by S. Cassese, IV, Milan, 2006, 3115 ss.; 
*Notarmuzi, Il codice dell&#8217;amministrazione digitale, in Astrid Rassegna, www.astrid-online.it, 2006, n. 12; Id., Il procedimento amministrativo informatico, ivi, n. 16; 
*Osnaghi, Firme elettroniche e documento informatico: il codice richiede ulteriori integrazioni, ivi, n. 10; 
*Rabbito. c., L'informatica al servizio della pubblica amministrazione. Dai principi della teleamministrazione ai piani di e-government, Gedit, 2007. 
*Rivest, Shamir e Adleman, A method for obtaining digital signature and public key cryptosystems, in Communications of the ACM, vol. 21, February 1978, 120-126 
*The telematic procedures in the European Union. Introducing a draft Directive, ricerca coordinata da Duni, g., in CNR ITTIG Informatica e diritto, Vol. XXI, 2012, N. 2, pp.&amp;nbsp;113&#8211;129 and in www.teleamministrazione.it. 
*Applicazioni della multimedialit&#224; nella P.A.: teleamministrazione e telelavoro&#8221;, in Funzione Pubblica, special issue &#8220;I convegni di FORUM P.A. &#8217;96&#8221;, volume I, p.&amp;nbsp;105.

==See also==
*[[Digital era governance]]
*[[Electronic document]]
*[[Electronic paper]]
*[[Paperless office]]
*[[Bureaucrat]]
*[[E-Government Act of 2002]]
*[[E-government]]
*[[Public administration]]

==References==
&lt;references/&gt;

[[Category:E-government]]
[[Category:Administrative law]]
[[Category:Public-key cryptography]]
[[Category:Electronic documents]]
[[Category:Public administration]]</text>
      <sha1>tv4f5t2ywyfitaj28s63ynesq320mpw</sha1>
    </revision>
  </page>
  <page>
    <title>SPIN bibliographic database</title>
    <ns>0</ns>
    <id>28010203</id>
    <revision>
      <id>678206401</id>
      <parentid>678205296</parentid>
      <timestamp>2015-08-28T00:15:29Z</timestamp>
      <contributor>
        <username>Cydebot</username>
        <id>1215485</id>
      </contributor>
      <minor />
      <comment>Robot - Moving category Bibliographic databases to [[:Category:Bibliographic databases and indexes]] per [[WP:CFD|CFD]] at [[Wikipedia:Categories for discussion/Log/2015 July 4]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5091" xml:space="preserve">{{Infobox Bibliographic Database
|title =SPIN  (Searchable Physics Information Notices)  
|image = 
|caption = 
|producer =[[American Institute of Physics]] (AIP) 
|country =USA, Russia, Ukraine
|history = 
|languages =English, [[Russian language|Russian]], [[Ukrainian language|Ukrainian]] 
|providers =[[Dialog (online database)|Dialog]], [[American Institute of Physics|AIP website]], [[SPIE|SPIE Digital Library]] 
|cost = 
|disciplines =Physics, Astronomy, Mathematics, Geophysics, Geosciences, Nuclear Science, Science &amp; Technology 
|depth =Word, Phrase, Abstract, Author and Author affiliations, Descriptor, Errata (coden, or date, or volume) Identifier, Title, Astronomical objects, CODEN, Conference (location, or title, or year), Journal name, and more...   
|formats =Journal Articles, Book Reviews, Conferences, Meetings, Patents, Symposia
|temporal =1975 to the present  
|geospatial =International 
|number =over 1.5 million 
|updates =Weekly 
|p_title =No print counterparts 
|p_dates = 
|ISSN =
|web =https://scitation.aip.org/jhtml/scitation/coverage.jsp 
|titles =  
}}

'''SPIN''' (Searchable Physics Information Notices) '''bibliographic database''' is an indexing and abstracting service produced by the [[American Institute of Physics]] (AIP). The content focus of SPIN is described as the most significant areas of [[physics]] [[research]]. This type of [[scientific literature|literature coverage]] spans the major [[scientific journal|physical science journals]] and magazines. Major [[conference proceedings]] that are reported by the American Institute of Physics, member societies, as well as affiliated organizations are also included as part of this database. References, or citations, provide access to more than 1.5 million articles as of 2010. ''SPIN''  has no print counterpart.&lt;ref name=DialogSpin/&gt;&lt;ref name=AIP-SPIN/&gt;

==Journals==
Delivery of timely indexing and abstracting is for, what are deemed to be, the significant or important [[physics]] and [[astronomy]] journals from the [[United States]], [[Russia]], and the [[Ukraine]]. Citations for journal articles are derived from original publications of the ''AIP'', which includes published translated works. At the same time, citations are included from member societies, and selectively chosen American journals. Citations become typically available online on the same date as the corresponding journal article.&lt;ref name=DialogSpin/&gt;&lt;ref name=AIP-SPIN&gt; {{Cite web
  | title =What is the SPIN database? 
  | work =Information about SPIN 
  | publisher =[[American Institute of Physics]] 
  | date =July 2010 
  | url =http://scitation.aip.org/servlet/HelpSystem?KEY=SCI&amp;TYPE=HELP/FAQ#ques3 
  | format = 
  | accessdate =2010-07-12}}&lt;/ref&gt;

==Sources==
Overall, the source citations are derived from material published by the AIP and member societies,  which are English-speaking, Russian, and Ukrainian journals and conference proceedings. Certain American physics-related articles are also sources of citations. About 60 journals have cover to cover indexing, and about 100 journals, overall, are indexed.&lt;ref name=DialogSpin/&gt;&lt;ref name=pub-coverage&gt;{{Cite web
  | title =SPIN Publication Coverage 
  | work =Complete list of publications covered and coverage years. 
  | publisher =American Institute of Physics 
  | date =July 2010 
  | url =http://scitation.aip.org/jhtml/scitation/spincodens.jsp 
  | format = 
  | accessdate =2010-07-12}}&lt;/ref&gt;  

==Scope==
Subject coverage encompasses the following: &lt;ref name=DialogSpin&gt;  {{Cite web
  | title =Indexes and Databases 
  | work =SPIN: Searchable Physics Information Notices
  | publisher =Raymond H. Fogler Library, The University of Maine
  | date =October 2010 
  | url =http://www.library.umaine.edu/indexesdb/dbdetails.asp?field=Name&amp;search=SPIN:+Searchable+Physics+Information+Notices 
  | format = 
  | accessdate =2010-07-12}}&lt;/ref&gt;

*[[Applied physics]], [[Electromagnetic spectrum|Electromagnetic]] technology, [[Microelectronics]] 
*[[Atomic physics]] and [[Molecular physics]] 
*[[Biological physics]] and [[Medical physics]] 
*[[Classical physics]] and [[Quantum physics]] 
*[[Condensed matter physics]] 
*[[Elementary particle physics]] 
*[[Physics|General physics]], [[Optics]], [[Acoustics]], and [[Fluid dynamics]] 
*[[Geophysics]], [[Astronomy]], [[Astrophysics]] 
*[[Materials science]] 
*[[Nuclear physics]] 
*[[Plasma physics]] 
*[[Physical chemistry]]

==See also==
*[[List of academic databases and search engines]]

==References==
{{Reflist}}

==External links==
*[http://www.aip.org/press_release/spin.html AIP'S SPIN Database Reaches One Million Records].  American Institute of Physics. March 1, 2002.
*[http://scholarlykitchen.sspnet.org/2009/06/17/physics-papers-and-the-arxiv/ Can everything published in physics can be found in the [[arXiv]]?]. The Scholarly Kitchen. [[Society for Scholarly Publishing]]. June, 2010.
*[http://www.pub4stm.org/ AIP partnerships] (society publishing). July 2010.


[[Category:Bibliographic databases and indexes]]
[[Category:Citation indices]]
[[Category:Scientific databases]]</text>
      <sha1>jyccaikszf3qpu59e9oecjsjl9nmp6d</sha1>
    </revision>
  </page>
  <page>
    <title>Citation index</title>
    <ns>0</ns>
    <id>423362</id>
    <revision>
      <id>760499794</id>
      <parentid>760499762</parentid>
      <timestamp>2017-01-17T10:49:48Z</timestamp>
      <contributor>
        <username>Cyrus noto3at bulaga</username>
        <id>29357210</id>
      </contributor>
      <comment>Reverted 1 edit by [[Special:Contributions/37.105.71.72|37.105.71.72]] ([[User talk:37.105.71.72|talk]]): Uncapitilized. ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4721" xml:space="preserve">{{distinguish|Citation metric}}

A '''citation index''' is a kind of [[bibliographic index]], an index of [[citation]]s between publications, allowing the user to easily establish which later documents cite which earlier documents. A form of citation index is first found in 12th-century Hebrew religious literature. Legal citation indexes are found in the 18th century and were made popular by [[citator]]s such as [[Shepard's Citations]] (1873). In 1960, [[Eugene Garfield]]'s [[Institute for Scientific Information]] (ISI) introduced the first citation index for papers published in [[academic journal]]s, first the ''[[Science Citation Index]]'' (SCI), and later the ''[[Social Sciences Citation Index]]'' (SSCI) and the ''[[Arts and Humanities Citation Index]]'' (AHCI). The first automated citation indexing was done by [[CiteSeer]] in 1997. Other sources for such data include [[Google Scholar]] and Elsevier's [[Scopus]].

==History==
The earliest known citation index is an index of biblical citations in [[rabbinic literature]], the ''Mafteah ha-Derashot'', attributed to [[Maimonides]] and probably dating to the 12th century. It is organized alphabetically by biblical phrase. Later biblical citation indexes are in the order of the canonical text. These citation indices were used both for general and for legal study. The Talmudic citation index ''En Mishpat'' (1714) even included a symbol to indicate whether a Talmudic decision had been overridden, just as in the 19th-century ''Shepard's Citations''.&lt;ref&gt;Bella Hass Weinberg, "The Earliest Hebrew Citation Indexes" in Trudi Bellardo Hahn, Michael Keeble Buckland, eds., ''Historical Studies in Information Science'', 1998, p. 51''ff''&lt;/ref&gt;&lt;ref&gt;Bella Hass Weinberg, "Predecessors of Scientific Indexing Structures in the Domain of Religion" in W. Boyden Rayward, Mary Ellen Bowden, ''The History and Heritage of Scientific and Technological Information Systems'', Proceedings of the 2002 Conference, 2004, p. 126''ff''&lt;/ref&gt; Unlike modern scholarly citation indexes, only references to one work, the Bible, were indexed.

In English legal literature, volumes of judicial reports included lists of cases cited in that volume starting with ''Raymond's Reports'' (1743) and followed by ''Douglas's Reports'' (1783). Simon Greenleaf (1821) published an alphabetical list of cases with notes on later decisions affecting the precedential authority of the original decision.&lt;ref name='shapiro'/&gt;

The first true citation index dates to the 1860 publication of Labatt's ''Table of Cases...California...'', followed in 1872 by Wait's ''Table of Cases...New York...''. But the most important and best-known citation index came with the 1873 publication of [[Shepard's Citations]].&lt;ref name='shapiro'&gt;Fred R. Shapiro, "Origins of Bibliometrics, Citation Indexing, and Citation Analysis: The Neglected Legal Literature" ''Journal of the American Society of Information Science'' '''43''':5:337-339 (1992)&lt;/ref&gt;

==Major citation indexing services==
{{main article|Indexing and abstracting service}}
{{main cat|Citation indices}}
General-purpose academic citation indexes include:
*[[Web of Science]] by [[Clarivate Analytics]] (previously the Intellectual Property and Science business of [[Thomson Reuters]])
*[[Scopus]] by [[Elsevier]], available online only, which similarly combines subject searching with citation browsing and tracking in the sciences and [[social sciences]].
*[[Indian Citation Index (ICI)|Indian Citation Index]] is an online citation data which covers [[peer review]]ed journals published from India. It covers major subject areas such as scientific, technical, medical, and [[social sciences]] and includes arts and humanities. The citation database is the first of its kind in India.
Each of these offer an index of citations between publications and a mechanism to establish which documents cite which other documents. They differ widely in cost: Web of Science and Scopus are available by subscription (generally to libraries).

In addition, [[CiteSeer]] and [[Google Scholar]] are freely available online.

==See also==
* [[Microsoft Academic Search]]
* [[Google Scholar]]
* [[Scopus]]
* [[Semantic Scholar]]
* [[Citation analysis]]
* [[Acknowledgment index]]
* [[CiteSeer]]
* [[CiteSeerX]]
* [[Scientific journal]]
* [[Science Citation Index]]
* [[Indian Citation Index]]
* [[Journal Citation Reports]]
* [[Emerging Sources Citation Index (ESCI)]]
* [[SciELO]]
* [[Redalyc]]
* [[Index Copernicus]]

==References==
{{Reflist}}

{{DEFAULTSORT:Citation Index}}
[[Category:Academic publishing]]
[[Category:Bibliometrics]]
[[Category:Bibliographic databases and indexes]]
[[Category:Reputation management]]
[[Category:Citation indices| ]]</text>
      <sha1>ja0p5y3d5cl9rl2p54m400qbaulshym</sha1>
    </revision>
  </page>
  <page>
    <title>Emerging Sources Citation Index</title>
    <ns>0</ns>
    <id>49394017</id>
    <revision>
      <id>747003768</id>
      <parentid>747002085</parentid>
      <timestamp>2016-10-30T22:02:14Z</timestamp>
      <contributor>
        <username>Rbleibenusw</username>
        <id>24206652</id>
      </contributor>
      <comment>Clarivate</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2028" xml:space="preserve">{{incomplete|date=January 2014}}
{{ infobox bibliographic database
| title = Emerging Sources Citation Index
| image = 
| caption = 
| producer = [[Thomson Reuters]]
| country = United States
| history = 2015-present
| languages = 
| providers = 
| cost = 
| disciplines = Multidisciplinary
| depth = 
| formats = 
| temporal = 
| geospatial = Worldwide
| number =
| updates = 
| p_title = 
| p_dates = 
| ISSN = 
| web = http://wokinfo.com/products_tools/multidisciplinary/esci/
| titles = 
}}
The '''Emerging Sources Citation Index''' is a [[citation index]] produced since 2015 by [[Thomson Reuters]], and now by [[Clarivate Analytics]]. It is accessible through the ''[[Web of Science]]''.&lt;ref name=AtoZ&gt;{{cite web |last=ISI Web of Knowledge platform |title =Available databases A to Z |publisher=Thomson Reuters |year=2015 |url=http://ip-science.thomsonreuters.com/mjl/}}&lt;/ref&gt; The index includes [[academic journal]]s "of regional importance and in emerging scientific fields".&lt;ref&gt;{{cite web |title=ESCI Fact Sheet |url=http://wokinfo.com/media/pdf/ESCI_Fact_Sheet.pdf?utm_source=false&amp;utm_medium=false&amp;utm_campaign=false |publisher=Thomson Reuters |accessdate=28 April 2016 |format=PDF}}&lt;/ref&gt; [[Jeffrey Beall]] has stated that, among the databases produced by Thomson Reuters, the Emerging Sources Citation Index is the easiest one to get into and as a result it contains many [[Predatory open access publishing|predatory journals]].&lt;ref name=Beall&gt;{{cite web |last1=Beall |first1=Jeffrey |authorlink1=Jeffrey Beall |title=The TR Master Journal List is not a Journal Whitelist |url=https://scholarlyoa.com/2016/04/28/the-tr-master-journal-list-is-not-a-journal-whitelist/ |website=Scholarly Open Access |publisher=WordPress.com |accessdate=28 April 2016 |date=28 April 2016}}&lt;/ref&gt;

==References==
{{Reflist}}

==External links==
*{{Official website|http://wokinfo.com/products_tools/multidisciplinary/esci/}}
{{Thomson Reuters}}

[[Category:Citation indices]]
[[Category:Online databases]]
[[Category:Thomson Reuters]]</text>
      <sha1>n9b53bm6h8q67j0ch5qlo9nyiw1viyi</sha1>
    </revision>
  </page>
  <page>
    <title>Thomson Directories</title>
    <ns>0</ns>
    <id>1322148</id>
    <revision>
      <id>651198200</id>
      <parentid>575654596</parentid>
      <timestamp>2015-03-13T14:53:09Z</timestamp>
      <contributor>
        <ip>86.4.89.109</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2039" xml:space="preserve">{{Advert|date=July 2009}}
{{Refimprove|date=July 2009}}
'''Thomson Directories''', more commonly referred to as Thomson Local, is a local business directory company based in [[Farnborough, Hampshire|Farnborough]], [[Hampshire]], [[England]], and offers business listings both in print and online following the launch of ThomsonLocal.com in 2003.

174 regional editions of the Thomson Local are produced and delivered free of charge to residential and commercial addresses throughout the UK.
 
The Chief Executive Officer is currently Elio Shiavo.&lt;ref&gt;CEO http://www.answers.com/topic/thomson-directories-1&lt;/ref&gt; 

The company was purchased by [[US West]], a telecommunications company in the United States, in 1994.&lt;ref&gt;{{cite news|url=http://www.independent.co.uk/news/business/us-west-pays-70m-pounds-for-thomson-directories-american-telephone-company-continues-to-develop-multimedia-in-uk-1437180.html|title=US West pays 70m pounds for Thomson Directories: American telephone company continues to develop multimedia in UK |last=Fagan|first=Mary|date=20 May 1994|work=The Independent|accessdate=2009-08-15}}&lt;/ref&gt; In 1999, the company was sold by [[3i]] to [[TDL Infomedia]], a subsidiary of [[Apax Partners]].&lt;ref&gt;{{cite news|url=http://www.independent.co.uk/news/business/3i-sells-thomson-guides-for-pounds-220m-1109799.html|title=  3i sells Thomson guides for pounds 220m |last=Baker|first=Lucy|date=31 July 1999|work=The Independent|accessdate=2009-08-15}}&lt;/ref&gt;

The company was placed in [[Administration (law)]] in August 2013, and acquired by Corporate Media Partners.&lt;ref&gt;{{cite news|url=http://www.bbc.co.uk/news/uk-england-hampshire-23710958|title= Thomson Local directory firm goes into administration|work=BBC News|accessdate=2013-09-11}}&lt;/ref&gt;

==References==
{{Reflist}}

==External links==
*[http://www.thomsondirectories.com/ Official Thomson Directory Site]

[[Category:Companies of the United Kingdom]]
[[Category:Directories]]
[[Category:Apax Partners companies]]
[[Category:3i Group companies]]


{{UK-company-stub}}</text>
      <sha1>rf9m9kjtoft36gers4t7szlqk8hlfki</sha1>
    </revision>
  </page>
  <page>
    <title>Oregon Blue Book</title>
    <ns>0</ns>
    <id>3214053</id>
    <revision>
      <id>728647156</id>
      <parentid>708833450</parentid>
      <timestamp>2016-07-06T18:11:18Z</timestamp>
      <contributor>
        <username>Bender235</username>
        <id>88026</id>
      </contributor>
      <minor />
      <comment>clean up; http-&gt;https (see [[WP:VPR/Archive 127#RfC: Should we convert existing Google and Internet Archive links to HTTPS?|this RfC]]) using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5802" xml:space="preserve">{{Infobox book 
| name          = Oregon Blue Book
| image         = OrBlueBookCover.png
| caption       = Cover of the 2005 edition
| editor        = [[Oregon Secretary of State]]
| country       = United States
| language      = English
| subject       = Oregon history, government
| genre         = Reference
| published     = Biennially, 1911&#8211;present
| media_type    = Print, online
| isbn          = 
| external_url  = http://bluebook.state.or.us/
}}
The '''''Oregon Blue Book''''' is the official directory and fact book for the U.S. state of [[Oregon]] prepared by the [[Oregon Secretary of State]]&lt;ref name="ORS"&gt;{{cite web|url = https://www.oregonlegislature.gov/bills_laws/ors/ors177.html|title = ORS 177.120|publisher = [[Oregon Legislative Counsel]]|accessdate = February 16, 2015}}&lt;/ref&gt; and published by the Office of the Secretary's [[Oregon State Archives|Archives Division]].

The ''Blue Book'' comes in both print and online editions. The [[Oregon Revised Statutes]] require the Secretary of State to publish the print edition "biennially on or about February 15 of the same year as the regular sessions of the [[Oregon Legislative Assembly|Legislative Assembly]],"&lt;ref name=ORS/&gt; which are during odd-numbered years; it has been so published since 1911.  The online edition is updated regularly.&lt;ref name=About&gt;{{cite web |url= http://bluebook.state.or.us/misc/about/about.htm |title= About the Oregon Blue Book |publisher= Oregon Secretary of State |accessdate= February 16, 2015}}&lt;/ref&gt;

==Contents==
The book contains information on the state, city, county, and federal governments in Oregon, educational institutions, finances, the economy, resources, population figures and demographics.&lt;ref name=ERG83&gt;{{cite news |url= https://news.google.com/newspapers?nid=1310&amp;dat=19830410&amp;id=jP5VAAAAIBAJ&amp;sjid=UeIDAAAAIBAJ&amp;pg=3537,2331038 |title= New Oregon Blue Book Published |author= [[United Press International]] |date= April 10, 1983 |newspaper= [[The Register-Guard]] |accessdate= February 16, 2015}}&lt;/ref&gt;

The 1919 edition contained a "statement of registered motor vehicles, chauffeurs, and dealers from 1905 to 1919", and "a general summary of in the taxable property in Oregon from 1858 to 1918".&lt;ref name=Received&gt;{{cite news |url= https://news.google.com/newspapers?nid=1243&amp;dat=19190911&amp;id=0NgsAAAAIBAJ&amp;sjid=HCAEAAAAIBAJ&amp;pg=3731,5589153 |date= September 11, 1919 |title= Blue Book is Received Here |newspaper= [[The Bulletin (Bend)|The Bulletin]] |accessdate= February 17, 2015}}&lt;/ref&gt;

==History==
Secretary of State [[Ben Olcott]] published the first edition in 1911 in response to an "increased demand for information of a general character concerning Oregon".&lt;ref name=Indispensable&gt;{{cite news |url= https://news.google.com/newspapers?nid=1310&amp;dat=19950525&amp;id=4ERWAAAAIBAJ&amp;sjid=7OoDAAAAIBAJ&amp;pg=3777,5960904 |title= Blue Book Indispensable |newspaper= The Register-Guard |date= May 25, 1995 |accessdate= February 16, 2015}}&lt;/ref&gt;

Early editions of the book were available free from the State.&lt;ref name=Received/&gt; By 1937, copies cost 25; in 1981 the book cost $4.&lt;ref&gt;{{cite news |url= https://news.google.com/newspapers?nid=1310&amp;dat=19370921&amp;id=dMpYAAAAIBAJ&amp;sjid=Q-gDAAAAIBAJ&amp;pg=2452,1169200 |title= Oregon Blue Book Being Distributed |author= United Press International |date= September 21, 1937 |newspaper= The Register-Guard |accessdate= February 17, 2015}}&lt;/ref&gt;&lt;ref&gt;{{cite news |url= https://news.google.com/newspapers?nid=1243&amp;dat=19810427&amp;id=tl0zAAAAIBAJ&amp;sjid=EPcDAAAAIBAJ&amp;pg=3755,5317981 |title= Oregon Blue Book Makes Biennial Appearance |author= United Press International |date= April 27, 1981 |newspaper= The Bulletin |accessdate= February 17, 2015}}&lt;/ref&gt;

In 1953, a legislative ways and means subcommittee, headed by Representative [[Francis Ziegler]], was going to confer with Secretary of State [[Earl T. Newbry]] about how to improve the ''Blue Book''.&lt;ref name=Revision&gt;{{cite news |url= https://news.google.com/newspapers?nid=1310&amp;dat=19530323&amp;id=URZWAAAAIBAJ&amp;sjid=veIDAAAAIBAJ&amp;pg=5600,1181962 |title= Legislative Group to Study Revision of 'Blue Book' |date= March 23, 1953 |newspaper= The Register-Guard |author= United Press International |accessdate= February 16, 2015}}&lt;/ref&gt; This was following complaints by Representative [[Monroe Sweetland]] that the book was "obsolete, carelessly edited, and only of limited use."&lt;ref name=Revision/&gt; Calling the book "an inferior job", Sweetland criticized the timing of book's publication long after elections, as well as the map in the back.&lt;ref name=Revision/&gt; As a result, the [[47th Oregon Legislative Assembly|1953 Legislative Assembly]] passed a law requiring the book be published soon after the legislature convenes.&lt;ref&gt;{{cite news |url= https://news.google.com/newspapers?nid=1310&amp;dat=19550125&amp;id=nPlVAAAAIBAJ&amp;sjid=p-IDAAAAIBAJ&amp;pg=6704,3150227 |title= Oregon Blue Book Printed But It Isn't Blue Any More |author= [[Associated Press]] |date= January 25, 1955 |newspaper= The Register-Guard |accessdate= February 17, 2015}}&lt;/ref&gt;

The 1993&#8211;94 edition of the book contained a four-page [[errata]].&lt;ref name=Indispensable/&gt; When [[Norma Paulus]] was Secretary of State, she would send a free copy of the book to the first person to find a mistake in each new edition.&lt;ref name=Indispensable/&gt; The 1995&#8211;96 edition was reduced in size from its predecessors.&lt;ref name=Indispensable/&gt;

==Reviews==
A 1995 ''[[Register-Guard]]'' editorial called the book "indispensable".&lt;ref name=Indispensable/&gt;

==See also==
*''[[The Oregon Encyclopedia]]''

==References==
{{reflist}}

==External links==
*[http://bluebook.state.or.us/ Oregon Blue Book] (official website)

[[Category:1911 books]]
[[Category:Government of Oregon|Blue Book]]
[[Category:Directories]]
[[Category:1911 establishments in Oregon]]</text>
      <sha1>9xlpy1z0pa4i0upfulw1ii5g1j1ee9p</sha1>
    </revision>
  </page>
  <page>
    <title>Vsya Rossiya</title>
    <ns>0</ns>
    <id>11017381</id>
    <revision>
      <id>736631813</id>
      <parentid>710446145</parentid>
      <timestamp>2016-08-28T20:33:19Z</timestamp>
      <contributor>
        <username>Iridescent</username>
        <id>937705</id>
      </contributor>
      <minor />
      <comment>/* top */[[WP:AWB/T|Typo fixing]], [[WP:AWB/T|typo(s) fixed]]: between 500 to &#8594; between 500 and using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2485" xml:space="preserve">{{italic title}}
'''''Vsya Rossiya''''' (literally translated "''All Russia''" or "''The whole Russia''") was the title of a series of directories of the [[Russian Empire]] published by [[Aleksei Sergeevich Suvorin]] on a yearly basis from 1895 to 1923 and was continued under the name '''''Ves SSSR''''' (Literally translated ''All of the USSR'' or ''The whole USSR'') from 1924 to 1931. Each volume was anywhere between 500 and 1500 pages long. The directories contained detailed lists of government offices, public services and medium and large businesses present in major cities across Russia including [[Kiev]], [[Minsk]], . These directories are often used by [[genealogists]] today to trace family members who were living in pre-revolutionary Russia and the early [[Soviet Union|Soviet]] period when [[vital records]] are missing or prove difficult to find. [[Historians]] use them to research the [[social histories]] of late 19th century and early 20th century Russia.

==Contents==

The following information can be found in most editions:
*a surname index of over 100,000 names and thousands of companies
*a directory of prominent landowners
*Lists members of the Imperial House of Russia and government officials 
*statistical information about the Russian Empire
*Population figures
*information and guidelines about trade and industry in Russia
*Lists of joint-stock companies
*Sub-sections detailing a directory of each district of each province, listing administrative officials, merchants, industrial and commercial manufacturers
*Original advertising

== Availability ==

Many original directories in the series (or [[microfiche]] copies thereof) can be found in libraries across the U.S., Europe (including the [[Baltic countries]], Finland the United Kingdom and Germany) however most only have an incomplete collection.

==Other city directories in Russia ==
Suvorin also published city directories for [[Saint Petersburg]] under the title ''[[Ves Petersburg]]'' (Literally translated ''All Petersburg'' or ''The Whole Saint Petersburg'') for the years 1894 to 1940 and for [[Moscow]] under the title ''[[Vsia Moskva]]'' (Literally translated ''All Moscow'' or ''The Whole Moscow'') for the years 1875 to 1936.

== External links ==

*[http://surname.litera-ru.ru/ A Russian website offering a search engine in Cyrillic for some city directories.]

[[Category:Directories]]
[[Category:Russian non-fiction books]]
[[Category:Russian Empire]]
[[Category:1895 books]]</text>
      <sha1>ph9z12kz4eu340k0kmdphwlrmu1osxf</sha1>
    </revision>
  </page>
  <page>
    <title>Crockford's Clerical Directory</title>
    <ns>0</ns>
    <id>9731047</id>
    <revision>
      <id>756495858</id>
      <parentid>753364568</parentid>
      <timestamp>2016-12-24T17:41:32Z</timestamp>
      <contributor>
        <username>DBD</username>
        <id>288189</id>
      </contributor>
      <minor />
      <comment>/* Locating previous issues */ UL's collection is missing only one</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="15541" xml:space="preserve">{{italictitle}}'''''Crockford's Clerical Directory''''' ('''''Crockford''''') is the authoritative directory of the [[Anglican Communion]] in the United Kingdom, containing details of English, Welsh and Irish benefices and churches, and biographies of around 26,000 clergy. It was first issued in 1858 by [[John Crockford]],{{sfn|Hough &amp; Matthew|2004}} a London printer and publisher whose father &#8211; also named John &#8211; had been a Somerset schoolmaster.

''Crockford'' is currently compiled and published for the [[Archbishops' Council]] by Church House Publishing.&lt;ref&gt;{{Cite web
| title = Crockford's Clerical Directory
| author =
| work = The Church of England
| date =
| accessdate = 2014-08-12
| url = https://www.churchofengland.org/clergy-office-holders/crockford.aspx
| ref={{sfnref|CofE}}
}}&lt;/ref&gt;  It covers in detail the whole of the [[Church of England]] (including the [[Diocese in Europe]]), the [[Church in Wales]], the [[Scottish Episcopal Church]], and the [[Church of Ireland]], and it also gives some information &#8211; now more limited &#8211; about the world-wide [[Anglican Communion]].

== Previous publishers ==

[[File:Crockford1868-titlepage.jpg|thumb|''Crockford's Clerical Directory 1868'', published by Horace Cox, London]]The actual title of the first edition was simply ''The Clerical Directory'', but a footnote showed that it was published by John Crockford, 29 Essex Street, [[Strand, London|the Strand]].  The original publisher died suddenly in 1865, shortly before the appearance of the third edition of what had by then become ''Crockford&#8217;s Clerical Directory''.  For many subsequent issues the volumes were anonymously edited, but they were published under the imprint of Horace Cox &#8211; the nephew of John Crockford&#8217;s closest business associate, solicitor and publisher [[Edward William Cox]] (1809&#937;&#8211;1879).  (His family was probably quite unrelated to the Charles Cox who coincidentally was the publisher of ''Crockford''{{'}}s chief rival, the ''[[Clergy List]]''.{{efn|A two-part article "Shop-talk and mordant wit" by Christopher Currie &amp; Glyn Paflin describes the background to the directory's first hundred editions, {{sfn|Currie &amp; Paflin|7 December 2007}} }}) Horace Cox died in 1918{{efn|Horace Cox&#8217;s very brief obituary in ''The Times'', 11 October 1918; p. 5, states that he had retired in 1912 and had ceased to take an active part in his business, which also produced ''The Field'', ''The Queen'' and ''The Law Times''}} and the title was subsequently sold in 1921 to the [[Oxford University Press]],{{sfn|Currie &amp; Paflin|7 December 2007}} who continued as publishers right up until the early 1980s.  For the 1985/86 issue publication was transferred to the [[Church Commissioners]] and their Central Board of Finance (who worked from their own administrative lists and databases).  It is now collated by Church House Publishing.

== Frequency of publication ==

The first four issues came  out in 1858, 1860 (with a supplement in 1861),{{efn|The 1861 supplement, experimentally issued when a switch to biennial publication was being contemplated, may be downloaded free of charge from Google Play}} 1865 {{efn|The 1865 edition was reprinted in a 1995 facsimile limited edition of 100 copies by Peter Bell (bookseller), Edinburgh.{{sfn|Bell|1995|p=}} It can also now be downloaded free of charge from Google play}} and 1868.  ''Crockford'' then reappeared biennially until 1876, when it began a long run of annual appearances which lasted until 1917. The next issue was a delayed 1918/19 edition, which had for the first time incorporated its main rival publication, the ''[[Clergy List]]''. Further issues appeared for 1920 and 1921/22; then between 1923 and 1927{{efn|There was no issue in 1928, for what the editor called "technical reasons". Production difficulties in 1941/42, 1943 and 1944 meant that it was only possible to issue short supplements to the 1941 edition. ''Crockford Prefaces: The Editor Looks Back'' (Oxford, 1947), pp. i, 257, 272, 283.}} and 1929&#8211;1940 the directory reappeared annually, followed by more late issues in 1941 and 1947/48. Since that time ''Crockford'' has generally appeared every two years, although gradually worsening delays meant that the 87th and 88th editions were dated 1977/79 and 1980/82, and the book failed to appear at all during 1983/84. Biennial publication was once again resumed in 1985/86, although the volume issued late in 1997 was designated the 1998/99 edition. The 100th edition &#8211; eventually published for 2008/09 &#8211; included within its hardback version a few facsimile pages from the first edition, together with an extended historical note describing some of the earlier volumes.

The 1858 edition was later described as seemingly &#8220;assembled in a very haphazard fashion, with names added &#8216;as fast as they could be obtained&#8217;, out of alphabetical order and with an unreliable index&#8221;. But nevertheless the 1860 directory &#8220;had become a very much more useful work of reference&#8221;.{{efn|Quoted by Brenda Hough in her biographical note on John Crockford, published in the 1998/99 ''Crockford'' and reprinted (with minor modifications) in all subsequent editions; also on the official Crockford's website.&lt;ref&gt;{{Cite web
| title = About John Crockford
| author = Brenda Hough
| work = Crockford's clerical Directory - online
| date =
| accessdate = 2014-08-12
| url = http://www.crockford.org.uk/standard.asp?id=126
| quote =
}}&lt;/ref&gt;}}  However the original volume was actually a consolidation of what in 1857 had been conceived as a mere series of supplements to an entirely different publication, the ''Clerical Journal''.{{efn|The ''Oxford Dictionary of National Biography'' article on Edward William Cox states that he, together with John Crockford, had founded the ''Clerical Journal'' in 1853.}}  The editors explained in the preface that they  wished it to be understood that it was &#8220;but the foundation of a great work which, with the Cordial aid of the clergy, we shall hope to make more and more perfect every year&#8221;.

== Scope of the directory ==

[[File:Crockford1910BpLichfield.jpg|thumb|''Crockford'', 1910: a biographical page in an older edition would typically include many abbreviations, including clergy academic backgrounds, and their dates ordained deacon [d] and priest [p] (the presiding bishop being indicated). Diocesan coats of arms were shown alongside episcopal entries; any publications were listed, and parish incomes and patrons were mentioned. Many overseas clergy would be covered.]] The 1858 issue was based on postal returns from the clergy in England and Wales, involving an outlay &#8211; as the preface pointed out &#8211; of "more than Five Hundred Pounds for Postage Stamps alone".  Simpler lists for the [[Scottish Episcopal Church]] and for a number of colonial clergy &#8211; obtained from alternative sources &#8211; had been added by the 1865 edition, whilst details of Irish clergy had also been extracted from [[Alexander Thom (almanac editor)|Alexander Thom]]'s ''Irish Almanack and Official Directory''.  From the 1870s onwards the scope was progressively extended to all parts of the Anglican communion with the notable exception of the [[Episcopal Church (United States)]].  The 1870 edition contained 940 pages, but this had increased to over 2,100 pages by 1892.

The earliest editions had also gradually added some details of diocesan office holders and administrators, together with the theological colleges, and the royal chapels.  They also acquired much fuller indexes &#8211; along with outline maps of dioceses, and increasingly complete lists of bishops, dating right back to the earliest years of their sees.  They further offered to all clergy an opportunity to list their publications, although these lists eventually had to be cut back as their overall length started to increase dramatically.

By the early 1980s severe economies had become necessary and 1985/86 edition had to be restricted to the "home" churches of England, Scotland and Wales.&lt;ref&gt;''Crockford&#8217;s Clerical Directory 1987/88'', pp. 47-48&lt;/ref&gt;  Retired clergy were temporarily restricted to just a few details of their final appointment, although it became possible to restore the Irish clergy in time for the 1987/88 edition.  Later editions saw a further return of the retired clergy, together with details of those overseas clergy who had originally been licensed or trained in the UK, or who occupied senior positions within their respective church hierarchies.  Details which had also become obtainable from the ''[[Church of England Yearbook]]'' or from similar sources were generally excluded. For a time too clergy who made their livings though secular jobs  were excluded from the biographies section, with the abbreviation NQ (Non-Qualifying Position) being used to cover such periods when clerics returned to parish work and were again eligible for inclusion. In that many such clergy retained diocesan licences or episcopal "Permissions to Officiate" during their periods of secular employment, this approach may have caused a degree of difficulty for clerics who needed to prove their clerical status.

By 1985/86 the first women deacons were being included (although [[Hong_Kong_Sheng_Kung_Hui#Social_issues|women priests ordained in Hong Kong]] were included even in the 1970s) while other more recent innovations &#8211; from the 1990s onwards &#8211; have included optional email addresses, together with lists of those clergy who have died since the previous edition.  Notes on "How to Address the Clergy"&lt;ref&gt;{{Cite web
| title = How to address the clergy
| author =
| work = Crockford's Clerical Directory
| date =
| accessdate = 2014-08-12
| url = http://www.crockford.org.uk/standard.asp?id=116
| quote =
}}&lt;/ref&gt; have been retained. A small number of clergy have been excluded at their own request, or have allowed their biographies to appear minus a contact address.  The Church Commissioners soon replaced the traditional black hardback bindings in favour of red and also introduced a separate softback alternative version.

Since 2004 there has also been a frequently updated Internet edition of ''Crockford'', which is available by subscription.&lt;ref&gt;{{Cite web
| title = Welcome to the Crockford web site...
| author =
| work = Crockford's Clerical Directory
| date =
| accessdate = 2014-08-12
| url = http://www.crockford.org.uk/
| subscription=yes
}}&lt;/ref&gt; More recently the directory has also joined in with [[social networking]], operating a [[Twitter]] account since 2012. 

An alternative to the main work, ''Crockford's Shorter Directory'', focused almost entirely on the Church of England and omitting all past biographical details, was issued as a single edition in 1953&#8211;54.

== Prefaces ==

The well-known tradition of having an extensive but anonymous preface offering a general review of events within the Anglican communion &#8211; together with some occasionally sharp and controversial commentary &#8211; evolved gradually during the early part of the 20th century.{{sfn|Currie &amp; Paflin|7 December 2007}} Previous prefaces had tended to be much briefer and they had often been limited merely to explaining the directory&#8217;s in-house policies.  After the events following the publication of the 1987/88 edition, which had ended with the death of Dr [[Gareth Bennett]], this tradition of the anonymous preface was discontinued.

An anthology ''Crockford Prefaces: The Editor Looks Back'', anonymously edited by [[Richard Henry Malden]]{{sfn|Currie &amp; Paflin|7 December 2007}} and covering the previous 25 years, was published by the Oxford University Press in 1947.{{sfn|Anon|1947|p=}}

== Locating previous issues ==

County libraries each have their own policies, but there are good collections in a number of major academic and ecclesiastical libraries, including [[Cambridge University Library]], [[Lambeth Palace Library]], [[Canterbury Cathedral]] Library, [[York Minster]] Library, the [[Guildhall Library]] and the [[Society of Genealogists]].

Besides the 1865 reprint,{{sfn|Bell|1995|p=}} a small number of early editions have been reissued in CD format by various publishers, including [[Archive CD Books]].  Scanned copies of other early editions have also begun to appear on the World Wide Web.

== Crockford references in fiction ==

Crockford is referenced in [[Dorothy Sayers]]'s 1927 detective novel ''[[Unnatural Death]]'' (chapter XI) where [[Lord Peter Wimsey]] uses "this valuable work of reference" in trying to trace a clergyman who is important for solving the book's mystery.

Another fictional character holding Crockford on his bookshelves was [[Sherlock Holmes]], who during one of his final short stories ("The Adventure of the Retired Colourman"), consulted his copy before dispatching his colleague Dr Watson, together with another companion, to a distant part of Essex. There they interviewed &#8220;a big solemn rather pompous clergyman&#8221; who received them angrily in his study.

The character Dulcie Mainwaring prefers Crockford's format to ''[[Who's Who]]'' while reflecting on researching in the [[Public Record Office]] in London in [[Barbara Pym]]'s ''No Fond Return of Love.  ''

== Footnotes ==
{{notelist}}

== Notes ==
{{reflist}}

== References ==
{{refbegin}}

* {{Citation

| title = Crockford, John (1824/5&#8211;1865)
| first1 = Brenda
| last1= Hough
| first2=H. C. G.
| last2= Matthew
| work = Oxford Dictionary of National Biography
| date =  2004
| accessdate = 2014-08-12
| url = http://dx.doi.org/10.1093/ref:odnb/37324
| language =
| ref={{sfnref|Hough &amp; Matthew|2004}}
}}

* {{Cite web
 |title=Shop-talk and mordant wit 
 |first1=Christopher 
 |last1=Currie 
 |first2=Glyn 
 |last2=Paflin 
 |work=The [[Church Times]] 
 |date=7 December 2007 
 |issue=7552 
 |url=http://www.churchtimes.co.uk/content.asp?id=48255 
 |accessdate=2014-08-12 
 |archive-url=https://web.archive.org/web/20120407024756/http://www.churchtimes.co.uk/content.asp?id=48255 
 |archive-date=2012-04-07 
 |ref={{sfnref|Currie &amp; Paflin|7 December 2007}} 
 |subscription=yes 
 |deadurl=yes 
 |df= 
}}

* {{cite book|ref={{sfnref|Bell|1995|p=}}|last=Bell|first=Peter|title=Crockford's Clerical Directory for 1865: Being a Biographical and Statistical Book of Reference for Facts Relating to the Clergy and the Church|url=https://books.google.com/books?id=oSqjAQAACAAJ|year=1995|publisher=Horace Cox  in 1865, republished by Peter Bell in 1995|location=Oxford and Edinburgh|isbn=978-1-871538-21-2}}
* {{cite book|ref={{sfnref|Anon|1947|p=}}|author=Anon|authorlink=Richard Henry Malden|title=Crockford prefaces: the editor looks back|url=https://books.google.com/books?id=rItbAAAAMAAJ|year=1947|publisher=Oxford Univ. Press}}
*{{cite book|ref=harv|author1=Church of England|author2=Central Board of Finance|author3=Church Commissioners|title=Crockford's Clerical Directory|url=https://books.google.com/books?id=BzkFAAAAYAAJ|year=1865|publisher=Oxford University Press}}

{{refend}}

== External links ==

* {{Official website|http://www.crockford.org.uk/}}
* [https://play.google.com/store/books/details/Church_of_England_Crockford_s_Clerical_Directory?id=BzkFAAAAYAAJ Crockford's Clerical Directory 1865 free download from Google play]
* [http://www.chpublishing.co.uk/ Church House Publishing]
* [https://archive.org/details/crockfordscleri00commgoog 1868 version available for free download at the archive.org]

[[Category:Directories]]
[[Category:Church of England]]
[[Category:Church in Wales]]
[[Category:Scottish Episcopal Church]]
[[Category:Anglicanism]]</text>
      <sha1>5rd83ikhxy872wq7ul51ce0023dtha4</sha1>
    </revision>
  </page>
  <page>
    <title>Reverse telephone directory</title>
    <ns>0</ns>
    <id>5279920</id>
    <revision>
      <id>755512943</id>
      <parentid>743175336</parentid>
      <timestamp>2016-12-18T14:16:57Z</timestamp>
      <contributor>
        <username>Rjwilmsi</username>
        <id>203434</id>
      </contributor>
      <minor />
      <comment>/* United States */Journal cites, added 1 DOI using [[Project:AWB|AWB]] (12130)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6844" xml:space="preserve">A '''reverse telephone directory''' (also known as a '''gray pages''' directory, criss-cross directory or '''reverse phone lookup''') is a collection of telephone numbers and associated customer details. However, unlike a standard [[telephone directory]], where the user uses customer's details (such as name and address) in order to retrieve the telephone number of that person or business, a reverse telephone directory allows users to search by a telephone service number in order to retrieve the customer details for that service.

Reverse telephone directories are used by law enforcement and other emergency services in order to determine the origin of any request for assistance, however these systems include both publicly accessible (listed) and private (unlisted) services. As such, these directories are restricted to internal use only. Some forms of [[city directory|city directories]] provide this form of lookup for listed services by phone number, along with address cross-referencing.

Publicly accessible reverse telephone directories may be provided as part of the standard directory services from the telecommunications carrier in some countries. In other countries these directories are often created by [[phreaking|phone phreaker]]s by collecting the information available via the publicly accessible directories and then providing a search function which allows users to search by the telephone service details.

==History==
Printed reverse phone directories have been produced by the telephone companies (in the United States) for decades, and were distributed to the phone companies, law enforcement, and [[public library|public libraries]].&lt;ref&gt;{{cite news | url=https://news.google.com/newspapers?nid=1454&amp;dat=19720102&amp;id=87osAAAAIBAJ&amp;sjid=vgkEAAAAIBAJ&amp;pg=3122,379459 | title=Clinton Directory Issued | date=Jan 2, 1972 | accessdate=9 February 2014 | location=Page 16}}&lt;/ref&gt; In the early 1990s, businesses started offering reverse telephone lookups for fees, and by the early 2000s advertising-based reverse directories were available online, prompting occasional alarms about privacy concerns.

==Australia==
In 2001, a legal case ''[[Telstra|Telstra Corporation Ltd]] v Desktop Marketing Systems Pty Ltd'' was heard in the Australian Federal Court.&lt;ref&gt;{{cite web|url=http://www.austlii.edu.au/au/cases/cth/federal_ct/2001/612.html|title=Telstra Corporation Limited v Desktop Marketing Systems Pty Ltd (2001) FCA 612 (25 May 2001)|author=[[Federal Court of Australia]]|publisher=Australasian Legal Information Institute|accessdate=2008-01-03}}&lt;/ref&gt;&lt;ref name=austliiPP&gt;{{cite web|url=http://www.austlii.edu.au/au/journals/PLPR/2001/25.html|title=Private parts - PLPR 25; (2001) 8 PLPR 24|publisher=Australasian Legal Information Institute|accessdate=2008-01-03}}&lt;/ref&gt; gave Telstra, the predominant carrier within Australia and the maintainer of the publicly accessible [[White Pages]] (residential) and [[Yellow Pages]] (commercial) directories, [[copyright]] over the content of these directories.

In February 2010 a Federal Court of Australia case ''[[Telstra|Telstra Corporation Ltd]] v Phone Directories Company Pty Ltd'' determined that Telstra does not hold copyright in the White Pages or the Yellow Pages.&lt;ref&gt;{{cite news|url=http://www.smh.com.au/business/copyright-to-enter-a-new-dimension-20101215-18y9o.html|title=Copyright to enter a new dimension|newspaper=[[The Sydney Morning Herald]]| first=Malcolm|last=Maiden|date=16 December 2010|accessdate=20 December 2012}}&lt;/ref&gt;

As it currently{{when|date=October 2014}} stands there is no legal way to ensure a particular number is not listed in the directories currently available.

==United States==

In United States, landline phone subscribers can pay a small fee to exclude their number from the directory. This service is usually called "Your Listing Not Published" and the cost ranges between $0.80 and $1.50 for residential customers.

As [[cellular phones]] become more popular, there has been debate about releasing cell phone numbers into public [[4-1-1|411]] and reverse number directories. (S. 1963, the "Wireless 411 Privacy Act" 9/2004). However, opposition led by leading consumer-protection organization [[Consumers Union]] presented several privacy concerns in their congressional [http://www.consumersunion.org/pub/wireless%20411%20senate%20testimony%20final.pdf testimony]. Right now,{{when|date=October 2014}} cell phone numbers are not available in any public 411 or reverse-number directories. However, several information companies provide reverse cell phone lookups that are obtained from utility resources, and are available online. Because there is no central database of cell phone numbers, reverse phone directories that claim to be free cannot return information on those numbers.&lt;ref&gt;{{cite journal | title=Evaluating the utility and accuracy of a reverse telephone directory to identify the location of survey respondents. | work=2005 Feb |vauthors=Schootman M, Jeffe D, Kinman E, Higgs G, Jackson-Thompson J | pmid=15652722 | doi=10.1016/j.annepidem.2004.06.005 | volume=15 | pages=160-6}}&lt;/ref&gt;

In recent years{{when|date=October 2014}} community web based services offer a reverse telephone directory of known telemarketers, debt collectors, fund raisers, and other solicitors which contact consumers by telephone.  Users of these services can perform a search of the telephone number which showed up on their caller ID and read through user comments to find the identity of the calling company or individual.

==United Kingdom==
In the United Kingdom proper, reverse directory information is not publicly available.&lt;ref&gt;{{cite web | url=http://ico.org.uk/for_organisations/privacy_and_electronic_communications/the_guide/directories_of_subscribers | title=Directories of subscribers | publisher=Information Commissioner's Office | accessdate=9 February 2014}}&lt;/ref&gt; However, in the [[Channel Islands]] it is provided in the printed telephone directories.

Although the information is, of necessity, available to emergency services, for other agencies it is treated as 'communication data' in the [[RIPA]] regime and subject to the same controls as requests for lists of and content of calls.

==References==
{{reflist}}

==External links==
&lt;!-- Do not delete these comments. --&gt;
&lt;!-- Do not put commercial links into this list. Doing so can get you blocked with no further warning. --&gt; 
*[https://web.archive.org/web/20010721175437/http://blackpages.2600.org.au/ Wayback Machine (21 July 2001) archive of http://blackpages.2600.org.au]
*[http://www.austlii.edu.au/au/cases/cth/federal_ct/2001/612.html Federal Court of Australia Case 612 (25 May 2001): Telstra Corporation Limited v Desktop Marketing Systems Pty Ltd]

[[Category:Telephone numbers]]
[[Category:Directories]]
[[Category:Information retrieval systems]]</text>
      <sha1>n7qp47ly0f5rqr5az9kaon4az2ljw38</sha1>
    </revision>
  </page>
  <page>
    <title>EADP</title>
    <ns>0</ns>
    <id>23508467</id>
    <revision>
      <id>725522998</id>
      <parentid>725522839</parentid>
      <timestamp>2016-06-16T05:36:01Z</timestamp>
      <contributor>
        <username>JoshMuirWikipedia</username>
        <id>25843868</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1256" xml:space="preserve">{{refimprove|date=August 2014}}

The '''European Association of Directory and Database Publishers''', known as '''EADP''', was founded in 1966. EADP is the key representative for the [[Europe|European]] directory and database publishing sector. As such, EADP has 180 members from 36 countries and represents the interests of some 340 [[Telephone directory|directory]] publishers. The associations members and affiliate members include [[Publishing|publishers]] and stakeholders from the industry such as suppliers and vendors.&lt;ref&gt;{{cite web|title=About EADP|url=http://www.eadp.org/index.php?q=aboutus|accessdate=25 July 2013}}&lt;/ref&gt;

EADP's activities include:

* Maintaining an up-to-date member directory
* Facilitating an annual congress and a separate annual conference
* Monitoring EU legal activities of relevance to the industry
* Compiling an annual statistical report and benchmarking studies

The [[North America|North American]] counterpart to the EADP is the [[Yellow Pages Association]] (YPA).

== References ==
&lt;references /&gt;

==External links==
*[http://www.ypassociation.org/ YPA web-site]
*[http://www.eadp.org/ EADP web-site]

{{DEFAULTSORT:Eadp}}
[[Category:Companies established in 1966]]
[[Category:Directories]]


{{telephony-stub}}</text>
      <sha1>hek52me4ie3hpprqz8b07z1rnpa09er</sha1>
    </revision>
  </page>
  <page>
    <title>Pamyatnaya Knizhka</title>
    <ns>0</ns>
    <id>26512894</id>
    <revision>
      <id>545872612</id>
      <parentid>539606602</parentid>
      <timestamp>2013-03-21T03:54:13Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor />
      <comment>[[User:Addbot|Bot:]] Migrating 1 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q4342965]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2801" xml:space="preserve">{{unreferenced|date=April 2010}}
'''Pamyatnaya knizhka''' Memorial Book &#1055;&#1072;&#1084;&#1103;&#1090;&#1085;&#1072;&#1103; &#1082;&#1085;&#1080;&#1078;&#1082;&#1072; is the title of official reference books of regions and towns in [[Russian empire]].

== History ==
The books were annually published by local authorities in 89 [[gubernia]]s and regions of [[Russian Empire]] starting from the mid-1830s till 1917. They provide information on population and businesses in the course of over 60 years. Over two thousand books have been found.

== Composition of Pamyatnaya knizhka ==
The books had some peculiarities in some [[gubernia]]s and not always comprised 4 main sections:
* [[address-calendar]] (index of all local official institutions and their staff),
* administrative reference book (information on administrative units in a [[gubernia]], post offices, roads, industrial and commercial enterprises, hospitals and chemists&#8217;, educational institutions, museums and libraries, book stores and print shops, periodicals, list of towns, major landowners etc.),
* statistic data (statistic tables on population, farming, education, incomes, fires etc.);
* historical background.

== Research project ==
The [[Russian National Library]] is carrying out a research project devoted to Pamyatnaya knizhka. The project is supervised by Mrs. Nadezhda Balatskaya &#1053;&#1072;&#1076;&#1077;&#1078;&#1076;&#1072; &#1052;&#1080;&#1093;&#1072;&#1081;&#1083;&#1086;&#1074;&#1085;&#1072; &#1041;&#1072;&#1083;&#1072;&#1094;&#1082;&#1072;&#1103; in the department Bibliografia and krayevedeniye of the [[Russian National Library]].
The project comprises official memorial books Pamyatnay knizhkas of all gubernias and regions of [[Russia]], including areas that are no longer within the [[Russian Federation]].
The main result of the project will be the publication of 15 volume bibliographic index book titled &#8220;&#1055;&#1072;&#1084;&#1103;&#1090;&#1085;&#1099;&#1077; &#1082;&#1085;&#1080;&#1078;&#1082;&#1080; &#1075;&#1091;&#1073;&#1077;&#1088;&#1085;&#1080;&#1081; &#1080; &#1086;&#1073;&#1083;&#1072;&#1089;&#1090;&#1077;&#1081; &#1056;&#1086;&#1089;&#1089;&#1080;&#1081;&#1089;&#1082;&#1086;&#1081; &#1080;&#1084;&#1087;&#1077;&#1088;&#1080;&#1080;: &#1059;&#1082;&#1072;&#1079;&#1072;&#1090;&#1077;&#1083;&#1100; &#1089;&#1086;&#1076;&#1077;&#1088;&#1078;&#1072;&#1085;&#1080;&#1103;&#8221;. Some of materials in work are available at [http://givc.ru national computer center]. &#1056;&#1072;&#1073;&#1086;&#1095;&#1080;&#1077; &#1084;&#1072;&#1090;&#1077;&#1088;&#1080;&#1072;&#1083;&#1099; &#1087;&#1088;&#1086;&#1077;&#1082;&#1090;&#1072; &#1087;&#1088;&#1077;&#1076;&#1089;&#1090;&#1072;&#1074;&#1083;&#1103;&#1102;&#1090; &#1086;&#1075;&#1088;&#1086;&#1084;&#1085;&#1099;&#1081; &#1080;&#1085;&#1090;&#1077;&#1088;&#1077;&#1089; &#1076;&#1083;&#1103; &#1083;&#1102;&#1076;&#1077;&#1081;, &#1079;&#1072;&#1085;&#1080;&#1084;&#1072;&#1102;&#1097;&#1080;&#1093;&#1089;&#1103; &#1075;&#1077;&#1085;&#1077;&#1072;&#1083;&#1086;&#1075;&#1080;&#1077;&#1081;. That is an important source for genealogic researchers.

==External links==
* [http://www.nlr.ru/pro/inv/mem_buks.htm the site of the project of the Russia&#8217;s National Library &#171;&#1055;&#1072;&#1084;&#1103;&#1090;&#1085;&#1099;&#1077; &#1082;&#1085;&#1080;&#1078;&#1082;&#1080; &#1075;&#1091;&#1073;&#1077;&#1088;&#1085;&#1080;&#1081; &#1080; &#1086;&#1073;&#1083;&#1072;&#1089;&#1090;&#1077;&#1081; &#1056;&#1086;&#1089;&#1089;&#1080;&#1081;&#1089;&#1082;&#1086;&#1081; &#1080;&#1084;&#1087;&#1077;&#1088;&#1080;&#1080;&#187;]
* [http://chigirin.narod.ru/book.html#spravochniki some memorial books of Russian Empire in pdf at Russian national computer center]
* [http://russian-family.ru/index.php?option=com_jdownloads&amp;task=viewcategory&amp;catid=23&amp;Itemid=27 Memorial books of some gubernias]
{{use dmy dates|date=December 2010}}

[[Category:Russian Empire]]
[[Category:Directories]]
[[Category:Genealogy publications]]</text>
      <sha1>afn382ycllxeeqjazi68evc31mmu4oy</sha1>
    </revision>
  </page>
  <page>
    <title>Blogged.com</title>
    <ns>0</ns>
    <id>15955818</id>
    <revision>
      <id>747766818</id>
      <parentid>712558949</parentid>
      <timestamp>2016-11-04T07:27:53Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 4 sources and tagging 0 as dead. #IABot (v1.2.6)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5159" xml:space="preserve">{{Orphan|date=February 2009}}
{{Infobox dot-com company
| name     = Blogged.com 
| logo     = 
| company_type     = [[Private company|Private]]
| foundation       = 2008
| founder          =
| location         = [[Alhambra, California]], [[United States]]
| key_people       = 
| revenue          = unknown
| operating_income = 
| net_income       = 
| owner            = 
| num_employees    = number unknown
| company_slogan   = 
| url              = [http://www.blogged.com/ www.blogged.com]
| screenshot       = [http://www.techcrunch.com/wp-content/blogged-small.png]
| caption          = Screenshot of Blogged.com home page
| alexa            = {{IncreaseNegative}} 16,125,052 ({{as of|2014|4|1|alt=April 2014}})&lt;ref name="alexa"&gt;{{cite web|url= http://www.alexa.com/siteinfo/blogged.com |title= Blogged.com Site Info | publisher= [[Alexa Internet]] |accessdate= 2014-04-01 }}&lt;/ref&gt;&lt;!--Updated monthly by OKBot.--&gt;
| website_type     = [[Wiki]] [[Blog directory]]
| language         = multilingual
| advertising      = 
| registration     = Optional
| launch_date      = {{launch date and age|2008|2|24|p=y}}
| current_status   = inactive{{Citation needed|date=May 2012}}
}}

'''Blogged.com''' is a blog directory that attempts to combine social networking with people's interests in blogging. It employs a method of niche social networking whereby people can connect to each other by their interests rather than their social connections. It attempts to use a method of [[crowdsourcing]] to help evaluate the quality of various blogs{{Citation needed|date=February 2008}}. Blogged symbolizes a trend of new sites that attempt to connect people with their interests rather than social connections{{Citation needed|date=February 2008}}. This type of niche social networking has been employed successfully by sites such as [[Flixster]], [[Yelp, Inc.|Yelp]], [[Last.FM]], and [[Stumbleupon]].  [[TechCrunch]] has recently compared Blogged.com to Yelp for blogs.&lt;ref&gt;
{{cite news
 | first = Erick 
 | last = Schonfeld
 | title = Blogged Hopes to Become the Yelp of Blog Directories
 | url = http://www.techcrunch.com/2008/02/24/blogged-hopes-to-become-the-yelp-of-blog-directories/
 |publisher=Tech Crunch
 |date=2008-02-24
 }}
&lt;/ref&gt;

== Method ==

Blogged.com focuses on blog discovery and displays expert reviews and ratings on popular blogs thereby providing a basis from which to introduce new blogs to a potential reader. Traditional blog search sites such as Technorati and Google Blog Search offer users a method of searching through individual blog entries or postings, but not the blog website itself. Therefore, it is sometimes difficult to gauge the quality or importance of the search results since the credibility of the website which contains the blog entry may be in question. This method of propagating high-quality blogs via user feedback has been used by websites such as Digg. Digg allows users to vote on the importance of articles and causes those articles which are most popular to rise to the top. This method, commonly called [[crowdsourcing]], is being used by Blogged to utilize user feedback to gauge the importance of various blogs.{{Citation needed|date=February 2008}}

== Status ==

As of 2/3/2016 BLOGGED.COM does not resolve to any site and returns a "Not Found" message in the upper right corner.

The last known active date for Blogged.com is July 25, 2011.&lt;ref&gt;{{cite web|url=http://www.blogged.com/ |title=Blogged.com Last Crawled Date: July 25, 2011 |publisher=Internet Archive Wayback Machine |date= |accessdate=2012-05-24 |deadurl=yes |archiveurl=https://web.archive.org/web/20110725121828/http://www.blogged.com/ |archivedate=July 25, 2011 }}&lt;/ref&gt; The site currently redirects to [[Chime.in]].{{Citation needed|date=May 2012}}

== See also ==

* [[Facebook]]
* [[Yelp, Inc.]]
* [[Flixster]]
* [[Friendster]]
* [[Stumbleupon]]

==Notes==
{{reflist}}

==References==
* "[http://marketwire.com/mw/release.do?id=825026&amp;k=blogged.com Blogged.com Connects Bloggers With Readers; Increases Traffic and Promotes Quality Content Filtered by People]," Marketwire, 2/25/2008.
* "[http://www.webware.com/8301-1_109-9877585-2.html Blogged.com launches blog directory, reviews]," ''[[CNET]] Blogs'', 2/25/2008.
* "[http://mashable.com/2008/02/24/bloggedcom/ Blogged.com. More than Just Another Blog Search Tool]," ''[[Mashable]]'', 2/25/2008.
* "[http://www.blogherald.com/2008/02/25/bloggedcom-new-blog-directory-officially-launches/ Blogged.com new blog directory officially launches]," Blogherald, 2/25/2008.
* "[http://www.bloggingtips.com/2008/02/25/bloggedcom-public-beta-goes-live/ Blogged.com Public Beta Goes Live]," BloggingTips, 2/25/2008.
* "[https://web.archive.org/web/20070214023246/http://publications.mediapost.com:80/index.cfm?fuseaction=Articles.showArticleHomePage Blogged.com Ranks Blogs For Consumers, Could Help Bloggers Monetize]," Online Media Daily, 2/25/2008

==External links==
* [https://web.archive.org/web/20110725121828/http://www.blogged.com/ Blogged.com]

{{DEFAULTSORT:Blogged.Com}}
[[Category:Blogging]]
[[Category:Directories]]
[[Category:American websites]]
[[Category:Alhambra, California]]</text>
      <sha1>9ptpw474x2r9pfiz1098yulbxokryt2</sha1>
    </revision>
  </page>
  <page>
    <title>List of yellow pages</title>
    <ns>0</ns>
    <id>12767772</id>
    <revision>
      <id>761527664</id>
      <parentid>753653344</parentid>
      <timestamp>2017-01-23T12:56:52Z</timestamp>
      <contributor>
        <username>John of Reading</username>
        <id>11308236</id>
      </contributor>
      <minor />
      <comment>Typo fixing, replaced: On April 2007 &#8594; In April 2007, .. &#8594; ., is been &#8594; has been using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="42152" xml:space="preserve">{{multiple issues|
{{Advert|date=May 2010}}
{{Refimprove|date=May 2010}}
}}

[[Yellow pages]] [[telephone directories]] of businesses:

{{compact ToC|side=yes|top=yes|num=yes}}

==A==
* '''Afghanistan''': In [[Afghanistan]], the Canadian INGO [[Peace Dividend Trust]] launched a free online directory with over 2700 verified and registered Afghan enterprises in late 2006.
* '''Africa''': In [[Africa]], a business directory is YelloPagesAfrica published by ''Yellopages Development Company Limited''. It is an online business directory. It ia an interactive online business directory with a mission to integrate Africa businesses. It covers the entire Africa continent and operates on a do-it-yourself basis.
* '''Albania''': In [[Albania]], the directory is called Flete te Verdha - Albanian Yellow Pages which is a registered trademark belonging to Maxidisk SH.P.K Group and Fleteteverdha sh.p.k from Tirana.
* '''Algeria''': In [[Algeria]], the Yellow Pages business directory is published in French as '''Les Pages Jaunes'''. It is also available online in English and in French.
* '''Anguilla''': In [[Anguilla]], the directory is published by [[Global Directories Limited]] and titled ''Anguilla Yellow Pages''. Print copies are distributed free to each telephone subscriber and is also available online at www.anguillayp.com.
* '''Armenia''': In [[Armenia]], "Spyur" Information Center introduces "Armenia Yellow Pages". Directory is printed in English, [[Armenian language|Armenian]] and [[Russian language|Russian]](17500 copies a year). Other directory Armenian Business Pages was launched in 2015 by Comfy LLC and represents only digital version of yellow pages of Armenia. The directory is in the process of Electronic Armenia&#174; trademark registration.&lt;ref&gt;http://www.pages.am&lt;/ref&gt;
* '''Aruba''': In [[Aruba]], the official telephone directory of Setar is published by [[Global Directories Limited]] and titled ''Aruba Yellow Pages''. 85.000 Print copies are distributed free to households and companies and is also available online
* '''Austria''': The "Yellow Pages" and [[Yellow Pages#Internet yellow pages|IYP]] services are provided by: HEROLD Business Data GmbH, a [[European Directories]] group company.
* '''Australia''': In Australia, the most comprehensive business directory is the Yellow Pages published by [[Telstra#Directories and advertising (Sensis)|Sensis]]. The directory is also available online, on mobile and via smartphone app.

==B==
* '''Bangladesh''': In [[Bangladesh]], the business directory is published by '''''Ad Yellowpages''''' '''''Pages''''' and titled '''''Ad Yellowpages Yellow Pages'''''. AdYP, a sister conern of Ad Yellowpages.com is an new concept brought forward by the founders of the site. The site provide a variety of information about local places and businesses in Bangladesh.&lt;ref&gt;https://localyaar.com&lt;/ref&gt;
* '''Bahrain''': In [[Bahrain]], the business directory is published by '''''Primedia International BSC (c)''''' and titled '''''Bahrain Yellow Pages'''''. Primedia International signifies a fundamental move away from the traditional business directories to new print &amp; online media.
* '''Barbados''': In [[Barbados]], the directory is published by [[Global Directories Limited]] and titled ''Barbados Yellow Pages''. Print copies are distributed free to each telephone subscriber and is also available online at www.barbadosyp.com and on mobile devices at yp2go.bb.
* '''Belarus''': In [[Belarus]], the directory is titled ''Business-Belarus'' ([[Russian language|Russian]]), it is also available online. There is an alternative directory, called ''Belarus XXI vek'' (Belarus 21st century), which is analogue to Yellow Pages; it is also available online.
* '''Belgium''': In Belgium, the directory is titled ''Pages d'Or'' (golden pages) (French) or ''Gouden Gids'' (golden guide) ([[Dutch (language)|Dutch]]), and is distributed free to each telephone subscriber, it is also available online.
* '''Bonaire''': In [[Bonaire]], the directory is published by [[Global Directories Limited]] and titled ''Bonaire Yellow Pages''. Print copies are distributed free to each telephone subscriber.
* '''Bolivia''': In [[Bolivia]], yellowpages exist online under the URL ''Yellow Pages.com.bo''.
* '''Bosnia''': In [[Bosnia and Herzegovina|Bosnia]], Yellow Pages exist online under YellowPages.ba.
* '''Brazil''': In [[Brazil]], the directory is titled ''P&#225;ginas Amarelas'' and is distributed free to each telephone subscriber. Available online by DYK Internet S/A.
* '''British Virgin Islands''': In the [[British Virgin Islands]], the directory is published by [[Global Directories Limited]] and titled ''British Virgin Islands Yellow Pages''. Print copies are distributed free to each telephone subscriber and is also available online at www.britishvirginislandsyp.com.

==C==
* '''Cambodia''': In [[Cambodia]], the official Yellow Pages directory is called [[Cambodia Yellow Pages]] and published under contract to local Ministry of Posts and Telecommunications by [[CAMYP Co., Ltd]].
* '''Canada''': In Canada, the company [[Yellow Pages Group]] owns the trademarks ''Yellow Pages'' and ''Pages Jaunes''. It produces and distributes directories in both English and French. Yellow Pages Group is the market leader in print and online commercial directories and one of the largest media companies in Canada, producing the official directories of [[Bell Canada]], [[Telus]], [[Aliant]], [[Manitoba Telecom Services|MTS]], and others. [[Saskatchewan]]'s [[SaskTel]], through subsidiary [[DirectWest]], is believed to be the last major [[incumbent local exchange carrier]] to publish its own directories. Competitive local directory publishers, such as PhoneGuide or DirectWest's operations in Manitoba and Alberta, usually include commercial directories on yellow paper.
* '''Cayman Islands''': In the [[Cayman Islands]], the directory is published by [[Global Directories Limited]] and titled ''Cayman Islands Yellow Pages''. Print copies are distributed free to each telephone subscriber and is also available online and on mobile devices.
* '''Chile''':
* '''China''': In China, the modern yellow pages industry was started in the late 1990s with the formation of two international joint ventures between US yellow pages publishers and China&#8217;s telecom operators, namely: a joint venture started in Shenzhen between [[RHDonnelley]] and [[China Unicom]] (later including Hong Kong&#8217;s PCCW and InfoSpace); and a joint venture between [[China Telecom Shanghai]] and what later came to be known as the yellow pages operations of [[Verizon]] {{Citation needed|date=May 2010}}. Later, another mainly state-owned telecom operator, [[China Netcom]] began to produce, either directly or on a sub-contracted basis, yellow pages in selected cities around the country. By early 2005, there were a number of independent local and international yellow pages operators in numerous cities including [[Yilong Huangbaoshu]], based in Hangzhou, Zhejiang Province with operations in Hangzhou and Ningbo {{Citation needed|date=May 2010}}. However, there is no nationwide Yellow pages in any format and only some international-trade related businesses including INBIT (USA), CHINAPAGES.COM and ALIBABA.COM (Chinese) are running some kind of national online databases based on business lists not from telephone companies. [[China Yellow Pages]] is also a common-place for finding manufacturers and exporters from China.
* '''Colombia''': In [[Colombia]], the standard yellow and [[White Pages]] are published and distributed every year free of charge by [[Publicar]], a Colombian subsidiary company of [[Carvajal Group|Carvajal]], which also publishes and distributes yellow and white pages in other Latin American countries.
* '''Croatia''': In [[Croatia]], the directory is called ''&#381;ute stranice'' (yellow pages), published by [[MTI Telefonski imenik/Zute stranice]]. Another directory is ''CroPages Business Directory/Poslovni Adresar'', published by [[Masmedia]].
* '''Cuba''': In [[Cuba]], the equivalent online directory is titled [[Paginas Amarillas]], with information on the whole of Cuba.
* '''Cyprus''': In [[Cyprus]], the Yellow Pages is edited by ID Yellowpages Ltd [[Cyprus Yellow Pages Directory]].
* '''Cyprus (North)'''; In [[Turkish Republic of Northern Cyprus]] [[CYPYP North Cyprus Yellow Pages]]
* '''Czech Republic''': In the [[Czech Republic]] and [[Slovakia]], the directory is titled ''Zlat&#233; str&#225;nky'' (golden pages), published by [[Mediatel]], Prague (a [[European Directories]] group company) and is distributed free to each telephone subscriber, usually in exchange for its previous version.

==D==
* '''Denmark''': In Denmark, a full online directory including most phone numbers is provided by ''De Gule Sider'' (a brand of Eniro, a Nordic search engine and directories company), with paper versions of yellow and white pages distributed to subscribers throughout the country; it was formerly a part of [[TDC Forlag]], a subsidiary of the national telecoms operator.
* '''Dominica''': In [[Dominica]], the directory is published by [[Global Directories Limited]] and titled ''Dominica Yellow Pages''. Print copies are distributed free to each telephone subscriber and is also available online at www.dominicayp.com.
* '''Dominican Republic''': In [[Dominican Republic]], published by [[Caribe Media]]. Publishing of printed and / or digital directories in the Dominican Republic.

==E==
* '''Egypt''': [[Egypt Yellow Pages Ltd]] is the official publisher of Yellow Pages branded products in Egypt. Egypt Yellow Pages Ltd, founded in 1991, is the owner of the Yellow Pages trademark in Egypt.
* '''Europe''': For whole Europe, the [[European Yellow Pages]] apply. The European Yellow Pages is an effort of providing harmonized data to different language environments through keeping the character of having localized search capabilities on a regional level. Harmonizing data in this context means providing information to global users mainly in English and to local users in their native language.
* '''Europe''': For [[Europe]] the directory [[Yellobook.eu]] is providing information about many branches and companies all around 33 major European countries.

==F==
* '''Finland''': In [[Finland]], the directories are called ''Keltaiset sivut'', Eniro.fi and [[Teloos.fi]]
* '''France''': In France, Yellow Pages are referred to as ''Pages Jaunes''. They are distributed free by Pagesjaunes'''.fr''', a company affiliated with [[Orange S.A.|France T&#233;l&#233;com]]. Pagesjaunes'''.com''', the .com version of ''Pages Jaunes'', was the issue of a major court case at [[World Intellectual Property Organization|WIPO]]; the original registrant, an individual from Los Angeles, won against France T&#233;l&#233;com. This court decision defended by the Parisian Lawyer, Andre Bertrand, was path-setting for the whole European Yellow Pages industry, as it decided that the phrase "Yellow Pages" cannot be considered the property of a single company. Previously, many former state monopoly telecom companies outside the US had tried to ban competition by claiming the term "yellow pages", or the translation of "yellow pages" into the vernacular, as their exclusive trademark. [[Vivendi|Vivendi Universal]] moved to enter the French Yellow Pages market in 2001 with scoot.fr, but the attempt was a killed by a reorganisation of the struggling company. Another French editor of Yellow Pages is [[Bottin]]. More competition is expected in November 2005 from the liberalisation of "12", the former unique "[[4-1-1]]" number of [[Renseignements Telephoniques]], French for Directory Inquiry. In November 2006 [[Orange S.A.|France T&#233;l&#233;com]] sold its majority share in pagesjaunes.fr to Mediannuaire. In August 2007 pagesjaunes'''.com''' finally became active, giving France two different ''Pages Jaunes''; thus creating agitation at pagesjaunes'''.fr''', which reshaped their site and started a massive advertisement campaign all over France.

==G==
* '''Georgia''': In Georgia, the directory is called &#4327;&#4309;&#4312;&#4311;&#4308;&#4314;&#4312; &#4324;&#4323;&#4320;&#4330;&#4314;&#4308;&#4305;&#4312; and published by "Yellow Pages Tbilisi" Ltd.
* '''Germany''': In Germany, a directory titled ''Die Gelben Seiten'' is distributed free to each subscriber, by the [[Deutsche Telekom]], owner of [[T-Mobile]]. Other Yellow pages are edited by ''Go Yellow.de'', ''Klicktel.de'' and [[Gelbex.de]]. In 2006 a lawsuit with the [[Deutsches Patentamt]] denied the validity of the German Trademark "Gelbe Seiten" which in fact is the German translation of the universal expression "Yellow Pages". Klaus Harisch, an Internet Pioneer from Munich and founder of Go Yellow.de had spent over 7 Million Euros on Lawyer Fees to fight for the cancellation of the German "Gelbe Seiten" trademark. Deutsche Telekom had also registered "Yellow Pages" as a German trademark which they lost at the same time. On a European Level Deutsche Telekom had failed to register "Gelbe Seiten Deutschland" or "Yellow Pages Germany" as a Euro Trademark with [[OMPI]].
* '''Gibraltar''': A combined White and Yellow Pages directory, along with an [[Yellow Pages#Internet yellow pages|IYP]] service, are provided by: gibyellow.gi, a [[European Directories]] group company.
* '''Greece''': In Greece, Yellow Pages are called ''"Chrysos Odigos"'' that can be translated as "The Golden Guide".
* '''Grenada''': In [[Grenada]], the directory is published by [[Global Directories Limited]] and titled ''Grenada Yellow Pages''. Print copies are distributed free to each telephone subscriber and is also available online at www.grenadayp.com.
* '''Guyana''': In [[Guyana]], the directory service is provided by "[[GT&amp;T]]" in printed format and in online services, there are quite a few, some of them are "[[YellowPagesGuyana]]", "[[YellowGuyana]]" and "[[GT&amp;T]]'s" own online yellowpages directory--"[[yellowpages.com.gy]]".

==H==
* '''Hong Kong''': In [[Hong Kong]], the phone directory is titled ''Hong Kong Yellow Pages'', published by [[PCCW|PCCW Media Limited]].
* '''Hungary''': In Hungary, the directory is called ''Arany Oldalak'' (gold pages); are published and distributed by [[MTT Magyar Telefonk&#246;nyvkiad&#243;]] Kft, Buda&#246;rs.

==I==
* '''India''': [[India]] is a very large country in terms of population, business activities and economy. There are multiple Yellow Pages being published by private sector companies. Some of them focus the whole nation and some are regional.
* '''Indonesia''': In [[Indonesia]], the telecommunication company [[Telkom (Indonesia)|Telkom]] with [[PT. Infomedia Nusantara]] (one of its subsidiaries), regularly publishes phone books. The company provides directory, call centre, and content services since 1984. The phone books consist of white pages and yellow pages, which are published in hard and soft copies.
* '''Iran''': In the Islamic Republic of [[Iran]], the directory is called ''The first book'' or in [[Persian language|Persian]] ''Keta:b e Avval''. This directory divides into different sections such as Directory of Businesses, jobs and maps and city guides. There is an official YellowPages in Iran owned and published by Iranian Yellow Page company. It has been developed in Persian and English languages, and contains different categories and locations of Iran. There is also an unofficial company that runs ''The Iran Yellow Pages''. This directory is published by Moballeghan Publishing and Advertising Company (1986) with the cooperation of The Trade Promotion Organization of Iran. By 2010 a new updated comprehensive directory called ''"The First Portal"'', or ''"First Eurasia E-commerce"'' or in [[Persian language|Persian]] ''"&#1578;&#1580;&#1575;&#1585;&#1578; &#1575;&#1604;&#1603;&#1578;&#1585;&#1608;&#1606;&#1610;&#1603; &#1575;&#1608;&#1604;"'' comes to the [[Iran]] high potential markets.
* '''Iraq''': In Iraq, the directory is called ''Yellow Pages'' or in Arabic (Al Safahat al Safraa). This directory divides into different sections such as directory of businesses, jobs and maps and city guides and contains thousands of businesses in many categories. The directory is published by Alam Al-Rooya Publishing and Advertising Company.
* '''Ireland''': In the [[Republic of Ireland]], the directory is called ''Golden Pages'' and is published by FCR Media. Ireland's free Yellow pages is called BusinessPages.i.e.
* '''Israel''': In Israel, the yellow pages Hebrew edition is called ''Dapei Zahav'' (Golden Pages) and the English edition is ''Golden Pages''. The print directories come out in separate issues based on Israel's different telephone area codes, published by Golden Pages Publications Ltd. Five million copies of the yellow pages are distributed annually.
* '''Italy''': in Italy, the directory is titled ''Pagine Gialle'' (Yellow Pages). The printed versions come out in separate issues for [[province]] as [[White pages]]. Some years ago, an alternative directory, called ''Pagine utili'' (Useful Page) was proposed.

==J==
* '''Jamaica''': In [[Jamaica]], the directory is published by [[Global Directories Limited]] and titled ''Jamaica Yellow Pages''. Print copies are distributed free to each telephone subscriber and is also available online at www.jamaicayp.com and to mobile subscribers at yp2go.com.jm.
* '''Japan''': In Japan, the Yellow Pages directory, are known as [[:ja:&#12479;&#12454;&#12531;&#12506;&#12540;&#12472;|Town Page]], and published by [[Nippon Telegraph and Telephone|NTT]].
* '''Jordan''': In [[Jordan]], the directory is titled '' Yellow Pages - Jordan'', Yellow Pages Jordan is operated since 2001 by PAGESJAUNES LIBAN, a subsidiary of the European PagesJaunes Group - France in 1997.

==K==
* '''Kazakhstan:''' In [[Kazakhstan]], the directory is ''Yellow Pages of Kazakhstan'', published by [[Yellow Pages Kazakhstan]] Management Group.
* '''South Korea:''' In [[South Korea]], the directory is published and distributed by many publishers:
** ''BiG Yellow Pages. Korean National Directory'', by [[Yellow Pages Korea]];
** ''Korea Yellow Pages'', by [[Korea Yellow Pages]];
** ''Korea English Yellow Pages'', by [[Korea Telecom Directory]].
* '''Kosovo:''' In [[Kosovo]], [[Faqe te Verdha]] is a trademark belonging to [[KOSOFT]], [[Pristina]].
* '''Kyrgyzstan:''' In [[Kyrgyzstan]], Yellowpages can be found under the URL "yellowpages.kg".

==L==
* '''Lebanon:''' In [[Lebanon]], the Yellow Pages business directory is published in [[Arabic language|Arabic]] and French by PAGESJAUNES LIBAN.

==M==
* '''Macau:''' In [[Macau]], the phone directory is titled ''Macau Yellow Pages/P&#225;ginas Amarelas'', publ. by Directel [[Macau Listas Telefonicas]] Lda.
* '''Madagascar:''' In [[Madagascar]] yellow pages can be found via the site Madagascar Yellow Pages.
* '''Malaysia:''' In [[Malaysia]], there are 4 large directories Malaysia Yellow Pages, Malaysia Super Pages, Malaysia Business Directory and BCZ.com
* '''Maldives:''' In [[Maldives]], the commercial phone directory is called Yell.
* '''Mali:''' In [[Mali]], the equivalent online directory is titled [[Malipages.com]].
* '''Malta:''' In [[Malta]], the Yellow Pages Directory is published by [[Ark Publishing Group]]. It has been publishing the Yellow Pages since 1997 and each year distributes 200,000 directories free of charge to the general public.
* '''Mauritius:''' In [[Mauritius]], the Yellow Pages Directory is published by [[Teleservices Ltd]] and is known as MT yellow pages 
* '''Mexico''': In Mexico, there are several commercial phone directories. The incumbent is called Seccion Amarilla.com.mx (Yellow Section) is published by Anuncios en Directorios, S.A. de C.V., a subsidiary of Telmex, the local Telco. Others are Paginas Amarillas.com.mx (Yellow Pages) published by Phonebook of the World, Paginas Amarillas.com published by Publicar, Mexico Data Online.com published by the Mexico Business Directory and Paginas Utiles.com.mx published by Ideas Intercativas, S.A.
* '''Moldova:''' In [[Moldova]] yellow pages can be found via the site [[www.yp.md]].
* '''Mongolia:''' In [[Mongolia]], the directory is called ''Mongolia Yellow Pages'' (yellow pages) and can be found via [[www.yp.mn]].
* '''Montserrat''': In [[Montserrat]], the directory is published by [[Global Directories Limited]] and titled ''Montserrat Yellow Pages''. Print copies are distributed free to each telephone subscriber and is also available online.
* '''Morocco:''' In [[Morocco]], the directory is called ''Pages Jaunes'' (yellow pages).
* '''Myanmar:''' In [[Myanmar]], the directory is called ''Yangon Directory'' (&#4123;&#4116;&#4154;&#4096;&#4143;&#4116;&#4154;&#4124;&#4121;&#4154;&#4152;&#4106;&#4157;&#4116;&#4154;).

==N==
* '''Netherlands:''' In [[Netherlands]], the directory is called ''Gouden Gids'' (literally "Golden Guide"), and within the district concerned, it is distributed free to each telephone subscriber, by De Telefoongids BV (a [[European Directories]] group company).
* '''New Zealand:''' In New Zealand, the yellow.co.nz directory is printed in 18 regional editions by Yellow Pages Group (YPG). YPG also publishes 18 regional editions of '[[White Pages]]' (combined government, residential and business listings), and a 'Local Directory' for some urban areas and sub-regions.
* '''Nigeria:''' In [[Nigeria]], Yellow Pages companies are privately owned. [[NigerianYellowPages.com]] is the official trademark owner of the walking finger logo with six (6) edition of its yellow pages in different formats. YellowPages.net.ng claimed to be the first Yellow Pages Directory in the world to emulate social media network concept.  
** '''Nigerian Yellow Pages''': Content of [[Nigerian Yellow Pages]] or commonly known as NigerianYellowPages.com is available in six formats (editions): ''CD-ROM directory''; ''MS Windows Desktop directory''; ''Internet directory''; ''Mobile Phone Internet directory''  ''Mobile Phone SMS text directory'' and ''Print directory''. There is also a mobile edition, which is accessible on mobile phones and other mobile devices such as PDA. There is a dedicated classifieds section in their yellow pages for jobs, properties, homes, rentals and announcements. It has its own toolbar, the [[Nigerian Yellow Pages Toolbar]].
** '''Africaonline business directory''': The other Yellowpage business directory in Nigeria is the Africaonline business directory, [[yellopages.com]]. This is an interactive online business directory that enables businesses to upload their profiles and place their adverts. Yellopages.com includes Nigerian content and serves to integrate Nigerian businesses.
* '''Norway:''' In Norway, the directory is called ''Gule Sider'' (Yellow Pages). The two second largest directories are [[Opplysningen 1881]] and [[Nettkatalogen]]. [[Gul.no]], [[180.no]], [[Avanti Media AS]], [[Bedriftss&#248;ket AS]], [[Gul Index]] and [[Finnfirma.no]] are som of the other directories in growth. The searchengine Sesam.no provides a business directory branded [[Sesam Katalog]].

==P==
* '''Pakistan:'''
** '''Jamal's Yellow Pages of Pakistan:''' is a B2B [[Trade Directory]] published by US Publishers (Pvt) Ltd. since 1983. The directory is published in printed form (3 volumes per set), CDROM version and online.
** '''Time's Trade Directory of Pakistan:''' Time Publishers (Pvt) Ltd. published "Time's Trade Directory of Pakistan - National Yellow Pages" since 2002. B2B Version also launch similarly as Time's e-Directory. The online version also provide comprehensive information about Pakistan Businesses to the web user worldwide.
** '''PhoneBook.com.pk:''' The [[Pakistan Telecommunication Company]] maintains a yellow pages and white pages directory. JS Enterprises Private Limited is publishing this directory, which is also associated with [[Jang Group of Newspapers]] and the GEO Television Network.
** '''Ebizpk.com: '''Online Green &amp; Yellow Pages of Pakistan. Launched in January 2010. Initially Listing 10 companies of each sector.
** '''Dmoz Pakistan:''' Database of [[Pakistani]] companies, [[Government]] departments and business organizations in categorized format.
** '''[http://ypages.pk Ypages.pk]:'''  Launched in June 2012, Online Yellow Pages of [[Pakistan]] provides all local business contact details. Ypages associated with ALM Advertising Agency.
** '''[yellowpagespk.com]: '''Marshall Online Yellow Pages  in Islamabad Pakistan &amp; Online Business Directory in Islamabad Rawalpindi Lahore Karachi Pakistan.
* '''Palestine:''' [[Palestine Yellow Pages Ltd]] is the official publisher of Yellow Pages branded products in Palestine. [[Palestine Yellow Pages]] is the exclusive owner of the Yellow Pages, Walking Fingers &amp; Design, and YellowPages.com.ps trademarks in Palestine. [[Palestine Yellow Pages]] is part of the [[Al Wahda-Express Group of Companies]]. Founded in 1986, [[Al Wahda-Express Group of Companies]] employs nearly 1000 employees publishing print, online and mobile Yellow Pages directories throughout 5 countries including Palestine.
* '''Panama:''' In [[Panama]], [[Yellow Pages Panama]]
* '''Peru:''' In United States, [[Peruvian Yellow Pages]], since 1993, the printed edition, is the first and oldest publication for Peruvians living in the USA. Now with the online version covering coast to coast the American territory. The online version of the Peruvian yellow pages is available at [[Peruvian Yellow Pages]].
* '''Philippines:''' In [[Philippines]], Directories Philippines Corporation (DPC), regularly publishes phone books of more than a dozen telecom companies in the country.
* '''Poland:''' In [[Poland]], it is called ''&#380;&#243;&#322;te strony'' and is distributed by [[Polskie Ksi&#261;&#380;ki Telefoniczne]] (a [[European Directories]] group company) as a part of their phone books. The second largest directory, published by [[Eniro]], is called "Panorama Firm" (panorama of companies). [[YellowPages.pl]]. It is the biggest online directory in Poland. Polish Yellow Pages has existed on the market since 1998. Yellow Pages enables them to search companies and products and services, it is a business platform, which helps to promote a company and to establish trade relations. In April 2007 started [[Zumi.pl]] - first local search web which connects maps and information about companies in Poland. Several historical directories from Poland are available online as scans, and can be searched via [[kalter.org]].
* '''Portugal:''' In [[Portugal]], the ''P&#225;ginas Amarelas'' are controlled by [[Portugal Telecom]] and the website is [[pai.pt]]. The printed version is distributed for free to all land line users. There is also available a residential listing called P&#225;ginas Brancas.

==Q==
* '''Qatar''': In [[Qatar]], the official Yellow Pages directory is called Qatar Yellow Pages and published by ''''Primedia Qatar W.L.L'.''' The Qatar yellowpages features comprehensive business listings for industrial and commercial establishments across the region markets. This Directory is one of the most economical media for business to showcase their products and services. The user has a choice to reference print or source online or mobile wap.

==R==
* '''Romania:''' In [[Romania]], the directory is called 'Pagini Aurii' (Golden Pages) [[paginiaurii.ro]].
* '''Russia:''' In Russia, KONTAKT EAST HOLDINGS (KEH.ST) established in 2006, is a Swedish holding company that owns Russian Company OOO ''&#1046;&#1077;&#1083;&#1090;&#1099;&#1077; &#1089;&#1090;&#1088;&#1072;&#1085;&#1080;&#1094;&#1099;'' ("JOLTI STRANITSI") (Russian translation of Yellow Pages). OOO "JOLTI STRANITSI" is the result of the successful merger in 2007 of YPI YELLOW PAGES, established in 1993, a leading publisher of Yellow Pages directories in the St. Petersburg and Perm markets and Eniro RUS-M, a publisher of leading Yellow Pages directories in Moscow, Samara and 7 other Russian cities in the Urals and Volga region.

Other directories in Russia include:

:*''&#1040;&#1076;&#1088;&#1077;&#1089; M&#1086;&#1089;&#1082;&#1074;&#1072;'' (Moscow address), by ZAO [[Verlag Euro Address]];
:*''&#1041;&#1086;&#1083;&#1100;&#1096;&#1072;&#1103; &#1058;&#1077;&#1083;&#1077;&#1092;&#1086;&#1085;&#1085;&#1072;&#1103; &#1050;&#1085;&#1080;&#1075;&#1072; &#1052;&#1086;&#1089;&#1082;&#1074;&#1099;'' (Big Phone Book of Moscow), by [[Extra M Media]];
:*''&#1042;&#1089;&#1103; &#1044;&#1077;&#1083;&#1086;&#1074;&#1072;&#1103; &#1052;&#1086;&#1089;&#1082;&#1074;&#1072;'' (all business Moscow), by [[Biznes-Karta Business Information Agency]];
:*''&#1052;&#1086;&#1089;&#1082;&#1086;&#1074;&#1089;&#1082;&#1080;&#1081; &#1041;&#1080;&#1079;&#1085;&#1077;&#1089; - Moscow Business Telephone Guide'' by [[&#1052;&#1086;&#1089;&#1082;&#1086;&#1074;&#1089;&#1082;&#1080;&#1081; &#1041;&#1080;&#1079;&#1085;&#1077;&#1089; - Moscow Business Telephone Guide]].

==S==
* '''Saint Kitts and Nevis''': In [[St Kitts &amp; Nevis]], the directory is published by [[Global Directories Limited]] and titled ''St Kitts and Nevis Yellow Pages''. Print copies are distributed free to each telephone subscriber and is also available online at www.stkittsandnevisyp.com.
* '''Saint Lucia''': In [[Saint Lucia]], the directory is published by [[Global Directories Limited]] and titled ''St Lucia Yellow Pages''. Print copies are distributed free to each telephone subscriber.
* '''Saint Vincent''': In [[Saint Vincent and the Grenadines]], the directory is published by [[Global Directories Limited]] and titled ''St Vincent Yellow Pages''. Print copies are distributed free to each telephone subscriber and is also available online at www.stvincentyp.com.
* '''Saudi Arabia''': In [[Saudi Arabia]], the directory is Saudianyellowpages.com' 'Saudiarabyellowpages.com'',. Established in 2001, is the LARGEST yellow pages of Saudi Arabia. Yellow Pages Saudi Arabia.
* '''Saudi Arabia''': Daleeli.com is an online business directory in [[Saudi Arabia]] to locate addresses, Phone numbers, maps, websites &amp; locations of Business Places and offices in Saudi Arabia.
* '''Serbia:''' In [[Serbia]], the directory is called '''Zute Strane - Yellow Pages''' (Serbian Business Directory) which is a registered trademark belonging to Yellow Pages Co. from Belgrade.
* '''Sierra Leone:''' In [[Sierra Leone]], the online yellow pages directory, [[LeoneDirect]] powered by [[Denza, LLC.]] provides contact information for local companies.
* '''Singapore:''' In [[Singapore]], it is known as "Yellow Pages" and is registered as a [[Public company]] under the name [[Yellow Pages Singapore|Yellow Pages (Singapore) Limited (Reg. no.:200304719G)]]. It is [[Public company|listed]] on the Singapore [[Singapore Exchange|SGX]] mainboard on 9 Dec 2004. It includes the Singapore Phone Book, the Chinese Yellow Pages and the Yellow Pages Buying and Commercial/Industrial Guides and advertisement sales. Yellow Pages Singapore also publishes and distributes niche directories and guides.
* '''Slovakia:''' In [[Slovakia]], it is called "Zlat&#233; str&#225;nky" (which means Golden Pages), published by [[Mediatel]] (a [[European Directories]] group company), Bratislava and is distributed free to each telephone subscriber, usually in exchange for its previous version. The online version is available at [[zlatestranky.sk]].
* '''Slovenia:''' In [[Slovenia]], the directory is called ''[[Rumene strani]]'' (Yellow Pages) which is a registered trademark belonging to Inter Marketing.
* '''South Africa:''' In [[South Africa]], the directory is called 'the Yellow Pages' which is distributed by Trudon [[yellowpages.co.za]], a subsidiary of World Directories which also publishes books in Belgium, Ireland, the Netherlands, Portugal and Romania. There are 19 regional editions covering the nine provinces. Each of the four metropolitan areas has a separate white and yellow pages book. The remaining 15 areas have both sections in one book. They also have a mobile version [[pocketbook.co.za]]
* '''Spain:''' In Spain, it is called ''P&#225;ginas Amarillas'', it was distributed by [[Telef&#243;nica|Telef&#243;nica Publicidad e Informaci&#243;n]] S.A. Yellowpages - now known as Yell Publicidad - can also be found via the Internet Address [[www.paginasamarillas.es]]. Since July 2006 the company is owned by Yell Group from the UK. A competitor is [[www.qdq.com]], a directory edited by Pages Jaunes Group from France. Another competitor is [[citiservi]], a different yellow pages service where Professionals search for Customers requesting services. Also there is an English-speaking expat directory of businesses along the south east of Spain called [[www.littleyellowpages.com]]. This site is aimed mainly at English speaking expatriates living in Spain.
* '''Sri Lanka:''' In [[Sri Lanka]], the official 'Yellow Pages' publisher is produced by [[Sri Lanka Telecom]]. However, competing publishers also use the term 'Rainbow Pages' though not the walking fingers logo.
* '''Sweden:''' In Sweden, it is called ''Gula Sidorna'', distributed by [[Eniro]]. yellowpages.se is a portal to different Yellowpages from Sweden. Gulex.se is an alternative Swedish directory, distributed by the Norwegian company Advista. Lokaldelen i Sverige AB (a [[European Directories]] group company) provide over 250 local directories in Sweden. Also hitta.se, an Online business directory by Norwegian company [[Schibsted]].
* '''Switzerland:''' In Switzerland the brand local.ch produces and distributes directories in several forms (printed, online and on mobile) including yellow and white pages - online available in [[German language|German]], French, [[Italian language|Italian]] and English.
* '''Syria:''' In [[Syria]], the directory is called &#1575;&#1604;&#1589;&#1601;&#1581;&#1575;&#1578; &#1575;&#1604;&#1589;&#1601;&#1585;&#1575;&#1569; (Yellow Pages).

==T==
* '''Thailand:''' In [[Thailand]] it is called ''Samood Nar Leung'' and also called ''Thailand YellowPages''. The company [[Teleinfo Media Public Company Limited]] produce and distribute Yellow pages nationwide. Thailand YellowPages is generated in several forms e.g. paper, Call Center no. 1188. Thailand YellowPages is produced both in Thai and English.
* '''Tunisia:''' In [[Tunisia]], it is called "&#1575;&#1604;&#1589;&#1601;&#1581;&#1575;&#1578; &#1575;&#1604;&#1589;&#1601;&#1585;&#1575;&#1569;" (Pages Jaunes) and it is owned by "Les Editions Techniques Sp&#233;cialis&#233;es", a Tunisian private company. The online version, available at [[pagesjaunes.com.tn]] for free was one of the first online directories in Arabic.
* '''Turkey:''' Yellow Medya Istanbul based Yellow Medya.
* '''Turkish Republic of Northern Cyprus''' In [Turkish Republic of Northern Cyprus]. Known as CYPYP it is found at [[cypyp.com]]
* '''Turks and Caicos Islands''': In the [[Turks and Caicos Islands]] there are two telephone directories. One is published by Olympia Publishing Company, a Turks &amp; Caicos Islands company, and carries listings from the two major telecommunications companies on the Island and the other is published by a subsidiary of Global Directories Limited, a Caymanian-based company, which carries the listings from one of the two major telecommunications companies on the Islands. Both publications are titled the ''Turks and Caicos Islands Yellow Pages'' and refer to themselves as "Local" but the Olympia Publishing Company directory is the larger and more definitive and it is the only local directory publisher.

==U==
* '''Ukraine''': In [[Ukraine]], the free business directory is titled ''PromUA'' ([[Russian language|Russian]]), it is available online at [[prom.ua]]. Other directories are: ukrindustrial.com, ukrbiznes.com, [[ukrpartner.com]].
* '''United Arab Emirates:''' Dubai-based Local Search UAE is an Online Business Directory UAE where all businesses across Abu Dhabi, Al Ain, Dubai, Fujairah, Sharjah, Ras Al Khaimah &amp; Umm Al Quwain are listed and can be searched.
* '''United Arab Emirates:''' Dubai-based [[Express Print (Publishers) L.L.C.]] is the official publisher of '''Etisalat Yellow Pages''' branded products in the UAE. Express Print (Publishers) L.L.C. publishes the Yellow Pages in both print and electronic formats. Etisalat Yellow Pages print edition consists of 3 regional directories for the areas of Abu Dhabi, Dubai and the Northern Emirates. Directories are published annually and distributed towards the end of the first quarter of each year. Express Print (Publishers) L.L.C. also publishes the Etisalat Yellow Pages on 2 electronic platforms -Online &amp; Mobile.
* '''United Arab Emirates:''' As of late 2016, Dubai-based [[ZOSER MEA]] is the official publisher of [[du Yellow Pages]] branded products in the UAE. ZOSER MEA publishes the Yellow Pages in both print and electronic formats. Directories are published annually and distributed in the month of January each year.
* '''United Arab Emirates:''' In [[United Arab Emirates]], the directory is titled ''Yellow Page Gulf UAE'',. Established in January 2011, is the LARGEST yellow pages of GULF. Yellow Pages Gulf.
* '''United Kingdom:''' The first Yellow Pages directory in the UK was produced by the [[Kingston upon Hull|Hull]] Corporation's telephone department (now [[Kingston Communications]]) in 1954. This was distributed with the alphabetical phone directory rather than as a stand-alone publication. The company now produces [[Hull Colour Pages|The Hull Colour Pages]].

:With the encouragement of [[The Thomson Corporation]], at the time an advertising sales agent for the nationalised [[General Post Office (United Kingdom)|General Post Office]]'s [[telephone directory]], a business telephone number directory named the Yellow Pages was first produced in 1966 by the GPO for the [[Brighton]] area, and was rolled out nationwide in 1973. The Thomson Corporation formed Thomson Yellow Pages in 1966 to publish and to distribute the directory to telephone subscribers for the GPO, and later for [[Royal Mail|The Post Office]].

:Thomson Yellow Pages was sold by The Thomson Corporation in 1980, at the same time as Post Office Telecommunications became the (then) state-owned [[British Telecom]] (BT). The Yellow Pages directory continued to be distributed to all telephone subscribers by BT. At the same time, The Thomson Corporation formed Thomson Directories Ltd, and began to publish the [[Thomson Local]] directory, which would remain the Yellow Pages' main, and often sole, competitor in the UK for more than the next two decades, and would be the competitive driving force behind such changes to Yellow Pages as the adoption (in 1999) of colour printing and "white knock out" listings.

:In 1984 the year that BT was privatised, the department producing the directory became a stand alone subsidiary of BT, named Yellow Pages. In the mid-1990s the Yellow Pages business was re-branded as [[Yell Group|Yell]], although the directory itself continued to be known as the Yellow Pages.

:Yell was bought by venture capitalists in 2001, and in 2003 was floated on the Stock Exchange. After the one year "no competition" clause expired BT too went into competition with the Yellow Pages, re-entering the market by adding similar content to their existing directory, "The Phone Book", adding a classified section to the traditional alphabetical domestic and business listings.

:Yellow Pages, [[Thomson Local]] and BT's [[The Phone Book]] display advertising and can be booked directly with advertising sales representatives.

:Nowadays the KC Yellow Pages is referred to as [[Hull Colour Pages]], and is separate from the White Pages. Yell now also publishes an East Yorkshire edition of Yellow Pages in competition.

[[Image:Bsyps.png|right|Bell System Yellow Pages Logo]]
* '''United States:''' In the past, AT&amp;T, Verizon, and Qwest, the three largest phone companies in the U.S., dominated the U.S. yellow pages industry; however, the term "yellow pages" and the ''Walking Fingers'' logo was heavily marketed by AT&amp;T pre [[Bell System Divestiture|divestiture]]. However, AT&amp;T never filed a trademark registration application for the current and most recognized version of the ''Walking Fingers'' logo, so it is in the public domain. AT&amp;T allowed the "independent yellow pages" industry to use the logo freely.&lt;ref&gt;[http://www.ll.georgetown.edu/federal/judicial/fed/opinions/9_opinions/91-1461.html Bellsouth v. Datanational]&lt;/ref&gt; The "independents" are unrelated to the incumbent phone company and are either pure advertising operations with no phone infrastructure or telephone companies who provide local telephone service elsewhere. Such independents include operators who typically focus on industry or business segments, or local market directories.

:Yellow pages publishers or their agents sell the right to place advertisements within the same category, next to the basic listings.

:For example, [[AT&amp;T]] is the dominant local telephone service provider in [[California]], but since Bell Atlantic and [[GTE]] merged to become [[Verizon]], it now provides service in many pockets such as [[West Los Angeles (region)|West Los Angeles]]. [[Los Angeles]] telephone users can select from telephone directories published by AT&amp;T, Verizon (published by [[Idearc Media|Idearc]]), Yellow Book USA, PDC Pages (Phone Directories Company) [[PDC Pages.com]] and other independent publishing companies. [[R. H. Donnelley]] is also in local markets across country with Dex Printed Directories and [[DexKnows.com]]. In Northern California, Valley Yellow Pages [[MyYP.com]] is a large regional independent publisher. Additionally, in the smaller markets, many yellow pages publishers are beginning to offer directories catering to specific niche business or industry segments, such as automotive, manufacturing, environmental/green products, imports, exports, and the like. One such example is the [[Export Yellow Pages]] (a yellow page directory published in partnership with the US Department of Commerce that focuses on U.S. exporters) and vertical directories offered by Yellow Pages Nationwide, Inc. Media an Online Digital Yellow Pages company, Consolidation and M&amp;A activity in the directory publishing market continues to remain very high in the U.S. and there is an increasing move toward internet based directories as internet usage for search increases and concerns over the possible negative environmental effects of the books becomes more evident.
: [[Yellowpages.com]] LLC is a subsidiary of AT&amp;T.

* '''Uzbekistan:''' In [[Uzbekistan]], the directory is called ''Yellow Pages of Uzbekistan'', published by Yellow Pages Ltd.

==V==
* '''Vietnam:''' In [[Vietnam]], the official title "Telephone Directory &amp; Yellow Pages'' in English and ''Nien Giam Dien Thoai Nhung Trang Vang va Nhung Trang Trang "in Vietnamese are produced and distributed nationwide by [http://yp.vn/ VietNam Yellow Pages Media JSC].

Vietnam Business Yellow Pages in Vietnamese and English is directory of Vietnam Business.

==Notes and references==
{{reflist}}

==See also==
* [[Telecommunications service]]
* [[Yellow pages]]

[[Category:Directories|*]]
[[Category:Yellow pages|*]]</text>
      <sha1>9p25zplpzn0jsna6shut4sbnyny4q5t</sha1>
    </revision>
  </page>
  <page>
    <title>Spotlight (Casting Services Company)</title>
    <ns>0</ns>
    <id>5813520</id>
    <revision>
      <id>680460083</id>
      <parentid>680459700</parentid>
      <timestamp>2015-09-11T00:12:24Z</timestamp>
      <contributor>
        <username>Dl2000</username>
        <id>917223</id>
      </contributor>
      <comment>rm invalid cat</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1513" xml:space="preserve">{{Use dmy dates|date=September 2015}}
{{Use British English|date=September 2015}}
{{multiple issues|
{{no footnotes|date=January 2014}}
{{Primary sources|date=April 2013}}
}}
[[File:Cinema Museum, London object 15.JPG|thumb|Spotlight volumes preserved at the Cinema Museum, London.]]
	
'''Spotlight''' was founded in 1927 and is the [[United Kingdom|UK's]] largest casting resource. Over 60,000 performers appear in Spotlight, including [[actor]]s and actresses, [[child actor|child artist]]s, [[presenter]]s, [[dancer]]s, and [[stuntman|stunt artists]]. Thousands of [[production company|production companies]], [[broadcasting|broadcasters]], [[advertising agency|ad agencies]], and independent casting directors use Spotlight as a casting resource. Their clients range from large organisations such as the [[BBC]], [[ITV (TV network)|ITV]], and [[Channel 4]], to small production companies and individual casting [[Television director|directors]].

Spotlight also publishes the [[handbook]] Contacts both in hard copy and as an [[e-book]]. It includes listings for over 5000 [[company|companies]], services and individuals across all branches of [[television]], [[theatre|stage]], [[film industry|film]], and [[radio]].

==External links==
* [http://www.spotlight.com Spotlight website]
* [http://www.contactshandbook.com Contacts website]

{{UK-stub}}
{{DEFAULTSORT:Spotlight}}
[[Category:Directories]]
[[Category:1927 establishments in the United Kingdom]]
[[Category:Casting companies]]

{{advertising-stub}}</text>
      <sha1>6kc6gksq2q8ge0fwvu6oiqngua5pf1v</sha1>
    </revision>
  </page>
  <page>
    <title>White's Directories</title>
    <ns>0</ns>
    <id>40453381</id>
    <revision>
      <id>747276051</id>
      <parentid>735301350</parentid>
      <timestamp>2016-11-01T13:29:08Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* top */http&amp;rarr;https for [[Google Books]] and [[Google News]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6082" xml:space="preserve">{{italic title}}
'''''White's Directories''''' were a series of directory publications issued by William White of [[Sheffield]], England, beginning in the 1820s.&lt;ref&gt;{{citation |chapter=White's Directories (advert) |quote=Established 1822 |url= https://books.google.com/books?id=dMwUAAAAQAAJ&amp;pg=PA83 |title=White's general and commercial directory of Hull |year=1882}}&lt;/ref&gt;&lt;ref&gt;{{Citation |url = http://openlibrary.org/books/ia:pigotcosnational1837dire/Pigot_and_Co.'s_national_commercial_directory_for_the_whole_of_Scotland_and_of_the_Isle_of_Man_..._t |title = Pigot and Co.'s National Commercial Directory for the Whole of Scotland and of the Isle of Man, ... Manchester, Liverpool, Leeds, Hull, Birmingham, Sheffiled, Carlisle, and Newcastle-upon-Tyne |publication-date = 1837 |location = London |publisher =[[James Pigot|J. Pigot &amp; Co.]] }}&lt;/ref&gt; White began his career in publishing by working for [[Edward Baines (1774&#8211;1848)|Edward Baines]].&lt;ref&gt;{{cite journal |title=Locational Behaviour of Urban Retailing during the Nineteenth Century: The Example of Kingston upon Hull |author= M. T. Wild and G. Shaw |journal=Transactions of the Institute of British Geographers |number= 61 |year=1974 |jstor=621602 }}&lt;/ref&gt;{{refn|group=nb|By the 1850s Sheffield had two professional directory publishers: William White (34 Collegiate Crescent, Broomhall Park) and Francis White (Broomhall Terrace, 104 Ecclesial New Road)&lt;ref&gt;{{cite book |title=Post office directory of Sheffield |year=1854 |publisher=Kelley and Co. |url=https://books.google.com/books?id=bO4NAAAAQAAJ }}&lt;/ref&gt;&lt;ref&gt;{{Citation |publisher = W. Satchell |publication-place = London |title = Book of British Topography: a Classified Catalogue of the Topographical Works in the Library of the British Museum Relating to Great Britain and Ireland |author = John Parker Anderson |publication-date = 1881 |chapter=Yorkshire: Sheffield |chapterurl=https://archive.org/stream/bookofbritishtop00andeuoft#page/327/mode/1up }}&lt;/ref&gt;}}

==Notes==
{{reflist|group=nb}}

==References==
{{reflist}}

==Further reading==

=== 1820s-1830s ===
* {{cite book |title=History, directory, and gazetteer, of the counties of Durham and Northumberland, and the towns and counties of Newcastle-upon-Tyne and Berwick-upon-Tweed |location=Newcastle |publisher= Printed for W. White &amp; Co. by E. Baines and Son |year= 1827&#8211;1828 |url= http://catalog.hathitrust.org/Record/009725890 }}
* {{cite book |title=Directory of the Borough of Leeds, the City of York, and the Clothing District of Yorkshire |location=Leeds |publisher= Printed for Wm. Parson &amp; Wm. White by Edward Baines and Son |year= 1830 |url=http://catalog.hathitrust.org/Record/007973427 }}
* {{Citation |publisher = Printed for the author by R. Leader |title = History, Gazetteer, and Directory of Norfolk, and the City and County of the City of Norwich |url = http://openlibrary.org/books/OL20613547M/History_Gazetteer_and_Directory_of_Norfolk_and_the_City_and_County_of_the_City_of_Norwich_... |author = William White |publication-date = 1836 |oclc = 25166377 }}
** [https://archive.org/stream/historygazettee01whitgoog#page/n3/mode/2up 1845 ed.]
** [https://archive.org/stream/historygazetteer00whit#page/n5/mode/2up 1864 ed.]
* {{Citation |publisher = W. White |publication-place = Sheffield |author =William White |title = History, Gazetteer, and Directory, of the West-Riding of Yorkshire, with the City of York and Port of Hull |publication-date = 1837 |url=https://archive.org/stream/historygazetteer01whit#page/n5/mode/2up }}
** {{cite book |title=History, gazetteer and directory of the East and North Ridings of Yorkshire |author=William White |location= Sheffield |publisher= Robert Leader for the author |year= 1840 |url=http://catalog.hathitrust.org/Record/011724851 }}

=== 1840s ===
* {{cite book |title=History, gazetteer, and directory of Suffolk, and the towns near its borders |location=Sheffield |publisher= Printed for the author by R. Leader and sold by W. White |year=1844 |url= http://catalog.hathitrust.org/Record/000194916 }}
** [http://catalog.hathitrust.org/Record/011595374 1874 ed.]
* {{Citation |url = http://openlibrary.org/books/ia:generaldirectory00whit/General_directory_of_the_town_and_borough_of_Sheffield_with_Rotherham_Chesterfield_and_all_the_paris |title = General directory of the town and borough of Sheffield |publication-date = 1845 |publisher = William White |location=Sheffield }}
* {{Citation |publisher = Printed for the author, by R. Leader |publication-place = Sheffield |author = William White |url = http://openlibrary.org/books/OL14012344M/History_gazetteer_and_directory_of_Leicestershire_and_the_small_county_of_Rutland |title = History, gazetteer, and directory of Leicestershire, and the small county of Rutland |publication-date = 1846 }}

=== 1870s ===
* {{cite book |title=History, gazetteer and directory of Lincolnshire, and the city and diocese of Lincoln |location=Sheffield |publisher= W. White |year= 1872 |url=http://catalog.hathitrust.org/Record/008912723 }}
* {{Citation |publisher = W. White |publication-place = Sheffield |title = History, gazetteer and directory of the county of Hampshire, including the Isle of Wight |url = http://catalog.hathitrust.org/Record/009009769 |publication-date = 1878 }}
* {{Citation |publisher = William White |publication-place = Sheffield |title = History, Gazetteer and Directory of the County of Devon including the City of Exeter |url = http://openlibrary.org/books/OL14012345M/History_gazetteer_and_directory_of_the_County_of_Devon_including_the_City_of_Exeter_and_comprising_a |publication-date = 1878 |edition=2nd }}

==External links==
* {{citation |title=Historical Directories |publisher=[[University of Leicester]] |location=UK |url=http://www.historicaldirectories.org/hd/findbykeyword.asp }}. Includes digitized White's directories, various dates
* {{citation |work=WorldCat |url=http://www.worldcat.org/wcidentities/lccn-n50-38455 |title=William White of Sheffield }}

[[Category:Directories]]
[[Category:Publications established in the 1820s]]


{{ref-book-stub}}</text>
      <sha1>3zol0sgxwlgrfut7z6o907lhw0rsg4m</sha1>
    </revision>
  </page>
  <page>
    <title>Trow's Directory</title>
    <ns>0</ns>
    <id>41863504</id>
    <redirect title="John Fowler Trow" />
    <revision>
      <id>594235871</id>
      <timestamp>2014-02-06T17:24:53Z</timestamp>
      <contributor>
        <username>M2545</username>
        <id>9455233</id>
      </contributor>
      <comment>[[WP:AES|&#8592;]]Redirected page to [[John Fowler Trow]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="132" xml:space="preserve">#REDIRECT[[John Fowler Trow]]

[[Category:Directories]]
[[Category:Books about New York City]]
[[Category:History of New York City]]</text>
      <sha1>kcmleenha1dmgbyvzayppoqkyhydvzn</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Web directories</title>
    <ns>14</ns>
    <id>826434</id>
    <revision>
      <id>747862427</id>
      <parentid>609345481</parentid>
      <timestamp>2016-11-04T20:36:29Z</timestamp>
      <contributor>
        <username>The Transhumanist</username>
        <id>1754504</id>
      </contributor>
      <comment>add link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="402" xml:space="preserve">{{Cat main|Web directory|List of web directories}}
A [[web directory]] is a [[directory (databases)|directory]] on the [[World Wide Web]] that specializes in [[hyperlink|linking]] to other [[web site]]s and categorizing those links. This category includes online web directories.


[[Category:Websites|Directories]]
[[Category:Indexes]]
[[Category:Directories]]
[[Category:Online services|Directories]]</text>
      <sha1>s3paw1xheciv1ox2nqbwwk5rj0upcbs</sha1>
    </revision>
  </page>
  <page>
    <title>World Leaders</title>
    <ns>0</ns>
    <id>233058</id>
    <revision>
      <id>694564422</id>
      <parentid>694564191</parentid>
      <timestamp>2015-12-10T01:34:54Z</timestamp>
      <contributor>
        <username>GabeIglesia</username>
        <id>17543794</id>
      </contributor>
      <minor />
      <comment>/* top */ [[MOS:BOLDSYN]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1020" xml:space="preserve">{{italic title}}
[[Image:ChiefsCover.jpg|right|200px|thumb|Cover of ''World Leaders'']]
'''''World Leaders''''', also known as '''''Chiefs of State and Cabinet Members of Foreign Governments''''', is a [[public domain]] directory published weekly by the United States [[Central Intelligence Agency]]. It lists different state officials for each country of the world: the [[head of state]] and/or [[head of government]] and other [[cabinet minister]]s, the chief of the [[central bank]], and the [[ambassador]]s to the [[United Nations]] and the United States.

==See also==
*[[World-Check]]
*[[List of current heads of state and government]]
*[[National Security Agency academic publications]]
*''[[International Who's Who]]''

==External links==
*[https://www.cia.gov/library/publications/world-leaders-1/ ''World Leaders'']

[[Category:Central Intelligence Agency publications]]
[[Category:Heads of state]]
[[Category:Heads of government]]
[[Category:Directories]]
[[Category:Public domain databases]]

{{US-gov-stub}}</text>
      <sha1>229ga9geppkddk9q9knsw6xlue6rzgm</sha1>
    </revision>
  </page>
  <page>
    <title>Telephone directory</title>
    <ns>0</ns>
    <id>162263</id>
    <revision>
      <id>762819645</id>
      <parentid>762819528</parentid>
      <timestamp>2017-01-30T22:34:51Z</timestamp>
      <contributor>
        <username>ESkog</username>
        <id>88149</id>
      </contributor>
      <minor />
      <comment>Reverted edits by [[Special:Contributions/2601:181:4102:29B0:888C:399F:9C05:862|2601:181:4102:29B0:888C:399F:9C05:862]] ([[User talk:2601:181:4102:29B0:888C:399F:9C05:862|talk]]): [[WP:PROMO|Advertising or promotion]] ([[WP:HG|HG]]) (3.1.22)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9175" xml:space="preserve">{{Redirect2|Phone book|White pages|a contact list|Contact list|other uses|White pages (disambiguation)}}
{{Use dmy dates|date=June 2012}}
{{refimprove|date=November 2008}}
[[File:Telefonkatalog 1928.jpg|thumb|[[Gothenburg]] telephone directory, 1928.]]

A '''telephone directory''', also known as a '''telephone book''', '''telephone address book''', '''phone book''', or the '''white/yellow pages''', is a listing of telephone [[subscriber]]s in a geographical area or subscribers to services provided by the organization that publishes the directory. Its purpose is to allow the telephone number of a subscriber identified by name and address to be found.

The advent of the Internet and [[smart phones]] in the 21st century greatly reduced the need for a paper phone book.  Some communities, such as [[Seattle]] and [[San Francisco]], sought to ban their unsolicited distribution as wasteful, unwanted and harmful to the environment.&lt;ref name=SF&gt;[http://www.sfgate.com/bayarea/article/Yellow-Pages-ruling-endangers-SF-ban-3951477.php Yellow Pages ruling endangers SF ban], Heather Knight, ''[[San Francisco Chronicle]]'', 15 October 2012; accessed 19 March 2013&lt;/ref&gt;&lt;ref&gt;[http://seattletimes.com/html/localnews/2019441687_yellowpages16m.html Appeals court rules against Seattle's curbs on yellow pages], Emily Heffter, ''[[Seattle Times]]'', 15 October 2012; accessed 19 March 2013&lt;/ref&gt;

== Content ==
Subscriber names are generally listed in alphabetical order, together with their postal or street address and [[telephone number]].  In principle every subscriber in the geographical coverage area is listed, but subscribers may request the exclusion of their number from the directory, often for a fee; their number is then said to be "unlisted" ([[American English]]), "ex-directory" ([[British English]]), "private" or '''private number''' (Australia and New Zealand), or "non-published" (Canada).&lt;ref&gt;{{cite web|url=http://support.bell.ca/Home_phone/Phone_line/How_to_unlist_my_Bell_Home_phone_number |title=How to get a non-published Bell Home phone number |publisher=Support.bell.ca |date=2013-06-17 |accessdate=2014-04-16}}&lt;/ref&gt;

A telephone directory may also provide instructions: how to use the [[Local telephone service|telephone service]], how to dial a particular number, be it local or international, what numbers to access important and [[emergency services]], utilities, hospitals, doctors, and organizations who can provide support in times of crisis. It may also have [[civil defense]] or [[emergency management]] information. There may be transit maps, postal code/zip code guides, international dialing codes or stadium seating charts, as well as advertising.

In the US, under current rules and practices, mobile phone and [[voice over IP]] listings are not included in telephone directories.  Efforts to create cellular directories have met stiff opposition from several fronts, including those who seek to avoid [[telemarketer]]s.{{Citation needed|date=January 2011}}

== Types ==
[[File:Telefonbog ubt-1.JPG|thumb|White pages.]]
A telephone directory and its content may be known by the color of the paper it is printed on.
* White pages&lt;!--redirects here--&gt; generally indicates personal or alphabetic listings.
* [[Yellow pages]], golden pages, A2Z, or classified directory is usually a "business directory", where businesses are listed alphabetically within each of many classifications (e.g., "lawyers"), almost always with paid advertising.
* [[Reverse telephone directory|Grey pages]], sometimes called a "reverse telephone directory", allowing subscriber details to be found for a given number. Not available in all jurisdictions.{{citation needed|date=March 2014}}  (These listings are often published separately, in a city directory, [[Polk directory]], or under another name, for a price, and made available to commercial and government agencies.)

Other colors may have other meanings; for example, information on [[government agencies]] is often printed on [[blue pages]] or green pages.{{Citation needed|date=September 2011}}

== Publication ==
[[File:New haven directory 1878.jpg|thumb|upright|New Haven directory, November, 1878.]]Telephone directories can be published in [[hard copy]] or in electronic form. In the latter case, the directory can be provided as an online service through proprietary terminals or over the Internet, or on physical media such as CD-ROM. In many countries directories are both published in book form and also available over the Internet. Printed directories were usually supplied free of charge.

== History ==
[[File:Unused Phonebooks.JPG|thumb|Recently delivered 2013&#8211;2014 phone books in the trash unopened; in the 21st century some communities have tried to stop the unsolicited distribution of the books&lt;ref name=SF/&gt;]]
{{Expand section|date=September 2011}}
Telephone directories are a type of [[city directory]]. Books listing the inhabitants of an entire city were widely published starting in the 18th century, before the invention of the telephone. 

The first telephone directory, consisting of a single piece of cardboard, was issued on 21 February 1878; it listed 50 individuals, businesses, and other offices in [[New Haven, Connecticut]] that had telephones.&lt;ref&gt;{{cite web| title= The Phone Book | url= http://failuremag.com/feature/article/the_phone_book/ | author=Jason Zasky | work=Failure Magazine |accessdate=2013-12-31}}&lt;/ref&gt;

The first British telephone directory was published on 15 January 1880  by The Telephone Company. It contained 248 names and addresses of individuals and businesses in London; telephone numbers were not used at the time as subscribers were asked for by name at the exchange.&lt;ref&gt;Records of the Telephone Company Limited (Bell's Patents), BT Archives reference TPA&lt;/ref&gt; The directory is preserved as part of the British phone book collection by [[BT Archives]].

In 1938, AT&amp;T commissioned the creation of a new type font, known as [[Bell Gothic|BELL GOTHIC]], the purpose of which was to be readable at very small font sizes when printed on newsprint where small imperfections were common.

In 1981 France was the first country to have an electronic directory&lt;ref&gt;{{cite web|url=http://whitepages.fr/minitel/ |title=Telephone History in France by |publisher=Whitepages.fr |date= |accessdate=2014-04-16}}&lt;/ref&gt; on a system called [[Minitel]]. The directory is called "11" after its telephone access number.

In 1991 the [[U.S. Supreme Court]] ruled (in ''[[Feist v. Rural]]'') that telephone companies do not have a [[copyright]] on telephone listings, because copyright protects creativity and not the mere labor of collecting existing information.

1996 is the year the first telephone directories go online in the USA. [[Yellowpages.com]] and [[Whitepages.com]] both see their start in April.&lt;ref&gt;[http://www.whitepages.fr/telecom-history-ft-late-with-internet.html Telephone Directory History by Whitepages.fr]&lt;/ref&gt;

In 1999, the first online telephone directories and people finding sites such as [[LookupUK.com]] go online in the UK. In 2003, more advanced UK searching including Electoral Roll become available on [[LocateFirst.com]].

In the 21st century, printed telephone directories are increasingly criticized as waste. In 2012, after some North American cities passed laws banning the distribution of telephone books, an industry group sued and obtained a court ruling permitting the distribution to continue.&lt;ref name=SF/&gt; Manufacture and distribution of telephone directories produces over 1,400,000 metric tons of [[greenhouse gases]] and consumes over 600,000 tons of paper annually.&lt;ref&gt;{{cite web|last= Paster |first= Pablo |url=http://www.treehugger.com/culture/ask-pablo-what-is-the-impact-of-all-those-unwanted-phone-books.html |title=Ask Pablo: What Is The Impact Of All Those Unwanted Phone Books? |publisher=TreeHugger |date=2010-01-11 |accessdate=2014-04-16}}&lt;/ref&gt;

== Reverse directories ==
{{main|Reverse telephone directory}}
A reverse telephone directory is sorted by number, which can be looked up to give the name and address of the subscriber.

== In popular culture ==
Ripping phone books in half has often been considered a [[Feats of strength|feat of strength]]. The Guinness World Record for ripping the most telephone directories is 27; the record for French telephone directories is 29, held by [[Georges Christen]].{{citation needed|date=October 2012}}

== See also ==
* [[Domain Name System|DNS]]
* [[Lightweight Directory Access Protocol|LDAP]]
* [[Silent number]]
* [[Whois]]
* [[City directory]]

== References ==
{{reflist|colwidth=30em}}

== Further reading ==
* {{cite book|title=The Phone Book: The Curious History of the Book That Everyone Uses But No One Reads| last= Shea|first=Ammon|publisher=Perigee Trade|year=2010|ISBN=978-0-399-53593-2}}

== External links ==
*{{Commonscat-inline|Phone books}}
*{{wikt-inline}}
* {{dmoz|Reference/Directories/Address_and_Phone_Numbers}}

{{Authority control}}

{{DEFAULTSORT:Telephone Directory}}
[[Category:Telephone numbers]]
[[Category:Directories]]
[[Category:History of the telephone]]
[[Category:American inventions]]
[[Category:1878 introductions]]</text>
      <sha1>eysyv004a9dsqlxx0b2a0gv2y34ijd6</sha1>
    </revision>
  </page>
  <page>
    <title>Sands Directory</title>
    <ns>0</ns>
    <id>45359850</id>
    <revision>
      <id>762235890</id>
      <parentid>689255928</parentid>
      <timestamp>2017-01-27T14:52:59Z</timestamp>
      <contributor>
        <username>Wittylama</username>
        <id>492056</id>
      </contributor>
      <comment>/* See also */ adding searchable version link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9775" xml:space="preserve">{{Use Australian English|date=March 2015}}
{{Use dmy dates|date=March 2015}}

[[File:Sands Directory 1899 (book).JPG|thumb|1899 edition of Sands Directory ([[National Library of Australia]])]]
The '''Sands Directories''', also published as the '''Sands and Kenny Directory''' and the '''Sands and McDougall Directory''' were annual publications in Australia.

They listed household, business, society, and Government contacts&lt;ref name=":0" /&gt; in [[Melbourne]], [[Adelaide]] and [[Sydney]] including some rural areas of Victoria and New South Wales from the 1850s.&lt;ref name=Bibliog&gt;{{cite book|last1=Eslick, Christine; Joy Hughes, and R. Ian Jack|title=Bibliography of New South Wales local history: an annotated bibliography of secondary works published before 1982 and New South Wales directories 1828 -1950|date=1987|publisher=New South Wales University Press|location=Kensington, NSW|url=http://library.sl.nsw.gov.au/record=b1187352~S2|isbn=0-86840-154-4|pages=372; 398}}&lt;/ref&gt; [[City directory|City directories]] are an important resource for historical research, allowing individual addresses and occupations to be linked to specific streets and suburbs.&lt;ref&gt;{{cite book|last1=Williams|first1=A.V.|title=The development and growth of city directories|date=1913|publisher=Williams directory co.|location=Cincinnati, Ohio|url=http://babel.hathitrust.org/cgi/pt?id=nyp.33433082423645;view=1up;seq=5|accessdate=5 March 2015}}&lt;/ref&gt;

==Publisher==
[[File:Sands Directory 1899 (cover).JPG|thumb|1899 edition of Sands Directory (cover)]]
[[John Sands (printer)|John Sands]] (1818-1873) was an engraver, printer and stationer.  Born in England he moved to [[Sydney, Australia|Sydney]] in 1837.&lt;ref name="ADB Sands"&gt;{{cite book|last1=Walsh|first1=G.P.|title='Sands, John (1818&#8211;1873)', Australian Dictionary of Biography|date=1976|publisher=National Centre of Biography, Australian National University|url=http://adb.anu.edu.au/biography/sands-john-4536|accessdate=5 March 2015}}&lt;/ref&gt;  Sands formed several business partnerships, in 1851 with his brother-in-law Thomas Kenny, and in 1860 with Dugald McDougall with the business being known as [[John Sands (company)|Sands, Kenny &amp; Co.]]&lt;ref name="ADB Sands"/&gt; Directory titles changed as the publisher changed partners, and at different points the Sands Directories were also published as the 'Sands and Kenny' or 'Sands and McDougall Directories'.&lt;ref name=Kingston /&gt;

==Sands, Kenny &amp; Co's commercial and general Melbourne directory==
The first Melbourne Directory was published by Sands and Kenny in 1857.&lt;ref name=Kingston&gt;{{cite web|title=Sands and McDougall Melbourne Directories|url=http://www.kingston.vic.gov.au/library/Library-Services/Family-History-Resources/Sands-and-McDougall-Melbourne-Directories|website=Kingston Libraries|publisher=Kingston Libraries|accessdate=10 February 2015|ref=Kingston}}&lt;/ref&gt; By 1858 the second edition of the directory was distributed to public libraries in the major seaports of Great Britain, Ireland, the United States of America, and Canada.&lt;ref&gt;{{cite news |url=http://nla.gov.au/nla.news-article154855152 |title=Publications Received |newspaper=[[The Age]] |location=Melbourne |date=1 February 1858 |accessdate=5 March 2015 |page=6 }}&lt;/ref&gt;  From 1862 to 1974 the Melbourne directories were published as the Sands and McDougall Melbourne Directory.&lt;ref name=Kingston /&gt;&lt;ref&gt;{{cite news|last1=Stephens|first1=Andrew|title=Sands &amp; McDougall directory exhibition brings old Melbourne back to life.|url=http://www.smh.com.au/entertainment/sands--mcdougall-directory-exhibition-brings-old-melbourne-back-to-life-20140811-102quc.html|accessdate=5 March 2015|work=Sydney Morning Herald|date=15 August 2014}}&lt;/ref&gt;

The 1860 Melbourne directory was 400 pages long and contained over 10,000 entries.&lt;ref name=":0"&gt;{{cite news |url=http://nla.gov.au/nla.news-article154880275 |title=Sands and Kenny's Melbourne Directory|newspaper=[[The Age]]|location=Melbourne |date=24 January 1860 |accessdate=5 March 2015 |page=5 }}&lt;/ref&gt;

==Sands Sydney, Suburban and Country Commercial Directory==
[[File:Sands Directory 1899 (spine).JPG|thumb|1899 edition of Sands Directory (spine)]]
The ''Sands Sydney, Suburban and Country Commercial Directory'', first published in 1858,&lt;ref name="FMP"&gt;{{cite web|title=New South Wales, Sydney Directory 1847-1913|url=http://www.findmypast.com.au/articles/world-records/full-list-of-australia-and-new-zealand-records/newspapers-directories-and-social-history/new-south-wales-sydney-directory-1847-1913|website=Find My Past|accessdate=5 March 2015}}&lt;/ref&gt; included a variety of information including street addresses and businesses, farms and country towns, stock numbers (e.g. horses, cattle and sheep on each station) as well as information about public watering places including dams, tanks and wells.&lt;ref&gt;{{cite news |url=http://nla.gov.au/nla.news-article118819151 |title=Sands' Directory |newspaper=[[The Evening News (Sydney)|The Evening News]] |location=Sydney |date=15 January 1923 |accessdate=5 March 2015 |page=9 }}&lt;/ref&gt;  With the primary function of post office directory it provides lists of householders, businesses, public institutions and officials.&lt;ref&gt;{{cite web|title=Family history and genealogy|url=http://www.sl.nsw.gov.au/about/collections/documenting/social/famhist.html|website=State Library of NSW|accessdate=5 March 2015}}&lt;/ref&gt;

The Sydney editions of the directory, covering the state of New South Wales, were published each year from 1858&#8211;59 to 1932&#8211;33.&lt;ref name=Sydney&gt;{{cite web|title=Sands Sydney, Suburban and Country Commercial Directory|url=http://www.cityofsydney.nsw.gov.au/learn/search-our-collections/sands-directory|website=The City of Sydney|publisher=The City of Sydney|accessdate=10 February 2015}}&lt;/ref&gt;  There were four years when the directory did not appear during this time, they were 1872, 1874, 1878 and 1881.&lt;ref name= CGHG&gt;{{Citation | author1=Cridland, Marilyn | author2=Central Coast Family History Group (N.S.W.) | title=A guide to the Sands Directory | publication-date=1997 | publisher=Central Coast Family History Group Inc | page =1|url=http://trove.nla.gov.au/work/35275389 | accessdate=5 March 2015 }}&lt;/ref&gt;  The directory is arranged by municipalities in which properties were located, listing the primary householder street by street.&lt;ref&gt;{{cite web|title=Sands Sydney Directory Guide|url=http://www.waverley.nsw.gov.au/__data/assets/pdf_file/0009/28557/Sands_Sydney_Directory_guide_for_LS_website_revised.pdf|website=Waverley Council|accessdate=5 March 2015}}&lt;/ref&gt; As a consequence, the household and business information in the directories is used for research into Sydney history,&lt;ref&gt;{{cite web|title=Sands Directory &#8211; Researching your house's history|url=http://insidehistorymagazine.blogspot.com.au/2011/11/sands-directory-researching-your-houses.html|website=Inside History magazine|accessdate=5 March 2015}}&lt;/ref&gt; with particular application for genealogical research.&lt;ref name= CGHG/&gt;&lt;ref name="Sands digital edition"&gt;{{cite web|title=Sands' Directory [digital edition]|url=http://www.cityofsydney.nsw.gov.au/learn/search-our-collections/sands-directory|website=City of Sydney|accessdate=5 March 2015}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last1=Royal Australian Historical Society|title=Sands Directories are now online!|url=http://www.rahs.org.au/sands-directories-are-now-online/|accessdate=5 March 2015}}&lt;/ref&gt;

By 1909 the Sydney directory contained over 1700 pages.&lt;ref name=SMH_Trove&gt;{{cite news |url=http://nla.gov.au/nla.news-article15027528 |title=Sands' Directory 1909. |newspaper=[[The Sydney Morning Herald]] |location=NSW |date=9 January 1909 |accessdate=11 February 2015 |page=11}}&lt;/ref&gt; The full title of the 1913 edition of the directory of Sydney is ''Sands Sydney, Suburban and Country Directory for 1913 comprising, amongst other information, street, alphabetical, trade and professional, country towns, country alphabetical, pastoral, educational, governmental, parliamentary, law and miscellaneous lists''.&lt;ref name="FMP"/&gt;

==Sands &amp; McDougall's South Australian directory==
Sands and McDougall arrived in [[Adelaide, South Australia|Adelaide]] in 1883.&lt;ref name="SLSA"&gt;{{cite web|title=South Australian directories|url=http://guides.slsa.sa.gov.au/directories|website=State Library of South Australia|accessdate=5 March 2015}}&lt;/ref&gt;  They took over the directory previously published by Josiah Boothby, publishing their first South Australian directory in January 1884.&lt;ref name="SLSA"/&gt;&lt;ref&gt;{{Citation | author1=Sands &amp; McDougall Limited | title=Sands &amp; McDougall's South Australian directory : with which is incorporated Boothby's South Australian directory | publication-date=1884 | publisher=Printed and published by Sands &amp; McDougall | url=http://trove.nla.gov.au/work/21481893 | accessdate=5 March 2015 }}&lt;/ref&gt;&lt;ref&gt;{{cite news |url=http://nla.gov.au/nla.news-article166349067 |title=South Australian Directory |newspaper=[[The Southern Cross (South Australia)|The Southern Cross]] |location=Adelaide |date=27 March 1896 |accessdate=5 March 2015 |page=4}}&lt;/ref&gt; The Sands &amp; McDougall's Directory of South Australia was published from 1884 to 1974.&lt;ref name=Trove_SA&gt;{{cite web | author1=Sands &amp; McDougall Limited | title=Sands &amp; McDougall's directory of South Australia | publication-date=1884 | publisher=Sands &amp; McDougall | url=http://trove.nla.gov.au/work/21481863 | accessdate=11 February 2015 }}&lt;/ref&gt;

==See also==
[[Western Australia Post Office Directory|Wise Directories]]

==External links==
* [http://www.cityofsydney.nsw.gov.au/learn/search-our-collections/sands-directory digitised Sydney sands directory], at the City of Sydney archives

==References==
{{reflist|2}}

[[Category:Gazetteers]]
[[Category:Directories]]
[[Category:Australian directories]]</text>
      <sha1>qk7sli30aa37lg7l1b86hic59z4ipqe</sha1>
    </revision>
  </page>
  <page>
    <title>Bengali film directory</title>
    <ns>0</ns>
    <id>19601961</id>
    <revision>
      <id>741661123</id>
      <parentid>740012480</parentid>
      <timestamp>2016-09-28T22:33:25Z</timestamp>
      <contributor>
        <username>Randy Kryn</username>
        <id>4796325</id>
      </contributor>
      <comment>italicize title, first mention and infobox title, add italics</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2609" xml:space="preserve">{{italic title}}
{{Infobox book
| name = Bengali Film Directory
| image = Bengali film directory.jpg
| caption =Cover page of ''Bengali Film Directory''
| author = [[Ansu Sur]]
| country = India
| language = English
| cover_artist =
| genre = [[Trade directory|Directory]]
| publisher = [[Nandan (Kolkata)|Nandan]]&lt;br /&gt; [[West Bengal]] Film Centre([[Calcutta]])
| release_date = 1999
| media_type = Print ([[Hardback]])
| pages =319
| preceded_by =
| followed_by =
}}

'''''Bengali Film Directory''''' is an archive of [[Cinema of West Bengal|Bengali film]]s (in English).&lt;ref&gt;{{cite web |url=http://www.rosland.freeserve.co.uk/filmbooks5.htm |title=The Howard Summers Cinema Website-National Filmographies-Asian Cinema |publisher=www.rosland.freeserve.co.uk |accessdate=2008-10-23 |archiveurl = https://web.archive.org/web/20080616195358/http://www.rosland.freeserve.co.uk/filmbooks5.htm &lt;!-- Bot retrieved archive --&gt; |archivedate = 2008-06-16}}&lt;/ref&gt; Published in March 1999 by [[Nandan (Kolkata)|Nandan]], [[West Bengal]] Film Centre ([[Calcutta]]), this directory was edited by Ansu Sur and was compiled by Abhijit Goswami. It includes all [[Bengali people|Bengali]] [[feature film]]s released from 1917 to 1998, described briefly, but including detailed cast and crew, director name, release date and release theater name.&lt;ref&gt;{{cite web |url=https://openlibrary.org/b/OL171681M |title=Bengali film directory (Open Library) |publisher=openlibrary.org |accessdate=2008-10-23}}
&lt;/ref&gt;

==Contents==
* Acknowledgements iv
* A Note from the Editor v
* Reference vi
* Abbreviations vii
* Filmography
** Silent era 1
** Sound era 9
* Studios and Post-Production Centres 267
* Production Companies 269
* Distributions 271
* Show-houses 274
* Useful Addresses 277
* Film Societies 278
* Film Journals 280
* Film Books 281
* Index
** Films 285
** Directors 298
** Actors and Actresses 303
* Appendix: Films released in 1998 315&lt;ref&gt;{{cite web |url=http://www.cscsarchive.org/MediaArchive/Library.nsf/(docid)/5BD370ADB88260A6652571AF0037A748?OpenDocument&amp;StartKey=Bengali&amp;Count=100 |title=Bengali film directory |publisher=www.cscsarchive.org |accessdate=2008-10-23}} {{Dead link|date=October 2010|bot=H3llBot}}&lt;/ref&gt;

==References==
{{Reflist}}

==External links==

{{Cinema of West Bengal}}
{{Cinema of Bangladesh}}
{{Cinema of India}}
{{World cinema}}

[[Category:Cinema of Bengal]]
[[Category:Bengali language]]
[[Category:1999 books]]
[[Category:Bengali-language media]]
[[Category:English-language media]]
[[Category:Film guides]]
[[Category:Archives in India]]
[[Category:Directories]]


{{film-org-stub}}</text>
      <sha1>mhwk0re7q1uw5t1dturemceu2rwoc7o</sha1>
    </revision>
  </page>
  <page>
    <title>Search/Retrieve via URL</title>
    <ns>0</ns>
    <id>8465350</id>
    <revision>
      <id>681050270</id>
      <parentid>679599001</parentid>
      <timestamp>2015-09-14T21:31:03Z</timestamp>
      <contributor>
        <username>Scott</username>
        <id>9867</id>
      </contributor>
      <comment>[[Help:Cat-a-lot|Cat-a-lot]]: Moving from [[Category:Uniform resource locator]] to [[Category:Uniform Resource Locator]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4641" xml:space="preserve">{{MOS|date=September 2015|reason=OMG! =)}}
'''Search/Retrieve via URL''' ('''SRU''') is a standard search protocol for [[Internet search]] queries, utilizing [[Contextual Query Language]] (CQL), a standard query syntax for representing queries.

Samplecode of a complete answer for this SRU Query-URL:
http://philosophy-science-humanities-controversies.com/XML/sru.php?version=1.1&amp;operation=searchRetrieve&amp;query=dc.title=Darwinism 

&lt;pre&gt;
&lt;?xml version="1.0"?&gt;
&lt;sru:searchRetrieveResponse xmlns:sru="http://www.loc.gov/zing/srw/" xmlns:diag="http://www.loc.gov/zing/srw/diagnostic/" xmlns:xcql="http://www.loc.gov/zing/cql/xcql/" xmlns:dc="http://purl.org/dc/elements/1.1/"&gt;
  &lt;sru:version&gt;1.1&lt;/sru:version&gt;
  &lt;sru:numberOfRecords&gt;4&lt;/sru:numberOfRecords&gt;
  &lt;sru:records&gt;
    &lt;sru:record&gt;
      &lt;sru:recordSchema&gt;info:srw/schema/1/dc-v1.1&lt;/sru:recordSchema&gt;
      &lt;sru:recordPacking&gt;XML&lt;/sru:recordPacking&gt;
      &lt;sru:recordData&gt;
        &lt;sru:dc&gt;
          &lt;sru:title&gt;Darwinism&lt;/sru:title&gt;
          &lt;sru:creator&gt;Dennett&lt;/sru:creator&gt;
          &lt;sru:subject&gt;The rule of the Local is a basic principle of Darwinism -  it corresponds to the principle that there is no Creator, no intelligent foresight. I 262&lt;/sru:subject&gt;
        &lt;/sru:dc&gt;
      &lt;/sru:recordData&gt;
      &lt;sru:recordNumber&gt;1&lt;/sru:recordNumber&gt;
    &lt;/sru:record&gt;
    &lt;sru:record&gt;
      &lt;sru:recordSchema&gt;info:srw/schema/1/dc-v1.1&lt;/sru:recordSchema&gt;
      &lt;sru:recordPacking&gt;XML&lt;/sru:recordPacking&gt;
      &lt;sru:recordData&gt;
        &lt;sru:dc&gt;
          &lt;sru:title&gt;Darwinism&lt;/sru:title&gt;
          &lt;sru:creator&gt;McGinn&lt;/sru:creator&gt;
          &lt;sru:subject&gt;Design argument/William Paley: organisms have a brilliant design: We have not designed them, so we have to assume that a foreign intelligence did it. Let s call this intelligence "God". So God exists. II 98
DarwinVsPaley: intelligent design does not require a Creator. Selection is sufficient. II 98
Mind/consciousness/evolution/McGinn: evolution does not explain consciousness! nor sensations. II 99
 Reason: sensation and consciousness cannot be explained through the means of Darwinian principles and physics, because if selection were to explain how sensations are supposed to be created by it, it must be possible to mold the mind from matter. II 100
 (s) Consciousness or sensations would have to be visible for selection! (Similar GouldVsDawkins)&lt;/sru:subject&gt;
        &lt;/sru:dc&gt;
      &lt;/sru:recordData&gt;
      &lt;sru:recordNumber&gt;2&lt;/sru:recordNumber&gt;
    &lt;/sru:record&gt;
    &lt;sru:record&gt;
      &lt;sru:recordSchema&gt;info:srw/schema/1/dc-v1.1&lt;/sru:recordSchema&gt;
      &lt;sru:recordPacking&gt;XML&lt;/sru:recordPacking&gt;
      &lt;sru:recordData&gt;
        &lt;sru:dc&gt;
          &lt;sru:title&gt;Darwinism&lt;/sru:title&gt;
          &lt;sru:creator&gt;Putnam&lt;/sru:creator&gt;
          &lt;sru:subject&gt;Rorty: Darwinism / Putnam: he does noit like the image of man as a more complicated animal  (scientistic and reductionist physicalism).
Rorty VI 63&lt;/sru:subject&gt;
        &lt;/sru:dc&gt;
      &lt;/sru:recordData&gt;
      &lt;sru:recordNumber&gt;3&lt;/sru:recordNumber&gt;
    &lt;/sru:record&gt;
    &lt;sru:record&gt;
      &lt;sru:recordSchema&gt;info:srw/schema/1/dc-v1.1&lt;/sru:recordSchema&gt;
      &lt;sru:recordPacking&gt;XML&lt;/sru:recordPacking&gt;
      &lt;sru:recordData&gt;
        &lt;sru:dc&gt;
          &lt;sru:title&gt;Darwinism&lt;/sru:title&gt;
          &lt;sru:creator&gt;Rorty&lt;/sru:creator&gt;
          &lt;sru:subject&gt;Darwinism/Rorty provides a useful vocabulary. "Darwinism": for me is a fable about human beings as animals with special skills and organs. But these organs and skills are just as little in a representational relation to the world as the anteater s snout. VI 69 ff
Darwinism/Rorty: it demands that we consider our doing and being as part of the same continuum, which also includes the existence of amoebae, spiders and squirrels. One way to do that is to say that our experience is just more complex. VI 424&lt;/sru:subject&gt;
        &lt;/sru:dc&gt;
      &lt;/sru:recordData&gt;
      &lt;sru:recordNumber&gt;4&lt;/sru:recordNumber&gt;
    &lt;/sru:record&gt;
  &lt;/sru:records&gt;
&lt;/sru:searchRetrieveResponse&gt;
&lt;/pre&gt;
==See also==
* [[Search/Retrieve Web Service]]

==External links==
* [http://www.loc.gov/standards/sru/ Search/Retrieve via URL] at [[Library of Congress]]
* http://www.philosophy-science-humanities-controversies.com/XML/index.php Sample Page from the Lexicon of Arguments.
* http://philosophy-science-humanities-controversies.com/XML/sru.php?version=1.1&amp;operation=searchRetrieve&amp;query=dc.title=Darwinism This is a complete example with query and answer.
{{Internet search}}

{{DEFAULTSORT:Search Retrieve via URL}}
[[Category:Internet search]]
[[Category:Information retrieval techniques]]
[[Category:Uniform Resource Locator]]

{{web-stub}}</text>
      <sha1>nnfopvhbporebhgy3r2wfk24kl5ll0n</sha1>
    </revision>
  </page>
  <page>
    <title>Hybrid search engine</title>
    <ns>0</ns>
    <id>2851233</id>
    <revision>
      <id>697148404</id>
      <parentid>666712831</parentid>
      <timestamp>2015-12-28T15:53:08Z</timestamp>
      <contributor>
        <ip>170.163.6.2</ip>
      </contributor>
      <comment>eLocalFinder.com(http://elocalfinder.com/HSearch.aspx) is one of the website that uses Hybrid Search</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="737" xml:space="preserve">{{Notability|date=December 2009}}
A '''hybrid search engine''' ('''HSE''') is a type of [[computer]] [[search engine]] that uses different types of data with or without ontologies to produce the [[algorithm]]ically generated results based on [[web crawling]]. Previous types of search engines only use text to generate their results.
Hybrid search engines use a combination of both crawler-based results and directory results. More and more search engines these days are moving to a hybrid-based model.
==References==
{{No footnotes|date=April 2010}}
*http://eprints.ecs.soton.ac.uk/17457/
*http://eprints.whiterose.ac.uk/3771/
*http://www.picollator.com
*http://elocalfinder.com/HSearch.aspx

[[Category:Internet search]]


{{web-stub}}</text>
      <sha1>b0d1ut17n7tfrh1zc5hld3dcjo8p8vx</sha1>
    </revision>
  </page>
  <page>
    <title>URL redirection</title>
    <ns>0</ns>
    <id>636686</id>
    <revision>
      <id>756414315</id>
      <parentid>753147294</parentid>
      <timestamp>2016-12-24T02:30:12Z</timestamp>
      <contributor>
        <username>FrescoBot</username>
        <id>9021902</id>
      </contributor>
      <minor />
      <comment>Bot: [[User:FrescoBot/Links|link syntax]] and minor changes</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="32585" xml:space="preserve">{{Selfref|For redirection on Wikipedia, see [[Wikipedia:Redirect]].}}
{{Merge from|Rewrite engine|discuss=Talk:URL redirection#Merge Rewrite engine|date=November 2015}}
{{refimprove|date=December 2015}}
'''URL redirection''', also called '''URL forwarding''', is a [[World Wide Web]] technique for making a [[web page]] available under more than one [[Uniform Resource Locator|URL]] address. When a [[web browser]] attempts to open a URL that has been redirected, a page with a different URL is opened. Similarly, domain redirection or domain forwarding is when all pages in a URL [[Domain name|domain]] are redirected to a different domain, as when [http://www.wikipedia.com wikipedia.com] and [http://www.wikipedia.net wikipedia.net] are automatically redirected to [http://www.wikipedia.org wikipedia.org]. URL redirection is done for various reasons: for [[URL shortening]]; to prevent [[link rot|broken links]] when web pages are moved; to allow multiple domain names belonging to the same owner to refer to a single [[website|web site]]; to guide navigation into and out of a website; for privacy protection; and for less innocuous purposes such as [[phishing]] attacks.

== Purposes ==
There are several reasons to use URL redirection:

=== Similar domain names ===
A user might mistype a URL, for example, "example.com" and "exmaple.com". Organizations often register these "misspelled" domains and redirect them to the "correct" location: example.com. The addresses example.com and example.net could both redirect to a single domain, or web page, such as example.org. This technique is often used to "reserve" other [[top-level domain]]s (TLD) with the same name, or make it easier for a true ".edu" or ".net" to redirect to a more recognizable ".com" domain.

=== Moving pages to a new domain ===
Web pages may be redirected to a new domain for three reasons:
* a site might desire, or need, to change its domain name;
* an author might move his or her individual pages to a new domain;
* two web sites might merge.

With URL redirects, incoming links to an outdated URL can be sent to the correct location. These links might be from other sites that have not realized that there is a change or from bookmarks/favorites that users have saved in their browsers. The same applies to [[search engine]]s. They often have the older/outdated domain names and links in their database and will send search users to these old URLs. By using a "moved permanently" redirect to the new URL, visitors will still end up at the correct page. Also, in the next search engine pass, the search engine should detect and use the newer URL.

=== Logging outgoing links ===
The access logs of most web servers keep detailed information about where visitors came from and how they browsed the hosted site.  They do not, however, log which links visitors left by.  This is because the visitor's browser has no need to communicate with the original server when the visitor clicks on an outgoing link. This information can be captured in several ways.  One way involves URL redirection.  Instead of sending the visitor straight to the other site, links on the site can direct to a URL on the original website's domain that automatically redirects to the real target. This technique bears the downside of the delay caused by the additional request to the original website's server. As this added request will leave a trace in the server log, revealing exactly which link was followed, it can also be a privacy issue.&lt;ref&gt;
{{cite journal
  | title = Google revives redirect snoopery
  | journal = blog.anta.net
  | date = 2009-01-29
  | url = http://blog.anta.net/2009/01/29/509/
  | issn = 1797-1993
  | archiveurl=https://web.archive.org/web/20110817024348/http://blog.anta.net/2009/01/29/509/
  | archivedate=2011-08-17
}}&lt;/ref&gt; The same technique is also used by some corporate websites to implement a statement that the subsequent content is at another site, and therefore not necessarily affiliated with the corporation. In such scenarios, displaying the warning causes an additional delay.

=== Short aliases for long URLs ===
{{Main article|URL shortening}}

Web applications often include lengthy descriptive attributes in their URLs which represent data hierarchies, command structures, transaction paths and session information. This practice results in a URL that is aesthetically unpleasant and difficult to remember, and which may not fit within the size limitations of [[microblogging]] sites. [[URL shortening]] services provide a solution to this problem by redirecting a user to a longer URL from a shorter one.

=== Meaningful, persistent aliases for long or changing URLs ===
{{See also|Permalink|PURL|Link rot}}

Sometimes the URL of a page changes even though the content stays the same. Therefore, URL redirection can help users who have bookmarks. This is routinely done on Wikipedia whenever a page is renamed.

=== Post/Redirect/Get ===
{{Main article|Post/Redirect/Get}}

Post/Redirect/Get (PRG) is a [[web development]] [[design pattern]] that prevents some duplicate [[form (web)|form]] submissions, creating a more intuitive interface for [[user agent]]s (users).

=== Device targeting and geotargeting ===

Redirects can be effectively used for targeting purposes like [[device targeting]] or [[geotargeting]]. Device targeting has become increasingly important with the rise of mobile clients. There are two approaches to serve mobile users: Make the website [[responsive web design|responsive]] or redirect to a mobile website version. If a mobile website version is offered, users with mobile clients will be automatically forwarded to the corresponding mobile content. For device targeting, client side redirects or non-cacheable server side redirects are used. Geotargeting is the approach to offer localized content and automatically forward the user to a localized version of the requested URL. This is helpful for websites that target audience in more than one location and/or language. Usually server side redirects are used for Geotargeting but client side redirects might be an option as well, depending on requirements.&lt;ref&gt;{{Cite web |url=https://audisto.com/insights/guides/31/ |title=Redirects &amp; SEO - The Total Guide |accessdate=2015-11-29 |publisher=Audisto}}&lt;/ref&gt;

=== Manipulating search engines ===
Redirects have been used to manipulate search engines with unethical intentions, e.g. [[sneaky redirects]] or [[URL hijacking]]. The goal of misleading redirects is to drive search traffic to landing pages, which do not have enough ranking power on their own or which are only remotely or not at all related to the search target. The approach requires a rank for a range of search terms with a number of URLs that would utilize sneaky redirects to forward the searcher to the target page. This method had a revival with the uprise of mobile devices and device targeting. URL hijacking is an off-domain redirect technique&lt;ref&gt;{{cite web|url=https://www.mattcutts.com/blog/seo-advice-discussing-302-redirects/ |title=SEO advice: discussing 302 redirects |date=4 January 2006 |publisher=Matt Cutts, former Head of Google Webspam Team}}&lt;/ref&gt; that exploited the nature of the search engine's handling for temporary redirects. If a temporary redirect is encountered, search engines have to decide whether they assign the ranking value to the URL that initializes the redirect or to the redirect target URL. The URL that initiates the redirect may be kept to show up in search results, as the redirect indicates a temporary nature. Under certain circumstances it was possible to exploit this behaviour by applying temporary redirects to well ranking URLs, leading to a replacement of the original URL in search results by the URL that initialized the redirect, therefore "stealing" the ranking. This method was usually combined with sneaky redirects to re-target the user stream from the search results to a target page. Search engines have developed efficient technologies to detect these kind of manipulative approaches. Major search engines usually apply harsh ranking penalties on sites that get caught applying techniques like these.&lt;ref&gt;{{cite web|url=https://support.google.com/webmasters/answer/2721217?hl=en |title=Sneaky Redirects |date=3 December 2015 |publisher=Google Webmaster Guidelines}}&lt;/ref&gt;

=== Manipulating visitors ===
URL redirection is sometimes used as a part of [[phishing]] attacks that confuse visitors about which web site they are visiting.&lt;ref&gt;{{cite web|url=https://www.owasp.org/index.php/Unvalidated_Redirects_and_Forwards_Cheat_Sheet |title=Unvalidated Redirects and Forwards Cheat Sheet |date=21 August 2014 |publisher=Open Web Application Security Project (OWASP)}}&lt;/ref&gt; Because modern browsers always show the real URL in the address bar, the threat is lessened. However, redirects can also take you to sites that will otherwise attempt to attack in other ways. For example, a redirect might take a user to a site that would attempt to trick them into downloading antivirus software and, ironically, installing a [[trojan horse (computing)|trojan]] of some sort instead.

=== Removing &lt;code&gt;referer&lt;/code&gt; information ===
When a link is clicked, the browser sends along in the [[HTTP request]] a field called [[HTTP referer|referer]] which indicates the source of the link. This field is populated with the URL of the current web page, and will end up in the [[server log|logs]] of the server serving the external link. Since sensitive pages may have sensitive URLs (for example, &lt;code&gt;&lt;nowiki&gt;http://company.com/plans-for-the-next-release-of-our-product&lt;/nowiki&gt;&lt;/code&gt;), it is not desirable for the &lt;code&gt;referer&lt;/code&gt; URL to leave the organization. A redirection page that performs [[Referer#Referrer hiding|referrer hiding]] could be embedded in all external URLs, transforming for example &lt;code&gt;&lt;nowiki&gt;http://externalsite.com/page&lt;/nowiki&gt;&lt;/code&gt; into &lt;code&gt;&lt;nowiki&gt;http://redirect.company.com/http://externalsite.com/page&lt;/nowiki&gt;&lt;/code&gt;. This technique also eliminates other potentially sensitive information from the referer URL, such as the [[session ID]], and can reduce the chance of [[phishing]] by indicating to the end user that they passed a clear gateway to another site.

== Implementation ==
Several different kinds of response to the browser will result in a redirection.  These vary in whether they affect [[HTTP headers]] or HTML content.  The techniques used typically depend on the role of the person implementing it and their access to different parts of the system.  For example, a web author with no control over the headers might use a [[meta refresh|Refresh meta tag]] whereas a web server administrator redirecting all pages on a site is more likely to use server configuration.

=== Manual redirect ===
The simplest technique is to ask the visitor to follow a link to the new page, usually using an HTML anchor like:

&lt;source lang="html4strict"&gt;
Please follow &lt;a href="http://www.example.com/"&gt;this link&lt;/a&gt;.
&lt;/source&gt;

This method is often used as a fall-back&amp;nbsp;&#8212; if the browser does not support the automatic redirect, the visitor can still reach the target document by following the link.

=== HTTP status codes 3xx ===
In the [[HTTP]] [[Protocol (computing)|protocol]] used by the [[World Wide Web]], a '''redirect''' is a response with a [[List of HTTP status codes|status code]] beginning with ''3'' that causes a browser to display a different page. If a client encounters a redirect, it needs to make a number of decisions how to handle the redirect. Different status codes are used by clients to understand the purpose of the redirect, how to handle caching and which request method to use for the subsequent request.

HTTP/1.1 defines several status codes for redirection (RFC 7231):
* [[HTTP 300|300 multiple choices]] (e.g. offer different languages)
* [[HTTP 301|301 moved permanently]]
* [[HTTP 302|302 found]] (originally "temporary redirect" in HTTP/1.0 and popularly used for CGI scripts; superseded by 303 and 307 in HTTP/1.1 but preserved for backward compatibility)
* [[HTTP 303|303 see other]] (forces a GET request to the new URL even if original request was POST)
* [[HTTP 307|307 temporary redirect]] (provides a new URL for the browser to resubmit a GET or POST request)
* [[HTTP 308|308 permanent redirect]] (provides a new URL for the browser to resubmit a GET or POST request)

==== Redirect status codes and characteristics ====

{| class="wikitable"
|-
! HTTP Status Code !! HTTP Version !! Temporary / Permanent !! Cacheable !! Request Method Subsequent Request
|-
| 301 || HTTP/1.0 || Permanent || yes || GET / POST may change
|-
| 302 || HTTP/1.0 || Temporary || not by default || GET / POST may change
|-
| 303 || HTTP/1.1 || Temporary || never || always GET
|-
| 307 || HTTP/1.1 || Temporary || not by default || may not change
|-
| 308 || HTTP/1.1 || Permanent || by default || may not change
|-
|}&lt;ref&gt;{{Cite web |url=https://audisto.com/insights/guides/31/ |title=Redirects &amp; SEO - The Complete Guide |accessdate=2015-11-29 |publisher=Audisto}}&lt;/ref&gt;

All of these status codes require the URL of the redirect target to be given in the Location: header of the HTTP response. The 300 multiple choices will usually list all choices in the body of the message and show the default choice in the Location: header.

(Status codes [[HTTP 304|304 not modified]] and [[HTTP 305|305 use proxy]] are not redirects).

==== Example HTTP response for a 301 redirect ====

A [[HTTP]] response with the 301 "moved permanently" redirect looks like this:

&lt;syntaxhighlight lang="http"&gt;
HTTP/1.1 301 Moved Permanently
Location: http://www.example.org/
Content-Type: text/html
Content-Length: 174

&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Moved&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;Moved&lt;/h1&gt;
&lt;p&gt;This page has moved to &lt;a href="http://www.example.org/"&gt;http://www.example.org/&lt;/a&gt;.&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;/syntaxhighlight&gt;

==== Using server-side scripting for redirection ====
Web authors producing HTML content can't usually create redirects using HTTP headers as these are generated automatically by the web server program when serving an HTML file.  The same is usually true even for programmers writing CGI scripts, though some servers allow scripts to add custom headers (e.g. by enabling "non-parsed-headers").  Many web servers will generate a 3xx status code if a script outputs a "Location:" header line.  For example, in [[PHP]], one can use the "header" function:

&lt;source lang="php"&gt;
header('HTTP/1.1 301 Moved Permanently');
header('Location: http://www.example.com/');
exit();
&lt;/source&gt;

More headers may be required to prevent caching.&lt;ref name="php-301-robust-solution"&gt;{{cite web|url=http://www.websitefactors.co.uk/php/2011/05/php-redirects-302-to-301-rock-solid-solution/ |title=PHP Redirects: 302 to 301 Rock Solid Robust Solution |publisher=WebSiteFactors.co.uk |archiveurl=https://web.archive.org/web/20121012042703/http://www.websitefactors.co.uk/php/2011/05/php-redirects-302-to-301-rock-solid-solution |archivedate=2012-10-12}}&lt;/ref&gt; The programmer must ensure that the headers are output before the body.  This may not fit easily with the natural flow of control through the code.  To help with this, some frameworks for server-side content generation can buffer the body data.  In the [[Active Server Pages|ASP scripting]] language, this can also be accomplished using &lt;code&gt;response.buffer=true&lt;/code&gt; and &lt;code&gt;response.redirect &lt;nowiki&gt;"http://www.example.com/"&lt;/nowiki&gt;&lt;/code&gt; HTTP/1.1 allows for either a relative URI reference or an absolute URI reference.&lt;ref&gt;{{cite IETF | title = Hypertext Transfer Protocol (HTTP/1.1): Semantics and Content | rfc = 7231 | section = 7.1.2 | sectionname = Location | page = 68 | editor1 = Roy T. Fielding | editor2 = Julian F. Reschke | year = 2014 | month = June | publisher = [[Internet Engineering Task Force|IETF]]}}&lt;/ref&gt; If the URI reference is relative the client computes the required absolute URI reference according to the rules defined in RFC 3986.&lt;ref&gt;{{cite IETF | title = Uniform Resource Identifier (URI): Generic Syntax | rfc = 3986 | section = 5 | sectionname = Reference Resolution | page = 28 | first1 = Tim | last1 = Berners-Lee | author1-link = Tim Berners-Lee | first2 = Roy T. | last2 = Fielding | author2-link = Roy Fielding | first3 = Larry | last3 = Masinter | year = 2005 | month = January | publisher = [[Internet Engineering Task Force|IETF]]}}&lt;/ref&gt;

==== Apache mod_rewrite ====
The [[Apache HTTP Server]] mod_alias extension can be used to redirect certain requests. Typical configuration directives look like:
&lt;source lang="apache"&gt;
Redirect permanent /oldpage.html http://www.example.com/newpage.html
Redirect 301 /oldpage.html http://www.example.com/newpage.html
&lt;/source&gt;

For more flexible [[URL rewriting]] and redirection, Apache mod_rewrite can be used. E.g., to redirect a requests to a canonical domain name:
&lt;source lang="apache"&gt;
RewriteEngine on
RewriteCond %{HTTP_HOST} ^([^.:]+\.)*oldsite\.example\.com\.?(:[0-9]*)?$ [NC]
RewriteRule ^(.*)$ http://newsite.example.net/$1 [R=301,L]
&lt;/source&gt;

Such configuration can be applied to one or all sites on the server through the server configuration files or to a single content directory through a &lt;code&gt;[[.htaccess]]&lt;/code&gt; file.

==== nginx rewrite ====
[[Nginx]] has an integrated http rewrite module,&lt;ref&gt;{{cite web|url=http://nginx.org/r/rewrite |title=Module ngx_http_rewrite_module - rewrite |publisher=nginx.org |date= |accessdate=24 December 2014}}&lt;/ref&gt; which can be used to perform advanced URL processing and even web-page generation (with the &lt;tt&gt;return&lt;/tt&gt; directive).  A showing example of such advanced use of the rewrite module is [http://mdoc.su/ mdoc.su], which implements a deterministic [[URL shortening]] service entirely with the help of nginx configuration language alone.&lt;ref&gt;{{cite mailing list |date=18 February 2013 |url=http://mailman.nginx.org/pipermail/nginx/2013-February/037592.html |mailinglist=nginx@nginx.org |title=A dynamic web-site written wholly in nginx.conf? Introducing mdoc.su! |first=Constantine A. |last=Murenin |accessdate=24 December 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://mdoc.su/ |title=mdoc.su &#8212; Short manual page URLs for FreeBSD, OpenBSD, NetBSD and DragonFly BSD |first=Constantine A. |last=Murenin |date=23 February 2013 |accessdate=25 December 2014}}&lt;/ref&gt;

For example, if a request for [http://mdoc.su/DragonFlyBSD/HAMMER.5 &lt;tt&gt;/DragonFlyBSD/HAMMER.5&lt;/tt&gt;] were to come along, it would first be redirected internally to &lt;tt&gt;/d/HAMMER.5&lt;/tt&gt; with the first rewrite directive below (only affecting the internal state, without any HTTP replies issued to the client just yet), and then with the second rewrite directive, an [[HTTP response]] with a [[HTTP 302|302 Found status code]] would be issued to the client to actually redirect to the external [[Common Gateway Interface|cgi script]] of web-[[man page|man]]:&lt;ref&gt;{{cite web |url=http://nginx.conf.mdoc.su/mdoc.su.nginx.conf |title=mdoc.su.nginx.conf |first=Constantine A. |last=Murenin |date=23 February 2013 |accessdate=25 December 2014}}&lt;/ref&gt;
&lt;source lang="nginx"&gt;
	location /DragonFly {
		rewrite	^/DragonFly(BSD)?([,/].*)?$	/d$2	last;
	}
	location /d {
		set	$db	"http://leaf.dragonflybsd.org/cgi/web-man?command=";
		set	$ds	"&amp;section=";
		rewrite	^/./([^/]+)\.([1-9])$		$db$1$ds$2	redirect;
	}
&lt;/source&gt;

=== Refresh Meta tag and HTTP refresh header ===
[[Netscape]] introduced the [[meta refresh]] feature which refreshes a page after a certain amount of time.  This can specify a new URL to replace one page with another.  This is supported by most web browsers.&lt;ref&gt;[http://www.w3schools.com/tags/tag_meta.asp HTML &lt;meta&gt; tag]&lt;/ref&gt;&lt;ref&gt;[http://web.archive.org/web/20020802170847/http://wp.netscape.com/assist/net_sites/pushpull.html An exploration of dynamic documents]&lt;/ref&gt; A timeout of zero seconds effects an immediate redirect. This is treated like a 301 permanent redirect by Google, allowing transfer of PageRank to the target page.&lt;ref&gt;[http://sebastians-pamphlets.com/google-and-yahoo-treat-undelayed-meta-refresh-as-301-redirect/ "Google and Yahoo accept undelayed meta refreshs as 301 redirects"]. Sebastian's Pamphlets. 3 September 2007.&lt;/ref&gt;

This is an example of a simple HTML document that uses this technique:
&lt;source lang="html4strict"&gt;
&lt;html&gt;
&lt;head&gt;
&lt;meta http-equiv="Refresh" content="0; url=http://www.example.com/" /&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;p&gt;Please follow &lt;a href="http://www.example.com/"&gt;this link&lt;/a&gt;.&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;/source&gt;

This technique can be used by [[Web designer|web authors]] because the meta tag is contained inside the document itself.  The meta tag must be placed in the "head" section of the HTML file.  The number "0" in this example may be replaced by another number to achieve a delay of that many seconds.  The anchor in the "body" section is for users whose browsers do not support this feature.

The same effect can be achieved with an HTTP &lt;code&gt;refresh&lt;/code&gt; header:
&lt;source lang="http"&gt;
HTTP/1.1 200 ok
Refresh: 0; url=http://www.example.com/
Content-type: text/html
Content-length: 78

Please follow &lt;a href="http://www.example.com/"&gt;this link&lt;/a&gt;.
&lt;/source&gt;

This response is easier to generate by CGI programs because one does not need to change the default status code.

Here is a simple CGI program that effects this redirect:
&lt;source lang="perl"&gt;
#!/usr/bin/perl
print "Refresh: 0; url=http://www.example.com/\r\n";
print "Content-type: text/html\r\n";
print "\r\n";
print "Please follow &lt;a href=\"http://www.example.com/\"&gt;this link&lt;/a&gt;!"
&lt;/source&gt;

Note: Usually, the HTTP server adds the status line and the Content-length header automatically.

The [[World Wide Web Consortium|W3C]] discourage the use of meta refresh, since it does not communicate any information about either the original or new resource, to the browser (or [[search engine]]). The W3C's [http://www.w3.org/TR/WAI-WEBCONTENT/#tech-no-periodic-refresh Web Content Accessibility Guidelines (7.4)] discourage the creation of auto-refreshing pages, since most web browsers do not allow the user to disable or control the refresh rate.  Some articles that they have written on the issue include [http://www.w3.org/TR/WAI-WEBCONTENT/#gl-movement W3C Web Content Accessibility Guidelines (1.0): Ensure user control of time-sensitive content changes], [http://www.w3.org/QA/Tips/reback Use standard redirects: don't break the back button!] and [http://www.w3.org/TR/WCAG10-CORE-TECHS/#auto-page-refresh Core Techniques for Web Content Accessibility Guidelines 1.0 section 7].

=== JavaScript redirects ===
[[JavaScript]] can cause a redirect by setting the &lt;code&gt;window.location&lt;/code&gt; attribute, e.g.:
&lt;syntaxhighlight lang="ecmascript"&gt;
window.location='http://www.example.com/'
&lt;/syntaxhighlight&gt;
Normally JavaScript pushes the redirector site's [[URL]] to the browser's history. It can cause redirect loops when users hit the back button. With the following command you can prevent this type of behaviour.&lt;ref&gt;{{cite web|url=http://insider.zone/tools/client-side-url-redirect-generator/|title=Cross-browser client side URL redirect generator|publisher=Insider Zone}}&lt;/ref&gt;
&lt;syntaxhighlight lang="ecmascript"&gt;
window.location.replace('http://www.example.com/')
&lt;/syntaxhighlight&gt;
However, HTTP headers or the refresh meta tag may be preferred for security reasons and because JavaScript will not be executed by some browsers and many [[web crawler]]s.

=== Frame redirects ===

A slightly different effect can be achieved by creating an inline frame:

&lt;source lang="html4strict"&gt;
&lt;iframe height="100%" width="100%" src="http://www.example.com/"&gt;
Please follow &lt;a href="http://www.example.com/"&gt;link&lt;/a&gt;.
&lt;/iframe&gt;
&lt;/source&gt;

One main difference to the above redirect methods is that for a frame redirect, the browser displays the URL of the frame document and not the URL of the target page in the URL bar. This ''cloaking'' technique may be used so that the reader sees a more memorable URL or to fraudulently conceal a [[phishing]] site as part of [[website spoofing]].&lt;ref&gt;Aaron Emigh (19 January 2005). [http://www.sfbay-infragard.org/Documents/phishing-sfectf-report.pdf "Anti-Phishing Technology"] (PDF). Radix Labs.&lt;/ref&gt;

Before HTML5,&lt;ref&gt;
https://www.w3.org/TR/html5/obsolete.html&lt;/ref&gt; the same effect could be done with an [[Framing (World Wide Web)|HTML frame]] that contains the target page:
&lt;source lang="html4strict"&gt;
&lt;frameset rows="100%"&gt;
  &lt;frame src="http://www.example.com/"&gt;
  &lt;noframes&gt;
    &lt;body&gt;Please follow &lt;a href="http://www.example.com/"&gt;link&lt;/a&gt;.&lt;/body&gt;
  &lt;/noframes&gt;
&lt;/frameset&gt;
&lt;/source&gt;

=== Redirect chains ===
One redirect may lead to another. For example, the URL [http://www.wikipedia.com/wiki/URL_redirection http://www.wikipedia'''.com'''/wiki/URL_redirection] (with [[domain name]] in [[.com]]) is first redirected to [[:www:URL redirection|http://www.wikipedia'''.org'''/wiki/URL redirection]] (with domain name in [[.org]]), then to the [[HTTPS]] URL [[:www:URL redirection|'''https:'''//www.wikipedia.org/wiki/URL redirection]] and finally to the language-specific site https://'''en'''.wikipedia.org/wiki/URL redirection. This is unavoidable if the different links in the chain are served by different servers though it should be minimised by ''[[rewriting]]'' the URL as much as possible on the server before returning it to the browser as a redirect.

=== Redirect loops ===
Sometimes a mistake can cause a page to end up redirecting back to itself, possibly via other pages, leading to an infinite sequence of redirects. Browsers should stop redirecting after a certain number of hops and display an error message.

The HTTP/1.1 Standard states:&lt;ref name="rfc7231sec6.4"&gt;{{cite IETF | title = Hypertext Transfer Protocol (HTTP/1.1): Semantics and Content | rfc = 7231 | section = 6.4 | sectionname = Redirection 3xx | page = 54 | editor1 = Roy T. Fielding | editor1-link = Roy Fielding | editor2 = Julian F. Reschke | year = 2014 | month = June | publisher = [[Internet Engineering Task Force|IETF]]}}&lt;/ref&gt;
&lt;blockquote&gt;
A client ''SHOULD'' detect and intervene in cyclical redirections (i.e., "infinite" redirection loops).

Note: An earlier version of this specification recommended a maximum of five redirections ([RFC 2068], Section 10.3).  Content developers need to be aware that some clients might implement such a fixed limitation.
&lt;/blockquote&gt;
Note that the URLs in the sequence might not repeat, e.g.: http://www.example.com/1 -&gt; http://www.example.com/2 -&gt; http://www.example.com/3 ...

== Services ==
There exist services that can perform URL redirection on demand, with no need for technical work or access to the web server your site is hosted on.

=== URL redirection services ===
A '''redirect service''' is an information management system, which provides an internet link that redirects users to the desired content. The typical benefit to the user is the use of a memorable domain name, and a reduction in the length of the URL or web address. A redirecting link can also be used as a permanent address for content that frequently changes hosts, similarly to the [[Domain Name System]]. Hyperlinks involving URL redirection services are frequently used in spam messages directed at blogs and wikis.  Thus, one way to reduce spam is to reject all edits and comments containing hyperlinks to known URL redirection services; however, this will also remove legitimate edits and comments and may not be an effective method to reduce spam.
Recently, URL redirection services have taken to using [[AJAX]] as an efficient, user friendly method for creating shortened URLs. A major drawback of some URL redirection services is the use of delay pages, or frame based advertising, to generate revenue.

==== History ====
The first redirect services took advantage of [[top-level domains]] (TLD) such as "[[.to]]" (Tonga), "[[.at]]" (Austria) and "[[.is]]" (Iceland). Their goal was to make memorable URLs. The first mainstream redirect service was V3.com that boasted 4 million users at its peak in 2000.  V3.com success was attributed to having a wide variety of short memorable domains including "r.im", "go.to", "i.am", "come.to" and "start.at".  V3.com was acquired by FortuneCity.com, a large free web hosting company, in early 1999.&lt;ref&gt;{{cite news| url=http://news.bbc.co.uk/2/hi/technology/6991719.stm | work=BBC News | title=Net gains for tiny Pacific nation | date=2007-09-14 | accessdate=2010-05-27}}&lt;/ref&gt; As the sales price of top level domains started falling from $70.00 per year to less than $10.00, use of redirection services declined. With the launch of [[TinyURL]] in 2002 a new kind of redirecting service was born, namely [[URL shortening]]. Their goal was to make long URLs short, to be able to post them on internet forums. Since 2006, with the 140 character limit on the extremely popular [[Twitter]] service, these short URL services have been heavily used.

=== Referrer masking ===
Redirection services can hide the [[referrer]] by placing an intermediate page between the page the link is on and its destination. Although these are conceptually similar to other URL redirection services, they serve a different purpose, and they rarely attempt to shorten or obfuscate the destination URL (as their only intended side-effect is to hide referrer information and provide a clear gateway between other websites.) This type of redirection is often used to prevent potentially-malicious links from gaining information using the referrer, for example a [[session ID]] in the query string. Many large community websites use link redirection on external links to lessen the chance of an exploit that could be used to steal account information, as well as make it clear when a user is leaving a service, to lessen the chance of effective [[phishing]]  .

Here is a simplistic example of such a service, written in [[PHP]].
&lt;source lang="html+php"&gt;
&lt;?php
$url = htmlspecialchars($_GET['url']);
header( 'Refresh: 0; url=http://'.$url );
?&gt;
&lt;!-- Fallback using meta refresh. --&gt;
&lt;html&gt;
 &lt;head&gt;
  &lt;title&gt;Redirecting...&lt;/title&gt;
  &lt;meta http-equiv="refresh" content="0;url=http://&lt;?php echo $url; ?&gt;"&gt;
 &lt;/head&gt;
 &lt;body&gt;
 Attempting to redirect to &lt;a href="http://&lt;?php echo $url; ?&gt;"&gt;http://&lt;?php echo $url; ?&gt;&lt;/a&gt;.
 &lt;/body&gt;
&lt;/html&gt;
&lt;/source&gt;

The above example does not check who called it (e.g. by referrer, although that could be spoofed).  Also, it does not check the url provided.  This means that a malicious person could link to the redirection page using a url parameter of his/her own selection, from any page, which uses the web server's resources.

==Security issues==
URL redirection can be abused by attackers for [[phishing]] attacks, such as [[Open Redirect|open redirect]] and [[Covert Redirect|covert redirect]]. "An open redirect is an application that takes a parameter and redirects a user to the parameter value without any validation."&lt;ref name="Open_Redirect"&gt;{{cite web | url=https://www.owasp.org/index.php/Open_redirect | title=Open Redirect |publisher= OWASP |date=16 March 2014 | accessdate=21 December 2014}}&lt;/ref&gt; "Covert redirect is an application that takes a parameter and redirects a user to the parameter value WITHOUT SUFFICIENT validation."&lt;ref name="Covert_Redirect"&gt;{{cite web | url=http://tetraph.com/covert_redirect/ | title=Covert Redirect |publisher= Tetraph |date=1 May 2014 | accessdate=21 December 2014}}&lt;/ref&gt; It was disclosed in May 2014 by a mathematical doctoral student Wang Jing from Nanyang Technological University, Singapore.&lt;ref name="CNET"&gt;{{cite web | url=http://www.cnet.com/news/serious-security-flaw-in-oauth-and-openid-discovered/ | title=Serious security flaw in OAuth, OpenID discovered |publisher= CNET |date=2 May 2014 | accessdate=21 December 2014}}&lt;/ref&gt;

==See also==
* [[Link rot]]
* [[Canonical link element]]
* [[Canonical meta tag]]
* [[Domain masking]]
* [[URL normalization]]
* [[Semantic URL]]

==References==
{{Reflist}}

==External links==
* [http://httpd.apache.org/docs/1.3/urlmapping.html Mapping URLs to Filesystem Locations]
* [http://www.cs.ucdavis.edu/~hchen/paper/www07.pdf Paper on redirection spam (UC Davis)] (403 Forbidden link)
* [http://projects.webappsec.org/URL-Redirector-Abuse Security vulnerabilities in URL Redirectors] The Web Application Security Consortium Threat Classification

{{Spamming}}

{{Use dmy dates|date=November 2010}}

{{DEFAULTSORT:Url Redirection}}
[[Category:Uniform Resource Locator]]
[[Category:Black hat search engine optimization]]
[[Category:Internet search]]
[[Category:Internet terminology]]</text>
      <sha1>swx8dpnrwvs691ebuiurh0wnhg81a09</sha1>
    </revision>
  </page>
  <page>
    <title>Text Retrieval Conference</title>
    <ns>0</ns>
    <id>1897206</id>
    <revision>
      <id>751208525</id>
      <parentid>723140423</parentid>
      <timestamp>2016-11-24T02:02:38Z</timestamp>
      <contributor>
        <username>Me, Myself, and I are Here</username>
        <id>17619453</id>
      </contributor>
      <minor />
      <comment>/* top */ adjust bold</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="12826" xml:space="preserve">{{Other uses of|TREC|TREC (disambiguation)}}
The '''Text REtrieval Conference''' ('''TREC''') is an ongoing series of [[workshop]]s focusing on a list of different [[information retrieval]] (IR) research areas, or ''tracks.'' It is co-sponsored by the [[National Institute of Standards and Technology]] (NIST) and the [[Intelligence Advanced Research Projects Activity]] (part of the office of the [[Director of National Intelligence]]), and began in 1992 as part of the [[DARPA TIPSTER Program|TIPSTER Text program]]. Its purpose is to support and encourage research within the information retrieval community by providing the infrastructure necessary for large-scale ''evaluation'' of [[text retrieval]] methodologies and to increase the speed of lab-to-product [[technology transfer|transfer of technology]].

Each track has a challenge wherein NIST provides participating groups with data sets and test problems. Depending on track, test problems might be questions, topics, or target extractable [[Features (pattern recognition)|features]]. Uniform scoring is performed so the systems can be fairly evaluated. After evaluation of the results, a workshop provides a place for participants to collect together thoughts and ideas and present current and future research work.

== Tracks ==

===Current tracks===
''New tracks are added as new research needs are identified, this list is current for TREC 2016.''&lt;ref&gt;http://trec.nist.gov/pubs/call2016.html&lt;/ref&gt;
* [http://www.trec-cds.org/ Clinical Decision Support Track] - '''Goal:''' to investigate techniques for linking medical cases to information relevant for patient care
* [http://sites.google.com/site/treccontext/ Contextual Suggestion Track] - '''Goal:''' to investigate search techniques for complex information needs that are highly dependent on context and user interests.
* [http://trec-dd.org/ Dynamic Domain Track] - '''Goal:'''  to investigate  domain-specific search algorithms that adapt to the dynamic information needs of professional users as they explore in complex domains. 
* [http://trec-liveqa.org/ LiveQA Track] -  '''Goal:'''  to  generate answers to real questions originating from real users via a live question stream, in real time. 
* [http://trec-open-search.org/ OpenSearch Track] - '''Goal:''' to  explore an evaluation paradigm for IR that involves real users of operational search engines. For this first year of the track the task will be ad hoc Academic Search.
* Real-Time Summarization Track -  '''Goal:''' to explore techniques for constructing real-time update summaries from social media streams in response to users' information needs. 
* [http://www.cs.ucl.ac.uk/tasks-track-2016/ Tasks Track] - '''Goal:'''  to test whether systems can induce the possible tasks users might be trying to accomplish given a query. 
* [http://trec-total-recall.org/ Total Recall Track] -  '''Goal:''': to evaluate methods to achieve very high recall, including methods that include a human assessor in the loop.

===Past tracks===
* Chemical Track - '''Goal:''' to develop and evaluate technology for large scale search in [[chemistry]]-related documents, including academic papers and patents, to better meet the needs of professional searchers, and specifically [[patent search]]ers and chemists.
* [[Crowdsourcing]] Track - '''Goal:''' to provide a collaborative venue for exploring [[crowdsourcing]] methods both for evaluating search and for performing search tasks. 
* [[TREC Genomics|Genomics Track]] - '''Goal:''' to study the retrieval of [[Genomics|genomic]] data, not just gene sequences but also supporting documentation such as research papers, lab reports, etc. Last ran on TREC 2007.
* [[Enterprise search|Enterprise Track]] - '''Goal:''' to study search over the data of an organization to complete some task. Last ran on TREC 2008.
* Entity Track - '''Goal:''' to perform entity-related search on Web data. These search tasks (such as finding entities and properties of entities) address common information needs that are not that well modeled as ad hoc document search.
* [[Cross-language information retrieval|Cross-Language]] Track - '''Goal:''' to investigate the ability of retrieval systems to find documents topically regardless of source language.
* [[Federated search|FedWeb]] Track - '''Goal:''' to select best resources to forward a query to, and merge the results so that most relevant are on the top.
* Federated Web Search Track - '''Goal:''' to investigate techniques for the selection and combination of search results from a large number of real on-line web search services.
* Filtering Track - '''Goal:''' to binarily decide retrieval of new incoming documents given a stable [[information need]].
* HARD Track - '''Goal:''' to achieve High Accuracy Retrieval from Documents by leveraging additional information about the searcher and/or the search context.
* Interactive Track - '''Goal:''' to study user [[Human-computer interaction|interaction]] with text retrieval systems.
* Knowledge Base Acceleration Track - '''Goal:''' to develop techniques to dramatically improve the efficiency of (human) knowledge base curators by having the system suggest modifications/extensions to the KB based on its monitoring of the data streams.
* Legal Track - '''Goal:''' to develop search technology that meets the needs of lawyers to engage in effective [[discovery (law)|discovery]] in digital document collections.
* Medical Records Track - '''Goal:''' to explore methods for searching unstructured information found in patient medical records. 
* [[Microblog]] Track - '''Goal:''' to examine the nature of real-time information needs and their satisfaction in the context of microblogging environments such as Twitter. 
* Novelty Track - '''Goal:''' to investigate systems' abilities to locate new (i.e., non-redundant) information.
* [[Question answering|Question Answering]] Track - '''Goal:''' to achieve more [[information retrieval]] than just [[document retrieval]] by answering factoid, list and definition-style questions.
* Robust Retrieval Track - '''Goal:''' to focus on individual topic effectiveness.
* [[Relevance feedback|Relevance Feedback]] Track - '''Goal:''' to further deep evaluation of relevance feedback processes.
* Session Track - '''Goal:''' to develop methods for measuring multiple-query sessions where information needs drift or get more or less specific over the session.
* [[Spam (electronic)|Spam]] Track - '''Goal:''' to provide a standard evaluation of current and proposed [[spam filter]]ing approaches.
* Temporal Summarization Track - '''Goal:''' to develop systems that allow users to efficiently monitor the information associated with an event over time.
* [[Terabyte]] Track - '''Goal:''' to investigate whether/how the [[information retrieval|IR]] community can scale traditional IR test-collection-based evaluation to significantly large collections.
* [[Video search engine|Video]] Track - '''Goal:''' to research in automatic segmentation, [[index (search engine)|index]]ing, and content-based retrieval of [[digital video]].
:In 2003, this track became its own independent evaluation named [[TRECVID]].
* Web Track - '''Goal:''' to explore information seeking behaviors common in general web search.

===Related events===
In 1997, a Japanese counterpart of TREC was launched (first workshop in 1999), called [http://research.nii.ac.jp/ntcir/ NTCIR] ([[National Institute of Informatics|NII]] Test Collection for IR Systems), and in 2000, a European counterpart was launched, called [http://www.clef-campaign.org/ CLEF] (Cross Language Evaluation Forum).

== Conference contributions to search effectiveness==

NIST claims that within the first six years of the workshops, the effectiveness of retrieval systems approximately doubled.&lt;ref&gt;[http://trec.nist.gov/overview.html From TREC homepage: "... effectiveness approximately doubled in the first six years of TREC"]&lt;/ref&gt; The conference was also the first to hold large-scale evaluations of non-English documents, speech, video and retrieval across languages. Additionally, the challenges have inspired a large body of [http://trec.nist.gov/pubs.html publications]. Technology first developed in TREC is now included in many of the world's commercial [[search engine]]s.  An independent report by RTII found that "about one-third of the improvement in web search engines from 1999 to 2009 is attributable to TREC. Those enhancements likely saved up to 3 billion hours of time using web search engines. ... Additionally, the report showed that for every $1 that NIST and its partners invested in TREC, at least $3.35 to $5.07 in benefits were accrued to U.S. information retrieval researchers in both the private sector and academia."
&lt;ref&gt;{{cite web|url=http://rti.org/page.cfm?objectid=75E125DC-5056-B100-31A5A6BDE897DE6D |title=NIST Investment Significantly Improved Search Engines |publisher=Rti.org |date= |accessdate=2012-01-19}}&lt;/ref&gt;
&lt;ref&gt;http://www.nist.gov/director/planning/upload/report10-1.pdf&lt;/ref&gt;

While one study suggests that the state of the art for ad hoc search has not advanced substantially in the past decade,&lt;ref&gt;Timothy G. Armstrong, Alistair Moffat, William Webber, Justin Zobel.  Improvements that don't add up: ad hoc retrieval results since 1998.  CIKM 2009.  ACM.&lt;/ref&gt; it is referring just to search for topically relevant documents in small news and web collections of a few gigabytes.  There have been advances in other types of ad hoc search in the past decade.  For example, test collections were created for known-item web search which found improvements from the use of anchor text, title weighting and url length, which were not useful techniques on the older ad hoc test collections.  In 2009, a new billion-page web collection was introduced, and spam filtering was found to be a useful technique for ad hoc web search, unlike in past test collections.

The test collections developed at TREC are useful not just for (potentially) helping researchers advance the state of the art, but also for allowing developers of new (commercial) retrieval products to evaluate their effectiveness on standard tests.  In the past decade, TREC has created new tests for enterprise e-mail search, genomics search, spam filtering, e-Discovery, and several other retrieval domains.

TREC systems often provide a baseline for further research.  Examples include:
* [[Hal Varian]], Chief Economist at [[Google]], says ''Better data makes for better science. The history of information retrieval illustrates this principle well," and describes TREC's contribution.&lt;ref&gt;[http://googleblog.blogspot.com/2008/03/why-data-matters.html Why Data Matters]&lt;/ref&gt;
* TREC's Legal track has influenced the e-Discovery community both in research and in evaluation of commercial vendors.&lt;ref&gt;[http://blogs.the451group.com/information_management/2009/01/29/standards-in-e-discovery-%E2%80%93-walking-the-walk/ The 451 Group: Standards in e-Discovery -- walking the walk]&lt;/ref&gt;
* The [[IBM]] researcher team building [[IBM Watson]] (aka [[DeepQA]]), which beat the world's best [[Jeopardy!]] players,&lt;ref&gt;[http://www-03.ibm.com/press/us/en/presskit/27297.wss IBM and Jeopardy! Relive History with Encore Presentation of Jeopardy!: The IBM Challenge]&lt;/ref&gt; used data and systems from TREC's QA Track as baseline performance measurements.&lt;ref&gt;[http://www.aaai.org/AITopics/articles&amp;columns/Ferrucci-Watson2010.pdf David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James Fan, David Gondek, Aditya A. Kalyanpur, Adam Lally, J. William Murdock, Eric Nyberg, John Prager, Nico Schlaefer, and Chris Welt. '''Building Watson:  An Overview of the DeepQA Project''']&lt;/ref&gt;

== Participation ==
The conference is made up of a varied, international group of researchers and developers.&lt;ref&gt;{{cite web|url=https://wiki.ir-facility.org/index.php/Participants |title=Participants - IRF Wiki |publisher=Wiki.ir-facility.org |date=2009-12-01 |accessdate=2012-01-19}}&lt;/ref&gt;&lt;ref&gt;http://trec.nist.gov/pubs/trec17/papers/LEGAL.OVERVIEW08.pdf&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://trec.nist.gov/pubs/trec17/appendices/million.query.results.html |title=Text REtrieval Conference (TREC) TREC 2008 Million Query Track Results |publisher=Trec.nist.gov |date= |accessdate=2012-01-19}}&lt;/ref&gt; In 2003, there were 93 groups from both academia and industry from 22 countries participating.

==References==
{{reflist}}

== External links ==
*[http://trec.nist.gov/ TREC website at NIST]
*[http://www.nist.gov/itl/div894/894.02/related_projects/tipster/ TIPSTER]
*[http://www.amazon.com/TREC-Experiment-Evaluation-Information-Electronic/dp/0262220733/ The TREC book (at Amazon)]

[[Category:Information retrieval organizations]]
[[Category:Computational linguistics]]
[[Category:Natural language processing]]
[[Category:Computer science competitions]]</text>
      <sha1>l9hlm7l5f15vosa497m85t8tkz5tewu</sha1>
    </revision>
  </page>
  <page>
    <title>DtSearch</title>
    <ns>0</ns>
    <id>14388058</id>
    <revision>
      <id>762587204</id>
      <parentid>758155363</parentid>
      <timestamp>2017-01-29T18:06:16Z</timestamp>
      <contributor>
        <username>Green Cardamom</username>
        <id>8931761</id>
      </contributor>
      <comment>remove comma</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5586" xml:space="preserve">{{Lowercase}}

{{Infobox company |
  name   = dtSearch Corp. |
  slogan = "The Smart Choice for Text Retrieval since 1991" |
  type   =  Private company |
  foundation     = 1991 |
  location       = [[Bethesda, Maryland|Bethesda]], [[Maryland]], [[United States|US]] |
  key_people     = David Thede, President |
  industry       = [[Software]] |

  homepage       = [http://www.dtsearch.com/ www.dtsearch.com]
}}

'''dtSearch Corp.''' is a [[software company]] which specializes in [[text retrieval]] software. It was founded in 1991, and is headquartered in [[Bethesda, Maryland|Bethesda]], [[Maryland]]. Its current range of software includes products for enterprise [[desktop search]], Intranet/Internet [[spidering]] and search, and [[search engines]] for developers ([[Software development kit|SDK]]) to integrate into other software applications.

==History==
dtSearch Corp was founded by David Thede;&lt;ref&gt;[http://www.lets-talk-computers.com/guests/dtsearch/6.2/index.htm Lets talk computers - Interview May 31, 2003]&lt;/ref&gt;&lt;ref&gt;[https://www.google.com/patents/US6782380 Method and system for indexing and searching contents of extensible mark-up language(XML) documents US 6782380 B1]&lt;/ref&gt;&lt;ref&gt;[https://www.google.com/patents/US7464098 Method for rapidly searching elements or attributes or for rapidly filtering fragments in binary representations of structured, for example, XML-based documents US 7464098 B2]&lt;/ref&gt; the company started research and development in text retrieval in 1988  and incorporated in Virginia in 1991 as D T Software. Marketing of dtSearch 1.0 a DOS Text Retrieval software product began in the first quarter of 1991. Initially it was distributed as [[Association of Shareware Professionals]]-approved [[shareware]]. The product was featured in an article entitled "Text Retrieval Software" in an early edition of ''[[PC Magazine]]''&lt;ref&gt;"Text Retrieval Software". (July 1992). [[PC Magazine]] (UK ed)&lt;/ref&gt; as a shareware alternative to the commercial products reviewed; these included [[ISYS Search Software|ISYS]], [[ZyLAB Technologies|ZyIndex]], Strix, [[askSam]], [[ideaList]], Assassin PC, [[Folio Corporation|Folio Views]] and Lotus SmartText.

In the first few years after its initial release, dtSearch was an end-user application only. Then, in 1994, [[Symantec]] approached dtSearch about including its search technology into one of the first applications for 32-bit Windows; the dtSearch end-user application was developed into a [[Dynamic-link library]] (DLL) which Symantec embedded in Norton Navigator, which was released alongside Microsoft&#8217;s initial release of its 32-bit Windows operating system, [[Windows 95]].&lt;ref&gt;[http://www.processor.com/editorial/article.asp?article=articles%2Fp3012%2F11p12%2F11p12.asp dtSearch Performs Incredible Feats. Processor Mag. March 21, 2008]{{dead link|date=December 2016 |bot=InternetArchiveBot |fix-attempted=yes }}&lt;/ref&gt;

In 2007 the company was listed in the [[EContent]] 100 list, a list of companies that matter most in the digital content industry.&lt;ref&gt;[http://www.econtentmag.com/Articles/ArticleReader.aspx?ArticleID=40160&amp;PageNum=22007 EContent 100 list]&lt;/ref&gt;

==Products==
The current (v 7.7) product range is [[Unicode]]-based and has an index that can handle over 1 [[terabyte|TB]] of data per index.

*dtSearch Desktop with Spider -  Windows client Desktop search software (32 and 64 bit indexers)
*dtSearch Network with Spider -  as dtSearch Desktop but licensed for Network use (32 and 64 bit indexers)
*dtSearch Web with Spider -  browser based search-only client for Intranet/Internet usage based on Microsoft IIS (32 and 64 bit indexers)
*dtSearch Engine with Spider - SDK with C++, .NET, COM, Java, Delphi APIs (32-bit and 64-bit versions)
*dtSearch Engine for Linux - SDK with C++ and Java APIs
*dtSearch Publish &lt;ref&gt;[http://www.law.com/jsp/lawtechnologynews/PubArticleLTN.jsp?id=1202463957873&amp;slreturn=1&amp;hbxlogin=1 dtSearch Publish for EDD Production Law Technology News July 29, 2010]&lt;/ref&gt; - a search front-end for CD\DVD publishing (32 and 64 bit indexers)

==Licensing Partners==
* COMPANY:  PRODUCT
* Docupoint, LLC:  DrawingSearcher
* FileHold Systems Inc.: FileHold Document Management System
* ...

==See also==
* [[Enterprise search]]
* [[List of enterprise search vendors]]

==References==
{{Reflist}}

==External links==
*[http://www.dtsearch.com/ Company Website]
*[http://www.searchtools.com/tools/dtsearch.html Product description on SearchTools.com ]
*[http://www.windowsitpro.com/article/desktop-management/dtsearch-7-desktop-with-spider.aspx The index is mightier than the sword - Windows IT Pro. August 27, 2008]
*[http://www.infoworld.com/t/platforms/desktop-search-gets-down-business-610 Desktop search gets down to business - InfoWorld. September 01, 2005]
*[http://www.ncbi.nlm.nih.gov/pmc/articles/PMC150357/ Integrating Query of Relational and Textual Data in Clinical Databases - J Am Med Inform Assoc. 2003 Jan&#8211;Feb]
*[http://radiographics.rsna.org/content/29/5/1233.full.pdf Informatics in Radiology. Render: An Online Searchable Radiology Study Repository - RadioGraphics 2009; 29:1233&#8211;1246] 
*[http://jms.ndmctsgh.edu.tw/fdarticlee%5C2606199.pdf Use Of Intelligent Computer Search for the Patterns of Abnormal Lymphatic Uptake by F-18 FDG PET in Primary Lung Cancers - J Med Sci 2006;26(6):199-204]{{dead link|date=December 2016 |bot=InternetArchiveBot |fix-attempted=yes }}

{{DEFAULTSORT:Dtsearch Corp.}}
[[Category:Desktop search engines]]
[[Category:Information retrieval organizations]]
[[Category:Software companies based in Maryland]]</text>
      <sha1>lnkx1u0l0xs8elsk9ye074fed2lcw2y</sha1>
    </revision>
  </page>
  <page>
    <title>Automatic Content Extraction</title>
    <ns>0</ns>
    <id>33675011</id>
    <revision>
      <id>743084246</id>
      <parentid>700387121</parentid>
      <timestamp>2016-10-07T18:19:47Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>/* Topics and Exercises */ remove HTML comment</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3209" xml:space="preserve">{{Multiple issues|
{{citation style|date=December 2011}}
{{technical|date=October 2012}}
{{abbreviations|date=October 2012}}
}}
'''Automatic Content Extraction (ACE)''' is a research program for developing advanced [[Information extraction]] [[technologies]] convened by the [[National Institute of Standards and Technology | NIST]] from 1999 to 2008, succeeding [[Message Understanding Conference | MUC]] and preceding [http://www.nist.gov/tac/ Text Analysis Conference]. 

==Goals and Efforts==
In general objective, the ACE program is motivated by and addresses the same issues as the MUC program that preceded it. The ACE program, however, defines the research objectives in terms of the target objects (i.e., the entities, the relations, and the events) rather than in terms of the words in the text. For example, the so-called &#8220;named entity&#8221; task, as defined in MUC, is to identify those words (on the page) that are names of entities. In ACE, on the other hand, the corresponding task is to identify the entity so named. This is a different task, one that is more abstract and that involves inference more explicitly in producing an answer. In a real sense, the task is to detect things that &#8220;aren&#8217;t there&#8221;.

While the ACE program is directed toward extraction of information from [[Sound|audio]] and [[image]] sources in addition to pure text, the research effort is restricted to information extraction from text. The actual [[transduction (machine learning)|transduction]] of audio and image data into text is not part of the ACE research effort, although the processing of [[Speech recognition | ASR]] and [[Optical character recognition | OCR]] output from such transducers is.

The effort involves:
* defining the research tasks in detail,
* collecting and annotating data needed for training, development, and evaluation,
* supporting the research with evaluation tools and [[research workshop]]s.

==Topics and exercises==
Given a text in [[natural language]], the ACE challenge is to detect:
# '''entities''' mentioned in the text, such as: persons, organizations, locations, facilities, weapons, vehicles, and geo-political entities.
# '''relations''' between entities, such as: person A is the manager of company B. Relation types include: role, part, located, near, and social.
# '''events''' mentioned in the text, such as: interaction, movement, transfer, creation and destruction.

The program relates to [[English language|English]], [[Arabic language|Arabic]] and [[Chinese language|Chinese]] texts.

The ACE corpus is one of the standard benchmarks for testing new information extraction [[algorithm]]s.

==References==
* George Doddington@NIS T, Alexis Mitchell@LD C, Mark Przybocki@NIS T, Lance Ramshaw@BB N, Stephanie Strassel@LD C, Ralph Weischedel@BB N. [http://www.citeulike.org/user/erelsegal-halevi/article/10003935 The automatic content extraction (ACE) program&#8211;tasks, data, and evaluation.] 2004

==External links==
* [http://www.itl.nist.gov/iaui/894.02/related_projects/muc/ MUC] - ACE's predecessor.
* [http://projects.ldc.upenn.edu/ace/ ACE] (LDC)
* [http://www.itl.nist.gov/iad/894.01/tests/ace/ ACE] (NIST)

[[Category:Information retrieval organizations]]</text>
      <sha1>5oczipw7dviu4b8aiw6gwx2mltidht5</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Search engine software</title>
    <ns>14</ns>
    <id>6521632</id>
    <revision>
      <id>666714595</id>
      <parentid>666705385</parentid>
      <timestamp>2015-06-13T03:50:19Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>-Category:Data search engines (redundant)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="138" xml:space="preserve">[[Category:Information retrieval systems]]
[[Category:Utility software by type]]
[[Category:Marketing software]]
[[Category:Web software]]</text>
      <sha1>7wag14eowzs4ou0ffskogmc97z143ne</sha1>
    </revision>
  </page>
  <page>
    <title>Find</title>
    <ns>0</ns>
    <id>1486231</id>
    <revision>
      <id>751905534</id>
      <parentid>751905302</parentid>
      <timestamp>2016-11-28T12:49:14Z</timestamp>
      <contributor>
        <username>TwoTwoHello</username>
        <id>17114440</id>
      </contributor>
      <minor />
      <comment>Reverted 3 edits by [[Special:Contributions/125.22.43.11|125.22.43.11]] ([[User talk:125.22.43.11|talk]]) to last revision by ClueBot NG. ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="17697" xml:space="preserve">{{other uses}}
{{refimprove|date=June 2016}}
{{lowercase|title=find}} 
In [[Unix-like]] and some other [[operating system]]s, &lt;code&gt;'''find'''&lt;/code&gt; is a [[command-line utility]] that [[Search engine (computing)|searches]] one or more [[directory tree]]s of a [[file system]], locates [[Computer file|file]]s based on some [[user (computing)|user]]-specified criteria and applies a user-specified action on each matched file. The possible search criteria include a [[pattern matching|pattern]] to match against the [[filename]] or a time range to match against the modification time or access time of the file. By default, &lt;code&gt;find&lt;/code&gt; returns a list of all files below the current [[working directory]].

The related &lt;code&gt;[[locate (Unix)|locate]]&lt;/code&gt; programs use a database of indexed files obtained through &lt;code&gt;find&lt;/code&gt; (updated at regular intervals, typically by &lt;code&gt;[[cron]]&lt;/code&gt; job) to provide a faster method of searching the entire file system for files by name.

==History==
&lt;code&gt;find&lt;/code&gt; appeared in [[Version 5 Unix]] as part of the [[PWB/UNIX|Programmer's Workbench]] project, and was written by Dick Haight alongside ''cpio'',&lt;ref name="reader"&gt;{{cite techreport |first1=M. D. |last1=McIlroy |authorlink1=Doug McIlroy |year=1987 |url=http://www.cs.dartmouth.edu/~doug/reader.pdf |title=A Research Unix reader: annotated excerpts from the Programmer's Manual, 1971&#8211;1986 |series=CSTR |number=139 |institution=Bell Labs}}&lt;/ref&gt; which were designed to be used together.&lt;ref&gt;{{Cite web|title = libarchive/libarchive|url = https://github.com/libarchive/libarchive/wiki/FormatCpio|website = GitHub|accessdate = 2015-10-04}}&lt;/ref&gt;

==Find syntax==
{{expand section|date=August 2008}}
&lt;source lang="bash"&gt;
$ find [-H] [-L] [-P] path... [expression]
&lt;/source&gt;
The three options control how the &lt;code&gt;find&lt;/code&gt; command should treat symbolic links. The default behaviour is never to follow symbolic links. This can be explicitly specified using the -P flag. The -L flag will cause the &lt;code&gt;find&lt;/code&gt; command to follow symbolic links. The -H flag will only follow symbolic links while processing the command line arguments. These flags are not available with some older versions of &lt;code&gt;find&lt;/code&gt;.

At least one path must precede the expression. &lt;code&gt;find&lt;/code&gt; is capable of interpreting [[Wildcard character|wildcards]] internally and commands must be constructed carefully in order to control [[Glob (programming)|shell globbing]].

Expression elements are whitespace-separated and evaluated from left to right. They can contain logical elements such as AND (&amp;#x2011;and or &amp;#x2011;a) and OR (&amp;#x2011;or &amp;#x2011;o) as well as more complex predicates.

The [[GNU Find Utilities|GNU]] &lt;code&gt;find&lt;/code&gt; has a large number of additional features not specified by POSIX.

==POSIX protection from infinite output==
Real-world file systems often contain looped structures created through the use of [[hard link|hard]] or [[symbolic link|soft links]]. The [[POSIX|POSIX standard]] requires that
{{Quotation|
The &lt;code&gt;find&lt;/code&gt; utility shall detect infinite loops; that is, entering a previously visited
directory that is an ancestor of the last file encountered. When it detects an infinite
loop, &lt;code&gt;find&lt;/code&gt; shall write a diagnostic message to standard error and shall either recover
its position in the hierarchy or terminate.
}}

==Operators==
Operators can be used to enhance the expressions of the find command. Operators are listed in order of decreasing precedence:
* '''( expr )''' - forces precedence;
* '''! expr''' - true if expr is false;
* '''expr1 expr2''' (or '''expr1 -a expr2''') - AND. expr2 is not evaluated if expr1 is false;
* '''expr1 -o expr2''' - OR. expr2 is not evaluated if expr1 is true.

&lt;source lang="bash"&gt;
$ find . -name 'fileA_*' -o -name 'fileB_*'
&lt;/source&gt;
This command searches the current working directory tree for files whose names start with "fileA_" or "fileB_".

&lt;source lang="bash"&gt;
$ find . -name 'foo.cpp' '!' -path '.svn'
&lt;/source&gt;
This command searches the current working directory tree except the subdirectory tree ".svn" for files whose name is "foo.cpp". We quote the &lt;code&gt;!&lt;/code&gt; so that it's not interpreted by the shell as the history substitution character.

==Type filter explanation==
Various type filters are supported by &lt;code&gt;find&lt;/code&gt;. They are activated using the configuration switch:
&lt;source lang="bash"&gt;
$ find -type x
&lt;/source&gt;
where x may be any of:
* '''b''' - [[Device file|block device (buffered)]];
* '''c''' - [[Device file|character device (unbuffered)]];
* '''d''' - '''[[Directory (computing)|directory]]''';
* '''f''' - '''[[regular file]]''';
* '''l''' - [[symbolic link]]. This is never true if the -L option or the -follow operator is in effect, unless the symbolic link is broken. If you want to search for symbolic links when -L is in effect, use -xtype (though that is a GNU extension);
* '''p''' - [[named pipe]];
* '''s''' - [[Unix domain socket|socket]];
* '''D''' - [[Doors (computing)|door]].

The configuration switches listed in bold are most commonly used.

==Examples==
{{howto|section|date=September 2016}}

===From the current working directory===
&lt;source lang="bash"&gt;
$ find . -name 'my*'
&lt;/source&gt;
This searches the current working directory tree for files whose names start with ''my''. The single quotes avoid the [[shell (computing)|shell]] expansion&#8212;without them the shell would replace ''my*'' with the list of files whose names begin with ''my'' in the current working directory. In newer versions of the program, the directory may be omitted, and it will imply the current working directory.

===Regular files only===
&lt;source lang="bash"&gt;
$ find . -name 'my*' -type f
&lt;/source&gt;
This limits the results of the above search to only regular files, therefore excluding directories, special files, symbolic links, etc. ''my*'' is enclosed in single quotes (apostrophes) as otherwise the shell would replace it with the list of files in the current working directory starting with ''my''&#8230;

===Commands===
The previous examples created listings of results because, by default, &lt;code&gt;find&lt;/code&gt; executes the &lt;code&gt;-print&lt;/code&gt; action. (Note that early versions of the &lt;code&gt;find&lt;/code&gt; command had no default action at all; therefore the resulting list of files would be discarded, to the bewilderment of users.)

&lt;source lang="bash"&gt;
$ find . -name 'my*' -type f -ls
&lt;/source&gt;
This prints extended file information.

===Search all directories===
&lt;source lang="bash"&gt;
$ find / -name myfile -type f -print
&lt;/source&gt;
This searches every directory for a regular file whose name is ''myfile'' and prints it to the screen. It is generally not a good idea to look for files this way. This can take a considerable amount of time, so it is best to specify the directory more precisely. Some operating systems may mount dynamic file systems that are not congenial to &lt;code&gt;find&lt;/code&gt;. More complex filenames including characters special to the shell may need to be enclosed in single quotes.

===Search all but one subdirectory tree===
&lt;source lang="bash"&gt;
$ find / -path excluded_path -prune -o -type f -name myfile -print
&lt;/source&gt;
This searches every directory except the subdirectory tree ''excluded_path'' (full path including the leading /) that is pruned by the &lt;code&gt;-prune&lt;/code&gt; action, for a regular file whose name is ''myfile''.

===Specify a directory===
&lt;source lang="bash"&gt;
$ find /home/weedly -name myfile -type f -print
&lt;/source&gt;
This searches the ''/home/weedly'' directory tree for regular files named ''myfile''. You should always specify the directory to the deepest level you can remember.

===Search several directories===
&lt;source lang="bash"&gt;
$ find local /tmp -name mydir -type d -print
&lt;/source&gt;
This searches the ''local'' subdirectory tree of the current working directory and the ''/tmp'' directory tree for directories named ''mydir''.

===Ignore errors===
If you're doing this as a user other than root, you might want to ignore permission denied (and any other) errors. Since errors are printed to [[stderr]], they can be suppressed by redirecting the output to /dev/null. The following example shows how to do this in the bash shell:
&lt;source lang="bash"&gt;
$ find / -name myfile -type f -print 2&gt; /dev/null
&lt;/source&gt;

If you are a [[C shell|csh]] or [[tcsh]] user, you cannot redirect [[stderr]] without redirecting [[stdout]] as well. You can use sh to run the &lt;code&gt;find&lt;/code&gt; command to get around this:
&lt;source lang="bash"&gt;
$ sh -c "find / -name myfile -type f -print 2&gt; /dev/null"
&lt;/source&gt;
An alternate method when using [[C shell|csh]] or [[tcsh]] is to pipe the output from [[stdout]] and [[stderr]] into a [[grep]] command. This example shows how to suppress lines that contain permission denied errors.
&lt;source lang="bash"&gt;
$ find . -name myfile |&amp; grep -v 'Permission denied'
&lt;/source&gt;

===Find any one of differently named files===
&lt;source lang="bash"&gt;
$ find . \( -name '*jsp' -o -name '*java' \) -type f -ls
&lt;/source&gt;
The &lt;code&gt;-ls&lt;/code&gt; operator prints extended information, and the example finds any regular file whose name ends with either 'jsp' or 'java'. Note that the parentheses are required. TIn many shells the parentheses must be escaped with a backslash (&lt;code&gt;\(&lt;/code&gt; and &lt;code&gt;\)&lt;/code&gt;) to prevent them from being interpreted as special shell characters. The &lt;code&gt;-ls&lt;/code&gt; operator is not available on all versions of &lt;code&gt;find&lt;/code&gt;.

===Execute an action===
&lt;source lang="bash"&gt;
$ find /var/ftp/mp3 -name '*.mp3' -type f -exec chmod 644 {} \;
&lt;/source&gt;
This command changes the [[File system permissions|permissions]] of all regular files whose names end with ''.mp3'' in the directory tree ''/var/ftp/mp3''. The action is carried out by specifying the statement &lt;code&gt;-exec [[chmod]] 644 {} \;&lt;/code&gt; in the command. For every regular file whose name ends in &lt;code&gt;.mp3&lt;/code&gt;, the command &lt;code&gt;chmod 644 {}&lt;/code&gt; is executed replacing &lt;code&gt;{}&lt;/code&gt; with the name of the file. The semicolon (backslashed to avoid the shell interpreting it as a command separator) indicates the end of the command. Permission &lt;code&gt;644&lt;/code&gt;, usually shown as &lt;code&gt;rw-r--r--&lt;/code&gt;, gives the file owner full permission to read and write the file, while other users have read-only access. In some shells, the &lt;code&gt;{}&lt;/code&gt; must be quoted. The trailing ";" is customarily quoted with a leading "\", but could just as effectively be enclosed in single quotes.

Note that the command itself should *not* be quoted; otherwise you get error messages like
&lt;source lang="console"&gt;
find: echo "mv ./3bfn rel071204": No such file or directory
&lt;/source&gt;
which means that &lt;code&gt;find&lt;/code&gt; is trying to run a file called 'echo "mv ./3bfn rel071204"' and failing.

If you will be executing over many results, it is more efficient to use a variant of the exec primary that collects filenames up to ARG_MAX and then executes COMMAND with a list of filenames.

&lt;source lang="bash"&gt;
$ find . -exec COMMAND {} +
&lt;/source&gt;
This will ensure that filenames with whitespaces are passed to the executed COMMAND without being split up by the shell.

===Delete files and directories===
The &lt;code&gt;-delete&lt;/code&gt; action is a GNU extension, and using it turns on &lt;code&gt;-depth&lt;/code&gt;. So, if you are testing a find command with &lt;code&gt;-print&lt;/code&gt; instead of &lt;code&gt;-delete&lt;/code&gt; in order to figure out what will happen before going for it, you need to use &lt;code&gt;-depth -print&lt;/code&gt;.

Delete empty files and print the names (note that &lt;code&gt;-empty&lt;/code&gt; is a vendor unique extension from GNU &lt;code&gt;find&lt;/code&gt; that may not be available in all &lt;code&gt;find&lt;/code&gt; implementations):
&lt;source lang="bash"&gt;
$ find . -empty -delete -print
&lt;/source&gt;

Delete empty regular files:
&lt;source lang="bash"&gt;
$ find . -type f -empty -delete
&lt;/source&gt;

Delete empty directories:
&lt;source lang="bash"&gt;
$ find . -type d -empty -delete
&lt;/source&gt;

Delete empty files named 'bad':
&lt;source lang="bash"&gt;
$ find . -name bad -empty -delete
&lt;/source&gt;

Warning. &#8212; The &lt;code&gt;-delete&lt;/code&gt; action should be used with conditions such as &lt;code&gt;-empty&lt;/code&gt; or &lt;code&gt;-name&lt;/code&gt;:
&lt;source lang="bash"&gt;
$ find . -delete # this deletes all in .
&lt;/source&gt;

===Search for a string===
This command will search all files from the /tmp directory tree for a string:
&lt;source lang="bash"&gt;
$ find /tmp -type f -exec grep 'search string' '{}' /dev/null \+
&lt;/source&gt;
The &lt;tt&gt;[[/dev/null]]&lt;/tt&gt; argument is used to show the name of the file before the text that is found. Without it, only the text found is printed. 
GNU &lt;code&gt;grep&lt;/code&gt; can be used on its own to perform this task:
&lt;source lang="bash"&gt;
$ grep -r 'search string' /tmp
&lt;/source&gt;

Example of search for "LOG" in jsmith's home directory tree:
&lt;source lang="bash"&gt;
$ find ~jsmith -exec grep LOG '{}' /dev/null \; -print
/home/jsmith/scripts/errpt.sh:cp $LOG $FIXEDLOGNAME
/home/jsmith/scripts/errpt.sh:cat $LOG
/home/jsmith/scripts/title:USER=$LOGNAME
&lt;/source&gt;

Example of search for the string "ERROR" in all XML files in the current working directory tree:
&lt;source lang="bash"&gt;
$ find . -name "*.xml" -exec grep "ERROR" /dev/null '{}' \+ 
&lt;/source&gt;
The double quotes (" ") surrounding the search string and single quotes (&lt;nowiki&gt;' '&lt;/nowiki&gt;) surrounding the braces are optional in this example, but needed to allow spaces and some other special characters in the string. Note with more complex text (notably in most popular shells descended from `sh` and `csh`) single quotes are often the easier choice, since ''double quotes do not prevent all special interpretation''. Quoting filenames which have English contractions demonstrates how this can get rather complicated, since a string with an apostrophe in it is easier to protect with double quotes:
&lt;source lang="bash"&gt;
$ find . -name "file-containing-can't" -exec grep "can't" '{}' \; -print
&lt;/source&gt;

===Search for all files owned by a user===
&lt;source lang="bash"&gt;
$ find . -user &lt;userid&gt;
&lt;/source&gt;

===Search in case insensitive mode===
Note that &lt;code&gt;-iname&lt;/code&gt; is not in the standard and may not be supported by all implementations.
&lt;source lang="bash"&gt;
$ find . -iname 'MyFile*'
&lt;/source&gt;

If the &lt;code&gt;-iname&lt;/code&gt; switch is not supported on your system then workaround techniques may be possible such as:
&lt;source lang="bash"&gt;
$ find . -name '[mM][yY][fF][iI][lL][eE]*'
&lt;/source&gt;

This uses [[Perl]] to build the above command for you (though in general this kind of usage is dangerous, since special characters are not properly quoted before being fed into the standard input of `sh`):
&lt;source lang="bash"&gt;
$ echo 'MyFile*' | perl -pe 's/([a-zA-Z])/[\L\1\U\1]/g;s/(.*)/find . -name \1/' | sh
&lt;/source&gt;

===Search files by size===
Searching files whose size is between 100 kilobytes and 500 kilobytes:
&lt;source lang="bash"&gt;
$ find . -size +100k -a -size -500k
&lt;/source&gt;

Searching empty files:
&lt;source lang="bash"&gt;
$ find . -size 0k
&lt;/source&gt;

Searching non-empty files:
&lt;source lang="bash"&gt;
$ find . ! -size 0k
&lt;/source&gt;

===Search files by name and size ===
&lt;source lang="bash"&gt;
$ find /usr/src ! \( -name '*,v' -o -name '.*,v' \) '{}' \; -print
&lt;/source&gt;
This command will search the /usr/src directory tree. All files that are of the form '*,v' and '.*,v' are excluded. Important arguments to note are in the [[tooltip]] that is displayed on mouse-over.

&lt;source lang="bash" enclose="div"&gt;
for file in `find /opt \( -name error_log -o -name 'access_log' -o -name 'ssl_engine_log' -o -name 'rewrite_log' -o
 -name 'catalina.out' \) -size +300000k -a -size -5000000k`; do 
 cat /dev/null &gt; $file
done
&lt;/source&gt;
The units should be one of [bckw], 'b' means 512-byte blocks, 'c' means byte, 'k' means kilobytes and 'w' means 2-byte words. The size does not count indirect blocks, but it does count blocks in sparse files that are not actually allocated.

==Related utilities==
* &lt;code&gt;[[locate (Unix)|locate]]&lt;/code&gt; is a Unix search tool that searches a prebuilt database of files instead of directory trees of a file system. This is faster than &lt;code&gt;find&lt;/code&gt; but less accurate because the database may not be up-to-date.
* &lt;code&gt;[[grep]]&lt;/code&gt; is a command-line utility for searching plain-text data sets for lines matching a regular expression and by default reporting matching lines on [[standard output]].
* &lt;code&gt;[[tree (Unix)|tree]]&lt;/code&gt; is a command-line utility that recursively lists files found in a directory tree, indenting the filenames according to their position in the file hierarchy.
* [[GNU Find Utilities]] (also known as findutils) is a [[GNU package]] which contains implementations of the tools &lt;code&gt;find&lt;/code&gt; and [[xargs]].
* [[BusyBox]] is a utility that provides several stripped-down Unix tools in a single executable file, intended for embedded operating systems with very limited resources. It also provides a version of &lt;code&gt;find&lt;/code&gt;.
* &lt;code&gt;[[dir (command)|dir]]&lt;/code&gt; has the /s option that recursively searches for files or directories.

==See also==
* [[mdfind]], a similar utility that utilizes metadata for [[Mac OS X]] and [[Darwin (operating system)|Darwin]]
* [[List of Unix programs]]
* [[List of DOS commands]]
* [[Filter (higher-order function)]]
* [[find (command)]], a DOS and Windows command that is very different from UNIX &lt;code&gt;find&lt;/code&gt;

==References==
{{reflist}}

==External links==
* {{man|cu|find|SUS|find files}}
* [https://www.gnu.org/software/findutils/manual/html_mono/find.html Official webpage for GNU find]
* [http://www.librebyte.net/en/gnulinux/command-find-25-practical-examples/ Command find &#8211; 25 practical examples]

{{Unix commands}}

[[Category:Information retrieval systems]]
[[Category:Standard Unix programs]]
[[Category:Unix SUS2008 utilities]]</text>
      <sha1>0mf3e3nk5lzt7ntvyk9esydy85iifxh</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Image search</title>
    <ns>14</ns>
    <id>25810647</id>
    <revision>
      <id>666714786</id>
      <parentid>337897553</parentid>
      <timestamp>2015-06-13T03:52:54Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>move to Category:Information retrieval systems</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="89" xml:space="preserve">

[[Category:Applications of computer vision]]
[[Category:Information retrieval systems]]</text>
      <sha1>po4imn1f4bvk9ggrkjink85i70hwk3o</sha1>
    </revision>
  </page>
  <page>
    <title>Quandl</title>
    <ns>0</ns>
    <id>39810775</id>
    <revision>
      <id>750645137</id>
      <parentid>746255330</parentid>
      <timestamp>2016-11-21T01:03:25Z</timestamp>
      <contributor>
        <ip>162.216.161.56</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6399" xml:space="preserve">{{Infobox dot-com company
|name               = Quandl, Inc.
|logo               = [[File:Quandl-logo.png|100px]]
|company_type       = [[Private company|Private]]
|founder            = {{Plainlist|
* Tammer Kamel
* Abraham Thomas
}}
|location           = [[Toronto]], [[Canada]]
|area_served        = Worldwide
|key_people         = {{Plainlist|
* Tammer Kamel &lt;small&gt;(CEO)&lt;/small&gt;
* Abraham Thomas &lt;small&gt;(CDO)&lt;/small&gt;
}}
|industry           = [[Internet]]
|products           = Quandl Data Marketplace
|services           = Data [[subscriptions]]
|num_employees      = 16
|url                = {{URL|quandl.com}}
|programming_language = [[Ruby (programming language)|Ruby]] and [[Java (programming language)|Java]]
|website_type       = [[E-commerce]]
|language           = English
|launch_date        = {{start date and age|2013|01|01|df=yes}}
}}

'''Quandl''' ({{IPAc-en|&#712;|k|w|&#593;&#720;|n|d|&#601;l}}) is a Toronto-based platform for financial, economic, and alternative data, serving investment professionals. Quandl sources data from over 500 publishers.&lt;ref&gt;{{Cite web|url=https://www.producthunt.com/tech/quandl|title=Quandl - Product Hunt|website=Product Hunt|language=en-US|access-date=2016-09-01}}&lt;/ref&gt;  All Quandl's data are accessible via an [[API]].&lt;ref&gt;{{cite web |url= http://www.econometricsbysimulation.com/2013/05/quandl-package-5000000-free-datasets-at.html |title= Quandl Package - 5,000,000 free datasets at the tip of your fingers! |date= 5 May 2013 |publisher=EconBS}}&lt;/ref&gt; API access is possible through packages for multiple programming languages including [[R (programming language)|R]], [[Python (programming language)|Python]], [[Matlab]], [[Maple (software)]] and [[Stata]].&lt;ref&gt;{{cite web |url= http://blogs.computerworld.com/business-intelligenceanalytics/21881/quandl-wikipedia-data |title= Quandl: Wikipedia for data  |date= 8 March 2013 |publisher=Computer World |last = Machlis |first = Sharon }}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=https://www.quandl.com/tools/full-list |title=Full list of tools supported on Quandl}}&lt;/ref&gt;

An Excel add-in allows access to data, including stock price information.
 
Quandl's sources include the [[United Nations|UN]], [[Worldbank]], [[CLS Group]], Zacks, and several hundred more.&lt;ref&gt;{{cite web |url= http://gigaom.com/2013/05/31/its-a-beautiful-thing-when-free-data-meets-free-analytics/ |title= It's a beautiful thing when free data meets free analytics |date= 31 May 2013 |publisher=Gigaom |last = Harris |first = Derrick }}&lt;/ref&gt;&lt;ref&gt;{{cite web |url= http://www.quandl.com/resources/data-sources |title= Quandl Data Sources}}&lt;/ref&gt;

== History ==
Quandl was founded in 2012 by Tammer Kamel and Abraham Thomas.&lt;ref&gt;{{Cite web|url=https://www.quandl.com/about|title=Quandl Financial and Economic Data|website=www.quandl.com|access-date=2016-09-01}}&lt;/ref&gt; In March 2013, Quandl raised $1.5m,&lt;ref&gt;{{Cite web|url=https://www.crunchbase.com/organization/quandl|title=Quandl {{!}} CrunchBase|website=www.crunchbase.com|access-date=2016-09-01}}&lt;/ref&gt; in seed funding followed by a $5.4m Series A from [[August Capital]] in 2014.&lt;ref&gt;{{Cite web|url=http://blogs.wsj.com/venturecapital/2014/11/13/quandl-raises-5-4-million-for-its-financial-data-marketplace/|title=Quandl Raises $5.4 Million for Its Financial-Data Marketplace|last=Gage|first=Deborah|access-date=2016-09-01}}&lt;/ref&gt;

Since its launch, Quandl has been discussed as a disruptive force in the anachronistic financial data sector.&lt;ref&gt;{{Cite web|url=http://mattturck.com/2014/03/19/can-the-bloomberg-terminal-be-toppled/|title=Can the Bloomberg Terminal be "Toppled"?|date=2014-03-19|website=Matt Turck|access-date=2016-09-01}}&lt;/ref&gt; With over 100,000 users&lt;ref&gt;{{Cite web|url=https://www.integrity-research.com/new-data-provider-quandl/|title=New Data Provider Quandl Moves Toward Alternative Data &#8226; Integrity Research|language=en-US|access-date=2016-09-01}}&lt;/ref&gt; Quandl is positioning itself as a possible replacement for both Bloomberg and Reuters terminals.&lt;ref&gt;{{Cite web|url=http://www.huffingtonpost.com/irene-aldridge/blindsided-by-innovation-_b_9025960.html|title=Blindsided by innovation like Bloomberg? Don't become a statistic.|last=AbleMarkets.com|first=Irene Aldridge Quantitative portfolio manager; MD at|last2=speaker|date=2016-01-22|website=The Huffington Post|access-date=2016-09-01|last3=author|last4=Trading'|first4='High-Frequency}}&lt;/ref&gt; Quandl is an alternative for people who are unable to afford the expensive licensing fees of Bloomberg and Reuters.&lt;ref&gt;{{Cite web|url=https://openforum.hbs.org/challenge/understand-digital-transformation-of-business/data/quandl-a-marketplace-for-financial-data|title=Quandl: A Marketplace for Financial Data|access-date=2016-09-01}}&lt;/ref&gt;

== Products ==
Quandl's main focus, and area of expertise, is in the realm of alternative data.&lt;ref name=":0"&gt;{{Cite web|url=http://www.waterstechnology.com/inside-market-data/news/2462823/quandl-embarks-on-quest-for-alternative-data|title=Quandl Embarks on Quest for Alternative Data|access-date=2016-09-01}}&lt;/ref&gt; Quandl sells alternative datasets, defined as "any data that is not typically made available to Wall Street firms by traditional sources".&lt;ref name=":0" /&gt; Quandl "sources, evaluates and productizes undiscovered data" and then sells it to financial institutions, who use it to enhance their trading strategies.&lt;ref&gt;{{Cite web|url=https://www.quandl.com/institutions|title=Quandl Financial and Economic Data|website=www.quandl.com|access-date=2016-09-01}}&lt;/ref&gt;

Quandl also offers market data through its marketplace. Some data sets are free while others require a subscription. Different datasets have different prices. They have hundreds of databases and providers ranging from stock price history to global fundamentals to commodities data to Asian market data.&lt;ref&gt;{{Cite web|url=http://www.quandl.com/vendors|title=Quandl Financial and Economic Data|website=www.quandl.com|access-date=2016-09-01}}&lt;/ref&gt;

Quandl has an exclusive relationship with CLS Group in London, and is the only source of commercial [[Foreign exchange market|FX]] volume data&lt;ref&gt;{{Cite web|url=http://www.waterstechnology.com/inside-market-data/news/2465222/quandl-adds-cls-fx-trade-volume-data-to-online-platform|title=Quandl Adds CLS FX Trade, Volume Data to Online Platform|access-date=2016-09-01}}&lt;/ref&gt;

== References ==
{{Reflist|30em}}

[[Category:Information retrieval systems]]</text>
      <sha1>17mfcufnje4fdsd2maxul4i60b6k86j</sha1>
    </revision>
  </page>
  <page>
    <title>Greenpilot</title>
    <ns>0</ns>
    <id>26926858</id>
    <revision>
      <id>666861313</id>
      <parentid>622945390</parentid>
      <timestamp>2015-06-14T05:52:42Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>Category:Information retrieval systems</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6902" xml:space="preserve">{{COI|date=April 2010}}
The online portal '''Greenpilot''' is a service provided by the German National Library of Medicine, ZB MED.

The project is funded by the German Research Foundation ([[Deutsche Forschungsgemeinschaft]]) and gets its technical support from  [[Averbis]] Ltd. The portal first went online May 29, 2009 and currently runs in the updated beta version. In the context of the 'Germany - Land of Ideas' (Deutschland - Land der Ideen) initiative under the patronage of the [[President of Germany]] [[Horst K&#246;hler]] the ZB MED was awarded the distinction 'Selected Landmark 2009' (Ausgew&#228;hlter Ort 2009).&lt;ref&gt;[http://idw-online.de/pages/de/news315583 Pressemitteilung im Informationsdienst Wissenschaft vom 15. Mai 2009 ]&lt;/ref&gt;

==Objective==
The Greenpilot portal is a [[digital library]] specialised in the fields of Nutritional, Agricultural and Environmental Sciences. It aims to provide researchers in the three fields with a collection of scientific literature which is easy to access and of high quality. Especially the [[gray literature]] is often difficult to find and retrieve for the average user so Greenpilot also aims to make access to these sources easier. The service addresses itself not only to scientists and students but also to the broadly interested public. Greenpilot has been modelled after the corresponding digital library for Medicine, Medpilot,&lt;ref&gt;[http://www.medpilot.de/ Medpilot portal]&lt;/ref&gt; also a project of the German National Library of Medicine. The ZB MED has chosen the slogan 'Greenpilot - all about life and science' as a motto. In Greenpilot scientifically relevant databases, library catalogues and websites can be searched by entering a search term and the results are presented in a standardised web interface.

==Technical Background==
Greenpilot is a search engine based on intuitive search engine technology. The portal's software was developed in the programming language [[Perl]]. The search engine technology is based upon the 'Averbis Search Platform' software developed by the Averbis Ltd. and uses the [[open source]] software [[Lucene]]. Functionally this is an expert search engine which centres around the intelligent semantic connection of search terms by means of a standardised vocabulary. This is made possible by Averbis's MSI software which provides:

* semantic search optimised for the fields of Medicine and Life Sciences
* a contextual analysis of texts taking synonyms and compounds into account
* multilingual and cross-language search
* linking of lay and expert vocabulary
The search results are generated from a search index.

Additionally a [[metasearch]] can be conducted in order to search other databases not contained in the index. This search is based upon individual results from the specific database searched.

==Contents==
The Greenpilot portal integrates various scientifically relevant information resources under a uniform search interface. These resources are diverse and encompass national and international expert databases, library catalogues of national libraries with a focus on specific topics, full text documents from [[open access (publishing)|open access]] journals as well as information contained on about one thousand scientifically relevant websites selected for Greenpilot.
The following is a list of sources from November 2009:&lt;ref&gt;[http://www.greenpilot.de/beta2/app/misc/help/8cafcf93601eb861aaef86b5ce99ecdc/Datenbanken List of databases in Greenpilot]&lt;/ref&gt;

===Library Catalogues===
* Catalogue of the German National Library of Medicine (ZB MED Nutrition. Environment. Agriculture)
* Catalogue of the German National Library of Medicine (ZB MED Medicine. Health)
* Catalogue of the Bonn University Library
* Library catalogues of scientifically relevant departments within the collective library network (GBV)
* Catalogue of the Federal Ministry of Food, Agriculture and Consumer Protection (BMELV)
* Catalogue of the Johann Heinrich von Th&#252;nen-Institut (vTI), Federal Research Institute for Rural Areas, Forestry and Fisheries
* Catalogue of the Julius K&#252;hn-Institut, Federal Research Centre for Cultivated Plants
* Catalogue of the Friedrich L&#246;ffler-Institut, Federal Research Institute for Animal Health
* Catalogue of the Max Rubner-Institut, Federal Research Institute for Nutrition and Food
* Catalogue of the Federal Institute for Risk Assessment
* Catalogue of the Leibniz Institute for Marine Science (IFM-GEOMAR)
* Catalogue of the Leibniz Institute for Plant Genetics and Crop Plant Research (IPK-Plant Genetics and Crop Plant)
* Catalogue of the Leibniz Institute for Plant Biochemistry (IPB-Plant Chemistry)
* Catalogue of the special collection inshore and deep-sea fishery
* Catalogue of the University of Veterinary Medicine Hannover (TiHo-Veterinary Sciences)
* Catalogue of the German National Library of Economics (ZBW)

===Bibliographic databases===
* AGRIS (1975&#8211;2008), FAO ( Food and Agriculture Organization of the United Nations)
* VITIS-VEA, Viticulture and Enology Abstracts
* Medline (2004&#8211;2009)
* UFORDAT, Environmental Research Database (UBA)
* ULIDAT, Environmental Literature Database (UBA)
* ELFIS, International Information System for the Agricultural Sciences and Technology

===Relevant Internet Sources===
* Reviewed list of [[URL]]s selected by the ZB MED Nutrition. Environment. Agriculture
* Open Access journals with full text documents

===Metasearch===
* GetInfo, the knowledge portal for Technical Science provided by the Library for Technical Sciences (TIB) and the professional information centres FIZ Technik Frankfurt, FIZ Karlsruhe and FIZ CHEMIE Berlin.
* ECONIS, Catalogue of the German National Library of Economics (ZBW).

==Other Features==

===Search and results page===
* Search and advanced search
* Context sensitive help function
* [[Truncation]] and [[Boolean function]]s
* Personalised refining of search results by filtering for a specific document type, language or database
* [[Bookmark]]s

===Document ordering===
* Ordering directly from the results page is made possible by using the document delivery service of the ZB MED or the Electronic Journals Library ([[Elektronische Zeitschriftenbibliothek]]).

===Personalisation===
* My Greenpilot: a feature requiring the user to sign up for an account. The service is free of charge and offers an overview of ordered documents as well as enabling individual managing of customer data.

==See also==
*[[List of digital library projects]]
*[[vascoda]]

==References==
&lt;references /&gt;

==External links==
* [http://www.greenpilot.de Greenpilot website]
* [http://www.zbmed.de/home.html?lang=en Website of the German National Library of Medicine, ZB MED]
* [http://www.land-of-ideas.org Germany - Land of Ideas website]

{{coord missing|Germany}}

[[Category:Libraries in Germany]]
[[Category:Information retrieval systems]]
[[Category:Internet search engines]]</text>
      <sha1>3oam3b5gw8os229s7xq51p1hp9iwiz4</sha1>
    </revision>
  </page>
  <page>
    <title>Dynatext</title>
    <ns>0</ns>
    <id>14460441</id>
    <revision>
      <id>740772359</id>
      <parentid>708496452</parentid>
      <timestamp>2016-09-23T05:29:59Z</timestamp>
      <contributor>
        <username>Cnilep</username>
        <id>3729738</id>
      </contributor>
      <comment>disambiguate</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4594" xml:space="preserve">{{primary sources|date=October 2011}}
'''DynaText''' is an [[SGML]] publishing tool. It was introduced in 1990, and was the first system to handle arbitrarily large SGML documents, and to render them according to multiple style-sheets that could be switched at will.

DynaText and its Web sibling DynaWeb won multiple [[Seybold]] and other awards [http://xml.coverpages.org/ebt-award.html][http://xml.coverpages.org/dynaweb3-dvi.html], and there are eleven US Patents related to the DynaText technology: 5,557,722; 5,644,776; 5,708,806; 5,893,109; 5,983,248; 6,055,544; 6,101,511; 6,101,512; 6,105,044; 6,167,409; and 6,546,406.

DynaText was developed by Electronic Book Technologies, Incorporated, of [[Providence, Rhode Island]]. EBT was founded by [[Louis Reynolds]], [[Steven DeRose]], [[Jeffrey Vogel]], and [[Andries van Dam]], and was sold to [[Inso]] corporation in 1996, when it had about 150 employees.

DynaText heavily influenced stylesheet technologies such as [[DSSSL]] and [[CSS]], and [[XML]] chairman [[Jon Bosak]] cites EBT chief architect [[Steven DeRose]] as one of the originators of the notion of [[well-formed document|well-formedness]] formalized in [[XML]], as well as DynaText for influencing the design of Web browsers in general [http://www.ibiblio.org/bosak/cv.htm].

[[Inso]] corporation went out of business in 2002.

==Technology==

DynaText accepted [[SGML]] as input, and built a binary representation of the structure (similar to [[Document Object Model|DOM]] for [[XML]], but persistent), as well as a full-text [[inverted index]] of the text, elements, and attributes. Customers typically distributed such compiled e-books on CD-ROM or via network servers. Later versions of DynaText could also read SGML on the fly, providing exactly the same interface.

Unlike many prior systems, DynaText was not limited to any particular [[Document type definition|DTD]] (or [[XML schema|schema]]). Rather, customers could build style sheets in a simple language (also SGML-based), using properties very much like the later [[DSSSL]], [[CSS]], and [[XSL-FO]]. However, every property could have an expression as its value, which would be evaluated (if necessary) for each element the style applied to. Graphics, tables, formulae, and plug-ins could be included in documents.

Unlike nearly all prior SGML systems, DynaText was not limited to documents that could fit in [[RAM]] on the viewing or serving computer system. Users commonly created documents in the tens to hundreds of MB. DynaText customers included aerospace, workstation and other computer industry firms, government, literary and technical publishers, and others.

Full-text searches were based on an inverted index of words and other tokens (except for Japanese text, which was handled specially). Dynatext could report the number of "hits" for a given search, that occur within each section in the table of contents (by default, the table of contents appeared in a separate pane as an expandable outline, and clicking on any entry scrolled the full-text pane to the start of the corresponding section). Searches could also restrict hits to particular SGML element types, or sequences of types; refer to attributes; and use Boolean operators and parentheses. The "and" operator restricted its operands to occurring near each other, by default in the same paragraph or comparable element.

==References==
*[http://www.w3.org/History/19921103-hypertext/hypertext/Products/DynaText/Overview.html DynaText Notes] by [[Tim Berners-Lee]] (this note refers to a pre-release or very early release of DynaText).
*{{cite journal
 | id = MS
 | last = Smith
 | first = MacKenzie
 | title = Review: DynaText: An Electronic Publishing System
 | journal = Computers and the Humanities
 | volume = 27
 | issue = 5/6
 | pages = 415&#8211;420
 | publisher = Springer
 | location =
 | date = 1993
 | jstor = http://www.jstor.org/stable/30204569
 | issn = 0010-4817
 }}

*{{cite book
 | url = http://techpubs.sgi.com/library/dynaweb_docs/0630/SGI_EndUser/books/IIDWeb_UG/sgi_html/ch05.html
 | title = IRIS InSight&#8482; DynaWeb&#8482; User's Guide: Chapter 5. Introduction to the DynaText Search Language
 | publisher = Silicon Graphics, Inc.
}} Document Number: 007-3229-001

*{{cite journal
 | url = http://www.w3.org/Conferences/WWW4/ora_951122/112.html
 | title = DynaWeb: Interfacing Large SGML Repositories and the WWW
 | journal = Fourth International World Wide Web Conference: ``The Web Revolution''
 | date = 1995
 | location = Boston
 | first = Gavin Thomas 
 | last = Nicol
}}

[[Category:Information retrieval systems]]</text>
      <sha1>kye32nudur6b79p136mi79hn09fkmfe</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Search algorithms</title>
    <ns>14</ns>
    <id>1406201</id>
    <revision>
      <id>666714135</id>
      <parentid>666703092</parentid>
      <timestamp>2015-06-13T03:43:27Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>move to Category:Information retrieval techniques</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="140" xml:space="preserve">{{Commons category|Search algorithms}}
{{Cat main|Search algorithms}}

[[Category:Algorithms]]
[[Category:Information retrieval techniques]]</text>
      <sha1>m2hbfmy4pnl1084029gbltgm055p9c2</sha1>
    </revision>
  </page>
  <page>
    <title>Divergence-from-randomness model</title>
    <ns>0</ns>
    <id>1798853</id>
    <revision>
      <id>666714973</id>
      <parentid>592501389</parentid>
      <timestamp>2015-06-13T03:55:33Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>move to Category:Information retrieval techniques</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="577" xml:space="preserve">In the field of [[information retrieval]], '''divergence from randomness''' is one type of [[probabilistic]] model.

Term weights are computed by measuring the divergence between a term distribution produced by a random process and the actual term distribution.

==External links==
*[http://terrier.org/docs/v3.5/dfr_description.html Terrier's DFR Web page]
*[http://ir.dcs.gla.ac.uk/wiki/DivergenceFromRandomness Glasgow IR group Wiki DFR page]

[[Category:Ranking functions]]
[[Category:Information retrieval techniques]]
[[Category:Probabilistic models]]


{{comp-sci-stub}}</text>
      <sha1>9jqoeehbfeci7kbkrbs27u56bpjzbin</sha1>
    </revision>
  </page>
  <page>
    <title>Ordered weighted averaging aggregation operator</title>
    <ns>0</ns>
    <id>14893994</id>
    <revision>
      <id>744927318</id>
      <parentid>738491971</parentid>
      <timestamp>2016-10-18T08:35:28Z</timestamp>
      <contributor>
        <ip>82.150.248.37</ip>
      </contributor>
      <comment>Undid revision 738491971 by [[Special:Contributions/122.252.249.67|122.252.249.67]] ([[User talk:122.252.249.67|talk]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9910" xml:space="preserve">In applied mathematics &#8211; specifically in [[fuzzy logic]] &#8211; the '''ordered weighted averaging (OWA) operators''' provide a [[parameter]]ized class of mean type aggregation operators. They were introduced by [[Ronald R. Yager]]. Many notable mean operators such as the max, [[arithmetic average]], median and min, are members of this class. They have been widely used in [[computational intelligence]] because of their ability to model linguistically expressed aggregation instructions.

== Definition ==

Formally an '''OWA''' operator of dimension &lt;math&gt; \ n &lt;/math&gt; is a mapping &lt;math&gt; F: R_n \rightarrow R &lt;/math&gt; that has an associated collection of weights &lt;math&gt; \  W = [w_1, \ldots, w_n] &lt;/math&gt; lying in the unit interval and summing to one and with 		

:&lt;math&gt; F(a_1, \ldots , a_n) =  \sum_{j=1}^n  w_j b_j&lt;/math&gt;

where &lt;math&gt; b_j &lt;/math&gt; is the ''j''&lt;sup&gt;th&lt;/sup&gt; largest of the &lt;math&gt; a_i &lt;/math&gt;.

By choosing different ''W'' one can implement different aggregation operators. The OWA operator is a non-linear operator as a result of the process of determining the ''b''&lt;sub&gt;''j''&lt;/sub&gt;.

== Properties ==

The OWA operator is a mean operator. It is [[Bounded operator|bounded]], [[monotonic]], [[symmetric operator|symmetric]], and [[idempotent]], as defined below.

{|class="wikitable"
|[[Bounded operator|Bounded]]
|&lt;math&gt;   \min(a_1, \ldots, a_n) \le F(a_1, \ldots, a_n) \le \max(a_1, \ldots, a_n) &lt;/math&gt;
|-
|[[Monotonic]]
|&lt;math&gt;   F(a_1, \ldots, a_n) \ge F(g_1, \ldots, g_n) &lt;/math&gt; if &lt;math&gt; a_i \ge g_i &lt;/math&gt; for &lt;math&gt;\ i = 1,2,\ldots,n &lt;/math&gt;
|-
|[[symmetric operator|Symmetric]]
|&lt;math&gt;   F(a_1, \ldots, a_n)  = F(a_\boldsymbol{\pi(1)}, \ldots, a_\boldsymbol{\pi(n)})&lt;/math&gt; if &lt;math&gt;\boldsymbol{\pi} &lt;/math&gt; is a permutation map
|-
|[[Idempotent]]
|&lt;math&gt;  \ F(a_1, \ldots, a_n)  =  a &lt;/math&gt; if all &lt;math&gt; \ a_i = a &lt;/math&gt;
|}

== Notable OWA operators ==
:&lt;math&gt; \ F(a_1, \ldots, a_n) = \max(a_1, \ldots, a_n) &lt;/math&gt; if &lt;math&gt; \ w_1 = 1 &lt;/math&gt; and &lt;math&gt; \ w_j = 0 &lt;/math&gt; for &lt;math&gt; j \ne 1 &lt;/math&gt;

:&lt;math&gt; \ F(a_1, \ldots, a_n) = \min(a_1, \ldots, a_n) &lt;/math&gt; if &lt;math&gt; \ w_n = 1 &lt;/math&gt; and &lt;math&gt; \ w_j = 0 &lt;/math&gt; for &lt;math&gt; j \ne n &lt;/math&gt;

== Characterizing features ==

Two features have been used to characterize the OWA operators. The first is the attitudinal character(orness).

This is defined as
:&lt;math&gt;A-C(W)= \frac{1}{n-1} \sum_{j=1}^n (n - j) w_j. &lt;/math&gt;

It is known that &lt;math&gt; A-C(W) \in [0, 1] &lt;/math&gt;.

In addition ''A''&amp;nbsp;&amp;minus;&amp;nbsp;''C''(max) = 1, A&amp;nbsp;&amp;minus;&amp;nbsp;C(ave) = A&amp;nbsp;&amp;minus;&amp;nbsp;C(med) = 0.5 and A&amp;nbsp;&amp;minus;&amp;nbsp;C(min) = 0. Thus the A&amp;nbsp;&amp;minus;&amp;nbsp;C goes from 1 to 0 as we go from Max to Min aggregation. The attitudinal character characterizes the similarity of aggregation to OR operation(OR is defined as the Max).

The second feature is the dispersion. This defined as

:&lt;math&gt;H(W) = -\sum_{j=1}^n w_j \ln (w_j).&lt;/math&gt;

An alternative definition is &lt;math&gt;E(W) = \sum_{j=1}^n w_j^2 .&lt;/math&gt; The dispersion characterizes how uniformly the arguments are being used
&#192;&#282;

== A literature survey: OWA (1988-2014)==
The historical reconstruction of scientific development of the OWA field, the identification of the dominant direction of knowledge accumulation that emerged since the publication of the first OWA paper, and to discover the most active lines of research has recently been published, (see: http://onlinelibrary.wiley.com/doi/10.1002/int.21673/full). The results suggest, as expected, that Yager's paper[1] (IEEE Trans. Systems Man Cybernet, 18(1), 183&#8211;190, 1988) is the most influential paper and the starting point of all other research using OWA. Starting from his contribution, other lines of research developed and we describe them. Full list of papers published in OWA is also available at http://onlinelibrary.wiley.com/doi/10.1002/int.21673/full)

== Type-1 OWA aggregation operators ==

The above Yager's OWA operators are used to aggregate the crisp values. Can we aggregate fuzzy sets in the OWA mechanism ? The
'''[[Type-1 OWA operators]]''' have been proposed for this purpose. So the '''[[type-1 OWA operators]]''' provides us with a new technique for directly aggregating uncertain information with uncertain weights via OWA mechanism in soft decision making and data mining, where these uncertain objects are modelled by fuzzy sets.

The '''[[Type-1 OWA operators|type-1 OWA operator]]''' is defined according to the alpha-cuts of fuzzy sets as follows:

Given the ''n'' linguistic weights &lt;math&gt;\left\{ {W^i} \right\}_{i =1}^n &lt;/math&gt; in the form of fuzzy sets defined on the domain of discourse &lt;math&gt;U = [0,\;\;1]&lt;/math&gt;, then for each &lt;math&gt;\alpha \in [0,\;1]&lt;/math&gt;, an &lt;math&gt;\alpha &lt;/math&gt;-level type-1 OWA operator with &lt;math&gt;\alpha &lt;/math&gt;-level sets &lt;math&gt;\left\{ {W_\alpha ^i } \right\}_{i = 1}^n &lt;/math&gt; to aggregate the &lt;math&gt;\alpha &lt;/math&gt;-cuts of fuzzy sets &lt;math&gt;\left\{ {A^i} \right\}_{i =1}^n &lt;/math&gt; is given as

: &lt;math&gt;
\Phi_\alpha \left( {A_\alpha ^1 , \ldots ,A_\alpha ^n } \right) =\left\{ {\frac{\sum\limits_{i = 1}^n {w_i a_{\sigma (i)} } }{\sum\limits_{i = 1}^n {w_i } }\left| {w_i \in W_\alpha ^i ,\;a_i } \right. \in A_\alpha ^i ,\;i = 1, \ldots ,n} \right\}&lt;/math&gt;

where &lt;math&gt;W_\alpha ^i= \{w| \mu_{W_i }(w) \geq \alpha \}, A_\alpha ^i=\{ x| \mu _{A_i }(x)\geq \alpha \}&lt;/math&gt;, and &lt;math&gt;\sigma :\{\;1, \ldots ,n\;\} \to \{\;1, \ldots ,n\;\}&lt;/math&gt; is a permutation function such that &lt;math&gt;a_{\sigma (i)} \ge a_{\sigma (i + 1)} ,\;\forall \;i = 1, \ldots ,n - 1&lt;/math&gt;, i.e., &lt;math&gt;a_{\sigma (i)} &lt;/math&gt; is the &lt;math&gt;i&lt;/math&gt;th largest
element in the set &lt;math&gt;\left\{ {a_1 , \ldots ,a_n } \right\}&lt;/math&gt;.

The computation of the '''[[Type-1 OWA operators|type-1 OWA]]''' output is implemented by computing the left end-points and right end-points of the intervals &lt;math&gt;\Phi _\alpha \left( {A_\alpha ^1 , \ldots ,A_\alpha ^n } \right)&lt;/math&gt;:
&lt;math&gt;\Phi _\alpha \left( {A_\alpha ^1 , \ldots ,A_\alpha ^n } \right)_{-} &lt;/math&gt; and &lt;math&gt;
\Phi _\alpha \left( {A_\alpha ^1 , \ldots ,A_\alpha ^n } \right)_ {+},&lt;/math&gt;
where &lt;math&gt;A_\alpha ^i=[A_{\alpha-}^i, A_{\alpha+}^i], W_\alpha ^i=[W_{\alpha-}^i, W_{\alpha+}^i]&lt;/math&gt;. Then membership function of resulting aggregation fuzzy set is:

:&lt;math&gt;\mu _{G} (x) = \mathop \vee _{\alpha :x \in \Phi _\alpha \left( {A_\alpha ^1 , \cdots
,A_\alpha ^n } \right)_\alpha } \alpha &lt;/math&gt;

For the left end-points, we need to solve the following programming problem:

:&lt;math&gt; \Phi _\alpha \left( {A_\alpha ^1 , \cdots ,A_\alpha ^n } \right)_{-} = \min\limits_{\begin{array}{l} W_{\alpha - }^i \le w_i \le W_{\alpha + }^i A_{\alpha - }^i \le a_i \le A_{\alpha + }^i  \end{array}} \sum\limits_{i = 1}^n {w_i a_{\sigma (i)} / \sum\limits_{i = 1}^n {w_i } } &lt;/math&gt;

while for the right end-points, we need to solve the following programming problem:

:&lt;math&gt;\Phi _\alpha \left( {A_\alpha ^1 , \cdots , A_\alpha ^n } \right)_{+} = \max\limits_{\begin{array}{l} W_{\alpha - }^i \le w_i \le W_{\alpha + }^i  A_{\alpha - }^i \le a_i \le A_{\alpha + }^i  \end{array}} \sum\limits_{i = 1}^n {w_i a_{\sigma (i)} / \sum\limits_{i =
1}^n {w_i } } &lt;/math&gt;

[http://dx.doi.org/10.1109/TKDE.2010.191 This paper] has presented a fast method to solve two programming problem so that the type-1 OWA aggregation operation can be performed efficiently.

== References ==

* Yager, R. R., "On ordered weighted averaging aggregation operators in multi-criteria decision making," IEEE Transactions on Systems, Man and Cybernetics 18, 183&#8211;190, 1988.
* Yager, R. R. and Kacprzyk, J., [http://www.amazon.com/dp/079239934X The Ordered Weighted Averaging Operators: Theory and Applications], Kluwer: Norwell, MA, 1997.
* Liu, X., "The solution equivalence of minimax disparity and minimum variance problems for OWA operators," International Journal of Approximate Reasoning 45, 68&#8211;81, 2007.
* Emrouznejad (2009) SAS/OWA: ordered weighted averaging in SAS optimization, Soft Computing [http://www.springerlink.com/content/7277l73334r108x5/]
* Emrouznejad, A. and M. Marra (2014), Ordered Weighted Averaging Operators 1988&#8211;2014: A citation-based literature survey, International Journal of Intelligent Systems, 29:994-1014 [http://onlinelibrary.wiley.com/doi/10.1002/int.21673/full  &amp; http://onlinelibrary.wiley.com/store/10.1002/int.21673/asset/supinfo/int21673-sup-0001-SupMat.docx?v=1&amp;s=c0d8bdd220a31c876eb5885521cfa16d191f334d].
* Torra, V. and Narukawa, Y., Modeling Decisions: Information Fusion and Aggregation Operators, Springer: Berlin, 2007.
* Majlender, P., "OWA operators with maximal R&#233;nyi entropy," Fuzzy Sets and Systems 155, 340&#8211;360, 2005.
* Szekely, G. J. and Buczolich, Z., " When is a weighted average of ordered sample elements a maximum likelihood estimator of the location parameter?" Advances in Applied Mathematics 10, 1989, 439&#8211;456.
* S.-M. Zhou, F. Chiclana, R. I. John and J. M. Garibaldi, "Type-1 OWA operators for aggregating uncertain information with uncertain weights induced by type-2 linguistic quantifiers," Fuzzy Sets and Systems, Vol.159, No.24, pp.&amp;nbsp;3281&#8211;3296, 2008 [http://dx.doi.org/10.1016/j.fss.2008.06.018]
* S.-M. Zhou, F. Chiclana, R. I. John and J. M. Garibaldi, "Alpha-level aggregation: a practical approach to type-1 OWA operation for aggregating uncertain information with applications to breast cancer treatments," IEEE Transactions on Knowledge and Data Engineering, vol. 23, no.10, 2011, pp.&amp;nbsp;1455&#8211;1468.[http://dx.doi.org/10.1109/TKDE.2010.191]
* S.-M. Zhou, R. I. John, F. Chiclana and J. M. Garibaldi, "On aggregating uncertain information by type-2 OWA operators for soft decision making," International Journal of Intelligent Systems, vol. 25, no.6, pp.&amp;nbsp;540&#8211;558, 2010.[http://dx.doi.org/10.1002/int.20420]

[[Category:Artificial intelligence]]
[[Category:Logic in computer science]]
[[Category:Fuzzy logic]]
[[Category:Information retrieval techniques]]</text>
      <sha1>05mic4dldwbuzqldfh0gk2p3xw0dkdl</sha1>
    </revision>
  </page>
  <page>
    <title>Stop words</title>
    <ns>0</ns>
    <id>1015600</id>
    <revision>
      <id>756242367</id>
      <parentid>756242223</parentid>
      <timestamp>2016-12-22T22:37:47Z</timestamp>
      <contributor>
        <ip>93.86.48.16</ip>
      </contributor>
      <comment>/* References */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4905" xml:space="preserve">{{distinguish|Safeword}}
In [[computing]], '''stop words''' are words which are filtered out before or after [[Natural language processing|processing of natural language]] data (text).&lt;ref&gt;{{Cite book | last1 = Rajaraman | first1 = A. | last2 = Ullman | first2 = J. D. | doi = 10.1017/CBO9781139058452.002 | chapter = Data Mining | title = Mining of Massive Datasets | pages = 1&#8211;17| year = 2011 | isbn = 9781139058452 | pmid =  | pmc = | url = http://i.stanford.edu/~ullman/mmds/ch1.pdf}}&lt;/ref&gt; Though '''stop words''' usually refer to the most common words in a language, there is no single universal list of stop words used by all [[natural language processing]] tools, and indeed not all tools even use such a list. Some tools specifically avoid removing these '''stop words''' to support [[phrase search]].

Any group of words can be chosen as the stop words for a given purpose. For some [[search engine]]s, these are some of the most common, short [[function word]]s, such as ''the'', ''is'', ''at'', ''which'', and ''on''. In this case, stop words can cause problems when searching for phrases that include them, particularly in names such as "[[The Who]]", "[[The The]]", or "[[Take That]]". Other search engines remove some of the most common words&#8212;including [[lexical word]]s, such as "want"&#8212;from a query in order to improve performance.&lt;ref&gt;[http://blog.stackoverflow.com/2008/12/podcast-32 Stackoverflow]: "One of our major performance optimizations for the "related questions" query is removing the top 10,000 most common English dictionary words (as determined by Google search) before submitting the query to the SQL Server 2008 full text engine. It&#8217;s shocking how little is left of most posts once you remove the top 10k English dictionary words. This helps limit and narrow the returned results, which makes the query dramatically faster".&lt;/ref&gt;

[[Hans Peter Luhn]], one of the pioneers in [[information retrieval]], is credited with coining the phrase and using the concept.&lt;ref&gt;{{Cite book|title = Keyword-in-Context Index for Technical Literature (KWIC Index)|last = Luhn|first = H. P.|publisher = International Business Machines Corp.|year = 1959|isbn = |location = Yorktown Heights, NY|pages = |doi = 10.1002/asi.5090110403}}&lt;/ref&gt; The phrase "stop word", which is not in Luhn's 1959 presentation, and the associated terms "stop list" and "stoplist" appear in the literature shortly afterwards.&lt;ref&gt;{{cite journal|last1=Flood|first1=Barbara J.|title=Historical note: The Start of a Stop List at Biological Abstracts|journal=Journal of the American Society for Information Science|date=1999|volume=50|issue=12|page=1066|doi=10.1002/(SICI)1097-4571(1999)50:12&lt;1066::AID-ASI5&gt;3.0.CO;2-A|url=http://dx.doi.org/10.1002/(SICI)1097-4571(1999)50:12&lt;1066::AID-ASI5&gt;3.0.CO;2-A|accessdate=16 February 2016}}&lt;/ref&gt;

A predecessor concept was used in creating some [[Bible concordance|concordance]]s. For example, the first Hebrew concordance, Me&#8217;ir nativ, contained a one-page list of unindexed words, with nonsubstantive prepositions and conjunctions which are similar to modern stop words.&lt;ref&gt;{{cite journal|last1=Weinberg|first1=Bella Hass|title=Predecessors of scientific indexing structures in the domain of religion|journal=Second Conference on the History and Heritage of Scientific and Technical Information Systems|date=2004|pages=126&#8211;134|url=https://www.asis.org/History/11-weinberg.pdf|accessdate=17 February 2016}}&lt;/ref&gt;

== See also ==
{{Div col|cols=3}}
* [[Text mining]]
* [[Concept mining]]
* [[Information extraction]]
* [[Natural language processing]]
* [[Query expansion]]
* [[Stemming]]
* [[Index (search engine)|Search engine indexing]]
* [[Poison words]]
* [[Function words]]
* [[Filler_(linguistics) | Filler]]
{{Div col end}}

==References==
{{Reflist|2}}

== External links ==
* [http://xpo6.com/list-of-english-stop-words/  List of English Stop Words (PHP array, CSV) ]
* [http://dev.mysql.com/doc/refman/5.5/en/fulltext-stopwords.html  Full-Text Stopwords in MySQL ]
* [http://www.textfixer.com/resources/common-english-words.txt English Stop Words (CSV)]
* [http://mail.sarai.net/private/prc/Week-of-Mon-20080204/001656.html Hindi Stop Words]
* [http://solariz.de/de/deutsche_stopwords.htm German Stop Words],[http://aniol-consulting.de/uebersicht-deutscher-stop-words/ German Stop Words and phrases], another list of [http://www.ranks.nl/stopwords/german.html German stop words]
* [[:pl:Wikipedia:Stopwords|Polish Stop Words]]
* [https://code.google.com/p/stop-words/ Collection of stop words in 29 languages] [https://web.archive.org/web/*/http://tonyb.sk/_my/ir/stop-words-collection-2014-02-24.zip]
* [http://www.text-analytics101.com/2014/10/all-about-stop-words-for-text-mining.html A Detailed Explanation of Stop Words by Kavita Ganesan]

{{Natural Language Processing}}
{{SearchEngineOptimization}}

[[Category:Information retrieval techniques]]</text>
      <sha1>kuho1bzliupsodxfii95nd3tpg59tw3</sha1>
    </revision>
  </page>
  <page>
    <title>Uncertain inference</title>
    <ns>0</ns>
    <id>25962276</id>
    <revision>
      <id>711417671</id>
      <parentid>704833825</parentid>
      <timestamp>2016-03-22T19:41:52Z</timestamp>
      <contributor>
        <username>Sietse Snel</username>
        <id>94775</id>
      </contributor>
      <minor />
      <comment>/* Further work */ fix deprecated reference syntax</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4193" xml:space="preserve">'''Uncertain inference''' was first described by [[C. J. van Rijsbergen]]&lt;ref&gt;{{cite | author=C. J. van Rijsbergen | title=A non-classical logic for information retrieval | publisher=The Computer Journal | pages=481&#8211;485 | year=1986}}&lt;/ref&gt; as a way to formally define a query and document relationship in [[Information retrieval]]. This formalization is a [[logical consequence|logical implication]] with an attached measure of uncertainty.

==Definitions==
Rijsbergen proposes that the measure of [[uncertainty]] of a document ''d'' to a query ''q'' be the probability of its logical implication, i.e.:

:&lt;math&gt;P(d \to q)&lt;/math&gt;

A user's query can be interpreted as a set of assertions about the desired document. It is the system's task to [[inference|infer]], given a particular document, if the query assertions are true. If they are, the document is retrieved.
In many cases the contents of documents are not sufficient to assert the queries. A [[knowledge base]] of facts and rules is needed, but some of them may be uncertain because there may be a probability associated to using them for inference. Therefore, we can also refer to this as ''plausible inference''. The [[plausibility]] of an inference &lt;math&gt;d \to q&lt;/math&gt; is a function of the plausibility of each query assertion. Rather than retrieving a document that exactly matches the query we should rank the documents based on their plausibility in regards to that query.
Since ''d'' and ''q'' are both generated by users, they are error prone; thus &lt;math&gt;d \to q&lt;/math&gt; is uncertain. This will affect the plausibility of a given query.

By doing this it accomplishes two things:
* Separate the processes of revising probabilities from the logic
* Separate the treatment of relevance from the treatment of requests

[[Multimedia]] documents, like images or videos, have different inference properties for each datatype. They are also different from text document properties. The framework of plausible inference allows us to measure and combine the probabilities coming from these different properties.

Uncertain inference generalizes the notions of [[autoepistemic logic]], where truth values are either known or unknown, and when known, they are true or false.

==Example==
If we have a query of the form:

:&lt;math&gt;q = A \wedge B \wedge C&lt;/math&gt;

where A, B and C are query assertions, then for a document D we want the probability:

:&lt;math&gt;P (D \to (A \wedge B \wedge C))&lt;/math&gt;

If we transform this into the [[conditional probability]] &lt;math&gt;P ((A \wedge B \wedge C) | D)&lt;/math&gt; and if the query assertions are independent we can calculate the overall probability of the implication as the product of the individual assertions probabilities.

==Further work==
Croft and Krovetz&lt;ref&gt;{{cite | title=Interactive retrieval office documents | url=http://doi.acm.org/10.1145/45410.45435 | author1=W. B. Croft | author2=R. Krovetz | year=1988 }}&lt;/ref&gt; applied uncertain inference to an information retrieval system for office documents they called ''OFFICER''. In office documents the independence assumption is valid since the query will focus on their individual attributes. Besides analysing the content of documents one can also query about the author, size, topic or collection for example. They devised methods to compare document and query attributes, infer their plausibility and combine it into an overall rating for each document. Besides that uncertainty of document and query contents also had to be addressed.

[[Probabilistic logic network]]s is a system for performing uncertain inference; crisp true/false truth values are replaced not only by a probability, but also by a confidence level, indicating the certitude of the probability.

[[Markov logic network]]s allow uncertain inference to be performed; uncertainties are computed using the [[maximum entropy principle]], in analogy to the way that [[Markov chain]]s describe the uncertainty of [[finite state machine]]s.

== See also ==
* [[Fuzzy logic]]
* [[Probabilistic logic]]
* [[Plausible reasoning]]
* [[Imprecise probability]]

==References==
{{reflist}}

[[Category:Fuzzy logic]]
[[Category:Information retrieval techniques]]
[[Category:Inference]]</text>
      <sha1>lr2gvhbvry14nyg136z3xz3vdmsrui9</sha1>
    </revision>
  </page>
  <page>
    <title>Term Discrimination</title>
    <ns>0</ns>
    <id>15101979</id>
    <revision>
      <id>712583609</id>
      <parentid>666715963</parentid>
      <timestamp>2016-03-29T23:10:26Z</timestamp>
      <contributor>
        <username>BG19bot</username>
        <id>14508071</id>
      </contributor>
      <minor />
      <comment>Remove blank line(s) between list items per [[WP:LISTGAP]] to fix an accessibility issue for users of [[screen reader]]s. Do [[WP:GENFIXES]] and cleanup if needed. Discuss this at [[Wikipedia talk:WikiProject Accessibility#LISTGAP]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2721" xml:space="preserve">'''Term Discrimination''' is a way to rank keywords in how useful they are for [[information retrieval]].

== Overview ==

This is a method similar to [[tf-idf]] but it deals with finding keywords suitable for [[information retrieval]] and ones that are not.  Please refer to [[Vector Space Model]] first.

This method uses the concept of ''Vector Space Density'' that the less dense an [[occurrence matrix]] is, the better an information retrieval query will be.

An optimal index term is one that can distinguish two different documents from each other and relate two similar documents.  On the other hand, a sub-optimal index term can not distinguish two different document from two similar documents.

The discrimination value is the difference in the occurrence matrix's vector-space density versus the same matrix's vector-space without the index term's density.

 Let:
 &lt;math&gt;A&lt;/math&gt; be the occurrence matrix
 &lt;math&gt;A_k&lt;/math&gt; be the occurrence matrix without the index term &lt;math&gt;k&lt;/math&gt;
 and &lt;math&gt;Q(A)&lt;/math&gt; be density of &lt;math&gt;A&lt;/math&gt;.
 Then:
 The discrimination value of the index term &lt;math&gt;k&lt;/math&gt; is: 
 &lt;math&gt;DV_k = Q(A) - Q(A_k)&lt;/math&gt;

== How to compute ==

Given an [[occurrency matrix]]: &lt;math&gt;A&lt;/math&gt; and one keyword: &lt;math&gt;k&lt;/math&gt;
* Find the global document [[centroid]]: &lt;math&gt;C&lt;/math&gt; (this is just the average document vector)
* Find the average [[euclidean distance]] from every document vector, &lt;math&gt;D_i&lt;/math&gt; to &lt;math&gt;C&lt;/math&gt;
* Find the average euclidean distance from every document vector, &lt;math&gt;D_i&lt;/math&gt; to &lt;math&gt;C&lt;/math&gt; ''IGNORING'' &lt;math&gt;k&lt;/math&gt;
* The difference between the two values in the above step is the ''discrimination value'' for keyword &lt;math&gt;K&lt;/math&gt;

A higher value is better because including the keyword will result in better information retrieval.

== Qualitative Observations ==
Keywords that are ''[[Sparse matrix|sparse]]'' should be poor discriminators because they have poor ''[[Precision and recall|recall]],''
whereas
keywords that are ''frequent'' should be poor discriminators because they have poor ''[[Precision and recall|precision]].''

== References ==
* [[Gerard Salton|G. Salton]], A. Wong, and C. S. Yang (1975), "[http://www.cs.uiuc.edu/class/fa05/cs511/Spring05/other_papers/p613-salton.pdf A Vector Space Model for Automatic Indexing]," ''Communications of the ACM'', vol. 18, nr. 11, pages 613&#8211;620. ''(The article in which the vector space model was first presented)''
* Can, F., Ozkarahan, E. A (1987), "Computation of term/document discrimination values by use of the cover coefficient concept." ''Journal of the American Society for Information Science'', vol. 38, nr. 3, pages 171-183.

[[Category:Information retrieval techniques]]</text>
      <sha1>65atdn29f7vbhj9xkredsfrg9bayu0l</sha1>
    </revision>
  </page>
  <page>
    <title>Search-oriented architecture</title>
    <ns>0</ns>
    <id>7470226</id>
    <revision>
      <id>666716905</id>
      <parentid>666710948</parentid>
      <timestamp>2015-06-13T04:24:35Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>Category:Information retrieval techniques</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1881" xml:space="preserve">{{unreferenced|date=October 2007}}
The use of [[search engine technology]] is the main integration component in an [[information system]]. In a traditional business environment the [[architectural layer]] usually occupied by a [[relational database management system]] (RDBMS) is supplemented or replaced with a search engine or the indexing technology used to build search engines. Queries for information which would usually be performed using [[Structured Query Language]] (SQL) are replaced by keyword or fielded (or field-enabled) searches for structured, [[Semi-structured model|semi-structured]], or unstructured data.

In a typical [[Multitier architecture|multi-tier]] or [[Multitier architecture|N tier]] architecture information is maintained in a data tier where it can be stored and retrieved from a database or file system. The data tier is queried by the logic or business tier when information is needed using a data retrieval language like SQL.

In a '''search-oriented architecture''' the data tier may be replaced or placed behind another tier which contains a search engine and search engine index which is queried instead of the database management system. Queries from the business tier are made in the search engine query language instead of SQL. The search engine itself crawls the relational database management system in addition to other traditional data sources such as web pages or traditional file systems and consolidates the results when queried.

The benefit of adding a search layer to the architecture stack is rapid response time large dynamic datasets made possible by search indexing technology such as an [[inverted index]]. 

== Contrast with ==
* [[Service-oriented architecture]] (SOA)
* [[Service-Oriented Modeling]]

== See also ==
* [[Hibernate search]]
 
[[Category:Software architecture]]
[[Category:Information retrieval techniques]]</text>
      <sha1>jt11nqrgnjyz9c6609g6wxayzuviups</sha1>
    </revision>
  </page>
  <page>
    <title>Latent semantic mapping</title>
    <ns>0</ns>
    <id>11989095</id>
    <revision>
      <id>666733714</id>
      <parentid>570019893</parentid>
      <timestamp>2015-06-13T08:02:49Z</timestamp>
      <contributor>
        <username>Qwertyus</username>
        <id>196471</id>
      </contributor>
      <comment>removed [[Category:Information retrieval]]; added [[Category:Information retrieval techniques]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1930" xml:space="preserve">'''Latent semantic mapping (LSM)''' is a data-driven framework to model globally meaningful relationships implicit in large volumes of (often textual) data. It is a generalization of [[latent semantic analysis]]. In information retrieval, LSA enables retrieval on the basis of conceptual content, instead of merely matching words between queries and documents.

LSM was derived from earlier work on latent semantic analysis.  There are 3 main characteristics of latent semantic analysis: Discrete entities, usually in the form of words and documents, are mapped onto continuous vectors, the mapping involves a form of global correlation pattern, and dimensionality reduction is an important aspect of the analysis process. These constitute generic properties, and have been identified as potentially useful in a variety of different contexts.  This usefulness has encouraged great interest in LSM. The intended product of latent semantic mapping, is a data-driven framework for modeling relationships in large volumes of data.

[[Mac OS X v10.5]] and later includes a [[Software framework|framework]] implementing latent semantic mapping.&lt;ref&gt;[http://developer.apple.com/documentation/TextFonts/Reference/LatentSemanticMapping/index.html API Reference: Latent Semantic Mapping Framework Reference&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;

== See also ==
* [[Latent semantic analysis]]

== Notes ==
{{reflist}}

== References ==
* {{cite journal
 | url=http://ieeexplore.ieee.org/iel5/79/32367/01511825.pdf
 | title=Latent semantic mapping [information retrieval]
 | author=Bellegarda, J.R.
 | date=2005
}}
* {{cite conference
 | url=https://www.securecms.com/ICASSP2006/Tutorial_06.asp
 | title=Latent semantic mapping: Principles and applications
 | author=J. Bellegarda
 | booktitle=ICASSP 2006
 | date=2006
}}

[[Category:Information retrieval techniques]]
[[Category:Natural language processing]]


{{semantics-stub}}
{{compu-stub}}</text>
      <sha1>131pnwdwdah5a9fif8huxzey1byao0g</sha1>
    </revision>
  </page>
  <page>
    <title>Semantic technology</title>
    <ns>0</ns>
    <id>4416107</id>
    <revision>
      <id>738468053</id>
      <parentid>737660756</parentid>
      <timestamp>2016-09-09T03:50:19Z</timestamp>
      <contributor>
        <username>Lourdes</username>
        <id>26951022</id>
      </contributor>
      <comment>[[Wikipedia:Articles for deletion/Semantic technology]] closed as keep</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3746" xml:space="preserve">{{no footnotes|date=March 2013}}
[[File:SemanticNetExample.jpg|thumb|Simplistic example of the sort of semantic net used in Semantic Web technology]]
In [[software]], '''semantic technology''' encodes meanings separately from data and content files, and separately from application code. 

This enables machines as well as people to understand, share and reason with them at execution time. With semantic technologies, adding, changing and implementing new relationships or interconnecting programs in a different way can be just as simple as changing the external model that these programs share.

With traditional [[information technology]], on the other hand, meanings and relationships must be predefined and &#8220;hard wired&#8221; into data formats and the application program code at design time. This means that when something changes, previously unexchanged information needs to be exchanged, or two programs need to interoperate in a new way, the humans must get involved.

Off-line, the parties must define and communicate between them the knowledge needed to make the change, and then recode the data structures and program logic to accommodate it, and then apply these changes to the database and the application. Then, and only then, can they implement the changes.

Semantic technologies are &#8220;meaning-centered.&#8221; They include tools for:

* autorecognition of topics and concepts, 
* information and meaning extraction, and
* categorization. 

Given a question, semantic technologies can directly search topics, concepts, associations that span a vast number of sources.

Semantic technologies provide an abstraction layer above existing IT technologies that enables bridging and interconnection of data, content, and processes. Second, from the portal perspective, semantic technologies can be thought of as a new level of depth that provides far more intelligent, capable, relevant, and responsive interaction than with information technologies alone.

== See also ==
* [[Business Intelligence 2.0]] (BI 2.0)
* [[Metadata]]
* [[Ontology (computer science)]]
* [[Semantic web]]

==References==

* J.T. Pollock, R. Hodgson. ''Adaptive Information: Improving Business Through Semantic Interoperability, Grid Computing, and Enterprise Integration.'' [[J. Wiley and Sons]], October 2004
* R. Guha, R. McCool, and E. Miller. Semantic search. In ''WWW2003 &#8212; Proc. of the 12th international conference on World Wide Web'', pp 700&#8211;709. [[ACM Press]], 2003.
* I. Polikoff and D. Allemang. [https://lists.oasis-open.org/archives/regrep-semantic/200402/pdf00000.pdf Semantic technology.] ''TopQuadrant Technology Briefing'' v1.1, September 2003.
* [[Tim Berners-Lee|T. Berners-Lee]], J. Hendler, and O. Lassila. The Semantic Web: A new form of Web content that is meaningful to computers will unleash a revolution of new possibilities. ''[[Scientific American]]'', May 2001.
* A.P. Sheth, C. Ramakrishnan. [http://corescholar.libraries.wright.edu/knoesis/970Technology%20In%20Action:%20Ontology%20Driven%20Information%20Systems%20For%20Search,%20Integration%20and%20Analysis. Semantic (Web) Technology In Action: Ontology Driven Information Systems For Search, Integration and Analysis.] ''[[IEEE Data Engineering Bulletin]]'', 2003.
* Steffen Staab, Rudi Studer  (Ed.), Handbook on Ontologies, Springer, 
* Mills Davis. The Business Value of Semantic Technologies. Presentation and Report. Semantic Technologies for E-Government, September
2004.
* P. Hitzler, M. Kr&#246;tzsch, S. Rudolph, Foundations of Semantic Web Technologies, Chapman&amp;Hall/CRC, 2009, ISBN 978-1-4200-9050-5

== External links ==
* [http://semtech2010.semanticuniverse.com Semantic Technology Conference]

[[Category:Information retrieval techniques]]
[[Category:Semantics]]</text>
      <sha1>4wsx74a60bs1nd7n5436x8ivbgkyxpn</sha1>
    </revision>
  </page>
  <page>
    <title>Document clustering</title>
    <ns>0</ns>
    <id>14663145</id>
    <revision>
      <id>756183038</id>
      <parentid>743106203</parentid>
      <timestamp>2016-12-22T14:59:59Z</timestamp>
      <contributor>
        <username>MrOllie</username>
        <id>6908984</id>
      </contributor>
      <comment>[[WP:NOT]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6232" xml:space="preserve">{{Multiple issues|
{{disputed|date=March 2014}}
{{more footnotes|date=March 2014}}
}}

'''Document clustering''' (or '''text clustering''') is the application of [[cluster analysis]] to textual documents. It has applications in automatic document organization, [[topic (linguistics)|topic]] extraction and fast [[information retrieval]] or filtering.

==Overview==
Document clustering involves the use of descriptors and descriptor extraction. Descriptors are sets of words that describe the contents within the cluster. Document clustering is generally considered to be a centralized process. Examples of document clustering include web document clustering for search users.

The application of document clustering can be categorized to two types, online and offline. Online applications are usually constrained by efficiency problems when compared to offline applications.

In general, there are two common algorithms. The first one is the hierarchical based algorithm, which includes single link, complete linkage, group average and Ward's method.  By aggregating or dividing, documents can be clustered into hierarchical structure, which is suitable for browsing. However, such an algorithm usually suffers from efficiency problems. The other algorithm is developed using the [[K-means algorithm]] and its variants. Generally hierarchical algorithms produce more in-depth information for detailed analyses, while algorithms based around variants of the [[K-means algorithm]] are more efficient and provide sufficient information for most purposes.&lt;ref name="manning"&gt;Manning, Chris, and Hinrich Sch&#252;tze, ''Foundations of Statistical Natural Language Processing'', MIT Press. Cambridge, MA: May 1999.&lt;/ref&gt;{{rp|Ch.14}}

These algorithms can further be classified as hard or soft clustering algorithms. Hard clustering computes a hard assignment &#8211; each document is a member of exactly one cluster. The assignment of soft clustering algorithms is soft &#8211; a document&#8217;s assignment is a distribution over all clusters. In a soft assignment, a document has fractional membership in several clusters.&lt;ref name="manning"/&gt;{{rp|499}} [[Dimensionality reduction]] methods can be considered a subtype of soft clustering; for documents, these include [[latent semantic indexing]] ([[truncated singular value decomposition]] on term histograms)&lt;ref&gt;http://nlp.stanford.edu/IR-book/pdf/16flat.pdf&lt;/ref&gt; and [[topic model]]s.

Other algorithms involve graph based clustering, ontology supported clustering and order sensitive clustering.

Given a clustering, it can be beneficial to automatically derive human-readable labels for the clusters. [[Cluster labeling|Various methods]] exist for this purpose.

==Clustering in search engines==
A [[web search engine]] often  returns thousands of pages in response to a broad query, making it difficult for users to browse or to identify relevant information.  Clustering methods can be used to automatically group the retrieved documents into a list of meaningful categories, as is achieved by e.g. open source software such as [[Carrot2]].

==Procedures==
In practice, document clustering often takes the following steps:
 
1. [[Tokenization (lexical analysis)|Tokenization]]

Tokenization is the process of parsing text data into smaller units (tokens) such as words and phrases. Commonly used tokenization methods include [[Bag-of-words model]] and [[N-gram model]].

2. [[Stemming]] and [[lemmatization]]

Different tokens might carry out similar information (e.g. tokenization and tokenizing). And we can avoid calculating similar information repeatedly by reducing all tokens to its base form using various stemming and lemmatization dictionaries.

3. Removing [[stop words]] and [[punctuation]]

Some tokens are less important than others. For instance, common words such as "the" might not be very helpful for revealing the essential characteristics of a text. So usually it is a good idea to eliminate stop words and punctuation marks before doing further analysis.

4. Computing term frequencies or [[tf-idf]]

After pre-processing the text data, we can then proceed to generate features. For document clustering, one of the most common ways to generate features for a document is to calculate the term frequencies of all its tokens. Although not perfect, these frequencies can usually provide some clues about the topic of the document. And sometimes it is also useful to weight the term frequencies by the inverse document frequencies. See [[tf-idf]] for detailed discussions.

5. Clustering

We can then cluster different documents based on the features we have generated. See the algorithm section in [[cluster analysis]] for different types of clustering methods.

6. Evaluation and visualization

Finally, the clustering models can be assessed by various metrics. And it is sometimes helpful to visualize the results by plotting the clusters into low (two) dimensional space. See [[multidimensional scaling]] as a possible approach.

== Clustering v. Classifying ==
Clustering algorithms in computational text analysis groups documents into what are called subsets or ''clusters'' where the algorithm's goal is to create internally coherent clusters that are distinct from one another.&lt;ref&gt;{{Cite web|url=http://nlp.stanford.edu/IR-book/|title=Introduction to Information Retrieval|website=nlp.stanford.edu|pages=349|access-date=2016-05-03}}&lt;/ref&gt; Classification on the other hand, is a form of [[supervised learning]] where the features of the documents are used to predict the "type" of documents.

== References ==
{{reflist}}
Publications:
* Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch&#252;tze. ''Flat Clustering'' in &lt;u&gt;Introduction to Information Retrieval.&lt;/u&gt; Cambridge University Press. 2008
* Nicholas O. Andrews and Edward A. Fox, Recent Developments in Document Clustering, October 16, 2007 [http://eprints.cs.vt.edu/archive/00001000/01/docclust.pdf]
* Claudio Carpineto, Stanislaw Osi&#324;ski, Giovanni Romano, Dawid Weiss. A survey of Web clustering engines. ACM Computing Surveys, Volume 41, Issue 3 (July 2009), Article No. 17, {{ISSN|0360-0300}}

==See also==
*[[Cluster Analysis]]
*[[Fuzzy clustering]]

[[Category:Information retrieval techniques]]</text>
      <sha1>sz15cayl5yo0x8phdkbo5emh4zk1bsn</sha1>
    </revision>
  </page>
  <page>
    <title>Communication engine</title>
    <ns>0</ns>
    <id>20895417</id>
    <revision>
      <id>666862131</id>
      <parentid>601817015</parentid>
      <timestamp>2015-06-14T05:57:23Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>Category:Information retrieval techniques</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="788" xml:space="preserve">{{Orphan|date=February 2009}}
A '''communication engine''' is a tool that sends user requests to several other [[communication protocols]] and/or [[database]]s and aggregates the results into a single list or displays them according to their source. Communication engines enable users to enter communication account authorization once and access several communication avenues simultaneously. Communication engines operate on the premise that the [[World Wide Web]] is too large for any one engine to index it all and that more productive results can be obtained by combining the results from several engines dynamically. This may save the user from having to use multiple engines separately.

[[Category:Information retrieval techniques]]
[[Category:Computing terminology]]


{{Web-stub}}</text>
      <sha1>lkkw3xdyj3egmrkz5m27v1tcdii414r</sha1>
    </revision>
  </page>
  <page>
    <title>Hashtag</title>
    <ns>0</ns>
    <id>20819040</id>
    <revision>
      <id>762744498</id>
      <parentid>762260923</parentid>
      <timestamp>2017-01-30T14:08:54Z</timestamp>
      <contributor>
        <username>Elmeter</username>
        <id>23744986</id>
      </contributor>
      <comment>/* Adaptations */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="36769" xml:space="preserve">{{Use mdy dates|date=May 2016}}
{{Use American English|date=May 2016}}

[[File:Global Summit to End Sexual Violence in Conflict (14203190979).jpg|thumb|A sign suggesting the use of a #TimeToAct hashtag at a 2014 conference]]

A '''hashtag''' is a type of label or [[Tag (metadata)|metadata tag]] used on [[Social networking service|social network]] and [[microblogging]] services which makes it easier for users to find messages with a specific theme or content. Users create and use hashtags by placing the [[Number sign|hash character &lt;code&gt;#&lt;/code&gt;]] (also known as the number sign or pound sign) in front of a word or unspaced phrase, either in the main text of a message or at the end. Searching for that hashtag will yield each message that has been tagged with it. A hashtag archive is consequently collected into a single stream under the same hashtag.&lt;ref&gt;{{Cite journal|last=Chang|first=Hsia-Ching|last2=Iyer|first2=Hemalata|title=Trends in Twitter Hashtag Applications: Design Features for Value-Added Dimensions to Future Library Catalogues|url=http://muse.jhu.edu/article/485537|journal=Library Trends|volume=61|issue=1|pages=248&#8211;258|doi=10.1353/lib.2012.0024|issn=1559-0682}}&lt;/ref&gt; For example, on the [[photo sharing|photo-sharing]] service [[Instagram]], the hashtag ''#bluesky'' allows users to find all the posts that have been tagged using that hashtag.

Because of its widespread use, ''hashtag'' was added to the ''[[Oxford English Dictionary]]'' in June 2014.&lt;ref&gt;{{cite web |title='Hashtag' added to the OED &#8211; but # isn't a hash, pound, nor number sign|url=http://www.theregister.co.uk/2014/06/13/hashtag_added_to_the_oed/|work=The Register|date=June 13, 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web |title=New words notes June 2014|url=http://public.oed.com/the-oed-today/recent-updates-to-the-oed/june-2014-update/new-words-notes-june-2014/|work=Oxford English Dictionary|date=June 2014}}&lt;/ref&gt;  The term ''hashtag'' can also refer to the hash symbol itself when used in the context of a hashtag.&lt;ref&gt;{{cite web |title=Oxford English Dictionary &#8211; Hash|url=http://www.oed.com/view/Entry/389023#eid301493073|work=Oxford English Dictionary|date=June 2014}}&lt;/ref&gt;

== Origin and use ==
The [[Number sign|pound sign]] or [[Number sign|hash symbol]] was often used in [[information technology]] to highlight a special meaning. (It should be noted that the words "Pound Sign" in the UK refer specifically to currency "&#163;" - extended ASCII character 156 - and not weight.) In 1970 for example, the number sign was used to denote ''immediate'' [[address mode]] in the assembly language of the [[PDP-11]]&lt;ref&gt;{{cite web|url=https://programmer209.wordpress.com/2011/08/03/the-pdp-11-assembly-language/ |title=PDP-11 assembly language |publisher=Programmer209.wordpress.com |date=August 3, 2011 |accessdate=August 25, 2014}}&lt;/ref&gt; when placed next to a symbol or a number. In 1978, [[Brian Kernighan]] and [[Dennis Ritchie]] used ''#'' in the [[C (programming language)|C programming language]] for special keywords that had to be processed first by the [[C preprocessor]].&lt;ref&gt;{{cite book|title=[[The C Programming Language]]|authors=B.W. Kernighan &amp; d. Ritchie|publisher=Prentice Hall|year=1978|pages=86 and 207|isbn=0-13-110163-3}}&lt;/ref&gt; Since before the invention of the hashtag, the pound sign has been called the "hash symbol" in some countries outside of North America.&lt;ref&gt;{{cite book|last1=Bourke|first1=Jane|title=Communication Technology Resource Book|date=2004|publisher=Ready-Ed Publications|pages=19|url=https://books.google.com/books?id=gPNBTmxzpIIC&amp;lpg=PA19&amp;dq=hash%20key%20telephone&amp;pg=PA19#v=onepage&amp;q=hash&amp;f=false|accessdate=November 7, 2014|isbn=978-1-86397-585-8}}&lt;/ref&gt;&lt;ref&gt;{{cite book|last1=Hargraves|first1=Orin|title=Mighty fine words and smashing expressions : making sense of transatlantic English|date=2003|publisher=Oxford Univ. Press|location=Oxford [u.a.]|isbn=978-0-19-515704-8|pages=33, 260|url=https://books.google.com/books?id=dUTdk93cq9UC&amp;lpg=PA260&amp;dq=hash%20telephone&amp;pg=PA260#v=onepage&amp;q=hash%20mark&amp;f=false}}&lt;/ref&gt;

The pound sign appeared and was used by people within [[Internet Relay Chat|IRC]] networks to label groups and topics.&lt;ref&gt;"Channel Scope". Section 2.2. RFC 2811&lt;/ref&gt; Channels or topics that are available across an entire IRC network are prefixed with a hash symbol # (as opposed to those local to a server, which use an [[ampersand]] '&amp;').&lt;ref&gt;{{cite IETF |title=Internet Relay Chat Protocol |rfc=1459 |sectionname=Channels |section=1.3 |page= |last1=Oikarinen |first1=Jarkko |authorlink1=Jarkko Oikarinen |last2=Reed |first2=Darren |authorlink2= |year=1993 |month=May |publisher=[[Internet Engineering Task Force|IETF]] |accessdate=June 3, 2014}}&lt;/ref&gt;

The use of the pound sign in IRC inspired&lt;ref&gt;{{cite web|url=http://www.cmu.edu/homepage/computing/2014/summer/originstory.shtml |title=#OriginStory|publisher=Carnegie Mellon University|date=August 29, 2014}}&lt;/ref&gt; [[Chris Messina (open source advocate)|Chris Messina]] to propose a similar system to be used on Twitter to tag topics of interest on the microblogging network.&lt;ref&gt;{{cite news | url=http://www.nytimes.com/2011/06/12/fashion/hashtags-a-new-way-for-tweets-cultural-studies.html?_r=1&amp;pagewanted=all | title=Twitter's Secret Handshake | work=The New York Times | date=June 10, 2011 | accessdate=July 26, 2011 | author=Parker, Ashley}}&lt;/ref&gt; He posted the first hashtag on Twitter:
{{quote |1=How do you feel about using # (pound) for groups. As in #barcamp [msg]? |author = Chris Messina |source = ("factoryjoe"), August 23, 2007&lt;ref&gt;{{cite web|url = https://twitter.com/factoryjoe/statuses/223115412|title = Twitter post|author = Chris Messina ("factoryjoe")|date = August 23, 2007&lt;!-- 3:25 PM--&gt;}}&lt;/ref&gt; |width  = 50% |align  = center }}
Messina&#8217;s suggestion to use the hashtag was not adopted by Twitter, but the practice took off after hashtags were widely used in tweets relating to the [[2007 San Diego forest fires]] in Southern California.&lt;ref&gt;[http://mashable.com/2013/10/08/what-is-hashtag/ What is hashtag?"], Mashable, 8 October 2013&lt;/ref&gt;&lt;ref&gt;https://factoryjoe.com/2007/10/22/twitter-hashtags-for-emergency-coordination-and-disaster-relief/&lt;/ref&gt;

According to Messina, he suggested use of the hashtag to make it easy for "lay" users to search for content and find specific relevant updates; they are for people who do not have the technological knowledge to navigate the site. Therefore, the hashtag &#8220;was created organically by Twitter users as a way to categorize messages." &lt;ref&gt;{{Cite journal|last=Scott|first=Kate|date=2015-05-01|title=The pragmatics of hashtags: Inference and conversational style on Twitter|url=http://www.sciencedirect.com/science/article/pii/S037821661500096X|journal=Journal of Pragmatics|volume=81|pages=8&#8211;20|doi=10.1016/j.pragma.2015.03.015}}&lt;/ref&gt;

Internationally, the hashtag became a practice of writing style for Twitter posts during the [[2009&#8211;2010 Iranian election protests]]; Twitter users inside and outside Iran used both English- and [[Persian language|Persian]]-language hashtags in communications during the events.&lt;ref&gt;{{cite news|title=The story of the hashtag began with Iranians|url=http://www.dw.de/&#1581;&#1705;&#1575;&#1740;&#1578;-&#1607;&#1588;&#1578;&#1711;&#1740;-&#1705;&#1607;-&#1575;&#1740;&#1585;&#1575;&#1606;&#1740;&#1575;&#1606;-&#1570;&#1594;&#1575;&#1586;-&#1705;&#1585;&#1583;&#1606;&#1583;/g-18012627|accessdate=March 12, 2015|publisher=Deutsche Welle Persian|date=2009}}&lt;/ref&gt;

The first published use of the term "hash tag" was in a blog post by Stowe Boyd, "Hash Tags = Twitter Groupings,"&lt;ref&gt;{{cite web|url=http://stoweboyd.com/post/39877198249/hash-tags-twitter-groupings |title=Stowe Boyd, Hash Tags = Twitter Groupings |publisher=Stoweboyd.com |date= |accessdate=September 19, 2013}}&lt;/ref&gt; on August 26, 2007, according to lexicographer [[Ben Zimmer]], chair of the American Dialect Society's New Words Committee.

Beginning July 2, 2009,&lt;ref&gt;{{cite web|title=Twitter Makes Hashtags More #Useful|url=http://techcrunch.com/2009/07/02/twitter-makes-hashtags-more-useful/|accessdate=December 27, 2015}}&lt;/ref&gt; Twitter began to hyperlink all hashtags in tweets to Twitter search results for the hashtagged word (and for the standard spelling of commonly misspelled words). In 2010, Twitter introduced "[[Twitter#Trending topics|Trending Topics]]" on the Twitter front page, displaying hashtags that are rapidly becoming popular. Twitter has an algorithm to tackle attempts to [[spamming|spam]] the trending list and ensure that hashtags trend naturally.&lt;ref&gt;{{cite web|url=http://www.allisayis.com/the-secret-of-twitters-trending-hashtags-with-insight-and-tips/ |title=The Secret of Twitter's Trending Hashtags With Insight and Tips |publisher=AllISayIs.com |date= |accessdate=December 3, 2014}}&lt;/ref&gt;

Although the hashtag started out most popularly on Twitter as the main social media platform for this use, the use has extended to other social media sites including Instagram, Facebook, Flickr, Tumblr, and Google+.&lt;ref&gt;{{Cite web|url=http://www.cnn.com/2013/06/12/tech/social-media/facebook-hashtags/index.html|title=Facebook finally gets #hashtags - CNN.com|last=Mashable|first=By Christina Warren|website=CNN|access-date=2016-05-16}}&lt;/ref&gt;

In China, microblogs [[Sina Weibo]] and [[Tencent Weibo]] use a double-hashtag #HashName# format, since the lack of spacing between [[Chinese characters]] necessitates a closing tag. In contrast, when using Chinese characters (and [[orthographies]] with similar spacing conventions) on [[Twitter]], users must insert spacing before and after the hashtagged element (e.g. '&#25105; #&#29233; &#20320;' instead of '&#25105;#&#29233;&#20320;')&lt;ref&gt;{{cite news|last1=Martin|first1=Rick|title=Twitter Rolls Out Hashtag Support for Japanese, Korean, Chinese, and Russian|url=https://www.techinasia.com/twitter-hashtag-languages/|accessdate=March 5, 2015|publisher=Tech in Asia|date=July 13, 2011}}&lt;/ref&gt; or insert a [[zero-width non-joiner]] character before and after the hashtagged element, to retain a linguistically natural appearance, such as '&#25105;#&#29233;&#20320;'.&lt;ref&gt;{{cite news|last1=International services team|title=Right-to-left languages on Twitter|url=https://blog.twitter.com/2012/right-to-left-languages-on-twitter|accessdate=March 5, 2015|publisher=Twitter|date=April 5, 2012}}&lt;/ref&gt;

== Style ==
On microblogging or social networking sites, hashtags can be inserted anywhere within a sentence, either preceding it, following it as a [[postscript]], or being included as a word within the sentence (e.g. "It is #sunny today").

The quantity of hashtags used in a post or tweet is just as important as the types of hashtags used. It is currently considered acceptable to tag a post once when contributing to a specific conversation. Two hashtags are considered acceptable when adding a location to the conversation. Three hashtags are seen by some as the "absolute maximum", and any contribution exceeding this risks "raising the ire of the community."&lt;ref&gt;{{cite web|title=What is a (#) Hashtag?|url=http://www.hashtags.org/how-to/history/what-is-a-hashtag/|publisher=Hashtags.org|accessdate=February 22, 2014}}&lt;/ref&gt;

As well as frustrating other users, the misuse of hashtags can lead to account suspensions. Twitter warns that adding hashtags to unrelated tweets, or repeated use of the same hashtag without adding to a conversation, could cause an account to be filtered from search, or even suspended.&lt;ref&gt;{{cite web|title=The Twitter Rules|url=https://support.twitter.com/groups/56-policies-violations/topics/236-twitter-rules-policies/articles/18311-the-twitter-rules|publisher=Twitter, Inc.|accessdate=February 22, 2014}}&lt;/ref&gt;{{failed verification|date=August 2014}}

[[Jimmy Fallon]] and [[Justin Timberlake]] performed a sketch parodying the often misused and misunderstood usage of hashtags on ''[[Late Night with Jimmy Fallon]]'' in September 2013.&lt;ref&gt;{{cite web|author=The Tonight Show Starring Jimmy Fallon |url=https://www.youtube.com/watch?v=57dzaMaouXA |title="#Hashtag" with Jimmy Fallon &amp; Justin Timberlake (Late Night with Jimmy Fallon) |publisher=YouTube |date=September 24, 2013 |accessdate=August 25, 2014}}&lt;/ref&gt;

== Function ==

[[File:Seguir hashtags.png|upright=1.3|right|thumb|Search bar in the header of a social networking site, searching for most recent posts containing the hashtag #science]]

Hashtags are mostly used in unmoderated, ad hoc discussion forums; any combination of characters led by a hash symbol is a hashtag, and any hashtag, if promoted by enough individuals, can "trend" and attract more individual users to discussion. On Twitter, when a hashtag becomes extremely popular, it will appear in the "Trending Topics" area of a user's homepage. The trending topics can be organized by geographic area or by all of Twitter. Hashtags are neither registered nor controlled by any one user or group of users. They cannot be "retired" from public usage, meaning that any given hashtag can theoretically be used in perpetuity. They do not contain any set definitions, meaning that a single hashtag can be used for any number of purposes, as chosen by those who make use of them.

Hashtags intended for discussion of a particular event tend to use an obscure wording to avoid being caught up with generic conversations on similar subjects, such as a cake festival using #cakefestival rather than simply #cake. However, this can also make it difficult for topics to become "trending topics" because people often use different spelling or words to refer to the same topic.  In order for topics to trend, there has to be a consensus, whether silent or stated, that the hashtag refers to that specific topic.

Hashtags also function as beacons in order for users to find and "follow" (subscribe) or "list" (organize into public contact lists) other users of similar interest.

In recent years, broadcasters such as [[Channel 4]] have employed the hashtag during the airing of programmes such as [[First Dates]] and [[The Undateables]]. Research has shown that audience numbers go up when individuals can be interactive - by tweeting while viewing a programme on TV.

Hashtags can be used on the social network [[Instagram]], by posting a picture and hashtagging it with its subject. As an example, a photo of oneself and a friend posted to the social network can be hashtagged #bffl or #friends. Instagram has banned certain hashtags, some because they are too generic, such as #photography #iPhone #iphoneography, and therefore do not fulfill a purpose. They have also blocked hashtags that can be linked to illegal activities, such as drug use.&lt;ref&gt;{{cite web|url=http://www.bbc.co.uk/news/technology-24842750 |title=Instagram banned hashtags | date = November 7, 2013|publisher=[[BBC.co.uk]] |accessdate=November 25, 2013}}&lt;/ref&gt; The ban against certain hashtags has a consequential role in the way that particular [[subaltern]] communities are built and maintained on Instagram. Despite Instagram's content policies, users are finding creative ways of maintaining their practices and ultimately circumventing censorship.&lt;ref&gt;Olszanowski, M. (2014). "Feminist Self-Imaging and Instagram: Tactics of Circumventing Sensorship{{sic|hide=y}}". ''Visual Communication Quarterly,'' 21(1), 83-95. Retrieved February 8, 2015, from http://www.tandfonline.com/doi/abs/10.1080/15551393.2014.928154#.VNgGT7DF-7FF-7F&lt;/ref&gt;

Hashtags are also used informally to express context around a given message, with no intent to categorize the message for later searching, sharing, or other reasons. One of the functions of the hashtag is to serve as a reflexive meta-commentary, which contributes to the idea of how written communication in new media can be paralleled to how pragmatic methodology is applied to speech.&lt;ref&gt;{{Cite web|url=http://www.linguistics.fi/julkaisut/SKY2014/Wikstrom.pdf|title=#srynotfunny: Communicative Functions of Hashtags on Twitter|last=Wilkstr&#246;m|first=Peter|date=2014|work=SKY Journal of Linguistics|publisher=|access-date=May 15, 2016}}&lt;/ref&gt;

This can help express contextual cues or offer more depth to the information or message that appears with the hashtag. "My arms are getting darker by the minute. #toomuchfaketan". Another function of the hashtag can be used to express personal feelings and emotions. For example, with "It's Monday!! #excited #sarcasm" in which the adjectives are directly indicating the emotions of the speaker. It can also be used as a disclaimer of the information that the hashtag accompanies, as in, "BREAKING: US GDP growth is back! #kidding". In this case, the hashtag provides an essential piece of information in which the meaning of the utterance is changed entirely by the disclaimer hashtag. This may also be conveyed with #sarcasm, as in the previous example. Self-mockery is another informal function of the hashtag used by writers, as in this tweet: "Feeling great about myself till I met an old friend who now races at the Master's level. Yup, there's today's #lessoninhumility," where the informality of the hashtag provides commentary on the tweet itself.&lt;ref name=":0"&gt;{{Cite web|url=http://www.skase.sk/Volumes/JTL28/pdf_doc/05.pdf|title=The &#8216;hashtag&#8217;: A new word or a new rule?|last=Caleffi|first=Paola-Maria|date=|website=Skase Journal of Theoretical Linguistics|publisher=|access-date=}}&lt;/ref&gt;

== Other uses ==
The feature has been added to other, non-short-message-oriented services, such as the user comment systems on [[YouTube]] and [[Gawker Media]]. In the case of the latter, hashtags for blog comments and directly submitted comments are used to maintain a more constant rate of user activity even when paid employees are not logged into the website.&lt;ref&gt;{{cite web|url = http://gawker.com/5382267/anarchy-in-the-machine-welcome-to-gawkers-open-forums|title = Anarchy in the Machine: Welcome to Gawker's Open Forums|author = Gabriel Snyder|publisher = Gawker|date = October 15, 2009&lt;!-- 3:25 PM--&gt;}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url = http://www.niemanlab.org/2009/10/got-a-tip-gawker-media-opens-tag-pages-to-masses-expecting-chaos/|title = Got a #tip? Gawker Media opens tag pages to masses, expecting "chaos"|author = Zachary M. Seward|publisher = Nieman Journalism Lab|date = October 15, 2009 &lt;!-- 8 a.m. --&gt;}}&lt;/ref&gt; Real-time search aggregators such as the former [[Google Real-Time Search]] also support hashtags in syndicated posts, meaning that hashtags inserted into Twitter posts can be hyperlinked to incoming posts falling under that same hashtag; this has further enabled a view of the "river" of Twitter posts that can result from search terms or hashtags.{{citation needed|date=September 2014}}

== Uses ==

=== Broadcast media ===

The use of hashtags has extended to [[television]]{{nsmdns}}a concept that began rising in prominence in the early 2010s. Broadcasters may display a hashtag as an on-screen [[digital on-screen graphic|bug]], encouraging viewers to participate in a [[backchannel]] of discussion via social media prior to, during, or after the program. [[Television commercial]]s have sometimes contained hashtags for similar purposes.&lt;ref&gt;{{cite web|url = http://www.tvguide.com/News/New-TV-Screen-1032111.aspx|title = New to Your TV Screen: Twitter Hashtags|date = April 21, 2011&lt;!-- 3:25 PM--&gt;|author = Michael Schneider|publisher = TV Guide}}&lt;/ref&gt; Hashtag bugs appear on either corner of the screen, or they may appear at the end of an advertisement.&lt;ref&gt;{{cite web|url = http://mashable.com/2012/12/03/mcdonalds-tv-ad-twitter-hashtag/|title = McDonald's Releases First TV Ad With Twitter Hashtag|date = Dec 3, 2012|author = Todd Wasserman|publisher = Mashable}}&lt;/ref&gt;

While personalities associated with broadcasts, such as hosts and correspondents, also promote their corporate or personal Twitter usernames in order to receive mentions and replies to posts, usage of related or "branded" hashtags alongside Twitter usernames (e.g., [[The Ed Show|#edshow]] as well as [[Ed Schultz|@edshow]]) is increasingly encouraged as a microblogging style in order to "trend" the hashtag (and, hence, the discussion topic) in Twitter and other search engines. Broadcasters also make use of such a style in order to index select posts for live broadcast. Chloe Sladden, Twitter's director of media partnerships, identified two types of television-formatted usage of hashtags: hashtags which identify a series being broadcast (i.e. [[It's Always Sunny in Philadelphia|#SunnyFX]]) and instantaneous, "temporary" hashtags issued by television personalities to gauge topical responses from viewers during broadcasts.&lt;ref&gt;{{cite web|url = http://www.fastcompany.com/1747437/twitter-tv-hashtag-tips-twitters-own-expert|title = Twitter TV Hashtag Tips From Twitter's Own Expert|author = Gregory Ferenstein|date = April 15, 2011|publisher = Fast Company}}&lt;/ref&gt; Some have speculated that hashtags might take the place of (or co-exist with) the [[Nielsen ratings|Nielsen television ratings system]].&lt;ref&gt;{{cite web|url=http://www.ibtimes.com/twitter-chatter-correlates-tv-ratings-good-or-bad-news-nielsen-1144311 |title=Twitter Chatter Correlates With TV Ratings, But Is That Good Or Bad News For Nielsen? |work=International Business Times |date=March 22, 2013 |accessdate=September 19, 2013}}&lt;/ref&gt;

An example of trending "temporary" hashtags garnering viewers during broadcasts is observed on ''[[The Tonight Show]]'' with [[Jimmy Fallon]], a variety [[talk show]] on [[NBC]]. Every Wednesday, Fallon hosts a segment on his show called "Tonight Show Hashtags," which engages viewers by inviting them via Twitter to post humorous stories based on a specific hashtag topic, such as #WhydidIsaythat, #Worstfirstdate, to #Onetimeinclass, reflecting on funny experiences in daily life. By using hashtags, Fallon creates a sense of community and solidarity among his viewers and draws a wider range of viewers through an online platform while they watch a classic, non-interactive television program. Because of its popularity, the "Tonight Show Hashtags" are usually the 'most tweeted hashtag' on Twitter, which promotes the show. By engaging viewers with a lighthearted subject and simple hashtags, Fallon can gauge topical responses from viewers during broadcasts and also use the hashtags to brand his show.{{citation needed|date=April 2016}}

The increased usage of hashtags as brand promotion devices has been compared to the promotion of branded "[[Index term|keywords]]" by [[AOL]] in the late 1990s and early 2000s, as such keywords were also promoted at the end of television commercials and series episodes.&lt;ref&gt;{{cite web|url = http://techcrunch.com/2012/06/10/twitter-hashtag-pages-aol-keywords/|title = Twitter's Hashtag Pages Could Be The New AOL Keywords &#8212; But Better|author = Ryan Lawler|date = June 10, 2012|publisher = Techcrunch}}&lt;/ref&gt;

The late-night television comedy [[game show]] [[@midnight]] with [[Chris Hardwick]] on [[Comedy Central]] features a daily game entitled "Hashtag Wars," in which three comedians compete against one another to come up with phrases based on a given hashtag theme.

Some hashtags have become famous worldwide. For instance the slogan "''[[Je suis Charlie]],''" which was first used on Twitter as the hashtag #jesuischarlie and #iamcharlie to indicate solidarity with ''Charlie Hebdo'' offices attacked in Paris, spread to the internet at large.

=== Purchasing ===

Since February 2013 Twitter and [[American Express]] have collaborated to enable users to pay for discounted goods online by tweeting a special hashtag.&lt;ref&gt;{{cite news | first = Kelly | last = Heather | title = Twitter and Amex let you pay with a hashtag | date = February 12, 2013 | url = http://edition.cnn.com/2013/02/11/tech/social-media/twitter-hashtag-purchases/| work = CNN | accessdate = November 25, 2013}}&lt;/ref&gt; American Express members can sync their card with Twitter and pay for offers by tweeting; American Express tweets a response to the member that confirms the purchase.&lt;ref&gt;{{cite web|url=https://sync.americanexpress.com/Twitter/Index |title=Sync with Twitter|publisher=Amex Sync |accessdate=November 25, 2013}}&lt;/ref&gt;

=== Event promotion ===

[[File:Occupy for Rights.JPG|thumb|[[Stencil graffiti]] promoting the hashtag #OccupyForRights]]

Organized real-world events have used hashtags and ad hoc lists for discussion and promotion among participants. Hashtags are used as beacons by event participants in order to find each other, both on Twitter and, in many cases, during actual physical events.

Companies and advocacy organizations have taken advantage of hashtag-based discussions for promotion of their products, services or campaigns.

Political protests and campaigns in the early 2010s, such as [[Occupy Wall Street|#OccupyWallStreet]] and [[2011 Libyan civil war|#LibyaFeb17]], have been organized around hashtags or have made extensive usage of hashtags for the promotion of discussion.

=== Consumer complaints ===
Hashtags are often used by consumers on social media platforms in order to complain about the customer service experience with large companies.  The term "bashtag" has been created to describe situations in which a user refers to a corporate social media hashtag in order to criticise the company or to tell others about poor customer service. For example, in January 2012, [[McDonald's]] created the #McDStories hashtag so that customers could share positive experiences about the restaurant chain. But, the marketing effort was cancelled after two hours when McDonald's received numerous complaint tweets rather than the positive stories they were anticipating.&lt;ref&gt;{{cite news | first = Alexis | last = Akwagyiram | title = Are Twitter and Facebook changing the way we complain? | date = May 17, 2012 | url = http://www.bbc.co.uk/news/uk-18081651 | work = BBC News | accessdate = June 12, 2012}}&lt;/ref&gt;

=== Sentiment analysis ===
The use of hashtags also reveals what feelings or sentiment an author attaches to a statement. This can range from the obvious, where a hashtag directly describes the state of mind, to the less obvious. For example, words in hashtags are the strongest predictor of whether or not a statement is [[sarcasm|sarcastic]].&lt;ref&gt;{{cite journal|last=Maynard|title=Who cares about sarcastic tweets? Investigating the impact of sarcasm on sentiment analysis|journal=Proceedings of the Conference on Language Resources and Evaluation|year=2014}}&lt;/ref&gt;&#8212;a difficult [[Artificial Intelligence|AI]] problem.&lt;ref&gt;http://www.huffingtonpost.com/entry/power-yourself-with-viral-marketing-become-a-hashtag_us_57bf13e6e4b06384eb3e7f1d?dxrywr9zcw30rizfr&lt;/ref&gt;

=== Sports ===
The YouTuber Spencer FC used the hashtag for the name and crest of his YouTube-based association football team, [[Hashtag United]].

== In popular culture ==
During the [[2011 Canadian leaders debates|April 2011 Canadian party leader debate]], [[Jack Layton]], then-leader of the [[New Democratic Party of Canada|New Democratic Party]], referred to [[Conservative Party of Canada|Conservative]] Prime Minister [[Stephen Harper]]'s crime policies as "a hashtag fail" (presumably #fail).&lt;ref&gt;{{cite news|url = http://www.theglobeandmail.com/news/politics/jack-laytons-debatable-hashtag-fail/article576224/|title = Jack Layton's debatable 'hashtag' #fail|author = Anna Mehler Paperny|publisher = The Globe and Mail|date = April 13, 2011 &lt;!-- , 6:00 AM EDT --&gt; }}&lt;/ref&gt;&lt;ref&gt;{{cite news|url = http://www.cbc.ca/news/politics/canadavotes2011/story/2011/04/13/cv-debate-twitter.html|title = Canadians atwitter throughout debate|date = April 13, 2011&lt;!-- 3:25 PM--&gt;|publisher = CBC News}}&lt;/ref&gt;

The term "hashtag [[Hip hop music|rap]]", coined by [[Kanye West]],&lt;ref&gt;{{cite web |url = http://blogs.villagevoice.com/music/2010/11/the_ten_best_qu.php|title = The Ten Best Quotes From Kanye West's Epic Hot 97 Interview With Funkmaster Flex|author = Zach Baron|publisher = The Village Voice|date = November 3, 2010}}&lt;/ref&gt; was developed in the 2010s to describe a style of rapping which, according to Rizoh of the ''[[Houston Press]],'' uses "three main ingredients: a metaphor, a pause, and a one-word [[punch line]], often placed at the end of a rhyme".&lt;ref&gt;{{cite web|url = http://blogs.houstonpress.com/rocks/2011/07/a_brief_history_of_hashtag_rap.php|title = A Brief History Of Hashtag Rap|author = Rizoh|publisher = Houston Press|date = July 7, 2011 &lt;!-- at 9:00 AM --&gt; }}&lt;/ref&gt; Rappers [[Nicki Minaj]], [[Big Sean]], [[Drake (rapper)|Drake]], and [[Lil Wayne]] are credited with the popularization of hashtag rap, while the style has been criticized by [[Ludacris]], [[The Lonely Island]],&lt;ref&gt;{{cite web|url = http://www.tucsonweekly.com/TheRange/archives/2013/05/22/the-lonely-island-puts-hashtag-rap-in-its-place-looking-at-you-drake|title = The Lonely Island Puts Hashtag Rap In Its Place (Looking at You, Drake)|author = David Mendez|date = May 22, 2013 &lt;!-- AT 11:43 AM --&gt; |publisher = Tucson Weekly}}&lt;/ref&gt; and various music writers.&lt;ref&gt;{{cite web|url = http://www.joplinglobe.com/enjoy/x1666506743/Jeremiah-Tucker-Hashtag-rap-is-2010s-lamest-trend|title = Jeremiah Tucker: Hashtag rap is 2010's lamest trend|author = Jeremiah Tucker|date = December 17, 2010|publisher = Joplin Globe}}&lt;/ref&gt;

On September 13, 2013, a hashtag, #TwitterIPO, appeared in the headline of a ''[[The New York Times|New York Times]]'' front page article regarding Twitter's [[initial public offering]].&lt;ref&gt;{{cite web
| title = Twitter / nickbilton: My first byline on A1 of the &#8230;
| url = https://twitter.com/nickbilton/status/378534272962793472/photo/1
| accessdate = September 14, 2013
 }}&lt;/ref&gt;

[[Bird's Eye]] foods released in 2014 a shaped [[mashed potato]] food that included forms of @-symbols and hashtags, called "Mashtags".&lt;ref&gt;{{cite web|title=Birds Eye launches Mashtags &#8211; social media potato shapes|url=http://www.thegrocer.co.uk/fmcg/birds-eye-launches-mashtags-potato-shapes/354514.article|work=The Grocer}}&lt;/ref&gt;

In May 2014, Twitter users began using the hashtag [[YesAllWomen|#YesAllWomen]] to raise awareness about personal experiences of [[sexism]] and [[violence against women]].&lt;ref name="Nytimes"&gt;{{cite news |last=Medina| first=Jennifer | title = Campus Killings Set Off Anguished Conversation About the Treatment of Women | work = [[The New York Times]] | accessdate = September 23, 2014 | date = May 27, 2014 | url =http://www.nytimes.com/2014/05/27/us/campus-killings-set-off-anguished-conversation-about-the-treatment-of-women.html?ref=us&amp;_r=0 }}&lt;/ref&gt;

In September 2014, in response to the "[[blame the victim]]" public reactions to videotaped footage of [[NFL]] player [[Ray Rice]] assaulting his then-fianc&#233;e Janay Palmer in the elevator of an [[Atlantic City]] casino, Beverly Gooden shared on Twitter her own story of [[domestic abuse]], using the hashtag #WhyIStayed, and encouraged others to share theirs.&lt;ref&gt;{{cite news|work=Today|title=WhyIStayed: Woman behind Ray Rice-inspired hashtag writes to past self, other abuse victims|author=Gooden, Beverly|date=September 10, 2014| url= http://www.today.com/news/whyistayed-woman-behind-ray-rice-inspired-hashtag-writes-past-self-1D80139011}}&lt;/ref&gt;&lt;ref&gt;{{cite news|work=The Leonard Lopate Show|authors=Lopate, Leonard &amp; Gooden, Beverly|title=#WhyIStayed|date=September 10, 2014}}&lt;/ref&gt;

Hashtags have been used verbally to make a humorous point in informal conversations,&lt;ref&gt;[http://www.macmillandictionary.com/dictionary/british/hashtag]&lt;/ref&gt; such as "I&#8217;m hashtag confused!"&lt;ref name=":0" /&gt; In August 2012, British journalist Tom Meltzer reported in ''[[The Guardian]]'' about a new [[hand gesture]] that mimicked the hashtag, sometimes called the "finger hashtag", in which both hands form a [[Peace sign#The V sign|peace sign]], and then the fingers are crossed to form the symbol of a hashtag.&lt;ref&gt;{{cite web |url=https://www.theguardian.com/technology/shortcuts/2012/aug/01/how-to-say-hashtag-fingers |title=How to say 'hashtag' with your fingers |work=[[The Guardian]] |author=Tom Meltzer |date=August 1, 2012 |accessdate=March 20, 2014}}&lt;/ref&gt; The emerging gesture was reported about in ''[[Wired (magazine)|Wired]]'' by [[Nimrod Kamer]],&lt;ref&gt;{{cite web |url=http://www.wired.co.uk/news/archive/2013-03/06/hashtags |title=Finger-Hashtags |work=[[Wired (magazine)|Wired]] |author=[[Nimrod Kamer]] |date=March 2013 |accessdate=March 20, 2014}}&lt;/ref&gt; and during 2013, it was seen on TV as used by [[Jimmy Fallon]], and on ''[[The Colbert Report]],'' among other programs.&lt;ref&gt;{{cite web |url=http://www.dailydot.com/lol/finger-hashtag-jimmy-fallon-twitter/ |title=I invented finger hashtags&#8212;and I regret nothing |work=[[The Daily Dot]] |author=[[Nimrod Kamer]] |date=February 26, 2014 |accessdate=March 20, 2014}}&lt;/ref&gt; Writing in 2015, Paola Maria Caleff considered this usage a [[fad]], but noted that people talking the way that they write was a consequence of computer-mediated communication.&lt;ref name=":0" /&gt;

=== Adaptations ===
*Hashflags: In 2010, Twitter introduced "hashflags" during the 2010 World Cup in South Africa.&lt;ref&gt;{{cite web|author=|url=http://www.ryanseacrest.com/2010/06/11/twitter-supports-world-cup-fever-with-hashflags/ |title=Twitter Supports World Cup Fever with Hashflags |publisher=Ryanseacrest.com |date=June 11, 2010 |accessdate=August 5, 2015 |archiveurl=https://web.archive.org/web/20101129201517/http://ryanseacrest.com/2010/06/11/twitter-supports-world-cup-fever-with-hashflags/ |archivedate=November 29, 2010}}&lt;/ref&gt; They reintroduced the feature on June 10, 2014, in time for the 2014 World Cup in Brazil,&lt;ref&gt;{{cite web|url=http://howto.digidefen.se/twitter/What-are-hashflags.php |title=What are Hashflags? |publisher=Howto.digidefen.se |date=June 10, 2014 |accessdate=August 25, 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web|author=Ben Woods |url=http://thenextweb.com/twitter/2014/06/10/twitter-brings-back-hashflags-just-time-world-cup-2014-kick/ |title=Twitter brings back hashflags just in time for World Cup 2014 kick-off |publisher=Thenextweb.com |date=June 10, 2014 |accessdate=August 25, 2014}}&lt;/ref&gt; and then again on April 10, 2015, with UK political party logos for the 2015 UK General Election.&lt;ref&gt;{{cite web|title=Twitter just launched election hashflags|url=http://www.bbc.co.uk/newsbeat/article/32249518/twitter-just-launched-election-hashflags|website=BBC News|accessdate=April 15, 2015}}&lt;/ref&gt; When a user tweets a hashtag consisting of the three letter country code of any of the 32 countries represented in the tournament, Twitter automatically embeds a flag emoticon for that country.
*Cashtags: In 2009, [[StockTwits]] used [[ticker symbol]]s preceded by the [[dollar sign]] to create "cashtags".&lt;ref name=Wong2012&gt;{{cite journal |author=Wong, Matthew |title=VCs and Start-Ups Pin Their Hopes on Pinterest |date=2012-08-17 |work=[[The Wall Street Journal]] |url=http://blogs.wsj.com/venturecapital/2012/08/17/vcs-and-start-ups-pin-their-hopes-on-pinterest/ |accessdate=2013-05-28 }}&lt;/ref&gt;&lt;ref name=Taylor2012&gt;{{cite journal |author=Taylor, Colleen |title=Howard Lindzon on Why He Sold His Twitter Stock, And The 'Hijack' Of StockTwits&#8217; Cashtags [TCTV] |date=2012-07-01 |publisher=[[TechCrunch]] |url=http://techcrunch.com/2012/08/01/howard-lindzon-on-why-he-sold-his-twitter-stock-and-the-hijack-of-stocktwits-cashtags-tctv/ |accessdate=2013-05-09 }}&lt;/ref&gt; In July 2012, Twitter adapted the hashtag style to make company ticker symbols preceded by the dollar sign clickable (as in [[Apple, Inc.|$AAPL]]), a method that Twitter dubbed the "cashtag".&lt;ref&gt;{{cite web|last=Kim |first=Erin |url=http://money.cnn.com/2012/07/31/technology/twitter-cashtag/ |title=Twitter unveils 'cashtags' to track stock symbols &#8211; Jul. 31, 2012 |publisher=Money.cnn.com |date=July 31, 2012 |accessdate=November 12, 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite web|author= |url=http://www.theverge.com/2012/7/30/3205284/twitter-stock-ticker-cashtag-links-official |title=Twitter makes stock symbol $ 'cashtag' links official, following # and @ |publisher=The Verge |date=July 30, 2012 |accessdate=November 12, 2013}}&lt;/ref&gt; This is intended to allow users to search posts discussing companies and their stocks. This is also used for discussion of currency fluctuations on twitter, eg. using #USDGBP or $USDGBP when mentioning the US Dollar's level expressed in Pounds Sterling.

== References ==

{{Reflist|30em}}

== External links ==

{{Commons category|Hashtags}}

* [//tools.wmflabs.org/hashtags/search/artandfeminism Wikipedia internal hashtag search engine] &#8211; for hashtags used in edit summaries

{{Microblogging}}
{{Online social networking}}
{{Web syndication}}
{{authority control}}

[[Category:Hashtags| ]]
[[Category:2010s slang]]
[[Category:Collective intelligence]]
[[Category:Computer jargon]]
[[Category:Information retrieval techniques]]
[[Category:Knowledge representation]]
[[Category:Metadata]]
[[Category:Reference]]
[[Category:Social media]]
[[Category:Web 2.0]]
[[Category:Twitter]]</text>
      <sha1>hm0hmbz5abqu8kstjfsq4z7un4twp2k</sha1>
    </revision>
  </page>
  <page>
    <title>Subject indexing</title>
    <ns>0</ns>
    <id>13200719</id>
    <revision>
      <id>762988353</id>
      <parentid>733746281</parentid>
      <timestamp>2017-01-31T20:13:03Z</timestamp>
      <contributor>
        <ip>50.53.1.33</ip>
      </contributor>
      <comment>unneeded pipe</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="17072" xml:space="preserve">'''Subject indexing''' is the act of describing or [[document classification|classifying]] a [[document]] by [[index term]]s or other symbols in order to indicate what the document is '''[[aboutness|about]],''' to summarize its [[content (media and publishing)|content]] or to increase its [[findability]]. In other words, it is about identifying and describing the '''[[Subject (documents)|subject]]''' of documents. Indexes are constructed, separately, on three distinct levels: terms in a document such as a book; objects in a collection such as a library; and documents (such as books and articles) within a field of knowledge.

Subject indexing is used in [[information retrieval]] especially to create [[bibliographic index]]es to retrieve documents on a particular subject. Examples of academic indexing services are [[Zentralblatt MATH]], [[Chemical Abstracts]] and [[PubMed]]. The index terms were mostly assigned by experts but author keywords are also common.

The process of indexing begins with any analysis of the subject of the document. The indexer must then identify terms which appropriately identify the subject either by extracting words directly from the document or assigning words from a [[controlled vocabulary]].&lt;ref name="Lancaster2003a"&gt;F. W. Lancaster (2003): "Indexing and abstracting in theory and practise". Third edition. London, Facet ISBN 1-85604-482-3. page 6&lt;/ref&gt; The terms in the index are then presented in a systematic order.

Indexers must decide how many terms to include and how specific the terms should be. Together this gives a depth of indexing.

== Subject analysis ==
The first step in indexing is to decide on the subject matter of the document. In manual indexing, the indexer would consider the subject matter in terms of answer to a set of questions such as "Does the document deal with a specific product, condition or phenomenon?".&lt;ref name="Chowdhury2004"&gt;G.G. Chowdhury (2004): "Introduction to modern information retrieval". Third Edition. London, Facet. ISBN 1-85604-480-7. page 71&lt;/ref&gt; As the analysis is influenced by the knowledge and experience of the indexer, it follows that two indexers may analyze the content differently and so come up with different index terms. This will impact on the success of retrieval.

=== Automatic vs. manual subject analysis ===
Automatic indexing follows set processes of analyzing frequencies of word patterns and comparing results to other documents in order to assign to subject categories. This requires no understanding of the material being indexed. This therefore leads to more uniform indexing but this is at the expense of the true meaning being interpreted. A computer program will not understand the meaning of statements and may therefore fail to assign some relevant terms or assign incorrectly. Human indexers focus their attention on certain parts of the document such as the title, abstract, summary and conclusions, as analyzing the full text in depth is costly and time consuming &lt;ref name="Lancaster2003b"&gt;F. W. Lancaster (2003): "Indexing and abstracting in theory and practice". Third edition. London, Facet ISBN 1-85604-482-3. page 24&lt;/ref&gt; An automated system takes away the time limit and allows the entire document to be analyzed, but also has the option to be directed to particular parts of the document.

== Term selection ==
The second stage of indexing involves the translation of the subject analysis into a set of [[keyword (search)|index terms]]. This can involve extracting from the document or assigning from a [[controlled vocabulary]]. With the ability to conduct a [[full text search]] widely available, many people have come to rely on their own expertise in conducting information searches and [[full text search]] has become very popular. Subject indexing and its experts, professional indexers, [[catalogers]], and [[librarians]], remains crucial to information organization and retrieval. These experts understand [[controlled vocabularies]] and are able to find information that cannot be located by [[full text search]]. The cost of expert analysis to create subject indexing is not easily compared to the cost of hardware, software and labor to manufacture a comparable set of full-text, fully searchable materials. With new web applications that allow every user to annotate documents, [[social tagging]] has gained popularity especially in the Web.&lt;ref name="Voss2007"&gt;{{cite conference
|last1=Voss |first1=Jakob
|title=Tagging, Folksonomy &amp; Co - Renaissance of Manual Indexing?
|booktitle=Proceedings of the International Symposium of Information Science
|pages=234&#8211;254
|year=2007
|arxiv=cs/0701072
}}&lt;/ref&gt;

One application of indexing, the [[Index (publishing)|book index]], remains relatively unchanged despite the information revolution.

=== Extraction/Derived indexing ===
Extraction indexing involves taking words directly from the document. It uses [[natural language]] and lends itself well to automated techniques where word frequencies are calculated and those with a frequency over a pre-determined threshold are used as index terms. A stop-list containing common words (such as "the", "and") would be referred to and such [[stop words]] would be excluded as index terms.

Automated extraction indexing may lead to loss of meaning of terms by indexing single words as opposed to phrases. Although it is possible to extract commonly occurring phrases, it becomes more difficult if key concepts are inconsistently worded in phrases. Automated extraction indexing also has the problem that, even with use of a stop-list to remove common words, some frequent words may not be useful for allowing discrimination between documents. For example, the term glucose is likely to occur frequently in any document related to diabetes. Therefore use of this term would likely return most or all the documents in the database. Post-co-ordinated indexing where terms are combined at the time of searching would reduce this effect but the onus would be on the searcher to link appropriate terms as opposed to the information professional. In addition terms that occur infrequently may be highly significant for example a new drug may be mentioned infrequently but the novelty of the subject makes any reference significant. One method for allowing rarer terms to be included and common words to be excluded by automated techniques would be a relative frequency approach where frequency of a word in a document is compared to frequency in the database as a whole. Therefore a term that occurs more often in a document than might be expected based on the rest of the database could then be used as an index term, and terms that occur equally frequently throughout will be excluded.
Another problem with automated extraction is that it does not recognise when a concept is discussed but is not identified in the text by an indexable keyword.&lt;ref name="Lamb2008"&gt;J. Lamb (2008): ''[http://www.indexers.org.uk/index.php?id=463 Human or computer produced indexes?]'' [online] Sheffield, Society of Indexers. Accessed 15 January 2009.&lt;/ref&gt;

=== Assignment indexing ===
An alternative is assignment indexing where index terms are taken from a controlled vocabulary. This has the advantage of controlling for [[synonym]]s as the preferred term is indexed and synonyms or related terms direct the user to the preferred term. This means the user can find articles regardless of the specific term used by the author and saves the user from having to know and check all possible synonyms.&lt;ref name="Tenopir"&gt;C. Tenopir (1999): "Human or automated, indexing is important". ''Library Journal'' '''124'''(18) pages 34-38.&lt;/ref&gt; It also removes any confusion caused by [[homograph]]s by inclusion of a qualifying term. A third advantage is that it allows the linking of related terms whether they are linked by hierarchy or association, e.g. an index entry for an oral medication may list other oral medications as related terms on the same level of the hierarchy but would also link to broader terms such as treatment. Assignment indexing is used in manual indexing to improve inter-indexer consistency as different indexers will have a controlled set of terms to choose from. Controlled vocabularies do not completely remove inconsistencies as two indexers may still interpret the subject differently.&lt;ref name="Chowdhury2004" /&gt;

== Index presentation ==
The final phase of indexing is to present the entries in a systematic order. This may involve linking entries. In a pre-coordinated index the indexer determines the order in which terms are linked in an entry by considering how a user may formulate their search. In a post-coordinated index, the entries are presented singly and the user can link the entries through searches, most commonly carried out by computer software. Post-coordination results in a loss of precision in comparison to pre-coordination &lt;ref name="Bodoff1998"&gt;D. Bodoff and A. Kambil, (1998): "Partial coordination. I. The best of pre-coordination and post-coordination." ''Journal of the American Society for Information Science'', '''49'''(14), 1254-1269.&lt;/ref&gt;

== Depth of Indexing ==
Indexers must make decisions about what entries should be included and how many entries an index should incorporate. The depth of indexing describes the thoroughness of the indexing process with reference to exhaustivity and specificity &lt;ref name="Cleveland2001"&gt;D.B. Cleveland and A.D. Cleveland (2001): "Introduction to indexing and abstracting". 3rd Ed. Englewood, libraries Unlimited, Inc. ISBN 1-56308-641-7. page 105&lt;/ref&gt;

=== Exhaustivity ===
An exhaustive index is one which lists all possible index terms. Greater exhaustivity gives a higher [[Recall (information retrieval)|recall]], or more likelihood of all the relevant articles being retrieved, however, this occurs at the expense of [[Precision (information retrieval)|precision]]. This means that the user may retrieve a larger number of irrelevant documents or documents which only deal with the subject in little depth. In a manual system a greater level of exhaustivity brings with it a greater cost as more man hours are required. The additional time taken in an automated system would be much less significant. At the other end of the scale, in a selective index only the most important aspects are covered.&lt;ref name="Weinberg1999"&gt;B.H. Weinberg (1990): "Exhaustivity of indexes: Books, journals, and electronic full texts; Summary of a workshop presented at the 1999 ASI Annual Conference". ''Key Words'', '''7'''(5), pages 1+.&lt;/ref&gt; Recall is reduced in a selective index as if an indexer does not include enough terms, a highly relevant article may be overlooked. Therefore indexers should strive for a balance and consider what the document may be used. They may also have to consider the implications of time and expense.

=== Specificity ===
The specificity describes how closely the index terms match the topics they represent &lt;ref name="Anderson1997"&gt;J.D. Anderson (1997): ''[http://www.niso.org/publications/tr/ Guidelines for indexes and related information retrieval devices]'' [online]. Bethesda, Maryland, Niso Press. 10 December 2008.&lt;/ref&gt; An index is said to be specific if the indexer uses parallel descriptors to the concept of the document and reflects the concepts precisely.&lt;ref name="Cleveland2001b"&gt;D.B. Cleveland and A.D. Cleveland (2001): "Introduction to indexing and abstracting". 3rd Ed. Englewood, libraries Unlimited, Inc. ISBN 1-56308-641-7. page 106&lt;/ref&gt; Specificity tends to increase with exhaustivity as the more terms you include, the narrower those terms will be.

==Indexing theory==
[[Birger Hj&#248;rland|Hj&#248;rland]] (2011)&lt;ref&gt;Hj&#248;rland, Birger (2011). The Importance of Theories of Knowledge: Indexing and Information retrieval as an example. ''Journal of the American Society for Information Science and Technology'', 62(1,), 72-77.&lt;/ref&gt; found that theories of indexing is at the deepest level connected to different theories of knowledge:

'''Rationalist theories of indexing''' (such as Ranganathan's theory) suggest that subjects are constructed logically from a fundamental set of categories. The basic method of subject analysis is then "analytic-synthetic", to isolate a set of basic categories (=analysis) and then to construct the subject of any given document by combining those categories according to some rules (=synthesis). '''Empiricist theories of indexing''' are based on selecting similar documents based on their properties, in particular by applying numerical statistical techniques. '''Historicist and hermeneutical theories of indexing''' suggest that the subject of a given document is relative to a given discourse or domain, why the indexing should reflect the need of a particular discourse or domain. According to hermeneutics is a document always written and interpreted from particular horizon. The same is the case with systems of knowledge organization and with all users searching such systems. Any question put to such a system is put from a particular horizon. All those horizons may be more or less in consensus or in conflict. To index a document is to try to contribute to the retrieval of &#8220;relevant&#8221; documents by knowing about those different horizons. '''Pragmatic and critical theories of indexing''' (such as Hj&#248;rland, 1997)&lt;ref&gt;Hj&#248;rland, B. (1997). Information Seeking and Subject Representation. An Activity-theoretical approach to Information Science. Westport &amp; London: Greenwood Press.&lt;/ref&gt; is in agreement with the historicist point of view that subjects are relative to specific discourses but emphasizes that subject analysis should support given goals and values and should consider the consequences of indexing one way or another. These theories believe that indexing cannot be neutral and that it is a wrong goal to try to index in a neutral way. Indexing is an act (and computer based indexing is acting according to the programmers intentions). Acts serve human goals. Libraries and information services also serve human goals, why their indexing should be done in a way that supports these goals as much as possible. At a first glance this looks strange because the goals of libraries and information services is to identify any document or piece of information. Nonetheless is any specific way of indexing always supporting some kind of uses at the expense of other. The documents to be indexed intend to serve some specific purposes in a community. Basically the indexing should intend serving the same purposes. Primary and secondary documents and information services are parts of the same overall social system. In such a system different theories, epistemologies, worldviews etc. may be at play and users need to be able to orient themselves and to navigate among those different views. This calls for a mapping of the different epistemologies in the field and classification of the single document into such a map. Excellent examples of such different paradigms and their consequences for indexing and classification systems are provided in the domain of art by &#216;rom (2003)&lt;ref&gt;&#216;rom, Anders (2003). Knowledge Organization in the domain of Art Studies - History, Transition and Conceptual Changes. Knowledge Organization. 30(3/4), 128-143.&lt;/ref&gt; and in music by Abrahamsen (2003).&lt;ref&gt;Abrahamsen, Knut T. (2003). Indexing of Musical Genres. An Epistemological Perspective. Knowledge Organization, 30(3/4), 144-169.&lt;/ref&gt;

The core of indexing is, as stated by Rowley &amp; Farrow&lt;ref name=rowley2000&gt;Rowley, J. E. &amp; Farrow, J. (2000). Organizing Knowledge: An Introduction to Managing Access to Information. 3rd. Alderstot: Gower Publishing Company&lt;/ref&gt; to evaluate a papers contribution to knowledge and index it accordingly. Or, with the words of Hj&#248;rland (1992,&lt;ref&gt;Hj&#248;rland, Birger (1992). The Concept of "Subject" in Information Science. Journal of Documentation. 48(2), 172-200. http://iva.dk/bh/Core%20Concepts%20in%20LIS/1992JDOC%5FSubject.PDF&lt;/ref&gt; 1997) to index its informative potentials.

"In order to achieve good consistent indexing, the indexer must have a thorough appreciation of the structure of the subject and the nature of the contribution that the document is making to the advancement of knowledge." (Rowley &amp; Farrow, 2000,&lt;ref name=rowley2000/&gt; p.&amp;nbsp;99).

== See also ==
{{Commons category|Subject indexing}}
* [[Indexing and abstracting service]]
* [[Document classification]]
* [[Metadata]]
* [[Overcategorization]]
* [[Thomas of Ireland]], a medieval pioneer in subject indexing

== References ==
{{reflist}}
* {{cite book|author=Fugman, Robert|year=1993|title=Subject analysis and indexing. Theoretical foundation and practical advice|place=Frankfurt/Main|publisher=Index Verlag}}
* {{cite journal|author=Frohmann, B.|year=1990|title=Rules of Indexing: A Critique of [[Mentalism]] in Information Retrieval Theory|journal=Journal of Documentation|volume=46|issue=2|pages=81&#8211;101|doi=10.1108/eb026855}}

[[Category:Index (publishing)]]
[[Category:Information science]]
[[Category:Information retrieval techniques]]</text>
      <sha1>41r106836zl90cvp35a7ju4coawnslt</sha1>
    </revision>
  </page>
  <page>
    <title>Latent semantic analysis</title>
    <ns>0</ns>
    <id>689427</id>
    <revision>
      <id>762103594</id>
      <parentid>760873000</parentid>
      <timestamp>2017-01-26T17:41:54Z</timestamp>
      <contributor>
        <username>Nicegilles</username>
        <id>29988969</id>
      </contributor>
      <minor />
      <comment>/* Additional uses of LSI */ added link to e-discovery wiki page</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="53375" xml:space="preserve">{{semantics}}
'''Latent semantic analysis''' ('''LSA''') is a technique in [[natural language processing]], in particular [[distributional semantics]], of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms.  LSA assumes that words that are close in meaning will occur in similar pieces of text.  A matrix containing word counts per paragraph (rows represent unique words and columns represent each paragraph) is constructed from a large piece of text and a mathematical technique called [[singular value decomposition]] (SVD) is used to reduce the number of rows while preserving the similarity structure among columns.  Words are then compared by taking the cosine of the angle between the two vectors (or the [[dot product]] between the [[Unit vector|normalizations]] of the two vectors) formed by any two rows.  Values close to 1 represent very similar words while values close to 0 represent very dissimilar words.&lt;ref&gt;{{cite journal | title=Latent Semantic Analysis | author=Susan T. Dumais |year=2005 | doi=10.1002/aris.1440380105 | journal=Annual Review of Information Science and Technology | volume=38 | pages=188&#8211;230}}&lt;/ref&gt;

An information retrieval technique using latent semantic structure was patented in 1988 ([http://patft.uspto.gov/netacgi/nph-Parser?patentnumber=4839853 US Patent 4,839,853], now expired) by [[Scott Deerwester]], [[Susan Dumais]], [[George Furnas]], [[Richard Harshman]], [[Thomas Landauer]], [[Karen Lochbaum]] and [[Lynn Streeter]]. In the context of its application to [[information retrieval]], it is sometimes called [[Latent semantic indexing|Latent Semantic Indexing '''(LSI)''']].&lt;ref&gt;{{cite web | url=http://lsa.colorado.edu/ | title=The Latent Semantic Indexing home page}}&lt;/ref&gt;

== Overview ==

=== Occurrence matrix ===
LSA can use a [[term-document matrix]] which describes the occurrences of terms in documents; it is a [[sparse matrix]] whose rows correspond to [[terminology|terms]] and whose columns correspond to documents. A typical example of the weighting of the elements of the matrix is [[tf-idf]] (term frequency&#8211;inverse document frequency): the weight of an element of the matrix is proportional to the number of times the terms appear in each document, where rare terms are upweighted to reflect their relative importance.

This matrix is also common to standard semantic models, though it is not necessarily explicitly expressed as a matrix, since the mathematical properties of matrices are not always used.

=== Rank lowering ===
After the construction of the occurrence matrix, LSA finds a [[low-rank approximation]]&lt;ref&gt;Markovsky I. (2012) Low-Rank Approximation: Algorithms, Implementation, Applications, Springer, 2012, ISBN 978-1-4471-2226-5 {{page needed|date=January 2012}}&lt;/ref&gt; to the [[term-document matrix]]. There could be various reasons for these approximations:

* The original term-document matrix is presumed too large for the computing resources; in this case, the approximated low rank  matrix is interpreted as an ''approximation'' (a "least and necessary evil").
* The original term-document matrix is presumed ''noisy'': for example, anecdotal instances of terms are to be eliminated. From this point of view, the approximated matrix is interpreted as a ''de-noisified matrix'' (a better matrix than the original).
* The original term-document matrix is presumed overly [[Sparse matrix|sparse]] relative to the "true" term-document matrix.  That is, the original matrix lists only the words actually ''in'' each document, whereas we might be interested in all words ''related to'' each document&#8212;generally a much larger set due to [[synonymy]].

The consequence of the rank lowering is that some dimensions are combined and depend on more than one term:

:: {(car), (truck), (flower)} --&gt;  {(1.3452 * car + 0.2828 * truck), (flower)}

This mitigates the problem of identifying synonymy, as the rank lowering is expected to merge the dimensions associated with terms that have similar meanings. It also mitigates the problem with [[polysemy]], since components of polysemous words that point in the "right" direction are added to the components of words that share a similar meaning. Conversely, components that point in other directions tend to either simply cancel out, or, at worst, to be smaller than components in the directions corresponding to the intended sense.

=== Derivation ===
Let &lt;math&gt;X&lt;/math&gt; be a matrix where element &lt;math&gt;(i,j)&lt;/math&gt; describes the occurrence of term &lt;math&gt;i&lt;/math&gt; in document &lt;math&gt;j&lt;/math&gt; (this can be, for example, the frequency). &lt;math&gt;X&lt;/math&gt; will look like this:

:&lt;math&gt;
\begin{matrix} 
 &amp; \textbf{d}_j \\
 &amp; \downarrow \\
\textbf{t}_i^T \rightarrow &amp;
\begin{bmatrix} 
x_{1,1} &amp; \dots &amp; x_{1,n} \\
\vdots &amp; \ddots &amp; \vdots \\
x_{m,1} &amp; \dots &amp; x_{m,n} \\
\end{bmatrix}
\end{matrix}
&lt;/math&gt;

Now a row in this matrix will be a vector corresponding to a term, giving its relation to each document:

:&lt;math&gt;\textbf{t}_i^T = \begin{bmatrix} x_{i,1} &amp; \dots &amp; x_{i,n} \end{bmatrix}&lt;/math&gt;

Likewise, a column in this matrix will be a vector corresponding to a document, giving its relation to each term:

:&lt;math&gt;\textbf{d}_j = \begin{bmatrix} x_{1,j} \\ \vdots \\ x_{m,j} \end{bmatrix}&lt;/math&gt;

Now the [[dot product]] &lt;math&gt;\textbf{t}_i^T \textbf{t}_p&lt;/math&gt; between two term vectors gives the [[correlation]] between the terms over the set of documents. The [[matrix product]] &lt;math&gt;X X^T&lt;/math&gt; contains all these dot products. Element &lt;math&gt;(i,p)&lt;/math&gt; (which is equal to element &lt;math&gt;(p,i)&lt;/math&gt;) contains the dot product &lt;math&gt;\textbf{t}_i^T \textbf{t}_p&lt;/math&gt; (&lt;math&gt; = \textbf{t}_p^T \textbf{t}_i&lt;/math&gt;). Likewise, the matrix &lt;math&gt;X^T X&lt;/math&gt; contains the dot products between all the document vectors, giving their correlation over the terms: &lt;math&gt;\textbf{d}_j^T \textbf{d}_q = \textbf{d}_q^T \textbf{d}_j&lt;/math&gt;.

Now, from the theory of linear algebra, there exists a decomposition of &lt;math&gt;X&lt;/math&gt; such that &lt;math&gt;U&lt;/math&gt; and &lt;math&gt;V&lt;/math&gt; are [[orthogonal matrix|orthogonal matrices]] and &lt;math&gt;\Sigma&lt;/math&gt; is a [[diagonal matrix]]. This is called a [[singular value decomposition]] (SVD):

:&lt;math&gt;
\begin{matrix}
X = U \Sigma V^T
\end{matrix}
&lt;/math&gt;

The matrix products giving us the term and document correlations then become

:&lt;math&gt;
\begin{matrix}
X X^T &amp;=&amp; (U \Sigma V^T) (U \Sigma V^T)^T = (U \Sigma V^T) (V^{T^T} \Sigma^T U^T) = U \Sigma V^T V \Sigma^T U^T = U \Sigma \Sigma^T U^T = U \Sigma^2 U^T \\
X^T X &amp;=&amp; (U \Sigma V^T)^T (U \Sigma V^T) = (V^{T^T} \Sigma^T U^T) (U \Sigma V^T) = V \Sigma^T U^T U \Sigma V^T = V \Sigma^T \Sigma V^T = V \Sigma^2 V^T
\end{matrix}
&lt;/math&gt;

Since &lt;math&gt;\Sigma \Sigma^T&lt;/math&gt; and &lt;math&gt;\Sigma^T \Sigma&lt;/math&gt; are diagonal we see that &lt;math&gt;U&lt;/math&gt; must contain the [[eigenvector]]s of &lt;math&gt;X X^T&lt;/math&gt;, while &lt;math&gt;V&lt;/math&gt; must be the eigenvectors of &lt;math&gt;X^T X&lt;/math&gt;. Both products have the same non-zero eigenvalues, given by the non-zero entries of &lt;math&gt;\Sigma \Sigma^T&lt;/math&gt;, or equally, by the non-zero entries of &lt;math&gt;\Sigma^T\Sigma&lt;/math&gt;. Now the decomposition looks like this:

:&lt;math&gt;
\begin{matrix} 
 &amp; X &amp; &amp; &amp; U &amp; &amp; \Sigma &amp; &amp; V^T \\
 &amp; (\textbf{d}_j) &amp; &amp; &amp; &amp; &amp; &amp; &amp; (\hat{\textbf{d}}_j) \\
 &amp; \downarrow &amp; &amp; &amp; &amp; &amp; &amp; &amp; \downarrow \\
(\textbf{t}_i^T) \rightarrow 
&amp;
\begin{bmatrix} 
x_{1,1} &amp; \dots &amp; x_{1,n} \\
\\
\vdots &amp; \ddots &amp; \vdots \\
\\
x_{m,1} &amp; \dots &amp; x_{m,n} \\
\end{bmatrix}
&amp;
=
&amp;
(\hat{\textbf{t}}_i^T) \rightarrow
&amp;
\begin{bmatrix} 
\begin{bmatrix} \, \\ \, \\ \textbf{u}_1 \\ \, \\ \,\end{bmatrix} 
\dots
\begin{bmatrix} \, \\ \, \\ \textbf{u}_l \\ \, \\ \, \end{bmatrix}
\end{bmatrix}
&amp;
\cdot
&amp;
\begin{bmatrix} 
\sigma_1 &amp; \dots &amp; 0 \\
\vdots &amp; \ddots &amp; \vdots \\
0 &amp; \dots &amp; \sigma_l \\
\end{bmatrix}
&amp;
\cdot
&amp;
\begin{bmatrix} 
\begin{bmatrix} &amp; &amp; \textbf{v}_1 &amp; &amp; \end{bmatrix} \\
\vdots \\
\begin{bmatrix} &amp; &amp; \textbf{v}_l &amp; &amp; \end{bmatrix}
\end{bmatrix}
\end{matrix}
&lt;/math&gt;

The values &lt;math&gt;\sigma_1, \dots, \sigma_l&lt;/math&gt; are called the singular values, and &lt;math&gt;u_1, \dots, u_l&lt;/math&gt; and &lt;math&gt;v_1, \dots, v_l&lt;/math&gt; the left and right singular vectors.
Notice the only part of &lt;math&gt;U&lt;/math&gt; that contributes to &lt;math&gt;\textbf{t}_i&lt;/math&gt; is the &lt;math&gt;i\textrm{'th}&lt;/math&gt; row.
Let this row vector be called &lt;math&gt;\hat{\textrm{t}}^T_i&lt;/math&gt;.
Likewise, the only part of &lt;math&gt;V^T&lt;/math&gt; that contributes to &lt;math&gt;\textbf{d}_j&lt;/math&gt; is the &lt;math&gt;j\textrm{'th}&lt;/math&gt; column, &lt;math&gt;\hat{ \textrm{d}}_j&lt;/math&gt;.
These are ''not'' the eigenvectors, but ''depend'' on ''all'' the eigenvectors.

It turns out that when you select the &lt;math&gt;k&lt;/math&gt; largest singular values, and their corresponding singular vectors from &lt;math&gt;U&lt;/math&gt; and &lt;math&gt;V&lt;/math&gt;, you get the rank &lt;math&gt;k&lt;/math&gt; approximation to &lt;math&gt;X&lt;/math&gt; with the smallest error ([[Frobenius norm]]). This approximation has a minimal error.  But more importantly we can now treat the term and document vectors as a "semantic space". The row "term" vector &lt;math&gt;\hat{\textbf{t}}^T_i&lt;/math&gt; then has &lt;math&gt;k&lt;/math&gt; entries mapping it to a lower-dimensional space dimensions. These new dimensions do not relate to any comprehensible concepts. They are a lower-dimensional approximation of the higher-dimensional space. Likewise, the "document" vector &lt;math&gt;\hat{\textbf{d}}_j&lt;/math&gt; is an approximation in this lower-dimensional space. We write this approximation as

:&lt;math&gt;X_k = U_k \Sigma_k V_k^T&lt;/math&gt;

You can now do the following:
* See how related documents &lt;math&gt;j&lt;/math&gt; and &lt;math&gt;q&lt;/math&gt; are in the low-dimensional space by comparing the vectors &lt;math&gt;\Sigma_k \hat{\textbf{d}}_j &lt;/math&gt; and &lt;math&gt;\Sigma_k \hat{\textbf{d}}_q &lt;/math&gt; (typically by [[vector space model|cosine similarity]]).
* Comparing terms &lt;math&gt;i&lt;/math&gt; and &lt;math&gt;p&lt;/math&gt; by comparing the vectors &lt;math&gt;\Sigma_k \hat{\textbf{t}}_i&lt;/math&gt; and &lt;math&gt;\Sigma_k \hat{\textbf{t}}_p&lt;/math&gt;. Note that &lt;math&gt;\hat{\textbf{t}}&lt;/math&gt; is now a column vector.
* Documents and term vector representations can be clustered using traditional clustering algorithms like k-means using similarity measures like cosine.
* Given a query, view this as a mini document, and compare it to your documents in the low-dimensional space.

To do the latter, you must first translate your query into the low-dimensional space. It is then intuitive that you must use the same transformation that you use on your documents:

:&lt;math&gt;\hat{\textbf{d}}_j = \Sigma_k^{-1} U_k^T \textbf{d}_j&lt;/math&gt;

Note here that the inverse of the diagonal matrix &lt;math&gt;\Sigma_k&lt;/math&gt; may be found by inverting each nonzero value within the matrix.

This means that if you have a query vector &lt;math&gt;q&lt;/math&gt;, you must do the translation &lt;math&gt;\hat{\textbf{q}} = \Sigma_k^{-1} U_k^T \textbf{q}&lt;/math&gt; before you compare it with the document vectors in the low-dimensional space. You can do the same for pseudo term vectors:

:&lt;math&gt;\textbf{t}_i^T = \hat{\textbf{t}}_i^T \Sigma_k V_k^T&lt;/math&gt;

:&lt;math&gt;\hat{\textbf{t}}_i^T = \textbf{t}_i^T V_k^{-T} \Sigma_k^{-1} = \textbf{t}_i^T V_k \Sigma_k^{-1}&lt;/math&gt;

:&lt;math&gt;\hat{\textbf{t}}_i = \Sigma_k^{-1}  V_k^T \textbf{t}_i&lt;/math&gt;

== Applications ==

The new low-dimensional space typically can be used to:
* Compare the documents in the low-dimensional space ([[data clustering]], [[document classification]]).
* Find similar documents across languages, after analyzing a base set of translated documents ([[cross language retrieval]]).
* Find relations between terms ([[synonymy]] and [[polysemy]]).
* Given a query of terms, translate it into the low-dimensional space, and find matching documents ([[information retrieval]]).
* Find the best similarity between small groups of terms, in a semantic way (i.e. in a context of a knowledge corpus), as for example in multi choice questions [[Multiple choice question|MCQ]] answering model.&lt;ref name="Alain2009"&gt;{{cite journal | url=http://hal.archives-ouvertes.fr/docs/00/38/41/43/PDF/eLSA1-brm20.pdf |format=PDF| title=Effect of tuned parameters on an LSA multiple choice questions answering model |author1=Alain Lifchitz |author2=Sandra Jhean-Larose |author3=Guy Denhi&#232;re | journal=Behavior Research Methods | volume=41 | issue=4 | pages=1201&#8211;1209 | year=2009  | doi=10.3758/BRM.41.4.1201 | pmid=19897829 }}&lt;/ref&gt;
* Expand the feature space of machine learning / text mining systems &lt;ref name="Galvez2017"&gt;{{cite journal | url=http://www.sciencedirect.com/science/article/pii/S1877750317300091 | title=Assessing the usefulness of online message board mining in automatic stock prediction systems |author1=Ramiro H. G&#225;lvez |author2=Agust&#237;n Gravano | journal=Journal of Computational Science | volume=19 | pages=1877-7503 | year=2017  | doi=10.1016/j.jocs.2017.01.001}}&lt;/ref&gt;

Synonymy and polysemy are fundamental problems in [[natural language processing]]: 
* Synonymy is the phenomenon where different words describe the same idea. Thus, a query in a search engine may fail to retrieve a relevant document that does not contain the words which appeared in the query. For example, a search for "doctors" may not return a document containing the word "[[physicians]]", even though the words have the same meaning.
* Polysemy is the phenomenon where the same word has multiple meanings. So a search may retrieve irrelevant documents containing the desired words in the wrong meaning. For example, a botanist and a computer scientist looking for the word "tree" probably desire different sets of documents.

=== Commercial applications ===

LSA has been used to assist in performing [[prior art]] searches for [[patents]].&lt;ref name="Gerry2007"&gt;{{Cite journal | author=Gerry J. Elman | title=Automated Patent Examination Support - A proposal | journal=Biotechnology Law Report | date=October 2007 | doi=10.1089/blr.2007.9896 | volume=26 | issue=5 | pages=435&#8211;436 | postscript=&lt;!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. --&gt;{{inconsistent citations}}}}&lt;/ref&gt;

=== Applications in human memory ===

The use of Latent Semantic Analysis has been prevalent in the study of human memory, especially in areas of [[free recall]] and memory search.  There is a positive correlation between the semantic similarity of two words (as measured by LSA) and the probability that the words would be recalled one after another in free recall tasks using study lists of random common nouns. They also noted that in these situations, the inter-response time between the similar words was much quicker than between dissimilar words.  These findings are referred to as the [[Semantic Proximity Effect]].&lt;ref&gt;{{cite journal | url=http://psycnet.apa.org/journals/xlm/25/4/923.pdf |format=PDF| title=Contextual Variability and Serial Position Effects in Free Recall |author1=Marc W. Howard |author2=Michael J. Kahana |year=1999}}&lt;/ref&gt;

When participants made mistakes in recalling studied items, these mistakes tended to be items that were more semantically related to the desired item and found in a previously studied list.  These prior-list intrusions, as they have come to be called, seem to compete with items on the current list for recall.&lt;ref&gt;{{cite journal | url=https://memory.psych.upenn.edu/files/pubs/ZaroEtal06.pdf |format=PDF| title=Temporal Associations and Prior-List Intrusions in Free Recall | author=Franklin M. Zaromb| booktitle=Interspeech'2005|year=2006|display-authors=etal}}&lt;/ref&gt;

Another model, termed [[Word Association Spaces]] (WAS) is also used in memory studies by collecting free association data from a series of experiments and which includes measures of word relatedness for over 72,000 distinct word pairs.&lt;ref&gt;{{cite web|last=Nelson|first=Douglas|title=The University of South Florida Word Association, Rhyme and Word Fragment Norms|url=http://w3.usf.edu/FreeAssociation/Intro.html|accessdate=May 8, 2011}}&lt;/ref&gt;

== Implementation ==

The [[Singular Value Decomposition|SVD]] is typically computed using large matrix methods (for example, [[Lanczos method]]s) but may also be computed incrementally and with greatly reduced resources via a [[neural network]]-like approach, which does not require the large, full-rank matrix to be held in memory.&lt;ref name="Genevi2005"&gt;{{cite conference | url=http://www.dcs.shef.ac.uk/~genevieve/gorrell_webb.pdf |format=PDF| title=Generalized Hebbian Algorithm for Latent Semantic Analysis |author1=Genevi&#232;ve Gorrell |author2=Brandyn Webb | booktitle=Interspeech'2005 |year=2005}}&lt;/ref&gt;
A fast, incremental, low-memory, large-matrix SVD algorithm has recently been developed.&lt;ref name="brand2006"&gt;{{cite journal | url=http://www.merl.com/reports/docs/TR2006-059.pdf |format=PDF| title=Fast Low-Rank Modifications of the Thin Singular Value Decomposition | author=Matthew Brand | journal=Linear Algebra and Its Applications | volume=415 | pages=20&#8211;30 | year=2006 | doi=10.1016/j.laa.2005.07.021 }}&lt;/ref&gt; [http://web.mit.edu/~wingated/www/resources.html MATLAB] and [http://radimrehurek.com/gensim Python] implementations of these fast algorithms are available. Unlike Gorrell and Webb's (2005) stochastic approximation, Brand's algorithm (2003) provides an exact solution.
In recent years progress has been made to reduce the computational complexity of SVD; for instance, by using a parallel ARPACK algorithm to perform parallel eigenvalue decomposition it is possible to speed up the SVD computation cost while providing comparable prediction quality.&lt;ref&gt;{{cite journal | doi = 10.1109/ICCSNT.2011.6182070 | title=A parallel implementation of Singular Value Decomposition based on Map-Reduce and PARPACK | journal=Proceedings of 2011 International Conference on Computer Science and Network Technology}}&lt;/ref&gt;

== Limitations ==
Some of LSA's drawbacks include:

* The resulting dimensions might be difficult to interpret. For instance, in
:: {(car), (truck), (flower)} &#8614;  {(1.3452 * car + 0.2828 * truck), (flower)}
:the (1.3452 * car + 0.2828 * truck) component could be interpreted as "vehicle". However, it is very likely that cases close to
:: {(car), (bottle), (flower)} &#8614;  {(1.3452 * car + 0.2828 * '''bottle'''), (flower)}
:will occur. This leads to results which can be justified on the mathematical level, but have no interpretable meaning in natural language.

* LSA cannot capture [[polysemy]] (i.e., multiple meanings of a word) because each occurrence of a word is treated as having the same meaning due to the word being represented as a single point in space.  For example, the occurrence of "chair" in a document containing "The Chair of the Board" and in a separate document containing "the chair maker" are considered the same.  The behavior results in the vector representation being an ''average'' of all the word's different meanings in the corpus, which can make it difficult for comparison.  However, the effect is often lessened due to words having a [[word sense disambiguation|predominant sense]] throughout a corpus (i.e. not all meanings are equally likely).
* Limitations of [[bag of words model]] (BOW), where a text is represented as an unordered collection of words. To address some of the limitation of [[bag of words model]] (BOW), [[N-gram|multi-gram]] dictionary can be used to find direct and indirect association as well as [[Higher-order statistics|higher-order]] [[co-occurrence]]s among terms.&lt;ref&gt;{{cite journal|url=http://www.translational-medicine.com/content/12/1/324|title=Empirical study using network of semantically related associations in bridging the knowledge gap|first1=Vida|last1=Abedi|first2=Mohammed|last2=Yeasin|first3=Ramin|last3=Zand|date=27 November 2014|publisher=|volume=12|issue=1|doi=10.1186/s12967-014-0324-9|pmid=25428570|pmc=4252998|journal=Journal of Translational Medicine}}&lt;/ref&gt;
* The [[probabilistic model]] of LSA does not match observed data: LSA assumes that words and documents form a joint [[normal distribution|Gaussian]] model ([[ergodic hypothesis]]), while a [[Poisson distribution]] has been observed.  Thus, a newer alternative is [[probabilistic latent semantic analysis]], based on a [[multinomial distribution|multinomial]] model, which is reported to give better results than standard LSA.&lt;ref name="Thomas1999"&gt;{{cite conference | url=http://www.cs.brown.edu/people/th/papers/Hofmann-UAI99.pdf |format=PDF| title=Probabilistic Latent Semantic Analysis | author=Thomas Hofmann | booktitle=Uncertainty in Artificial Intelligence |year=1999}}&lt;/ref&gt;

==Alternative methods==

===Semantic hashing===
In semantic hashing &lt;ref&gt;Salakhutdinov, Ruslan, and Geoffrey Hinton. "Semantic hashing." RBM 500.3 (2007): 500.&lt;/ref&gt; documents are mapped to memory addresses by means of a [[neural network]] in such a way that semantically similar documents are located at nearby addresses. [[Deep learning|Deep neural network]] essentially builds a [[graphical model]] of the word-count vectors obtained from a large set of documents. Documents similar to a query document can then be found by simply accessing all the addresses that differ by only a few bits from the address of the query document. This way of extending the efficiency of hash-coding to approximate matching is much faster than [[locality sensitive hashing]], which is the fastest current method.

== Latent semantic indexing ==
'''Latent semantic indexing''' ('''LSI''') is an indexing and retrieval method that uses a mathematical technique called [[singular value decomposition]] (SVD) to identify patterns in the relationships between the [[terminology|term]]s and [[concept]]s contained in an unstructured collection of text.  LSI is based on the principle that words that are used in the same contexts tend to have similar meanings.  A key feature of LSI is its ability to extract the conceptual content of a [[Text corpus|body of text]] by establishing associations between those terms that occur in similar [[context (language use)|context]]s.&lt;ref name=deerwester1988&gt;Deerwester, S., et al, Improving Information Retrieval with Latent Semantic Indexing, Proceedings of the 51st Annual Meeting of the American Society for Information Science 25, 1988, pp. 36&#8211;40.&lt;/ref&gt;

LSI is also an application of [[correspondence analysis]], a multivariate statistical technique developed by [[Jean-Paul Benz&#233;cri]]&lt;ref&gt;{{ cite book
 | author = Benz&#233;cri, J.-P.
 | publisher=Dunod |location= Paris, France
 | year = 1973
 | title = L'Analyse des Donn&#233;es. Volume II. L'Analyse des Correspondences
 }}&lt;/ref&gt; in the early 1970s, to a [[contingency table]] built from word counts in documents.

Called Latent Semantic Indexing because of its ability to correlate semantically related terms that are latent in a collection of text, it was first applied to text at [[Bellcore]] in the late 1980s.   The method, also called latent semantic analysis (LSA), uncovers the underlying latent semantic structure in the usage of words in a body of text and how it can be used to extract the meaning of the text in response to user queries, commonly referred to as concept searches.  Queries, or concept searches, against a set of documents that have undergone LSI will return results that are conceptually similar in meaning to the search criteria even if the results don&#8217;t share a specific word or words with the search criteria.

== Benefits of LSI ==

LSI overcomes two of the most problematic constraints of Boolean [[keyword search|keyword queries]]:  multiple words that have similar meanings ([[synonymy]]) and words that have more than one meaning ([[polysemy]]).  Synonymy is often the cause of mismatches in the vocabulary used by the authors of documents and the users of [[information retrieval]] systems.&lt;ref&gt;{{Cite journal | last1 = Furnas | first1 = G. W. | last2 = Landauer | first2 = T. K. | last3 = Gomez | first3 = L. M. | last4 = Dumais | first4 = S. T. | title = The vocabulary problem in human-system communication | doi = 10.1145/32206.32212 | journal = Communications of the ACM | volume = 30 | issue = 11 | pages = 964&#8211;971 | year = 1987 | pmid =  | pmc = }}&lt;/ref&gt;  As a result, Boolean or keyword queries often return irrelevant results and miss information that is relevant.

LSI is also used to perform automated [[document categorization]].  In fact, several experiments have demonstrated that there are a number of correlations between the way LSI and humans process and categorize text.&lt;ref name=landauer2008&gt;Landauer, T., et al., Learning Human-like Knowledge by Singular Value Decomposition: A Progress Report, M. I. Jordan, M. J. Kearns &amp; S. A. Solla (Eds.), Advances in Neural Information Processing Systems 10, Cambridge: MIT Press, 1998, pp. 45&#8211;51.&lt;/ref&gt;    Document categorization is the assignment of documents to one or more predefined categories based on their similarity to the conceptual content of the categories.&lt;ref&gt;{{Cite book | last1 = Dumais | first1 = S. | last2 = Platt | first2 = J. | last3 = Heckerman | first3 = D. | last4 = Sahami | first4 = M. | chapter = Inductive learning algorithms and representations for text categorization | doi = 10.1145/288627.288651 | title = Proceedings of the seventh international conference on Information and knowledge management  - CIKM '98 | pages = 148 | year = 1998 | isbn = 1581130619 | url = http://research.microsoft.com/en-us/um/people/jplatt/cikm98.pdf| pmid =  | pmc = }}&lt;/ref&gt;   LSI uses ''example'' documents to establish the conceptual basis for each category.  During categorization processing, the concepts contained in the documents being categorized are compared to the concepts contained in the example items, and a category (or categories) is assigned to the documents based on the similarities between the concepts they contain and the concepts that are contained in the example documents.

Dynamic clustering based on the conceptual content of documents can also be accomplished using LSI.  Clustering is a way to group documents based on their conceptual similarity to each other without using example documents to establish the conceptual basis for each cluster.  This is very useful when dealing with an unknown collection of unstructured text.

Because it uses a strictly mathematical approach, LSI is inherently independent of language.  This enables LSI to elicit the semantic content of information written in any language without requiring the use of auxiliary structures, such as dictionaries and thesauri.  LSI can also perform cross-linguistic concept searching and example-based categorization.  For example, queries can be made in one language, such as English, and conceptually similar results will be returned even if they are composed of an entirely different language or of multiple languages.{{Citation needed|date=July 2015}}

LSI is not restricted to working only with words.  It can also process arbitrary character strings.  Any object that can be expressed as text can be represented in an LSI vector space.&lt;ref&gt;Zukas, Anthony, Price, Robert J., Document Categorization Using Latent Semantic Indexing, White Paper, [[Content Analyst Company]], LLC&lt;/ref&gt;   For example, tests with MEDLINE abstracts have shown that LSI is able to effectively classify genes based on conceptual modeling of the biological information contained in the titles and abstracts of the MEDLINE citations.&lt;ref&gt;{{Cite journal | last1 = Homayouni | first1 = R. | last2 = Heinrich | first2 = K. | last3 = Wei | first3 = L. | last4 = Berry | first4 = M. W. | title = Gene clustering by Latent Semantic Indexing of MEDLINE abstracts | doi = 10.1093/bioinformatics/bth464 | journal = Bioinformatics | volume = 21 | issue = 1 | pages = 104&#8211;115 | year = 2004 | pmid =  15308538| pmc = }}&lt;/ref&gt;

LSI automatically adapts to new and changing terminology, and has been shown to be very tolerant of noise (i.e., misspelled words, typographical errors, unreadable characters, etc.).&lt;ref&gt;{{Cite book | last1 = Price | first1 = R. J. | last2 = Zukas | first2 = A. E. | chapter = Application of Latent Semantic Indexing to Processing of Noisy Text | doi = 10.1007/11427995_68 | title = Intelligence and Security Informatics | series = Lecture Notes in Computer Science | volume = 3495 | pages = 602 | year = 2005 | isbn = 978-3-540-25999-2 | pmid =  | pmc = }}&lt;/ref&gt;   This is especially important for applications using text derived from Optical Character Recognition (OCR) and speech-to-text conversion.  LSI also deals effectively with sparse, ambiguous, and contradictory data.

Text does not need to be in sentence form for LSI to be effective.  It can work with lists, free-form notes, email, Web-based content, etc.  As long as a collection of text contains multiple terms, LSI can be used to identify patterns in the relationships between the important terms and concepts contained in the text.

LSI has proven to be a useful solution to a number of conceptual matching problems.&lt;ref&gt;Ding, C., A Similarity-based Probability Model for Latent Semantic Indexing, Proceedings of the 22nd International ACM SIGIR Conference on Research and Development in Information Retrieval, 1999, pp. 59&#8211;65.&lt;/ref&gt;&lt;ref&gt;Bartell, B., Cottrell, G., and Belew, R., Latent Semantic Indexing is an Optimal Special Case of Multidimensional Scaling, Proceedings, ACM SIGIR Conference on Research and Development in Information Retrieval, 1992, pp. 161&#8211;167.&lt;/ref&gt;  The technique has been shown to capture key relationship information, including causal, goal-oriented, and taxonomic information.&lt;ref&gt;{{cite journal|url=http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.23.5444&amp;rep=rep1&amp;type=pdf|author1=Graesser, A. |author2=Karnavat, A.|title=Latent Semantic Analysis Captures Causal, Goal-oriented, and Taxonomic Structures|journal=Proceedings of CogSci 2000|pages=184&#8211;189}}&lt;/ref&gt;

== LSI timeline ==

*'''Mid-1960s''' &#8211; Factor analysis technique first described and tested (H. Borko and M. Bernick)
*'''1988''' &#8211; Seminal paper on LSI technique published &lt;ref name=deerwester1988/&gt;
*'''1989''' &#8211; Original patent granted &lt;ref name=deerwester1988/&gt;
*'''1992''' &#8211; First use of LSI to assign articles to reviewers&lt;ref&gt;{{cite journal|last1=Dumais |first1=S. |last2=Nielsen |first2=J. |title=Automating the Assignment of Submitted Manuscripts to Reviewers|journal=Proceedings of the Fifteenth Annual International Conference on Research and Development in Information Retrieval|year=1992|pages=233&#8211;244|doi=10.1145/133160.133205|isbn=0897915232 }}&lt;/ref&gt;
*'''1994''' &#8211; Patent granted for the cross-lingual application of LSI (Landauer et al.)
*'''1995''' &#8211; First use of LSI for grading essays (Foltz, et al., Landauer et al.)
*'''1999''' &#8211; First implementation of LSI technology for intelligence community for analyzing unstructured text ([[Science Applications International Corporation|SAIC]]).
*'''2002''' &#8211; LSI-based product offering to intelligence-based government agencies (SAIC)
*'''2005''' &#8211; First vertical-specific application &#8211; publishing &#8211; EDB (EBSCO, [[Content Analyst Company]])

== Mathematics of LSI ==

LSI uses common linear algebra techniques to learn the conceptual correlations in a collection of text.  In general, the process involves constructing a weighted term-document matrix, performing a [[Singular value decomposition|'''Singular Value Decomposition''']] on the matrix, and using the matrix to identify the concepts contained in the text.

=== Term-document matrix ===

LSI begins by constructing a term-document matrix, &lt;math&gt;A&lt;/math&gt;, to identify the occurrences of the &lt;math&gt;m&lt;/math&gt; unique terms within a collection of &lt;math&gt;n&lt;/math&gt; documents.  In a term-document matrix, each term is represented by a row, and each document is represented by a column, with each matrix cell, &lt;math&gt;a_{ij}&lt;/math&gt;, initially representing the number of times the associated term appears in the indicated document, &lt;math&gt;\mathrm{tf_{ij}}&lt;/math&gt;.  This matrix is usually very large and very sparse.

Once a term-document matrix is constructed, local and global weighting functions can be applied to it to condition the data.  The weighting functions transform each cell, &lt;math&gt;a_{ij}&lt;/math&gt; of &lt;math&gt;A&lt;/math&gt;, to be the product of a local term weight, &lt;math&gt;l_{ij}&lt;/math&gt;, which describes the relative frequency of a term in a document, and a global weight, &lt;math&gt;g_i&lt;/math&gt;, which describes the relative frequency of the term within the entire collection of documents.

Some common local weighting functions &lt;ref&gt;
Berry, M. W., and Browne, M., Understanding Search Engines: Mathematical Modeling and Text Retrieval, Society for Industrial and Applied Mathematics, Philadelphia, (2005).&lt;/ref&gt; are defined in the following table.

{| style="width:60%" cellpadding="25" cellspacing="5" align="center"
|-
|  style="width:22%" | '''Binary''' ||
| &lt;math&gt;l_{ij} = 1&lt;/math&gt; if the term exists in the document, or else &lt;math&gt;0&lt;/math&gt;
|-
|  style="width:22%" | '''TermFrequency''' ||
| &lt;math&gt;l_{ij} = \mathrm{tf}_{ij}&lt;/math&gt;, the number of occurrences of term &lt;math&gt;i&lt;/math&gt; in document &lt;math&gt;j&lt;/math&gt;
|-
|  style="width:22%" | '''Log''' ||
| &lt;math&gt;l_{ij} = \log(\mathrm{tf}_{ij} + 1)&lt;/math&gt;
|-
|  style="width:22%" | '''Augnorm''' ||
| &lt;math&gt;l_{ij} = \frac{\Big(\frac{\mathrm{tf}_{ij}}{\max_i(\mathrm{tf}_{ij})}\Big) + 1}{2}&lt;/math&gt;
|}

Some common global weighting functions are defined in the following table.

{| style="width:60%" cellpadding="25" cellspacing="5" align="center"
|-
| style="width:22%" | '''Binary''' ||
| &lt;math&gt;g_i = 1&lt;/math&gt;
|-
| style="width:22%" | '''Normal''' ||
| &lt;math&gt;g_i = \frac{1}{\sqrt{\sum_j \mathrm{tf}_{ij}^2}}&lt;/math&gt;
|-
| style="width:22%" | '''GfIdf''' ||
| &lt;math&gt;g_i = \mathrm{gf}_i / \mathrm{df}_i&lt;/math&gt;, where &lt;math&gt;\mathrm{gf}_i&lt;/math&gt; is the total number of times term &lt;math&gt;i&lt;/math&gt; occurs in the whole collection, and &lt;math&gt;\mathrm{df}_i&lt;/math&gt; is the number of documents in which term &lt;math&gt;i&lt;/math&gt; occurs.
|-
| style="width:22%" | '''[[Tf&#8211;idf#Inverse document frequency 2|Idf (Inverse Document Frequency)]]''' ||
| &lt;math&gt;g_i = \log_2 \frac{n}{1+ \mathrm{df}_i}&lt;/math&gt;
|-
| style="width:22%" | '''Entropy''' ||
| &lt;math&gt;g_i = 1 + \sum_j \frac{p_{ij} \log p_{ij}}{\log n}&lt;/math&gt;, where &lt;math&gt;p_{ij} = \frac{\mathrm{tf}_{ij}}{\mathrm{gf}_i}&lt;/math&gt;
|}

Empirical studies with LSI report that the Log and Entropy weighting functions work well, in practice, with many data sets.&lt;ref&gt;Landauer, T., et al., Handbook of Latent Semantic Analysis, Lawrence Erlbaum Associates, 2007.&lt;/ref&gt;  In other words, each entry &lt;math&gt;a_{ij}&lt;/math&gt; of &lt;math&gt;A&lt;/math&gt; is computed as:

:&lt;math&gt;g_i = 1 + \sum_j \frac{p_{ij} \log p_{ij}}{\log n}&lt;/math&gt;

:&lt;math&gt;a_{ij} = g_i \ \log (\mathrm{tf}_{ij} + 1)&lt;/math&gt;

=== Rank-reduced singular value decomposition ===

A rank-reduced, [[singular value decomposition]] is performed on the matrix to determine patterns in the relationships between the terms and concepts contained in the text.  The SVD forms the foundation for LSI.&lt;ref&gt;Berry, Michael W., Dumais, Susan T., O'Brien, Gavin W., Using Linear Algebra for Intelligent Information Retrieval, December 1994, SIAM Review 37:4 (1995), pp. 573&#8211;595.&lt;/ref&gt;   It computes the term and document vector spaces by approximating the single term-frequency matrix, &lt;math&gt;A&lt;/math&gt;, into three other matrices&#8212; an '''''m''''' by '''''r'''''  term-concept vector matrix &lt;math&gt;T&lt;/math&gt;, an '''''r''''' by '''''r''''' singular values matrix &lt;math&gt;S&lt;/math&gt;, and a '''''n''''' by '''''r''''' concept-document vector matrix, &lt;math&gt;D&lt;/math&gt;, which satisfy the following relations:

&lt;math&gt;A \approx TSD^T&lt;/math&gt;

&lt;math&gt;T^T T = I_r \quad D^T D = I_r &lt;/math&gt;

&lt;math&gt;S_{1,1} \geq S_{2,2} \geq \ldots \geq  S_{r,r} &gt; 0 \quad S_{i,j} = 0 \; \text{where} \; i \neq j&lt;/math&gt;

In the formula, '''A''' is the supplied '''''m''''' by '''''n''''' weighted matrix of term frequencies in a collection of text where '''''m''''' is the number of unique terms, and '''''n''''' is the number of documents.  '''T''' is a computed '''''m''''' by '''''r''''' matrix of term vectors where '''''r''''' is the rank of '''A'''&#8212;a measure of its unique dimensions '''&#8804; min(''m,n'')'''.  '''S''' is a computed '''''r''''' by '''''r''''' diagonal matrix of decreasing singular values, and '''D''' is a computed '''''n''''' by '''''r''''' matrix of document vectors.

The SVD is then [[Singular value decomposition#Truncated SVD|truncated]] to reduce the rank by keeping only the largest '''''k''''' &#171; '''''r''''' diagonal entries in the singular value matrix '''S''',
where '''''k''''' is typically on the order 100 to 300 dimensions.
This effectively reduces the term and document vector matrix sizes to '''''m''''' by '''''k''''' and '''''n''''' by '''''k''''' respectively.  The SVD operation, along with this reduction, has the effect of preserving the most important semantic information in the text while reducing noise and other undesirable artifacts of the original space of '''A'''.  This reduced set of matrices is often denoted with a modified formula such as:

:::::::'''A &#8776; A''&lt;sub&gt;k''&lt;/sub&gt; = T''&lt;sub&gt;k''&lt;/sub&gt; S''&lt;sub&gt;k''&lt;/sub&gt; D''&lt;sub&gt;k''&lt;/sub&gt;&lt;sup&gt;T&lt;/sup&gt;'''

Efficient LSI algorithms only compute the first '''''k''''' singular values and term and document vectors as opposed to computing a full SVD and then truncating it.

Note that this rank reduction is essentially the same as doing [[Principal Component Analysis]] (PCA) on the matrix '''A''', except that PCA subtracts off the means.  PCA loses the sparseness of the '''A''' matrix, which can make it infeasible for large lexicons.

== Querying and augmenting LSI vector spaces ==

The computed '''T''&lt;sub&gt;k''&lt;/sub&gt;''' and '''D''&lt;sub&gt;k''&lt;/sub&gt;''' matrices define the term and document vector spaces, which with the computed singular values, '''S''&lt;sub&gt;k''&lt;/sub&gt;''', embody the conceptual information derived from the document collection.  The similarity of terms or documents within these spaces is a factor of how close they are to each other in these spaces, typically computed as a function of the angle between the corresponding vectors.

The same steps are used to locate the vectors representing the text of queries and new documents within the document space of an existing LSI index.  By a simple transformation of the '''A = T S D&lt;sup&gt;T&lt;/sup&gt;''' equation into the equivalent '''D = A&lt;sup&gt;T&lt;/sup&gt; T S&lt;sup&gt;&#8722;1&lt;/sup&gt;''' equation, a new vector, '''''d''''', for a query or for a new document can be created by computing a new column in '''A''' and then multiplying the new column by '''T S&lt;sup&gt;&#8722;1&lt;/sup&gt;'''.  The new column in '''A''' is computed using the originally derived global term weights and applying the same local weighting function to the terms in the query or in the new document.

A drawback to computing vectors in this way, when adding new searchable documents, is that terms that were not known during the SVD phase for the original index are ignored.  These terms will have no impact on the global weights and learned correlations derived from the original collection of text.  However, the computed vectors for the new text are still very relevant for similarity comparisons with all other document vectors.

The process of augmenting the document vector spaces for an LSI index with new documents in this manner is called ''folding in''.  Although the folding-in process does not account for the new semantic content of the new text, adding a substantial number of documents in this way will still provide good results for queries as long as the terms and concepts they contain are well represented within the LSI index to which they are being added.  When the terms and concepts of a new set of documents need to be included in an LSI index, either the term-document matrix, and the SVD, must be recomputed or an incremental update method (such as the one described in &lt;ref name="brand2006"/&gt;) be used.

== Additional uses of LSI ==

It is generally acknowledged that the ability to work with text on a semantic basis is essential to modern information retrieval systems.  As a result, the use of LSI has significantly expanded in recent years as earlier challenges in scalability and performance have been overcome.

LSI is being used in a variety of information retrieval and text processing applications, although its primary application has been for concept searching and automated document categorization.&lt;ref&gt;Dumais, S., Latent Semantic Analysis, ARIST Review of Information Science and Technology, vol. 38, 2004, Chapter 4.&lt;/ref&gt;   Below are some other ways in which LSI is being used:

* Information discovery&lt;ref&gt;Best Practices Commentary on the Use of Search and Information Retrieval Methods in E-Discovery, the Sedona Conference, 2007, pp. 189&#8211;223.&lt;/ref&gt;  ([[Electronic Discovery|eDiscovery]], Government/Intelligence community, Publishing)
* Automated document classification (eDiscovery, Government/Intelligence community, Publishing)&lt;ref&gt;Foltz, P. W. and Dumais, S. T. Personalized Information Delivery:  An analysis of information filtering methods, Communications of the ACM, 1992, 34(12), 51-60.&lt;/ref&gt;
* Text summarization&lt;ref&gt;Gong, Y., and Liu, X., Creating Generic Text Summaries, Proceedings, Sixth International Conference on Document Analysis and Recognition, 2001, pp. 903&#8211;907.&lt;/ref&gt;  (eDiscovery, Publishing)
* Relationship discovery&lt;ref&gt;Bradford, R., Efficient Discovery of New Information in Large Text Databases, Proceedings, IEEE International Conference on Intelligence and Security Informatics, Atlanta, Georgia, LNCS Vol. 3495, Springer, 2005, pp. 374&#8211;380.&lt;/ref&gt;  (Government, Intelligence community, Social Networking)
* Automatic generation of link charts of individuals and organizations&lt;ref&gt;Bradford, R., Application of Latent Semantic Indexing in Generating Graphs of Terrorist Networks, in: Proceedings, IEEE International Conference on Intelligence and Security Informatics, ISI 2006, San Diego, CA, USA, May 23&#8211;24, 2006, Springer, LNCS vol. 3975, pp. 674&#8211;675.&lt;/ref&gt;  (Government, Intelligence community)
* Matching technical papers and grants with reviewers&lt;ref&gt;Yarowsky, D., and Florian, R., Taking the Load off the Conference Chairs: Towards a Digital Paper-routing Assistant, Proceedings of the 1999 Joint SIGDAT Conference on Empirical Methods in NLP and Very-Large Corpora, 1999, pp. 220&#8211;230.&lt;/ref&gt;  (Government)
* Online customer support&lt;ref&gt;Caron, J., Applying LSA to Online Customer Support: A Trial Study, Unpublished Master's Thesis, May 2000.&lt;/ref&gt;  (Customer Management)
* Determining document authorship&lt;ref&gt;Soboroff, I., et al, Visualizing Document Authorship Using N-grams and Latent Semantic Indexing,   Workshop on New Paradigms in Information Visualization and Manipulation, 1997, pp. 43&#8211;48.&lt;/ref&gt;  (Education)
* Automatic keyword annotation of images&lt;ref&gt;Monay, F., and Gatica-Perez, D., On Image Auto-annotation with Latent Space Models, Proceedings of the 11th ACM international conference on Multimedia, Berkeley, CA, 2003, pp. 275&#8211;278.&lt;/ref&gt;
* Understanding software source code&lt;ref&gt;{{cite journal|author1=Maletic, J. |author2=Marcus, A.|title=Using Latent Semantic Analysis to Identify Similarities in Source Code to Support Program Understanding|journal=Proceedings of 12th IEEE International Conference on Tools with Artificial Intelligence|location=Vancouver, British Columbia|date=November 13&#8211;15, 2000|pages= 46&#8211;53|doi=10.1109/TAI.2000.889845|isbn=0-7695-0909-6}}&lt;/ref&gt;  (Software Engineering)
* Filtering [[Spam (electronic)|spam]]&lt;ref&gt;Gee, K., Using Latent Semantic Indexing to Filter Spam, in: Proceedings, 2003 ACM Symposium on Applied Computing, Melbourne, Florida, pp. 460&#8211;464.&lt;/ref&gt;  (System Administration)
* Information visualization&lt;ref name=landauer2004&gt;Landauer, T., Laham, D., and Derr, M., From Paragraph to Graph: Latent Semantic Analysis for Information Visualization, Proceedings of the National Academy of Sciences, 101, 2004, pp. 5214&#8211;5219.&lt;/ref&gt;
* [[Automated essay scoring|Essay scoring]]&lt;ref&gt;Foltz, Peter W., Laham, Darrell, and Landauer, Thomas K., Automated Essay Scoring: Applications to Educational Technology, Proceedings of EdMedia,  1999.&lt;/ref&gt;  (Education)
* [[Literature-based discovery]]&lt;ref&gt;Gordon, M., and Dumais, S., Using Latent Semantic Indexing for Literature Based Discovery, Journal of the American Society for Information Science, 49(8), 1998, pp. 674&#8211;685.&lt;/ref&gt;
* Stock resturns prediction&lt;ref name="Galvez2017"&gt;{{cite journal | url=http://www.sciencedirect.com/science/article/pii/S1877750317300091 | title=Assessing the usefulness of online message board mining in automatic stock prediction systems |author1=Ramiro H. G&#225;lvez |author2=Agust&#237;n Gravano | journal=Journal of Computational Science | volume=19 | pages=1877-7503 | year=2017  | doi=10.1016/j.jocs.2017.01.001}}&lt;/ref&gt;

LSI is increasingly being used for electronic document discovery (eDiscovery) to help enterprises prepare for litigation.  In eDiscovery, the ability to cluster, categorize, and search large collections of unstructured text on a conceptual basis is essential.  Concept-based searching using LSI has been applied to the eDiscovery process by leading providers as early as 2003.&lt;ref&gt;There Has to be a Better Way to Search, 2008, White Paper, Fios, Inc.&lt;/ref&gt;

== Challenges to LSI ==

Early challenges to LSI focused on scalability and performance.  LSI requires relatively high computational performance and memory in comparison to other information retrieval techniques.&lt;ref&gt;Karypis, G., Han, E., Fast Supervised Dimensionality Reduction Algorithm with Applications to Document Categorization and Retrieval, Proceedings of CIKM-00, 9th ACM Conference on Information and Knowledge Management.&lt;/ref&gt;  However, with the implementation of modern high-speed processors and the availability of inexpensive memory, these considerations have been largely overcome.  Real-world applications involving more than 30 million documents that were fully processed through the matrix and SVD computations are common in some LSI applications. A fully scalable (unlimited number of documents, online training) implementation of LSI is contained in the open source [[gensim]] software package.&lt;ref name="rehurek2011"&gt;{{cite journal | url=http://dx.doi.org/10.1007/978-3-642-20161-5_29 |format=PDF| title=Subspace Tracking for Latent Semantic Analysis | author=Radim &#344;eh&#367;&#345;ek | journal=Advances in Information Retrieval - 33rd European Conference on IR Research, ECIR 2011 | volume=6611 | pages=289&#8211;300 | year=2011 | doi=10.1007/978-3-642-20161-5_29 |series=Lecture Notes in Computer Science|isbn=978-3-642-20160-8}}&lt;/ref&gt;

Another challenge to LSI has been the alleged difficulty in determining the optimal number of dimensions to use for performing the SVD.  As a general rule, fewer dimensions allow for broader comparisons of the concepts contained in a collection of text, while a higher number of dimensions enable more specific (or more relevant) comparisons of concepts.  The actual number of dimensions that can be used is limited by the number of documents in the collection.  Research has demonstrated that around 300 dimensions will usually provide the best results with moderate-sized document collections (hundreds of thousands of documents) and perhaps 400 dimensions for larger document collections (millions of documents).&lt;ref&gt;Bradford, R., An Empirical Study of Required Dimensionality for Large-scale Latent Semantic Indexing Applications, Proceedings of the 17th ACM Conference on Information and Knowledge Management, Napa Valley, California, USA, 2008, pp. 153&#8211;162.&lt;/ref&gt;   However, recent studies indicate that 50-1000 dimensions are suitable depending on the size and nature of the document collection.&lt;ref name=landauer2008&gt;Landauer, Thomas K., and Dumais, Susan T., Latent Semantic Analysis, Scholarpedia, 3(11):4356, 2008.&lt;/ref&gt;

Checking the amount of variance in the data after computing the SVD can be used to determine the optimal number of dimensions to retain.  The variance contained in the data can be viewed by plotting the singular values (S) in a [[scree plot]].  Some LSI practitioners select the dimensionality associated with the knee of the curve as the cut-off point for the number of dimensions to retain.  Others argue that some quantity of the variance must be retained, and the amount of variance in the data should dictate the proper dimensionality to retain.  Seventy percent is often mentioned as the amount of variance in the data that should be used to select the optimal dimensionality for recomputing the SVD.&lt;ref&gt;Cangelosi, R., Goriely A., Component Retention In Principal Component Analysis With Application to Cdna Microarray Data, BMC Biology Direct 2(2) (2007).&lt;/ref&gt;&lt;ref&gt;Jolliffe, L. T., Principal Component Analysis, Springer-Verlag, New York, (1986).&lt;/ref&gt;&lt;ref&gt;Hu, X., Z. Cai, et al., LSA: First Dimension and Dimensional Weighting, 25th Annual Meeting of the Cognitive Science Society, Boston, MA.&lt;/ref&gt;

== See also ==
* [[Compound term processing]]
* [[Explicit semantic analysis]]
* [[Latent semantic mapping]]
* [[Latent Semantic Structure Indexing]]
* [[Principal components analysis]]
* [[Probabilistic latent semantic analysis]]
* [[Spamdexing]]
* [[Topic model]]
** [[Latent Dirichlet allocation]]
* [[Distributional semantics]]
* [[Coh-Metrix]]

== References ==
{{Reflist|30em}}

==Further reading==
* {{cite journal
 | url=http://lsa.colorado.edu/papers/dp1.LSAintro.pdf
 | format=PDF
 | title=Introduction to Latent Semantic Analysis
 | author-link1= Thomas Landauer |first1=Thomas |last1=Landauer |first2=Peter W. |last2=Foltz |first3=Darrell |last3=Laham
 | journal=Discourse Processes
 | volume=25
 | pages=259&#8211;284
 | year=1998
 | doi=10.1080/01638539809545028
 | issue=2&#8211;3
}}
* {{cite journal
 | url=http://lsi.research.telcordia.com/lsi/papers/JASIS90.pdf 
 | format=PDF| title=Indexing by Latent Semantic Analysis
 | first1=Scott |last1=Deerwester |first2=Susan T. |last2=Dumais |first3=George W. |last3=Furnas |first4=Thomas K. |last4=Landauer |first5=Richard |last5=Harshman
 | author-link1=Scott Deerwester |author-link2=Susan Dumais |author-link3=George Furnas |author-link4=Thomas Landauer |author-link5=Richard Harshman
 | journal=Journal of the American Society for Information Science
 | volume=41
 | issue=6
 | pages=391&#8211;407
 | year=1990 
 | doi=10.1002/(SICI)1097-4571(199009)41:6&lt;391::AID-ASI1&gt;3.0.CO;2-9
}} Original article where the model was first exposed.
* {{cite journal
 | url=http://citeseer.ist.psu.edu/berry95using.html
 | title=Using Linear Algebra for Intelligent Information Retrieval
 | first1=Michael |last1= Berry |first2=Susan T. |last2=Dumais  |first3=Gavin W. |last3=O'Brien
 | author-link1=Susan Dumais
 |year=1995
}} [http://lsirwww.epfl.ch/courses/dis/2003ws/papers/ut-cs-94-270.pdf (PDF)]. Illustration of the application of LSA to document retrieval.
* {{cite web
 | url=http://iv.slis.indiana.edu/sw/lsa.html
 | title=Latent Semantic Analysis
 | publisher=InfoVis
}}
* {{cite web
 | url=http://cran.at.r-project.org/web/packages/lsa/index.html
 | title=An Open Source LSA Package for R
 | publisher=CRAN
 | author=Fridolin Wild
 | date=November 23, 2005
 | accessdate=November 20, 2006
}}
* {{ cite web
 | url=http://www.welchco.com/02/14/01/60/96/02/2901.HTM
 | title=A Solution to Plato's Problem: The Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge
 | author=[[Thomas Landauer]], [[Susan Dumais|Susan T. Dumais]]
 | accessdate=2007-07-02
}}

==External links==

===Articles on LSA===
* [http://www.scholarpedia.org/article/Latent_semantic_analysis Latent Semantic Analysis], a scholarpedia article on LSA written by Tom Landauer, one of the creators of LSA.

===Talks and demonstrations===
* [http://videolectures.net/slsfs05_hofmann_lsvm/ LSA Overview], talk by Prof. [http://www.cs.brown.edu/~th/ Thomas Hofmann] describing LSA, its applications in Information Retrieval, and its connections to [[probabilistic latent semantic analysis]].
* [http://www.semanticquery.com/archive/semanticsearchart/researchLSA.html Complete LSA sample code in C# for Windows]. The demo code includes enumeration of text files, filtering stop words, stemming, making a document-term matrix and SVD.

===Implementations===

Due to its cross-domain applications in [[Information Retrieval]], [[Natural Language Processing]] (NLP), [[Cognitive Science]] and [[Computational Linguistics]], LSA has been implemented to support many different kinds of applications.
* [http://www.d.umn.edu/~tpederse/senseclusters.html Sense Clusters], an Information Retrieval-oriented perl implementation of LSA
* [http://code.google.com/p/airhead-research/ S-Space Package], a Computational Linguistics and Cognitive Science-oriented Java implementation of LSA
* [http://code.google.com/p/semanticvectors/ Semantic Vectors] applies Random Projection, LSA, and Reflective Random Indexing to [[Lucene]] term-document matrices
* [http://infomap-nlp.sourceforge.net/ Infomap Project], an NLP-oriented C implementation of LSA (superseded by semanticvectors project)
* [http://scgroup20.ceid.upatras.gr:8000/tmg/index.php/Main_Page Text to Matrix Generator], A MATLAB Toolbox for generating term-document matrices from text collections, with support for LSA
* [[Gensim]] contains a Python implementation of LSA for matrices larger than RAM.

[[Category:Information retrieval techniques]]
[[Category:Natural language processing]]
[[Category:Latent variable models]]</text>
      <sha1>q00jsvi1va7q04kjpowabj0pshg9b3j</sha1>
    </revision>
  </page>
  <page>
    <title>Enterprise search</title>
    <ns>0</ns>
    <id>12101316</id>
    <revision>
      <id>751208693</id>
      <parentid>751208272</parentid>
      <timestamp>2016-11-24T02:04:16Z</timestamp>
      <contributor>
        <username>Me, Myself, and I are Here</username>
        <id>17619453</id>
      </contributor>
      <comment>Filled in 3 bare reference(s) with [[:en:WP:REFILL|reFill]] ()</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9817" xml:space="preserve">{{original research|date=November 2015}}
'''Enterprise search''' is the practice of making content from multiple enterprise-type sources, such as [[database]]s and [[intranet]]s, searchable to a defined audience.

"Enterprise search" is used to describe the software of search information within an enterprise (though the search function and its results may still be public).&lt;ref&gt;{{cite web|url=http://www.aiim.org/What-is-Enterprise-Search|title=What is Enterprise Search?|publisher=}}&lt;/ref&gt; Enterprise search can be contrasted with [[web search]], which applies search technology to documents on the open web, and [[desktop search]], which applies search technology to the content on a single computer.

Enterprise search systems index data and documents from a variety of sources such as: [[file systems]], [[intranets]], [[document management system]]s, [[e-mail]], and [[databases]]. Many enterprise search systems integrate structured and unstructured data in their collections.&lt;ref&gt;[http://www.arma.org/bookstore/files/Delgado.pdf The New Face of Enterprise Search: Bridging Structured and Unstructured Information]&lt;/ref&gt; Enterprise search systems also use access controls to enforce a security policy on their users.&lt;ref&gt;{{cite web|url=http://www.ideaeng.com/tabId/98/itemId/118/Mapping-Security-Requirements-to-Enterprise-Search.aspx|title=Security Requirements to Enterprise Search: part 1 - New Idea Engineering|publisher=}}&lt;/ref&gt;

Enterprise search can be seen as a type of [[vertical search]] of an enterprise.

==Components of an enterprise search system==
In an enterprise search system, content goes through various phases from source repository to search results:

=== Content awareness ===
Content awareness (or "content collection") is usually either a push or pull model. In the push model, a source system is integrated with the search engine in such a way that it connects to it and pushes new content directly to its [[API]]s. This model is used when realtime indexing is important. In the pull model, the software gathers content from sources using a connector such as a [[web crawler]] or a [[database]] connector. The connector typically polls the source with certain intervals to look for new, updated or deleted content.&lt;ref&gt;{{cite web|url=http://www.information-management.com/issues/20_7/content_management_data_integration_indexing_metadata-10019105-1.html|title=Understanding Content Collection and Indexing|publisher=}}&lt;/ref&gt;

=== Content processing and analysis ===
Content from different sources may have many different formats or document types, such as XML, HTML, Office document formats or plain text. The content processing phase processes the incoming documents to plain text using document filters. It is also often necessary to normalize content in various ways to improve [[Recall (information retrieval)|recall]] or [[Precision (information retrieval)|precision]]. These may include [[stemming]], [[lemmatization]], [[synonym]] expansion, [[entity extraction]], [[part of speech]] tagging.

As part of processing and analysis, [[tokenization (lexical analysis)|tokenization]] is applied to split the content into [[Lexical analysis#Token|tokens]] which is the basic matching unit. It is also common to normalize tokens to lower case to provide case-insensitive search, as well as to normalize accents to provide better recall.

=== Indexing ===
The resulting text is stored in an [[Index (search engine)|index]], which is optimized for quick lookups without storing the full text of the document. The index may contain the dictionary of all unique words in the corpus as well as information about ranking and [[term frequency]].

=== Query processing ===
Using a web page, the user issues a [[Web search query|query]] to the system. The query consists of any terms the user enters as well as navigational actions such as [[faceted search|faceting]] and paging information.

=== Matching ===
The processed query is then compared to the stored index, and the search system returns results (or "hits") referencing source documents that match. Some systems are able to present the document as it was indexed.

==Differences from web search==
{{unreferenced section|date=November 2015}}
Beyond the difference in the kinds of materials being indexed, enterprise search systems also typically include functionality that is not associated with the mainstream [[web search engine]]s. These include:
*Adapters to index content from a variety of repositories, such as [[databases]] and [[content management systems]].
*[[Federated search]], which consists of
# transforming a query and broadcasting it to a group of disparate databases or external content sources with the appropriate syntax,
# merging the results collected from the databases,
# presenting them in a succinct and unified format with minimal duplication, and
# providing a means, performed either automatically or by the portal user, to sort the merged result set.
*[[Enterprise bookmarking]], collaborative [[tag (metadata)|tagging]] systems for capturing knowledge about structured and semi-structured enterprise data.
*[[Entity extraction]] that seeks to locate and classify elements in text into predefined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc.
*[[Faceted search]], a technique for accessing a collection of information represented using a [[faceted classification]], allowing users to explore by filtering available information.
*Access control, usually in the form of an [[Access control list]] (ACL), is often required to restrict access to documents based on individual user identities. There are many types of access control mechanisms for different content sources making this a complex task to address comprehensively in an enterprise search environment (see below).
*[[Text clustering]], which groups the top several hundred search results into topics that are computed on the fly from the search-results descriptions, typically titles, excerpts (snippets), and meta-data.  This technique lets users navigate the content by topic rather than by the meta-data that is used in faceting. Clustering compensates for the problem of incompatible meta-data across multiple enterprise repositories, which hinders the usefulness of faceting.
*[[User interfaces]], which in web search are deliberately kept simple in order not to distract the user from clicking on ads, which generates the revenue.  Although the business model for enterprise search could include showing ads, in practice this is not done.  To enhance end user productivity, enterprise vendors continually experiment with rich UI functionality which occupies significant screen space, which would be problematic for web search.

==Relevance factors for enterprise search==
{{unreferenced section|date=November 2015}}
The factors that determine the relevance of search results within the context of an enterprise overlap with but are different from those that apply to web search. In general, enterprise search engines cannot take advantage of the rich [[hyperlink|link structure]] as is found on the web's [[hypertext]] content, however, a new breed of Enterprise search engines based on a bottom-up [[Web 2.0]] technology are providing both a contributory approach and [[hyperlink]]ing within the enterprise. Algorithms like [[PageRank]] exploit hyperlink structure to assign authority to documents, and then use that authority as a query-independent relevance factor. In contrast, enterprises typically have to use other query-independent factors, such as a document's recency or popularity, along with query-dependent factors traditionally associated with [[information retrieval]] algorithms.  Also, the rich functionality of enterprise search UIs, such as clustering and faceting, diminish reliance on ranking as the means to direct the user's attention.

==Access control: early binding vs late binding==
Security and restricted access to documents is an important matter in enterprise search. There are two main approaches to apply restricted access: early binding vs late binding.&lt;ref&gt;[http://enterprisesearch.co/enterprise-search-document-access-control/ Enterprise Search: document access control]&lt;/ref&gt;

===Late binding===
Permissions are analyzed and assigned to documents at query stage. Query engine generates a document set and before returning it to a user this set is filtered based on user access rights. It is costly process but accurate (based on user permissions at the moment of query).

===Early binding===
Permissions are analyzed and assigned to documents at indexing stage. It is much more effective than late binding, but could be inaccurate (user might be granted or revoked permissions between in the period between indexing and querying).

==Search relevance testing options==
Search application relevance can be determined by following relevance testing options like&lt;ref&gt;[http://searchhub.org/2009/09/02/debugging-search-application-relevance-issues/  Debugging Search Application Relevance Issues]&lt;/ref&gt;
*Focus groups
*Reference evaluation protocol (based on relevance judgements of results from agreed-upon queries performed against common document corpuses)
*Empirical testing
*[[A/B testing]]
*Log analysis on a Beta production site
*Online ratings

==See also==
*[[Collaborative search engine]]
*[[Comparison of enterprise search software]]
*[[Data defined storage]] 
*[[Enterprise bookmarking]]
*[[Enterprise information access]]
*[[Faceted search]]
*[[Information extraction]]
*[[Knowledge management]]
*[[List of enterprise search vendors]]
*[[List of search engines]]
*[[Text mining]]
*[[Vertical search]]

==References==
{{Reflist}}

{{DEFAULTSORT:Enterprise Search}}
[[Category:Information retrieval genres]]</text>
      <sha1>68jitf88jrca0kg84h7uh75yeg0h19s</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Personalized search</title>
    <ns>14</ns>
    <id>38954543</id>
    <revision>
      <id>727440755</id>
      <parentid>666717227</parentid>
      <timestamp>2016-06-29T01:05:28Z</timestamp>
      <contributor>
        <ip>70.51.200.20</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="91" xml:space="preserve">{{catmore}}

[[Category:Information retrieval genres]]
[[Category:Internet search engines]]</text>
      <sha1>bfm95ck6s3gvekknwd6uwnbykr9t0o9</sha1>
    </revision>
  </page>
  <page>
    <title>Exploratory search</title>
    <ns>0</ns>
    <id>4881262</id>
    <revision>
      <id>715773250</id>
      <parentid>715575692</parentid>
      <timestamp>2016-04-17T22:42:41Z</timestamp>
      <contributor>
        <username>Dtunkelang</username>
        <id>5293022</id>
      </contributor>
      <comment>/* Major figures */ updated Nick Belkin's home page URL</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="12623" xml:space="preserve">'''Exploratory search''' is a specialization of information exploration which represents the activities carried out by searchers who are:&lt;ref&gt;Ryen W. White and Resa A. Roth (2009). ''Exploratory Search: Beyond the Query-Response Paradigm'', San Rafael, CA: Morgan and Claypool.&lt;/ref&gt;
* unfamiliar with the domain of their goal (i.e. need to learn about the topic in order to understand how to achieve their goal)
* or unsure about the ways to achieve their goals (either the technology or the process)
* or unsure about their goals in the first place.

Consequently, exploratory search covers a broader class of activities than typical [[information retrieval]], such as investigating, evaluating, comparing, and synthesizing, where new information is sought in a defined conceptual area; [[exploratory data analysis]] is another example of an information exploration activity. Typically, therefore, such users generally combine querying and browsing strategies to foster learning and investigation.

==History==
Exploratory search is a topic that has grown from the fields of [[information retrieval]] and [[information seeking]] but has become more concerned with alternatives to the kind of search that has received the majority of focus (returning the most relevant documents to a [[Google]]-like keyword search). The research is motivated by questions like "what if the user doesn't know which keywords to use?" or "what if the user isn't looking for a single answer?". Consequently, research has begun to focus on defining the broader set of ''information behaviors'' in order to learn about the situations when a user is, or feels, limited by only having the ability to perform a keyword search.

In the last few years,{{When|date=April 2016}} a series of workshops has been held at various related and key events. In 2005, the Exploratory Search Interfaces workshop focused on beginning to define some of the key challenges in the field.&lt;ref&gt;{{cite web|url=http://research.microsoft.com/~ryenw/xsi/index.html|title=HCIL SOH 2005 Workshop on Exploratory Search Interfaces|publisher=Microsoft|accessdate=8 April 2016}}&lt;/ref&gt; Since then a series of other workshops has been held at related conferences: Evaluating Exploratory Search&lt;ref&gt;{{cite web|url=http://research.microsoft.com/~ryenw/eess/index.html|title=SIGIR 2006 Workshop - Evaluating Exploratory Search Systems|publisher=Microsoft|accessdate=8 April 2016}}&lt;/ref&gt; at SIGIR06&lt;ref&gt;{{cite web|url=http://www.sigir2006.org|title=Sigir 2006|publisher=|accessdate=8 April 2016}}&lt;/ref&gt; and Exploratory Search and HCI&lt;ref&gt;{{cite web|url=http://research.microsoft.com/~ryenw/esi/index.html|title=CHI 2007 Workshop - Exploratory Search and HCI|publisher=Microsoft|accessdate=8 April 2016}}&lt;/ref&gt; at CHI07&lt;ref&gt;{{cite web|url=http://www.chi2007.org|title=CHI 2007 Reach Beyond - welcome|publisher=|accessdate=8 April 2016}}&lt;/ref&gt; (in order to meet with the experts in [[human&#8211;computer interaction]]).

In March 2008, an ''Information Processing and Management'' special issue&lt;ref&gt;{{cite web|url=http://www.sciencedirect.com/science/journal/03064573|title=Information Processing &amp; Management|publisher=|accessdate=8 April 2016}}&lt;/ref&gt;&lt;ref&gt;Ryen W. White, Gary Marchionini, Gheorghe Muresan (2008). ''Evaluating exploratory search systems: Introduction to special topic issue of information processing and management'' Vol. 44, Issue 2, (2008), pp.&amp;nbsp;433&#8211;436&lt;/ref&gt; focused particularly on the challenges of evaluating exploratory search, given the reduced assumptions that can be made about scenarios of use.

In June 2008, the [[National Science Foundation]] sponsored an invitational workshop to identify a research agenda for exploratory search and similar fields for the coming years.&lt;ref&gt;{{cite web|url=http://www.ils.unc.edu/ISSS_workshop/|title=Moved|publisher=|accessdate=8 April 2016}}&lt;/ref&gt;

==Research challenges==

===Important scenarios===
With the majority of research in the [[information retrieval]] community focusing on typical keyword search scenarios, one challenge for exploratory search is to further understand the scenarios of use for when keyword search is not sufficient. An example scenario, often used to motivate the research by mSpace,&lt;ref&gt;[http://mspace.fm mSpace]&lt;/ref&gt; states: if a user does not know much about classical music, how should they even begin to find a piece that they might like.

===Designing new interfaces===
With one of the motivations being to support users when keyword search is not enough, some research has focused on identifying alternative user interfaces and interaction models that support the user in different ways. An example is [[Faceted classification|faceted search]] which presents diverse category-style options to the users, so that they can choose from a list instead of guess a possible keyword query.

Many of the [[human&#8211;computer information retrieval|interactive forms of search]], including [[faceted browser]]s, are being considered for their support of exploratory search conditions.

Computational cognitive models of exploratory search have been developed to capture the cognitive complexities involved in exploratory search. Model-based dynamic presentation of information cues are proposed to facilitate exploratory search performance.&lt;ref&gt;Fu, W.-T., Kannampalill, T. G., &amp; Kang, R. (2010). [http://portal.acm.org/citation.cfm?id=1719970.1719998 Facilitating exploratory search by model-based navigational cues.] In Proceedings of the ACM International conference on Intelligent User Interface. 199&#8211;208. &lt;/ref&gt;

===Evaluating interfaces===
As the tasks and goals involved with exploratory search are largely undefined or unpredictable, it is very hard to evaluate systems with the measures often used in information retrieval. Accuracy was typically used to show that a user had found a correct answer, but when the user is trying to summarize a domain of information, the ''correct'' answer is near impossible to identify, if not entirely subjective (for example: possible hotels to stay in Paris). In exploration, it is also arguable that spending more time (where time efficiency is typically desirable) researching a topic shows that a system provides increased support for investigation. Finally, and perhaps most importantly, giving study participants a well specified task could immediately prevent them from exhibiting exploratory behavior.{{cn|date=April 2016}}

===Models of exploratory search behavior===
There have been recent{{When|date=April 2016}} attempts to develop a process model of exploratory search behavior, especially in social information system (e.g., see [[models of collaborative tagging]].&lt;ref&gt;{{Citation
  | doi = 10.1145/1460563.1460600
  | last1 = Fu  | first1 = Wai-Tat
  | title = The Microstructures of Social Tagging: A Rational Model
  | journal = Proceedings of the ACM 2008 conference on Computer Supported Cooperative Work.
  | pages = 66&#8211;72
  | date = April 2008
  | url = http://portal.acm.org/citation.cfm?id=1460600
  | isbn = 978-1-60558-007-4 }}
&lt;/ref&gt;
&lt;ref&gt;{{Citation
  | last1 = Fu  | first1 = Wai-Tat
  | title = A Semantic Imitation Model of Social Tagging
  | journal = Proceedings of the IEEE conference on Social Computing
  | pages = 66&#8211;72
  | date = Aug 2009
  | url = http://www.humanfactors.illinois.edu/Reports&amp;PapersPDFs/IEEESocialcom09/A%20Semantic%20Imitation%20Model%20of%20Social%20Tag%20Choices%20(2).pdf }}&lt;/ref&gt; The process model assumes that user-generated information cues, such as social tags, can act as navigational cues that facilitate exploration of information that others have found and shared with other users on a social information system (such as [[social bookmarking]] system). These models provided extension to existing process model of information search that characterizes information-seeking behavior in traditional fact-retrievals using search engines.&lt;ref&gt;
{{Citation
  | last1 = Fu  | first1 = Wai-Tat
  | last2 = Pirolli  | first2 = Peter
  | title = SNIF-ACT: a cognitive model of user navigation on the world wide web
  | journal = Human-Computer Interaction
  | pages = 335&#8211;412
  | year = 2007
  | url = http://portal.acm.org/citation.cfm?id=1466608
  | volume = 22}}&lt;/ref&gt;&lt;ref&gt;Kitajima, M., Blackmon, M. H., &amp; Polson, P. G. (2000). A comprehension-based model of Web navigation and its application to Web usability analysis. In S. Mc-Donald, Y. Waern, &amp; G. Cockton (Eds.), People and computers XIV&#8212;Usability or else!
New York: Springer-Verlag.&lt;/ref&gt;&lt;ref&gt;Miller, C. S., &amp; Remington, R.W. (2004). Modeling information navigation: Implications for information architecture. Human Computer Interaction, 19, 225&#8211;271.&lt;/ref&gt;
Recent{{When|date=April 2016}} development in exploratory search is often concentrated in predicting users' search intents in interaction with the user.&lt;ref&gt;
{{Citation
  | last1 = Ruotsalo  | first1 = Tuukka
  | last2 = Athukorala  | first2 = Kumaripaba
  | last3 = Glowacka  | first3 = Dorota
  | last4 = Konuyshkova  | first4 = Ksenia
  | last5 = Oulasvrita  | first5 = Antti
  | last6 = Kaipiainen  | first6 = Samuli
  | last7 = Kaski  | first7 = Samuel
  | last8 = Jacucci  | first8 = Giulio
  | title = Supporting exploratory search tasks with interactive user modeling
  | journal = Proceedings of the 76th Annual Meeting of the American Society for Information Science and Technology ASIS&amp;T
  | year = 2013}}
&lt;/ref&gt;
Such predictive user modeling, also referred as intent modeling, can help users to get accustomed to a body of domain knowledge and help users to make sense of the potential directions to be explored around their initial, often vague, expression of information needs.&lt;ref&gt;
{{Citation
  | last1 = Ruotsalo  | first1 = Tuukka
  | last2 = Peltonen  | first2 = Jaakko
  | last3 = Eugster | first3 = Manuel J.A.
  | last4 = Glowacka  | first4 = Dorota
  | last5 = Konuyshkova  | first5 = Ksenia
  | last6 = Athukorala  | first6 = Kumaripaba
  | last7 = Kosunen | first7 = Ilkka   
  | last8 = Reijonen  | first8 = Aki
  | last9 = Myllym&#228;ki | first9 = Petri
  | last10 = Kaski  | first10 = Samuel
  | last11 = Jacucci  | first11 = Giulio
  | title = Directing Exploratory Search with Interactive Intent Modeling
  | journal = Proceedings of the ACM Conference of Information and Knowledge Management CIKM
  | year = 2013}}
&lt;/ref&gt;
&lt;ref&gt;
{{Citation
  | last1 = Glowacka  | first1 = Dorota
  | last2 = Ruotsalo  | first2 = Tuukka
  | last3 = Konuyshkova  | first3 = Ksenia
  | last4 = Athukorala  | first4 = Kumaripaba
  | last5 = Kaski  | first5 = Samuel
  | last6 = Jacucci  | first6 = Giulio
  | title = Directing exploratory search: Reinforcement learning from user interactions with keywords
  | journal = Proceedings of the ACM Conference of Intelligent User Interfaces IUI
  | url = http://dl.acm.org/citation.cfm?id=2449413
  | pages = 117&#8211;128
  | year = 2013}}
&lt;/ref&gt;

==Major figures==
Key figures, including experts from both [[information seeking]] and [[human&#8211;computer interaction]], are:{{Says who|date=April 2016}}
* [[Marcia Bates]]
* Nicholas Belkin&lt;ref&gt;{{cite web|url=http://comminfo.rutgers.edu/~belkin/|title=Nick's home page|publisher=|accessdate=17 April 2016}}&lt;/ref&gt;
* Gary Marchionini&lt;ref&gt;{{cite web|url=http://ils.unc.edu/~march|title=Gary's Home Page|publisher=|accessdate=8 April 2016}}&lt;/ref&gt;
* m.c. schraefel&lt;ref&gt;{{cite web|url=http://users.ecs.soton.ac.uk/mc|title=m.c. schraefel: design for innovation, creativity, discovery|publisher=|accessdate=8 April 2016}}&lt;/ref&gt;
* Ryen White&lt;ref&gt;{{cite web|url=http://research.microsoft.com/~ryenw|title=Ryen W. White|publisher=Microsoft|accessdate=8 April 2016}}&lt;/ref&gt;

==References==
&lt;references /&gt;

==Sources==
# White, R.W., Kules, B., Drucker, S.M., and schraefel, m.c. (2006). ''Supporting Exploratory Search'', Introduction to Special Section of Communications of the ACM, Vol. 49, Issue 4, (2006), pp.&amp;nbsp;36&#8211;39.
# Ryen W. White, Gary Marchionini, Gheorghe Muresan (2008). ''Evaluating exploratory search systems: Introduction to special topic issue of information processing and management'' Vol. 44, Issue 2, (2008), pp.&amp;nbsp;433&#8211;436
# Ryen W. White and Resa A. Roth (2009). ''Exploratory Search: Beyond the Query-Response Paradigm'', San Rafael, CA: Morgan and Claypool.
# P. Papadakos, S. Kopidaki, N. Armenatzoglou and Y. Tzitzikas (2009). ''Exploratory Web Searching with Dynamic Taxonomies and Results Clustering'',13th European Conference on Digital Libraries (ECDL'09), Corfu, Greece, Sep-Oct 2009

{{DEFAULTSORT:Exploratory Search}}
[[Category:Human&#8211;computer interaction]]
[[Category:Information retrieval genres]]
[[Category:Information science]]</text>
      <sha1>3ggj6me67fgkdgl700myz5ruxjj8uia</sha1>
    </revision>
  </page>
  <page>
    <title>Audio mining</title>
    <ns>0</ns>
    <id>14004969</id>
    <revision>
      <id>666861886</id>
      <parentid>545015261</parentid>
      <timestamp>2015-06-14T05:55:42Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>Category:Information retrieval genres, Category:Music information retrieval</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1827" xml:space="preserve">{{unreferenced|date=January 2012}}
'''Audio mining''' is a technique by which the content of an audio signal can be automatically analysed and searched. It is most commonly used in the field of [[speech recognition|automatic speech recognition]], where the analysis tries to identify any speech within the audio. The audio will typically be processed by a speech recognition system in order to identify word or [[phoneme]] units that are likely to occur in the spoken content. This information may either be used immediately in pre-defined searches for keywords or phrases (a real-time "word spotting" system), or the output of the speech recogniser may be stored in an index file. One or more audio mining index files can then be loaded at a later date in order to run searches for keywords or phrases.

The results of a search will normally be in terms of hits, which are regions within files that are good matches for the chosen keywords. The user may then be able to listen to the audio corresponding to these hits in order to verify if a correct match was found.

Audio mining systems used in the field of speech recognition are often divided into two groups: those that use [[Large Vocabulary Continuous Speech Recogniser]]s (LVCSR) and those that use phonetic recognition. 

Musical audio mining (also known as [[music information retrieval]]) relates to the identification of perceptually important characteristics of a piece of music such as melodic, harmonic or rhythmic structure. Searches can then be carried out to find pieces of music that are similar in terms of their melodic, harmonic and/or rhythmic characteristics.

==See also==
* [[Speech Analytics]]


[[Category:Speech recognition]]
[[Category:Music information retrieval]]
[[Category:Information retrieval genres]]
[[Category:Computational linguistics]]</text>
      <sha1>bqhv14kmvpgp0oeq98cum362ludlrz5</sha1>
    </revision>
  </page>
  <page>
    <title>Susan Dumais</title>
    <ns>0</ns>
    <id>2232087</id>
    <revision>
      <id>736646137</id>
      <parentid>706527914</parentid>
      <timestamp>2016-08-28T22:37:23Z</timestamp>
      <contributor>
        <username>GorillaWarfare</username>
        <id>4968133</id>
      </contributor>
      <comment>not a stub</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5618" xml:space="preserve">{{ Infobox scientist
| name              = Susan T. Dumais
| image             = Susan Dumais.jpg
| image_size        = 200px
| caption           = Susan Dumais in 2009 in her office at Microsoft Research.
| birth_date        = 
| birth_place       = [[Maine]], [[United States|US]]  
| death_date        = 
| death_place       = 
| nationality       = American
| fields            = [[Computer Science]]
| workplaces        = [[Microsoft Research]]
| alma_mater        = [[Indiana University]] &lt;br /&gt;[[Bates College]]
| doctoral_advisor  = 
| doctoral_students = 
| known_for         = [[Human Computer Interaction]]&lt;br /&gt; [[Information Retrieval]]
| website           = {{URL|http://research.microsoft.com/~sdumais/}} 
| awards            = ACM-W Athena Lecturer Award (2014)
}}

'''Susan Dumais''' is an American computer scientist who is a leader in the field of information retrieval, and has been a significant contributor to Microsoft's search technologies.&lt;ref&gt;{{cite news|title=100 Top Women in Seattle Tech|url=http://www.bizjournals.com/seattle/blog/techflash/2009/05/Top_100_Women_in_Seattle_Tech_44225472.html|accessdate=23 February 2016|newspaper=Puget Sound Business Journal|date=8 May 2009}}&lt;/ref&gt;
According to Mary Jane Irwin, who heads the Athena Lecture awards committee, &#8220;Her sustained contributions have shaped the thinking and direction of human-computer interaction and information retrieval."&lt;ref&gt;{{cite news|last=Burns|first=Jay|title=Microsoft&#8217;s Susan Dumais &#8217;75 Is a Big Reason Why, Computer-Wise, You Find What You Seek|url=https://www.bates.edu/news/2014/05/01/microsoft-susan-dumais-75/|accessdate=23 February 2016|newspaper=Bates News|date=28 October 2015}}&lt;/ref&gt;

==Biography==

Susan Dumais is a Distinguished Scientist at Microsoft and deputy managing director of the [[Microsoft Research]] lab in Redmond. She is also an Affiliate Professor at the [[University of Washington Information School]].

Before joining Microsoft in 1997, Dumais was a researcher at Bellcore (now [[Telcordia Technologies]]), where she and her colleagues conducted research into what is now called the [[vocabulary problem]] in [[information retrieval]].&lt;ref&gt;{{cite journal
| title=The Vocabulary Problem in Human-System Communication
| journal=Communications of the ACM
| author=[[George Furnas|G. W. Furnas]], [[Thomas Landauer|T. K. Landauer]], L. M. Gomez, S. T. Dumais
| volume = 30
| pages = 964&#8211;971
| year = 1987
| url = http://citeseer.ist.psu.edu/furnas87vocabulary.html
| doi=10.1145/32206.32212
| issue = 11
}}&lt;/ref&gt; Their study demonstrated, through a variety of experiments, that different people use different vocabulary to describe the same thing, and that even choosing the "best" term to describe something is not enough for others to find it.  One implication of this work is that because the author of a document may use different vocabulary than someone searching for the document, traditional [[information retrieval]] methods will have limited success.

Dumais and the other Bellcore researchers then began investigating ways to build search systems that avoided the vocabulary problem.  The result was their invention of [[Latent Semantic Indexing]].&lt;ref&gt;
 {{cite journal
 | url=http://lsi.research.telcordia.com/lsi/papers/JASIS90.pdf 
 | title=Indexing by Latent Semantic Analysis
 | author=[[Scott Deerwester|S. Deerwester]], Susan Dumais, [[George Furnas|G. W. Furnas]], [[Thomas Landauer|T. K. Landauer]], [[Richard Harshman|R. Harshman]]
 | journal=Journal of the American Society for Information Science
 | volume=41
 | issue=6
 | pages=391&#8211;407
 |year=1990 
 | doi=10.1002/(SICI)1097-4571(199009)41:6&lt;391::AID-ASI1&gt;3.0.CO;2-9
}}&lt;/ref&gt;

==Awards==

In 2006, Dumais was inducted as a [[Fellow]] of the [[Association for Computing Machinery]]. In 2009, she received the [[Gerard Salton Award]], an information retrieval lifetime achievement award. In 2011, she was inducted to the [[National Academy of Engineering]] for innovation and leadership in organizing, accessing, and interacting with information. In 2014, Dumais received the Athena Lecturer Award for "fundamental contributions to computer science.".&lt;ref&gt;{{cite news|last=Knies|first=Rob|title=Dumais Receives Athena Lecturer Award|url=http://blogs.technet.com/b/inside_microsoft_research/archive/2014/04/08/dumais-receives-athena-lecturer-award.aspx|accessdate=28 April 2014|newspaper=Inside Microsoft Research|date=April 2014}}&lt;/ref&gt; and the [[Tony Strix]] Award for "sustained contributions that are both innovative and practical" with "significant impact". &lt;ref&gt;{{cite web|title=The winner of the 2014 Tony Kent Strix Award is Dr Susan Dumais|url=http://www.ukeig.org.uk/awards/tony-kent-strix|accessdate=17 September 2014}}&lt;/ref&gt;
In 2015, she was inducted into the [[American Academy of Arts and Sciences]].&lt;ref&gt;{{cite news|last=Tice|first=Lindsay|title=Lewiston native inducted into American Academy of Arts and Sciences|url=http://www.sunjournal.com/news/lewiston-auburn/0001/11/30/lewiston-native-inducted-american-academy-arts-and-sciences/1808943|accessdate=23 February 2016|newspaper=Lewinston-Auburn Sun-Journal|date=28 October 2015}}&lt;/ref&gt;

==References==
{{reflist}}

==External links==
* [http://research.microsoft.com/~sdumais/ Home page at Microsoft Research]

{{DEFAULTSORT:Dumais, Susan}}
[[Category:People in information technology]]
[[Category:Fellows of the Association for Computing Machinery]]
[[Category:Microsoft employees]]
[[Category:Living people]]
[[Category:Women computer scientists]]
[[Category:University of Washington faculty]]
[[Category:Information retrieval researchers]]</text>
      <sha1>2ykimtt87um12yxv97oszb6vnqa2xx5</sha1>
    </revision>
  </page>
  <page>
    <title>Jaime Teevan</title>
    <ns>0</ns>
    <id>31120153</id>
    <revision>
      <id>759866842</id>
      <parentid>759866553</parentid>
      <timestamp>2017-01-13T16:05:04Z</timestamp>
      <contributor>
        <ip>50.47.104.39</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4739" xml:space="preserve">{{Infobox scientist
| name        = Jaime Teevan
| image       = 
| caption     =
| birth_date  = {{Birth year and age|1976}}
| birth_place = 
| death_date  =
| death_place =
| nationality = 
| residence   = 
| fields      = [[Computer science]]&lt;br/ &gt;[[Human-Computer Interaction]]&lt;br/ &gt;[[Information Retrieval]]
| work_institution = [[Microsoft Research]]
| alma_mater  = [[Massachusetts Institute of Technology]]&lt;br/&gt;[[Yale University]]
| known_for   =
| doctoral_advisor = [[David Karger]]
| awards = [[TR35]] (2009)&lt;br/ &gt;Borg Early Career Award (2014)&lt;br/ &gt;[[Karen Sp&#228;rck Jones]] Award (2016)
| website           = {{URL|http://teevan.org}} 
}}

'''Jaime Teevan''' is an American [[computer scientist]] known for her research in [[human-computer interaction]] and [[information retrieval]]. She is particularly known for the work she has done on [[personalized search]]. According to the [[Technology Review]], Teevan "is a leader in using data about people's knowledge, preferences, and habits to help them manage information.&lt;ref name="tr35"&gt;{{cite news|last=Kleiner|first=Kurt|title=TR35: Jaime Teevan, 32|url=http://www.technologyreview.com/tr35/Profile.aspx?Cand=T&amp;TRID=778|accessdate=10 March 2011|newspaper=Technology Review|date=August 2009}}&lt;/ref&gt;"

==Biography==
Teevan received and a [[Bachelor of Science|B.S.]] in [[Computer Science]] from [[Yale University]] and a Ph.D. and S.M. from [[MIT]].&lt;ref&gt;http://www.csail.mit.edu/~teevan/work/publications/theses/phd/thesis.pdf&lt;/ref&gt;&lt;ref&gt;http://www.csail.mit.edu/~teevan/work/publications/theses/masters/thesis.pdf&lt;/ref&gt;

She is currently a researcher at [[Microsoft Research]] and an affiliate professor at the [[University of Washington]]. There she co-authored the first book on [[collaborative information seeking]],.&lt;ref&gt;{{cite book|last=Morris|first=Meredith Ringel and Teevan, Jaime|title=Collaborative Search: Who, What, Where, When, Why, and How|year=2010|publisher=Morgan and Claypool Publishers|isbn=1-60845-121-6|url=http://www.amazon.com/dp/1608451216/}}&lt;/ref&gt; She also edited a book on [[Personal Information Management]] (PIM),&lt;ref&gt;{{cite book|editor=Jones, William |editor2=Teevan, Jaime|title=Personal Information Management|year=2007|publisher=University of Washington Press|isbn=0-295-98737-5|url=http://www.amazon.com/dp/0295987375}}&lt;/ref&gt; 
edited a special issue of Communications of the ACM on the topic, and organized workshops on [[PIM (software)|PIM]] and query log analysis. She has published numerous technical papers, including several best papers, and was chair of the Web Search and Data Mining (WSDM) 2012 conference.

==Awards==

Teevan was named a Technology Review ([[TR35]]) 2009 Young Innovator for her research on [[personalized search]]&lt;ref name="tr35" /&gt; and received the [[CRA-W]] Borg Early Career Award (BECA) in 2014.&lt;ref name="borg"&gt;{{cite news|last=Knies|first=Rob|title=Researcher Teevan Wins Borg Early Career Award|url=http://blogs.technet.com/b/inside_microsoft_research/archive/2014/04/22/researcher-teevan-wins-borg-early-career-award.aspx|accessdate=28 April 2014|newspaper=Inside Microsoft Research|date=April 2014}}&lt;/ref&gt; In 2016 she received the [[Karen Sp&#228;rck Jones]] award from the [[British Computer Society]] for her "technically strong and exceptionally creative contributions to the intersection of information retrieval, user experience and social media." &lt;ref&gt;http://irsg.bcs.org/ksjaward.php&lt;/ref&gt;

==Personal==
Teevan is married to Alexander Hehmeyer.&lt;ref&gt;{{cite news|title=WEDDINGS; Jaime Teevan, Alexander Hehmeyer|url=http://www.nytimes.com/2002/06/16/style/weddings-jaime-teevan-alexander-hehmeyer.html|accessdate=14 September 2015|newspaper=New York Times|date=June 16, 2002}}&lt;/ref&gt;
The couple live in [[Bellevue, Washington]]
and have four children.&lt;ref&gt;{{cite news|last=Vanderkam|first=Laura|title=Women with Big Jobs and Big Families: Balancing Really Isn't That Hard|url=http://fortune.com/2015/06/06/women-with-big-jobs-and-big-families-balancing-really-isnt-that-hard/|accessdate=14 September 2015|newspaper=Fortune|date=6 June 2015}}&lt;/ref&gt;
Teevan is an advocate for helping researchers successfully integrate parenthood and academic efforts.&lt;ref name="borg" /&gt;

==References==
&lt;references /&gt;

==External links==
* [http://teevan.org/ Professional home page]

{{DEFAULTSORT:Teevan, Jaime}}
[[Category:People in information technology]]
[[Category:Information retrieval researchers]]
[[Category:Human&#8211;computer interaction researchers]]
[[Category:Women computer scientists]]
[[Category:Microsoft employees]]
[[Category:Living people]]
[[Category:Yale University alumni]]
[[Category:Massachusetts Institute of Technology alumni]]
[[Category:University of Washington faculty]]
[[Category:1976 births]]</text>
      <sha1>j36ley9ctdjqvsmjwps47aqnhf0g9zw</sha1>
    </revision>
  </page>
  <page>
    <title>Suggested Upper Merged Ontology</title>
    <ns>0</ns>
    <id>247601</id>
    <revision>
      <id>725922084</id>
      <parentid>696941014</parentid>
      <timestamp>2016-06-18T19:11:36Z</timestamp>
      <contributor>
        <ip>68.65.169.236</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2048" xml:space="preserve">The '''Suggested Upper Merged Ontology''' or '''SUMO''' is an [[Upper ontology (information science)|upper ontology]] intended as a foundation [[ontology (computer science)|ontology]] for a variety of computer information processing systems. It was originally developed by the Teknowledge Corporation and now is maintained by [http://www.articulatesoftware.com Articulate Software]. SUMO is [[open source]].

SUMO originally concerned itself with meta-level concepts (general entities that do not belong to a specific problem domain), and thereby would lead naturally to a categorization scheme for encyclopedias.  It has now been considerably expanded to include a mid-level ontology and dozens of domain ontologies.

SUMO was first released in December 2000. It defines a hierarchy of ''SUMO classes'' and related rules and relationships. These are formulated in a version of the language [[SUO-KIF]] which has a [[LISP]]-like syntax. A [[Map (mathematics)|mapping]] from [[WordNet]] [[synsets]]  to SUMO has also been defined.  

SUMO is organized for interoperability of automated [[reasoning engine]]s. To maximize compatibility, [[logical schema|schema]] designers can try to assure that their [[naming convention]]s use the same meanings as SUMO for identical words (for example, "agent" or "process").  SUMO has an associated open source [[Sigma knowledge engineering environment]].

==See also==
* [[Semantic translation]]
* [[Upper ontology]]

== External links ==
* [http://www.ontologyportal.org/ Main page for SUMO]
* [http://suo.ieee.org/ Home page of the IEEE Standard Upper Ontology working group]
* The [http://sigmakee.sourceforge.net Sigma] reasoning system for SUMO
* [http://54.183.42.206:8080/sigma/Browse.jsp?kb=SUMO Online browser for SUMO]
* [http://www.adampease.org/professional/ Adam Pease, current Technical Editor of the standard]
[[Category:Java platform software]]
[[Category:Knowledge representation]]
[[Category:Ontology (information science)]]
[[Category:Open data]]
[[Category:Knowledge bases]]
{{Compu-AI-stub}}</text>
      <sha1>5q9qt6792ttbjcq6d8o5pex6kx1tbol</sha1>
    </revision>
  </page>
  <page>
    <title>Resource Description Framework</title>
    <ns>0</ns>
    <id>53847</id>
    <revision>
      <id>760123809</id>
      <parentid>760123227</parentid>
      <timestamp>2017-01-15T03:01:30Z</timestamp>
      <contributor>
        <username>K6ka</username>
        <id>11801436</id>
      </contributor>
      <minor />
      <comment>Reverted edits by [[Special:Contribs/66.87.152.25|66.87.152.25]] ([[User talk:66.87.152.25|talk]]) to last version by FockeWulf FW 190</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="38557" xml:space="preserve">{{Infobox technology standard
| title             = RDF 1.1 Concepts and Abstract Syntax
| status            = Published, W3C Recommendation
| year_started      = 1997
| editors           = Richard Cyganiak, David Wood, Markus Lanthaler
| base_standards    = [[URI]]
| related_standards = [[RDFS]], [[Web Ontology Language|OWL]], [[Rule Interchange Format|RIF]], [[RDFa]]
| domain            = [[Semantic Web]]
| abbreviation      = RDF
| website           = {{url|http://www.w3.org/TR/2014/REC-rdf11-concepts-20140225/}}
}}

The '''Resource Description Framework''' ('''RDF''') is a family of [[World Wide Web Consortium]] (W3C) [[specification]]s&lt;ref&gt;{{cite web|url=http://www.dblab.ntua.gr/~bikakis/XMLSemanticWebW3CTimeline.pdf |title=XML and Semantic Web W3C Standards Timeline
|date=2012-02-04}}&lt;/ref&gt; originally designed as a [[metadata]] [[data model]]. It has come to be used as a general method for conceptual description or modeling of information that is implemented in [[web resource]]s, using a variety of syntax notations and [[data serialization]] formats. It is also used in [[knowledge management]] applications.

RDF was adopted as a W3C recommendation in 1999. The RDF 1.0 specification was published in 2004, the RDF 1.1 specification in 2014.

== Overview ==
The RDF data model&lt;ref&gt;http://www.w3.org/TR/PR-rdf-syntax/ "Resource Description Framework
(RDF) Model and Syntax Specification"&lt;/ref&gt;  is similar to classical conceptual modeling approaches (such as [[entity&#8211;relationship model|entity&#8211;relationship]] or [[class diagram]]s). It is based upon the idea of making [[statement (programming)|statement]]s about [[resource (computer science)|resource]]s (in particular [[web resource]]s) expressions, known as ''[[Semantic triple|triples]]''.  Triples are so named because they follow a &lt;var&gt;subject&lt;/var&gt;&#8211;&lt;var&gt;predicate&lt;/var&gt;&#8211;&lt;var&gt;object&lt;/var&gt; structure. The &lt;var&gt;subject&lt;/var&gt; denotes the resource, and the &lt;var&gt;predicate&lt;/var&gt; denotes traits or aspects of the resource, and expresses a relationship between the &lt;var&gt;subject&lt;/var&gt; and the &lt;var&gt;object&lt;/var&gt;. 

For example, one way to represent the notion "The sky has the color blue" in RDF is as the triple: a [[Subject (grammar)|subject]] denoting "the sky", a [[Predicate (grammar)|predicate]] denoting "has the color", and an [[Object (grammar)|object]] denoting "blue". Therefore, RDF swaps &lt;var&gt;object&lt;/var&gt; for &lt;var&gt;subject&lt;/var&gt; in contrast to the typical approach of an [[entity&#8211;attribute&#8211;value model]] in [[object-oriented design]]: entity (sky), attribute (color), and value (blue).  

RDF is an abstract model with several [[Serialization|serialization formats]] (i.e. file formats), so the particular encoding for resources or triples varies from format to format.

This mechanism for describing resources is a major [[software componentry|component]] in the W3C's [[Semantic Web]] activity: an evolutionary stage of the [[World Wide Web]] in which automated software can store, exchange, and use machine-readable information distributed throughout the Web, in turn enabling users to deal with the information with greater efficiency and [[certainty]]. RDF's simple data model and ability to model disparate, abstract concepts has also led to its increasing use in [[knowledge management]] applications unrelated to Semantic Web activity.

A collection of RDF statements intrinsically represents a [[Glossary of graph theory|labeled, directed multi-graph]]. This theoretically makes an RDF [[data model]] better suited to certain kinds of [[knowledge representation]] than other [[relational model|relational]] or [[Ontology (computer science)|ontological]] models. However, in practice, RDF data is often persisted in [[RDBMS|relational database]] or native representations (also called [[Triplestore]]s&#8212;or Quad stores, if context (i.e. the [[named graph]]) is also persisted for each RDF triple).&lt;ref&gt;[http://sw.deri.org/2005/02/dexa/yars.pdf Optimized Index Structures for Querying RDF from the Web] Andreas Harth, Stefan Decker, 3rd Latin American Web Congress, Buenos Aires, Argentina, October 31 to November 2, 2005, pp. 71&#8211;80&lt;/ref&gt; 

ShEX, or Shape Expressions,&lt;ref&gt;[http://www.w3.org/2001/sw/wiki/ShEx]  Shape Expressions language&lt;/ref&gt; is a language for expressing constraints on RDF graphs. It includes the cardinality constraints from [[Open Services for Lifecycle Collaboration|OSLC]] Resource Shapes and [[Dublin Core]] Description Set Profiles, as well as logical connectives for disjunction and polymorphism. 

As [[RDFS]] and [[Web Ontology Language|OWL]] demonstrate, one can build additional [[ontology language]]s upon RDF.

== History ==
The initial RDF design, intended to "build a vendor-neutral and operating system-independent system of metadata,"&lt;ref name="press-release-1997"&gt;{{Cite news| last = | first = | title = World Wide Web Consortium Publishes Public Draft of Resource Description Framework| work = W3C| location = Cambridge, MA| date = 1997-10-03| url = http://www.w3.org/Press/RDF}}&lt;/ref&gt;  derived from the W3C's [[Platform for Internet Content Selection]] (PICS), an early web content labelling system,&lt;ref name="lash" /&gt; but the project was also  shaped by ideas from [[Dublin Core]], and from the [[Meta Content Framework]] (MCF),&lt;ref name="press-release-1997" /&gt; which had been developed during 1995&#8211;1997 by [[Ramanathan V. Guha]] at [[Apple Computer|Apple]] and [[Tim Bray]] at [[Netscape Communications Corporation|Netscape]].&lt;ref&gt;{{Cite book| publisher = O&#8217;Reilly| isbn = 0-596-00881-3| last = Hammersley| first = Ben| title = Developing Feeds with RSS and Atom| pages=2&#8211;3|location = Sebastopol| date = 2005}}&lt;/ref&gt;

A first public draft of RDF appeared in October 1997,&lt;ref&gt;{{Cite web| last1 = Lassila| first1 = Ora| last2 = Swick| first2 = Ralph R.| title = Resource Description Framework (RDF): Model and Syntax| work = W3C| accessdate = 2015-11-24| date = 1997-10-02| url = http://www.w3.org/TR/WD-rdf-syntax-971002/}}&lt;/ref&gt;&lt;ref&gt;{{Cite web|last=Swick |first=Ralph |title=Resource Description Framework (RDF) |work=W3C |accessdate=2015-11-24 |date=1997-12-11 |url=http://www13.w3.org/RDF/Overview.html |deadurl=yes |archiveurl=https://web.archive.org/web/19980214043631/http://www13.w3.org/RDF/Overview.html |archivedate=February 14, 1998 }}&lt;/ref&gt; issued by a W3C working group that included representatives from [[IBM]], [[Microsoft]], [[Netscape]], [[Nokia]], [[Reuters]], [[SoftQuad Software|SoftQuad]], and the [[University of Michigan]].&lt;ref name="lash"&gt;{{Cite news|last=Lash |first=Alex |title=W3C takes first step toward RDF spec |work=CNET News |accessdate=2015-11-28 |date=1997-10-03 |url=http://news.cnet.com/2100-1001-203893.html |deadurl=yes |archiveurl=https://web.archive.org/web/20110616023126/http://news.cnet.com/2100-1001-203893.html |archivedate=June 16, 2011 }}&lt;/ref&gt;

The W3C published a specification of RDF's data model and an [[XML]] serialization as a recommendation in February 1999.&lt;ref&gt;{{cite web|url=http://www.w3.org/TR/1999/REC-rdf-syntax-19990222|title=Resource Description Framework (RDF) Model and Syntax Specification| date=22 Feb 1999|accessdate=5 May 2014}}&lt;/ref&gt;

Two persistent misunderstandings developed around RDF at this time: firstly, from the MCF influence and the RDF "Resource Description" acronym, the idea that RDF was specifically for use in representing metadata. Secondly that RDF was an XML format, rather than RDF being a data model and only the RDF/XML serialisation being XML-based. RDF saw little take-up in this period, but there was significant work carried out in [[Bristol]], around ILRT at [[Bristol University]] and [[HP Labs]], and also in Boston at [[MIT]]. [[RSS 1.0]] and [[FOAF (ontology)|FOAF]] became exemplar applications for RDF in this period.

The recommendation of 1999 was replaced in 2004 by a set of six specifications: "The RDF Primer",&lt;ref&gt;{{Citation| publisher = W3C| last1 = Manola| first1 = Frank| last2 = Miller| first2 = Eric| title = RDF Primer| accessdate = 2015-11-21| date = 2004-02-10| url = http://www.w3.org/TR/2004/REC-rdf-primer-20040210/}}&lt;/ref&gt; "RDF Concepts and Abstract",&lt;ref&gt;{{Citation| publisher = W3C| last1 = Klyne| first1 = Graham| last2 = Carroll| first2 = Jeremy J.| title = Resource Description Framework (RDF): Concepts and Abstract Syntax| accessdate = 2015-11-21| date = 2004-02-10| url = http://www.w3.org/TR/2004/REC-rdf-concepts-20040210/}}&lt;/ref&gt; "RDF/XML Syntax Specification (revised)",&lt;ref&gt;{{Citation| publisher = W3C| last = Beckett| first = Dave| title = RDF/XML Syntax Specification (Revised)| accessdate = 2015-11-21| date = 2004-02-10| url = http://www.w3.org/TR/2004/REC-rdf-syntax-grammar-20040210/}}&lt;/ref&gt; "RDF Semantics",&lt;ref&gt;{{Citation| last = Hayes| first = Patrick| title = RDF Semantics| accessdate = 2015-11-21| date = 2014-02-10| url = http://www.w3.org/TR/2004/REC-rdf-mt-20040210/}}&lt;/ref&gt; "RDF Vocabulary Description Language 1.0",&lt;ref&gt;{{Citation| publisher = W3C| last1 = Brickley| first1 = Dan| last2 = Guha| first2 = R.V.| title = RDF Vocabulary Description Language 1.0: RDF Schema: W3C Recommendation 10 February 2004| accessdate = 2015-11-21| date = 2004-02-10| url = http://www.w3.org/TR/2004/REC-rdf-schema-20040210/}}&lt;/ref&gt; and "The RDF Test Cases".&lt;ref&gt;{{Citation| publisher = W3C| last1 = Grant| first1 = Jan| last2 = Beckett| first2 = Dave| title = RDF Test Cases| accessdate = 2015-11-21| date = 2004-02-10| url = http://www.w3.org/TR/2004/REC-rdf-testcases-20040210/}}&lt;/ref&gt;

This series was superseded in 2014 by the following six "RDF 1.1" documents: "RDF 1.1 Primer,"&lt;ref&gt;{{Citation| publisher = W3C| last1 = Schreiber| first1 = Guus| last2 = Raimond| first2 = Yves| title = RDF 1.1 Primer| accessdate = 2015-11-22| date = 2014-06-24| url = http://www.w3.org/TR/2014/NOTE-rdf11-primer-20140624/}}&lt;/ref&gt; "RDF 1.1 Concepts and Abstract Syntax,"&lt;ref&gt;{{Citation| publisher = W3C| last1 = Cyganiak| first1 = Richard| last2 = Wood| first2 = David| last3 = Lanthaler| first3 = Markus| title = RDF 1.1 Concepts and Abstract Syntax| accessdate = 2015-11-22| date = 2014-02-25| url = http://www.w3.org/TR/2014/REC-rdf11-concepts-20140225/}}&lt;/ref&gt; "RDF 1.1 XML Syntax,"&lt;ref&gt;{{Citation| publisher = W3C| last1 = Gandon| first1 = Fabien| last2 = Schreiber| first2 = Guus| title = RDF 1.1 XML Syntax| accessdate = 2015-11-22| date = 2014-02-25| url = http://www.w3.org/TR/rdf-syntax-grammar/}}&lt;/ref&gt;
"RDF 1.1 Semantics,"&lt;ref&gt;{{Citation| publisher = W3C| last1 = Hayes| first1 = Patrick J.| last2 = Patel-Schneider| first2 = Peter F.| title = RDF 1.1 Semantics| accessdate = 2015-11-22| date = 2014-02-25| url = http://www.w3.org/TR/2014/REC-rdf11-mt-20140225/}}&lt;/ref&gt; "RDF Schema 1.1,"&lt;ref&gt;{{Citation| publisher = W3C| last1 = Brickley| first1 = Dan| last2 = Guha| first2 = R.V.| title = RDF Schema 1.1| accessdate = 2015-11-22| date = 2014-02-25| url = http://www.w3.org/TR/rdf-schema/}}&lt;/ref&gt; and "RDF 1.1 Test Cases".&lt;ref&gt;{{Citation| publisher = W3C| last1 = Kellogg| first1 = Gregg| last2 = Lanthaler| first2 = Markus| title = RDF 1.1 Test Cases| accessdate = 2015-11-22| date = 2014-02-25| url = http://www.w3.org/TR/2014/NOTE-rdf11-testcases-20140225/}}&lt;/ref&gt;

==RDF topics==

===RDF vocabulary===
The vocabulary defined by the RDF specification is as follows:&lt;ref name="rdfschema"&gt;{{cite web|url=http://www.w3.org/TR/rdf-schema/|title=RDF Vocabulary Description Language 1.0: RDF Schema|publisher=[[W3C]]|date=2004-02-10|accessdate=2011-01-05}}&lt;/ref&gt;

====Classes====

===== rdf =====
* '''&lt;code&gt;rdf:XMLLiteral&lt;/code&gt;''' &#8211; the class of XML literal values
* '''&lt;code&gt;rdf:Property&lt;/code&gt;''' &#8211; the class of properties
* '''&lt;code&gt;rdf:Statement&lt;/code&gt;''' &#8211; the class of RDF statements
* '''&lt;code&gt;rdf:Alt&lt;/code&gt;''', '''&lt;code&gt;rdf:Bag&lt;/code&gt;''', '''&lt;code&gt;rdf:Seq&lt;/code&gt;''' &#8211; containers of alternatives, unordered containers, and ordered containers (&lt;code&gt;rdfs:Container&lt;/code&gt; is a super-class of the three)
* '''&lt;code&gt;rdf:List&lt;/code&gt;''' &#8211; the class of RDF Lists
* '''&lt;code&gt;rdf:nil&lt;/code&gt;''' &#8211; an instance of &lt;code&gt;rdf:List&lt;/code&gt; representing the empty list

===== rdfs =====
* '''&lt;code&gt;rdfs:Resource&lt;/code&gt;''' &#8211; the class resource, everything
* '''&lt;code&gt;rdfs:Literal&lt;/code&gt;''' &#8211; the class of literal values, e.g. [[string literal|string]]s and [[integer]]s
* '''&lt;code&gt;rdfs:Class&lt;/code&gt;''' &#8211; the class of classes
* '''&lt;code&gt;rdfs:Datatype&lt;/code&gt;''' &#8211; the class of RDF datatypes
* '''&lt;code&gt;rdfs:Container&lt;/code&gt;''' &#8211; the class of RDF containers
* '''&lt;code&gt;rdfs:ContainerMembershipProperty&lt;/code&gt;''' &#8211; the class of container membership properties, &lt;code&gt;rdf:_1&lt;/code&gt;, &lt;code&gt;rdf:_2&lt;/code&gt;, ..., all of which are sub-properties of &lt;code&gt;rdfs:member&lt;/code&gt;

====Properties====

=====rdf=====
* '''&lt;code&gt;rdf:type&lt;/code&gt;''' &#8211; an instance of &lt;code&gt;rdf:Property&lt;/code&gt; used to state that a resource is an instance of a class
* '''&lt;code&gt;rdf:first&lt;/code&gt;''' &#8211; the first item in the subject RDF list
* '''&lt;code&gt;rdf:rest&lt;/code&gt;''' &#8211; the rest of the subject RDF list after &lt;code&gt;rdf:first&lt;/code&gt;
* '''&lt;code&gt;rdf:value&lt;/code&gt;''' &#8211; idiomatic property used for structured values
* '''&lt;code&gt;rdf:subject&lt;/code&gt;''' &#8211; the subject of the subject RDF statement
* '''&lt;code&gt;rdf:predicate&lt;/code&gt;''' &#8211; the predicate of the subject RDF statement
* '''&lt;code&gt;rdf:object&lt;/code&gt;''' &#8211; the object of the subject RDF statement

&lt;code&gt;rdf:Statement&lt;/code&gt;, &lt;code&gt;rdf:subject&lt;/code&gt;, &lt;code&gt;rdf:predicate&lt;/code&gt;, &lt;code&gt;rdf:object&lt;/code&gt; are used for [[reification (knowledge representation)|reification]] (see [[#Statement reification and context|below]]).

=====rdfs=====
* '''&lt;code&gt;rdfs:subClassOf&lt;/code&gt;''' &#8211; the subject is a subclass of a class
* '''&lt;code&gt;rdfs:subPropertyOf&lt;/code&gt;''' &#8211; the subject is a subproperty of a property
* '''&lt;code&gt;rdfs:domain&lt;/code&gt;''' &#8211; a domain of the subject property
* '''&lt;code&gt;rdfs:range&lt;/code&gt;''' &#8211; a range of the subject property
* '''&lt;code&gt;rdfs:label&lt;/code&gt;''' &#8211; a human-readable name for the subject
* '''&lt;code&gt;rdfs:comment&lt;/code&gt;''' &#8211; a description of the subject resource
* '''&lt;code&gt;rdfs:member&lt;/code&gt;''' &#8211; a member of the subject resource
* '''&lt;code&gt;rdfs:seeAlso&lt;/code&gt;''' &#8211; further information about the subject resource
* '''&lt;code&gt;rdfs:isDefinedBy&lt;/code&gt;''' &#8211; the definition of the subject resource

This vocabulary is used as a foundation for [[RDF Schema]] where it is extended.

=== Serialization formats ===
{{Infobox file format
| name = RDF 1.1 Turtle serialization
| icon = 
| extension = .ttl
| mime = text/turtle&lt;ref&gt;{{cite web |url=http://www.w3.org/TR/turtle/#h2_sec-mediaReg |title=RDF 1.1 Turtle: Terse RDF Triple Language |publisher=W3C |date=9 Jan 2014 |accessdate=2014-02-22}}&lt;/ref&gt;
| owner = [[World Wide Web Consortium]]
| standard = [http://www.w3.org/TR/turtle/ RDF 1.1 Turtle: Terse RDF Triple Language] {{release date and age|2014|01|09}}
| free = Yes
}}

{{Infobox file format
| name = RDF/XML serialization
| icon = [[Image:XML.svg|100px]]
| extension = .rdf
| mime = application/rdf+xml&lt;ref&gt;{{cite web |url=http://tools.ietf.org/html/rfc3870 |title=application/rdf+xml Media Type Registration |page=2 |publisher=IETF |date=September 2004 |accessdate=2011-01-08}}&lt;/ref&gt;
| owner = [[World Wide Web Consortium]]
| standard = [http://www.w3.org/TR/2004/REC-rdf-concepts-20040210/ Concepts and Abstract Syntax] {{release date and age|2004|02|10}}
| free = Yes
}}

Several common [[Serialization|serialization formats]] are in use, including:
* '''[[Turtle (syntax)|Turtle]],'''&lt;ref name="turtle"&gt;{{cite web
  |title=RDF 1.1 Turtle: Terse RDF Triple Language
  |url=http://www.w3.org/TR/turtle/
  |date=9 January 2014
  |publisher=W3C
}}&lt;/ref&gt; a compact, human-friendly format.
* '''[[N-Triples]],'''&lt;ref name="n-triples" &gt;{{cite web
  |title=RDF 1.1 N-Triples: A line-based syntax for an RDF graph
  |date=9 January 2014
  |url=http://www.w3.org/TR/n-triples/
  |publisher=[[W3C]]
}}&lt;/ref&gt; a very simple, easy-to-parse, line-based format that is not as compact as Turtle.
* '''[[N-Quads]],'''&lt;ref&gt;{{cite web
  |title=N-Quads: Extending N-Triples with Context
  |date=2012-06-25 
  |url=http://sw.deri.org/2008/07/n-quads/
}}&lt;/ref&gt;&lt;ref name="n-quads" &gt;{{cite web
  |title=RDF 1.1 N-Quads
  |date=January 2014
  |url=http://www.w3.org/TR/n-quads/
  |publisher=[[W3C]]
}}&lt;/ref&gt; a superset of N-Triples, for serializing multiple RDF graphs.
* '''[[JSON-LD]],'''&lt;ref name="json-ld"&gt;{{cite web|title=
JSON-LD 1.0: A JSON-based Serialization for Linked Data|url=http://www.w3.org/TR/json-ld/|publisher=W3C}}&lt;/ref&gt; a [[JSON]]-based serialization.
* '''N3''' or [[Notation3]], a non-standard serialization that is very similar to Turtle, but has some additional features, such as the ability to define inference rules.
* '''[[RDF/XML]]''',&lt;ref name="rdf-xml" &gt;{{cite web
  |title=RDF 1.1 XML Syntax
  |date=25 February 2014
  |url=http://www.w3.org/TR/rdf-syntax-grammar/
  |publisher=[[W3C]]
}}&lt;/ref&gt; an XML-based syntax that was the first standard format for serializing RDF.

RDF/XML is sometimes misleadingly called simply RDF because it was introduced among the other W3C specifications defining RDF and it was historically the first W3C standard RDF serialization format. However, it is important to distinguish the RDF/XML format from the abstract RDF model itself. Although the RDF/XML format is still in use, other RDF serializations are now preferred by many RDF users, both because they are more human-friendly,&lt;ref name="rdf-xml-syntax-criticism"&gt;{{cite web|title=
Problems of the RDF syntax|url=http://milicicvuk.com/blog/2011/07/21/problems-of-the-rdf-syntax/|publisher=Vuk Mili&#269;i&#263;}}&lt;/ref&gt;  and because some RDF graphs are not representable in RDF/XML due to restrictions on the syntax of XML [[QName]]s.

With a little effort, virtually any arbitrary [[XML]] may also be interpreted as RDF using [[GRDDL]] (pronounced 'griddle'), Gleaning Resource Descriptions from Dialects of Languages.

RDF triples may be stored in a type of database called a [[triplestore]].

=== Resource identification ===
The subject of an RDF statement is either a [[uniform resource identifier]] (URI) or a [[blank node]], both of which denote  [[web resource|resource]]s. Resources indicated by [[blank node]]s are called anonymous resources. They are not directly identifiable from the RDF statement. The predicate is a URI which also indicates a resource, representing a relationship. The object is a URI, blank node or a [[Unicode]] [[string literal]]. 
As of RDF 1.1 resources are identified by IRI's. IRI is a generalization of URI.&lt;ref&gt;RDF 1.1 Concepts and Abstract Syntax https://www.w3.org/TR/rdf11-concepts/&lt;/ref&gt;

In Semantic Web applications, and in relatively popular applications of RDF like [[RSS (file format)|RSS]] and [[FOAF (software)|FOAF]] (Friend of a Friend), resources tend to be represented by URIs that intentionally denote, and can be used to access, actual data on the World Wide Web. But RDF, in general, is not limited to the description of Internet-based resources. In fact, the URI that names a resource does not have to be dereferenceable at all. For example, a URI that begins with "http:" and is used as the subject of an RDF statement does not necessarily have to represent a resource that is accessible via [[HTTP]], nor does it need to represent a tangible, network-accessible resource &#8212; such a URI could represent absolutely anything. However, there is broad agreement that a bare URI (without a # symbol) which returns a 300-level coded response when used in an HTTP GET request should be treated as denoting the internet resource that it succeeds in accessing.

Therefore, producers and consumers of RDF statements must agree on the semantics of resource identifiers. Such agreement is not inherent to RDF itself, although there are some controlled vocabularies in common use, such as [[Dublin Core]] Metadata, which is partially mapped to a URI space for use in RDF. The intent of publishing RDF-based ontologies on the Web is often to establish, or circumscribe, the intended meanings of the resource identifiers used to express data in RDF. For example, the URI:
&lt;blockquote&gt;
&lt;code&gt;
&lt;nowiki&gt;
http://www.w3.org/TR/2004/REC-owl-guide-20040210/wine#Merlot
&lt;/nowiki&gt;
&lt;/code&gt;
&lt;/blockquote&gt;
is intended by its owners to refer to the class of all [[Merlot]] red wines by vintner (i.e., instances of the above URI each represent the class of all wine produced by a single vintner), a definition which is expressed by the OWL ontology &#8212; itself an RDF document &#8212; in which it occurs.  Without careful analysis of the definition, one might erroneously conclude that an instance of the above URI was something physical, instead of a type of wine.

Note that this is not a 'bare' resource identifier, but is rather a [[Uniform Resource Identifier#URI reference|URI reference]], containing the '#' character and ending with a [[fragment identifier]].

=== Statement reification and context ===
The body of knowledge modeled by a collection of statements may be subjected to [[Reification (knowledge representation)|reification]], in which each ''statement'' (that is each triple ''subject-predicate-object'' altogether) is assigned a URI and treated as a resource about which additional statements can be made, as in "''Jane says that'' John is the author of document X". Reification is sometimes important in order to deduce a level of confidence or degree of usefulness for each statement.

In a reified RDF database, each original statement, being a resource, itself, most likely has at least three additional statements made about it: one to assert that its subject is some resource, one to assert that its predicate is some resource, and one to assert that its object is some resource or literal. More statements about the original statement may also exist, depending on the application's needs.

Borrowing from concepts available in [[logic]] (and as illustrated in graphical notations such as [[conceptual graphs]] and [[topic map]]s), some RDF model implementations acknowledge that it is sometimes useful to group statements according to different criteria, called ''situations'', ''contexts'', or ''scopes'', as discussed in articles by RDF specification co-editor [[Graham Klyne]].&lt;ref&gt;[http://www.ninebynine.org/RDFNotes/RDFContexts.html Contexts for RDF Information Modelling]&lt;/ref&gt;&lt;ref&gt;[http://www.ninebynine.org/RDFNotes/UsingContextsWithRDF.html Circumstance, Provenance and Partial Knowledge]&lt;/ref&gt; For example, a statement can be associated with a context, named by a URI, in order to assert an "is true in" relationship. As another example, it is sometimes convenient to group statements by their source, which can be identified by a URI, such as the URI of a particular RDF/XML document. Then, when updates are made to the source, corresponding statements can be changed in the model, as well.

Implementation of scopes does not necessarily require fully reified statements. Some implementations allow a single scope identifier to be associated with a statement that has not been assigned a URI, itself.&lt;ref&gt;[http://uche.ogbuji.net/tech/akara/nodes/2003-01-01/scopes The Concept of 4Suite RDF Scopes]&lt;/ref&gt;&lt;ref&gt;[http://librdf.org/notes/contexts.html Redland RDF Library &#8211; Contexts]&lt;/ref&gt;  Likewise ''named graphs'' in which a set of triples is named by a URI can represent context without the need to reify the triples.&lt;ref&gt;[http://www.w3.org/2004/03/trix/ Named Graphs]&lt;/ref&gt;

=== Query and inference languages ===
{{main|RDF query language}}
The predominant query language for RDF graphs is [[SPARQL]]. SPARQL is an [[SQL]]-like language, and a [[W3C recommendation|recommendation]] of the [[W3C]] as of January 15, 2008.

An example of a SPARQL query to show country capitals in Africa, using a fictional ontology.
&lt;source lang="sparql"&gt;
PREFIX ex: &lt;http://example.com/exampleOntology#&gt;
SELECT ?capital ?country
WHERE {
  ?x ex:cityname ?capital ;
     ex:isCapitalOf ?y .
  ?y ex:countryname ?country ;
     ex:isInContinent ex:Africa .
}
&lt;/source&gt;

Other non-standard ways to query RDF graphs include:
* [[RDQL]], precursor to [[SPARQL]], SQL-like
* Versa, compact syntax (non&#8211;SQL-like), solely implemented in [[4Suite]] ([[Python (programming language)|Python]])
* RQL, one of the first declarative languages for uniformly querying RDF schemas and resource descriptions, implemented in RDFSuite.&lt;ref name=RQL&gt;{{cite web|title=The RDF Query Language (RQL)|url=http://139.91.183.30:9090/RDF/RQL/index.html|work=The ICS-FORTH RDFSuite|publisher=ICS-FORTH}}&lt;/ref&gt;
* [[SeRQL]], part of [[Sesame (framework)|Sesame]]
* [[XUL]] has a template element in which to declare rules for matching data in RDF. XUL uses RDF extensively for databinding.

== Examples ==

=== Example 1: RDF Description of a person named Eric Miller&lt;ref name="rdf-primer"&gt;{{cite web|url= http://www.w3.org/TR/rdf-primer/|title= RDF Primer |publisher=[[W3C]]|accessdate=2009-03-13}}&lt;/ref&gt; ===

The following  example is taken from the W3C website&lt;ref name="rdf-primer" /&gt; describing a resource with statements "there is a Person identified by &lt;nowiki&gt;http://www.w3.org/People/EM/contact#me&lt;/nowiki&gt;, whose name is Eric Miller, whose email address is e.miller123(at)example (changed for security purposes), and whose title is Dr. [[Image:Rdf graph for Eric Miller.png|thumb|An RDF Graph Describing Eric Miller&lt;ref name="rdf-primer" /&gt;]]

The resource "&lt;nowiki&gt;http://www.w3.org/People/EM/contact#me&lt;/nowiki&gt;" is the subject.

The objects are:
* "Eric Miller" (with a predicate "whose name is"),
* &lt;nowiki&gt;mailto:e.miller123&lt;/nowiki&gt;(at)example (with a predicate "whose email address is"), and
* "Dr." (with a predicate "whose title is").

The subject is a URI.

The predicates also have URIs. For example, the URI for each predicate:
* "whose name is" is &lt;nowiki&gt;http://www.w3.org/2000/10/swap/pim/contact#fullName&lt;/nowiki&gt;,
* "whose email address is" is &lt;nowiki&gt;http://www.w3.org/2000/10/swap/pim/contact#mailbox&lt;/nowiki&gt;,
* "whose title is" is &lt;nowiki&gt;http://www.w3.org/2000/10/swap/pim/contact#personalTitle&lt;/nowiki&gt;.

In addition, the subject has a type (with URI &lt;nowiki&gt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&lt;/nowiki&gt;), which is person (with URI &lt;nowiki&gt;http://www.w3.org/2000/10/swap/pim/contact#Person&lt;/nowiki&gt;).

Therefore, the following "subject, predicate, object" RDF triples can be expressed:
* &lt;nowiki&gt;http://www.w3.org/People/EM/contact#me, http://www.w3.org/2000/10/swap/pim/contact#fullName,&lt;/nowiki&gt; "Eric Miller"
* &lt;nowiki&gt;http://www.w3.org/People/EM/contact#me, http://www.w3.org/2000/10/swap/pim/contact#mailbox, mailto:e.miller123(at)example&lt;/nowiki&gt;
* &lt;nowiki&gt;http://www.w3.org/People/EM/contact#me, http://www.w3.org/2000/10/swap/pim/contact#personalTitle,&lt;/nowiki&gt; "Dr."
* &lt;nowiki&gt;http://www.w3.org/People/EM/contact#me, http://www.w3.org/1999/02/22-rdf-syntax-ns#type, http://www.w3.org/2000/10/swap/pim/contact#Person&lt;/nowiki&gt;

In standard N-Triples format, this RDF can be written as:
&lt;source lang="turtle"&gt;
&lt;http://www.w3.org/People/EM/contact#me&gt; &lt;http://www.w3.org/2000/10/swap/pim/contact#fullName&gt; "Eric Miller" .
&lt;http://www.w3.org/People/EM/contact#me&gt; &lt;http://www.w3.org/2000/10/swap/pim/contact#mailbox&gt; &lt;mailto:e.miller123(at)example&gt; .
&lt;http://www.w3.org/People/EM/contact#me&gt; &lt;http://www.w3.org/2000/10/swap/pim/contact#personalTitle&gt; "Dr." .
&lt;http://www.w3.org/People/EM/contact#me&gt; &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt; &lt;http://www.w3.org/2000/10/swap/pim/contact#Person&gt; .
&lt;/source&gt;

Equivalently, it can be written in standard Turtle (syntax) format as:

&lt;source lang="turtle"&gt;
@prefix eric:    &lt;http://www.w3.org/People/EM/contact#&gt; .
@prefix contact: &lt;http://www.w3.org/2000/10/swap/pim/contact#&gt; .
@prefix rdf:     &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; .

eric:me contact:fullName "Eric Miller" .
eric:me contact:mailbox &lt;mailto:e.miller123(at)example&gt; .
eric:me contact:personalTitle "Dr." .
eric:me rdf:type contact:Person .
&lt;/source&gt;

Or, it can be written in RDF/XML format as:
&lt;source lang="xml" enclose="div"&gt;
&lt;?xml version="1.0" encoding="utf-8"?&gt;
&lt;rdf:RDF xmlns:contact="http://www.w3.org/2000/10/swap/pim/contact#" xmlns:eric="http://www.w3.org/People/EM/contact#" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"&gt;
  &lt;rdf:Description rdf:about="http://www.w3.org/People/EM/contact#me"&gt;
    &lt;contact:fullName&gt;Eric Miller&lt;/contact:fullName&gt;
  &lt;/rdf:Description&gt;
  &lt;rdf:Description rdf:about="http://www.w3.org/People/EM/contact#me"&gt;
    &lt;contact:mailbox rdf:resource="mailto:e.miller123(at)example"/&gt;
  &lt;/rdf:Description&gt;
  &lt;rdf:Description rdf:about="http://www.w3.org/People/EM/contact#me"&gt;
    &lt;contact:personalTitle&gt;Dr.&lt;/contact:personalTitle&gt;
  &lt;/rdf:Description&gt;
  &lt;rdf:Description rdf:about="http://www.w3.org/People/EM/contact#me"&gt;
    &lt;rdf:type rdf:resource="http://www.w3.org/2000/10/swap/pim/contact#Person"/&gt;
  &lt;/rdf:Description&gt;
&lt;/rdf:RDF&gt;
&lt;/source&gt;

=== Example 2: The postal abbreviation for New York ===

Certain concepts in RDF are taken from [[logic]] and [[linguistics]], where subject-predicate and subject-predicate-object structures have meanings similar to, yet distinct from, the uses of those terms in RDF. This example demonstrates:

In the [[English language]] statement '' 'New York has the postal abbreviation NY' '','' 'New York' '' would be the subject, '' 'has the postal abbreviation' '' the predicate and '' 'NY' '' the object.

Encoded as an RDF triple, the subject and predicate would have to be resources named by URIs. The object could be a resource or literal element. For example, in the N-Triples form of RDF, the statement might look like:

&lt;source lang="turtle"&gt;
&lt;urn:x-states:New%20York&gt; &lt;http://purl.org/dc/terms/alternative&gt; "NY" .
&lt;/source&gt;

In this example, "&lt;nowiki&gt;urn:x-states:New%20York&lt;/nowiki&gt;" is the URI for a resource that denotes the US state [[New York (state)|New York]], "&lt;nowiki&gt;http://purl.org/dc/terms/alternative&lt;/nowiki&gt;" is the URI for a predicate (whose human-readable definition can be found at here &lt;ref&gt;[http://dublincore.org/documents/dcmi-terms/index.shtml#terms-alternative DCMI Metadata Terms]. Dublincore.org. Retrieved on 2014-05-30.&lt;/ref&gt;), and "NY" is a literal string.  Note that the URIs chosen here are not standard, and don't need to be, as long as their meaning is known to whatever is reading them.

=== Example 3: A Wikipedia article about Tony Benn ===
In a like manner, given that &lt;nowiki&gt;"http://en.wikipedia.org/wiki/Tony_Benn"&lt;/nowiki&gt; identifies a particular resource (regardless of whether that URI could be traversed as a hyperlink, or whether the resource is ''actually'' the [[Wikipedia]] article about [[Tony Benn]]), to say that the title of this resource is "Tony Benn" and its publisher is "Wikipedia" would be two assertions that could be expressed as valid RDF statements. In the N-Triples form of RDF, these statements might look like the following:

&lt;source lang="turtle"&gt;
&lt;http://en.wikipedia.org/wiki/Tony_Benn&gt; &lt;http://purl.org/dc/elements/1.1/title&gt; "Tony Benn" .
&lt;http://en.wikipedia.org/wiki/Tony_Benn&gt; &lt;http://purl.org/dc/elements/1.1/publisher&gt; "Wikipedia" .
&lt;/source&gt;

To an English-speaking person, the same information could be represented simply as:
&lt;blockquote&gt;The title of this resource, which is published by Wikipedia, is 'Tony Benn'&lt;/blockquote&gt;
However, RDF puts the information in a formal way that a machine can understand. The purpose of RDF is to provide an [[Semantics encoding|encoding]] and interpretation mechanism so that [[Resource (computer science)|resources]] can be described in a way that particular [[software]] can understand it; in other words, so that software can access and use information that it otherwise couldn't use.

Both versions of the statements above are wordy because one requirement for an RDF resource (as a subject or a predicate) is that it be unique. The subject resource must be unique in an attempt to pinpoint the exact resource being described. The predicate needs to be unique in order to reduce the chance that the idea of [[Title]] or [[Publisher]] will be ambiguous to software working with the description. If the software recognizes  ''&lt;nowiki&gt;http://purl.org/dc/elements/1.1/title&lt;/nowiki&gt;'' (a specific [[definition]] for the [[concept]] of a title established by the [[Dublin Core]] Metadata Initiative), it will also know that this title is different from a land title or an honorary title or just the letters t-i-t-l-e put together.

The following example, written in Turtle, shows how such simple claims can be elaborated on, by combining multiple RDF vocabularies. Here, we note that the primary topic of the Wikipedia page is a "Person" whose name is "Tony Benn":

&lt;source lang="turtle"&gt;
@prefix rdf:  &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; .
@prefix foaf: &lt;http://xmlns.com/foaf/0.1/&gt; .
@prefix dc:   &lt;http://purl.org/dc/elements/1.1/&gt; .

&lt;http://en.wikipedia.org/wiki/Tony_Benn&gt;
    dc:publisher "Wikipedia" ;
    dc:title "Tony Benn" ;
    foaf:primaryTopic [
        a foaf:Person ;
        foaf:name "Tony Benn"
    ] .
&lt;/source&gt;

== Applications ==
* [[DBpedia]] &#8211; Extracts facts from Wikipedia articles and publishes them as RDF data.
* [[Creative Commons]] &#8211; Uses RDF to embed license information in web pages and mp3 files.
* [[FOAF (software)|FOAF (Friend of a Friend)]] &#8211; designed to describe [[person|people]], their interests and interconnections.
* [[Haystack (PIM)|Haystack client]] &#8211; Semantic web browser from MIT CS &amp; AI lab.&lt;ref&gt;[http://groups.csail.mit.edu/haystack/ Haystack]&lt;/ref&gt;
* [[IDEAS Group]] &#8211; developing a formal [[Ontology components|4D ontology]] for [[Enterprise Architecture]] using RDF as the encoding.&lt;ref&gt;[http://www.ideasgroup.org The IDEAS Group Website]&lt;/ref&gt;
* Microsoft shipped a product, Connected Services Framework,&lt;ref&gt;[http://www.microsoft.com/serviceproviders/solutions/connectedservicesframework.mspx Connected Services Framework]&lt;/ref&gt; which provides RDF-based Profile Management capabilities.
* [[MusicBrainz]] &#8211; Publishes information about Music Albums.&lt;ref&gt;[http://wiki.musicbrainz.org/RDF RDF on MusicBrainz Wiki]&lt;/ref&gt;
* [[NEPOMUK (framework)|NEPOMUK]], an open-source software specification for a Social Semantic desktop uses RDF as a storage format for collected metadata. NEPOMUK is mostly known because of its integration into the [[KDE Software Compilation 4|KDE SC 4]] desktop environment.
* [[Press Association]] is a news agency in the UK. They use ontologies to dynamically identify and link their NoSQL data to do [[semantic publishing]] but in a dynamic, rules based way that creates custom content on the fly.&lt;ref&gt;[http://www.datalanguage.com/blog/2012/05/17/ontology-driven-software-engineering/]&lt;/ref&gt;
* RDF Site Summary &#8211; one of several "[[RSS (file format)|RSS]]" languages for publishing information about updates made to a web page; it is often used for disseminating news article summaries and sharing [[weblog]] content.
* [[Simple Knowledge Organization System]] (SKOS) &#8211; a KR representation intended to support vocabulary/thesaurus applications
* [[SIOC|SIOC (Semantically-Interlinked Online Communities)]] &#8211; designed to describe online communities and to create connections between Internet-based discussions from message boards, weblogs and mailing lists.&lt;ref&gt;[http://sioc-project.org/ SIOC (Semantically-Interlinked Online Communities)]&lt;/ref&gt;
* [[Smart-M3]] &#8211; provides an infrastructure for using RDF and specifically uses the ontology agnostic nature of RDF to enable heterogeneous mashing-up of information&lt;ref&gt;Oliver Ian, Honkola Jukka, Ziegler Jurgen (2008). &#8220;Dynamic, Localized Space Based Semantic Webs&#8221;. IADIS WWW/Internet 2008. Proceedings, p.426, IADIS Press, ISBN 978-972-8924-68-3&lt;/ref&gt;

Some uses of RDF include research into social networking. It will also help people in business fields understand better their relationships with members of industries that could be of use for product placement.&lt;ref&gt;An RDF Approach for Discovering the Relevant Semantic Associations in a Social Network By Thushar A.K, and P. Santhi Thilagam&lt;/ref&gt;  It will also help scientists understand how people are connected to one another.

RDF is being used to have a better understanding of road traffic patterns.  This is because the information regarding traffic patterns is  on different websites, and RDF is used to integrate information from different sources on the web. Before, the common methodology was using keyword searching, but this method is problematic because it does not  consider synonyms. This is why ontologies are useful in this situation. But one of the issues that comes up when trying to efficiently study traffic is that to fully understand traffic,  concepts related to people, streets, and roads must be well understood. Since these are human  concepts, they require the addition of [[fuzzy logic]]. This is because values that are useful  when describing roads, like slipperiness, are not precise concepts and cannot be measured. This would imply that the best solution would incorporate both fuzzy logic and ontology.&lt;ref&gt;Traffic Information Retrieval Based on Fuzzy Ontology and RDF on the Semantic Web By Jun Zhai, Yi Yu, Yiduo Liang, and Jiatao Jiang (2008)&lt;/ref&gt;

== See also ==
;Notations for RDF
* [[TriG (syntax)|TRiG]]
* [[TriX (syntax)|TRiX]]
* [[RDF/XML]]
* [[RDFa]]
* [[JSON-LD]]
;Similar concepts
* [[Entity-attribute-value model]]
* [[Graph theory]] &#8211; An RDF model is a labeled, directed multi-graph.
* [[Website Parse Template]]
* [[Tag (metadata)|Tagging]]
* [[SciCrunch]]
* [[Semantic network]]
; Other (unsorted):
*[[Associative model of data]]
*[[Business Intelligence 2.0]] (BI 2.0)
*DataPortability
* [[EU Open Data Portal]]
*[[Folksonomy]]
*[[LSID|Life Science Identifiers]]
*[[Swoogle]]
*[[Universal Networking Language]] (UNL)

== References ==
{{Reflist|2}}

== Further reading ==
* [http://www.w3.org/RDF/ W3C's RDF at W3C]: specifications, guides, and resources
* [http://www.w3.org/TR/2004/REC-rdf-mt-20040210/ RDF Semantics]: specification of semantics, and complete systems of inference rules for both RDF and RDFS

== External links ==
{{Commons category|Resource Description Framework}}
*{{DMOZ|Reference/Libraries/Library_and_Information_Science/Technical_Services/Cataloguing/Metadata/RDF/}}
{{Semantic Web}}
{{W3C Standards}}
{{Data Exchange}}

{{Authority control}}

[[Category:Resource Description Framework| ]]
[[Category:Knowledge representation]]
[[Category:World Wide Web Consortium standards]]
[[Category:XML]]
[[Category:XML-based standards]]
[[Category:Metadata]]
[[Category:Semantic Web]]
[[Category:Bibliography file formats]]</text>
      <sha1>6s0927s9w5cip40tkm4qcbhbfnjoqgc</sha1>
    </revision>
  </page>
  <page>
    <title>Findability</title>
    <ns>0</ns>
    <id>1025538</id>
    <revision>
      <id>761083092</id>
      <parentid>760704391</parentid>
      <timestamp>2017-01-20T19:51:03Z</timestamp>
      <contributor>
        <username>Riceissa</username>
        <id>20698937</id>
      </contributor>
      <comment>/* History */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="11863" xml:space="preserve">'''Findability''' is a term for the ease with which information contained on a [[website]] can be found, both from outside the website (using [[search engine]]s and the like) and by users already on the website.&lt;ref&gt;{{Cite journal|url = |title = Information architecture|author1=Jacob, Elin K.  |author2=Loehrlein, Aaron|date = 2009|journal = Annual Review of Information Science and Technology|doi = 10.1002/aris.2009.1440430110|pmid = |access-date = |publication-date = }}&lt;/ref&gt; Although findability has relevance outside the [[World Wide Web]], the term is usually used in that context. Most relevant websites do not come up in the top results because designers and engineers do not cater to the way ranking algorithms work currently.&lt;ref&gt;{{Cite book|title=Ambient Findability|last=Morville|first=Peter|publisher=Oreilly|year=2005|isbn=978-0-596-00765-2|location=Sebastopol, CA|pages=|quote=|via=}}&lt;/ref&gt; Its importance can be determined from the first law of [[e-commerce]], which states "If the user can&#8217;t find the product, the user can&#8217;t buy the product."&lt;ref&gt;{{Cite web|url = http://www.nngroup.com/reports/ecommerce|title = E-Commerce user experience: High-level strategy, Nielsen Norman Group|date = 2001|accessdate = |website = |publisher = }}&lt;/ref&gt; As of December 2014, out of 10.3 billion monthly [[Google]] searches by Internet users in the [[United States]], an estimated 78% are made to research products and services online.&lt;ref&gt;{{Cite web|url = http://www.cmocouncil.org/facts-stats-categories.php?category=internet-marketing|title = Internet Marketing|date = |accessdate = |website = |publisher = }}&lt;/ref&gt;

Findability encompasses aspects of [[information architecture]], [[user interface design]], [[accessibility]] and [[search engine optimization]] (SEO), among others.

==Introduction==
Findability is similar to, but different from [[discoverability]], which is defined as the ability of something, especially a piece of content or information, to be found. It is different from web search in that the word 'find' refers to locating something in a known space while 'search' is in an unknown space or not in an expected location.&lt;ref name="every-page" /&gt;

Mark Baker, the author of "Every Page is Page One",&lt;ref name="every-page"&gt;{{Cite book|title = Every Page is Page One|last = Baker|first = Mark|publisher = XML Press|year = 2013|isbn = 978-1937434281|location = |pages = }}&lt;/ref&gt; mentions that findability "is a content problem, not a search problem".&lt;ref&gt;{{cite web|last1=Baker|first1=Mark|title=Findability is a Content Problem, not a Search Problem|url=http://everypageispageone.com/2013/05/28/findability-is-a-content-problem-not-a-search-problem/|website=Every Page is Page One|accessdate=2015-04-25}}&lt;/ref&gt; Even when the right content is present, users often find themselves deep within the content of a website but not in the right place. He further adds that findability is intractable, perfect findability is unattainable, but we need to focus on reducing the effort for finding that a user would have to do for themselves.

Findability can be divided into external findability and on-site findability, based on where the customers need to find the information.

==History==
[[Heather Lutze]] is thought to have created the term in the early 2000s.&lt;ref&gt;{{cite web | url=http://www.huffingtonpost.com/liz-wainger/the-shtickiness-factor_b_3471675.html | title=The Shtickiness Factor |last1=Wainger | first1=Liz | publisher=The Huffington Post | date=20 June 2013 | accessdate=12 September 2013}}&lt;/ref&gt; The popularization of the term "findability" for the Web is usually credited to [[Peter Morville]].{{citation needed|date=April 2015}} In 2005 he defined it as: "the ability of users to identify an appropriate Web site and navigate the pages of the site to discover and retrieve relevant information resources", though it appears to have been first coined in a public context referring to the web and information retrieval by Alkis Papadopoullos in a 2005 article entitled "Findability".&lt;ref&gt;{{cite journal|author=Alkis Papadopoulos|title=The Key to Enterprise Search|journal=KM World|date=April 1, 2005|url=http://news-business.vlex.com/vid/findability-key-to-enterprise-search-62406335}}&lt;/ref&gt;&lt;ref&gt;Though the word has been used to mean "ease of finding information" since at least 1943: see Urban A. Avery, "The 'Findability' of the Law", ''Chicago Bar Record'' '''24''':272, April 1943, reprinted in the ''Journal of the American Judicature Society'' '''27''':25 [http://heinonline.org/HOL/LandingPage?collection=journals&amp;handle=hein.journals/judica27&amp;div=12&amp;id=&amp;page=]&lt;/ref&gt;

==External findability==
External findability is the domain of [[internet marketing]] and [[Search engine optimization|search engine optimization (SEO)]] tactics. Several factors affect external findability:&lt;ref&gt;{{cite web|title=Findability Factors Found|url=http://www.econtentstrategies.com/Article_FindabilityFactorsFoundFinal_EContent_200701.pdf}}&lt;/ref&gt;
#''Search Engine Indexing'': As the very first step, webpages need to be found by indexing crawler in order to be shown in the search results. It would be helpful to avoid factors that may lead to webpages being ignored by indexing crawlers. Those factors may include elements that require user interaction, such as entering log-in credentials. Algorithms for indexing vary by the search engine which means the number of webpages of a website successfully being indexed may be very different between Google and Yahoo!'s search engines. Also, in countries like [[China]], [[Great Firewall|government policies]] could significantly influence the indexing algorithms. In this case, local knowledge about laws and policies could be valuable.&lt;ref&gt;{{cite web|title=Online Marketing in China|url=http://chineseseoshifu.com/china-online-marketing/}}&lt;/ref&gt;
#''Page Descriptions in Search Results'': Now that the webpages are successfully indexed by web crawlers and show in the search results with decent ranking, the next step is to attract customers to click the link to the web pages. However, the customers can't see the whole web pages at this point; they can only see an excerpt of the webpage's content and metadata. Therefore, displaying meaningful information in a limited space, usually a couple of sentences, in search results is important for increasing click traffic of the webpages, and thus the findability of the web content on your webpages. 
#''Keyword Matching'': At a semantic level, terminology used by the searcher and the content producer be different. Bridging the gap between the terms used by customers and developers is helpful for making web content more findable to more potential content consumers.

==On-site findability==
On-site findability is concerned with the ability of a potential customer to find what they are looking for within a specific site. More than 90 percent of customers use internal searches in a website compared to browsing. Of those, only 50 percent find what they are looking for.&lt;ref name="findability-solution"&gt;{{cite web|title=The Findability Solution|url=http://marriottschool.byu.edu/strategy/docs/TheFindabilitySolution-StrategyWhitePaper.pdf}}&lt;/ref&gt; Improving the quality of on-site searches highly improves the business of the website. Several factors affect findability on a website:

#''Site search'': If searchers within a site do not find what they are looking for, they tend to leave rather than browse through the website. Users who had successful site searches are twice as likely to ultimately convert.&lt;ref name="findability-solution" /&gt;
#''Related Links and Products'': User experience can be enhanced by trying to understand the needs of the customer and provide suggestions for other, related information.
#''Site Match to Customer Needs and Preferences'': Site design, content creation, and recommendations are major factors for affecting the customer experience.
#''Cross Device Experience'': With the rise of computing devices other than desktop computers, companies like Microsoft have focused more on smoothing the transition between devices to increase customer satisfaction.&lt;ref&gt;{{Cite web|url = http://research.microsoft.com/en-us/projects/courier/|title = Cross-Device User Experiences|date = |accessdate = |website = |publisher = }}&lt;/ref&gt;

==Evaluation and measures==
'''Baseline Findability''' is the existing findability before changes are made in order to improve it. This is measured by participants who represent the customer base of the website, who try to locate a sample set of items using the existing navigation of the website.&lt;ref&gt;{{Cite book|title = Customer Analytics For Dummies|last = Sauro|first = Jeff|publisher = John Wiley &amp; Sons|year = |isbn = 978-1-118-93759-4|location = |pages = |url = http://www.wiley.com/WileyCDA/WileyTitle/productCd-1118937597.html}}&lt;/ref&gt;&lt;ref&gt;{{Cite web|url = http://www.measuringu.com/blog/measure-findability.php|title = How to Measure Findability|date = |accessdate = |website = |publisher = }}&lt;/ref&gt;

In order to evaluate how easily information can be found by searching a site using a search engine or information retrieval system, [[retrievability]] measures were developed, and similarly, navigability measures now measure ease of information access through browsing a site (e.g. [[PageRank]], MNav, InfoScent (see [[Information foraging|Information Foraging]]), etc.).

Findability also can be evaluated via the following techniques:
* [[Usability testing]]: Conducted to find out how and why users navigate through a website to accomplish tasks.
* [[Tree testing]]: An [[information architecture]] based technique, to determine if critical information can be found on the website.
* [[Card sorting|Closed card sorting]]: A usability technique based on information architecture, for evaluating the strength of categories.
* [[Click testing]]: Accounts for the implicit data collected through clicks on the user interface.&lt;ref&gt;{{Cite web|url = http://www.nngroup.com/articles/navigation-ia-tests/|title = Low Findability and Discoverability: Four Testing Methods to Identify the Causes|date = July 6, 2014|accessdate = |website = |publisher = }}&lt;/ref&gt;

==Beyond findability==
Findability Sciences defines a findability index in terms of each user's influence, context, and sentiments. For seamless search, current websites focus on a combination of structured hypertext-based information architectures and rich Internet application-enabled visualization techniques.&lt;ref&gt;{{Cite journal|url = http://journalofia.org/volume2/issue1/03-spagnolo/|title = Beyond Findability - Search-Enhanced Information Architecture for Content-Intensive Rich Internet Applications|date = 2010|journal = |doi = |pmid = |access-date = }}&lt;/ref&gt;

==See also==
* [[Information retrieval]]
* [[Knowledge mining]]
* [[Search engine optimization]]
* [[Subject (documents)]]
* [[Usability]]
* [[User interface]]

==References==
{{reflist}}

==Further reading==
* Morville, P. (2005) Ambient findability. Sebastopol, CA: O'Reilly
* Wurman, R.S. (1996). Information architects. New York: Graphis.

==External links==
* [http://findability.org/ findability.org]: a collection of links to people, software, organizations, and content related to findability
* [http://semanticstudios.com/publications/semantics/000007.php The age of findability] (article)
* [http://www.useit.com/alertbox/search-keywords.html Use Old Words When Writing for Findability] (article on the findability impact of a site's choice of words)
* [http://buildingfindablewebsites.com/ Building Findable Websites: Web Standards SEO and Beyond] (book)
* [http://www.FindabilityFormula.com The Findability Formula: The Easy, Non-Technical Guide to Search Engine Marketing by Heather Lutze]

[[Category:Web design]]
[[Category:Knowledge representation]]
[[Category:Information science]]
[[Category:Information architecture]]</text>
      <sha1>0wgm46rzdgvvvepnd9tdr6geo4h9fid</sha1>
    </revision>
  </page>
  <page>
    <title>Agent Communications Language</title>
    <ns>0</ns>
    <id>3348350</id>
    <revision>
      <id>753455131</id>
      <parentid>709886768</parentid>
      <timestamp>2016-12-07T07:12:02Z</timestamp>
      <contributor>
        <username>Jessicapierce</username>
        <id>2003421</id>
      </contributor>
      <minor />
      <comment>minor copy edits</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3007" xml:space="preserve">{{Refimprove|date=September 2014}}
'''Agent Communication Language''' ('''ACL'''), proposed by the [[Foundation for Intelligent Physical Agents]] (FIPA), is a proposed standard language for [[Software agent|agent]] communications.  [[Knowledge Query and Manipulation Language]] (KQML) is another proposed standard.

The most popular ACLs are:
* FIPA-ACL &lt;ref&gt;{{cite journal|last=Poslad|first=Stefan|title=Specifying Protocols for Multi-agent System Interaction|year=2007|journal=ACM Transactions on Autonomous and Adaptive Systems|volume=4|issue=4|doi=10.1145/1293731.1293735}}&lt;/ref&gt; (by the [[Foundation for Intelligent Physical Agents]], a standardization consortium)
* [[KQML]] &lt;ref&gt;{{cite journal|last=Finin|first=Tim|author2= Richard Fritzson, Don McKay and Robin McEntire |title=KQML as an agent communication language|year=1994|conference=Proceedings of the third international conference on Information and knowledge management, CIKM '94 |pages= 456&#8211;463}}&lt;/ref&gt; (Knowledge Query and Manipulation Language)

Both rely on [[speech act]] theory developed by [[John Searle|Searle]] in the 1960s &lt;ref&gt;{{cite book|last= Searle|first=J.R.|year=1969|title=Speech Acts|publisher=Cambridge University Press, Cambridge, UK}}&lt;/ref&gt; and enhanced by [[Terry Winograd|Winograd]] and [[Fernando Flores|Flores]] in the 1970s. They define a set of [[Performative utterance|performatives]], also called Communicative Acts, and their meaning (e.g. ask-one). The content of the performative is not standardized, but varies from system to system.

To make agents understand each other they have to not only speak the same language, but also have a common [[Ontology (computer science)|ontology]]. An ontology is a part of the agent's knowledge base that describes what kind of things an agent can deal with and how they are related to each other.

Examples of frameworks that implement a standard agent communication language (FIPA-ACL) include FIPA-OS &lt;ref&gt;{{cite conference|last=Poslad|first=Stefan|author2=Philip Buckle and Robert Hadingham|title=The FIPA-OS agent platform: Open Source for Open Standards|year=2000|conference=Proceedings of 5th International Conference on the Practical Application Of Intelligent Agents And Multi-Agent Technology (PAAM)|pages=355&#8211;368}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|last=Poslad|first= S|author2=Buckle P, Hadingham R.G|title=Open Source, Standards and Scaleable Agencies|journal=Lecture Notes in Computer Science|year= 2001|volume=1887|pages=296&#8211;303|DOI=10.1007/3-540-47772-1_30}}&lt;/ref&gt;
and [[Java Agent Development Framework|Jade]]. &lt;ref&gt;{{cite conference|last=Bellifeminee|first=Fabio|author2=Agostino Poggi and Giovanni Rimassa|title=JADE: a FIPA2000 compliant agent development environment|year=2001|conference=Proceedings of the fifth international conference on Autonomous agents|pages=216&#8211;217}}&lt;/ref&gt;

==References==
{{reflist}}

[[Category:Formal languages]]
[[Category:Knowledge representation]]
[[Category:Multi-agent systems]]


{{Measurement-stub}}
{{computing-stub}}</text>
      <sha1>depv2ta47dl7kf6nl993hup24n5w5uw</sha1>
    </revision>
  </page>
  <page>
    <title>F-logic</title>
    <ns>0</ns>
    <id>4880312</id>
    <revision>
      <id>746560901</id>
      <parentid>726999355</parentid>
      <timestamp>2016-10-28T04:43:12Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* top */http&amp;rarr;https for [[Google Books]] and [[Google News]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5759" xml:space="preserve">'''F-logic''' ([[frame (data structure)|frame]] [[Logic programming|logic]]) is a [[knowledge representation]] and [[ontology language]]. F-logic combines the advantages of conceptual modeling with object-oriented, frame-based languages and offers a declarative, compact and simple syntax, as well as the well-defined semantics of a logic-based language. 
Features include, among others, object identity, complex objects, [[inheritance (computer science)|inheritance]], [[polymorphism (computer science)|polymorphism]], query methods, [[encapsulation (computer science)|encapsulation]]. F-logic stands in the same relationship to [[object-oriented programming]] as classical [[predicate calculus]] stands to [[relational database]] programming.

F-logic was developed by [[Michael Kifer]] at [[Stony Brook University]] and [[Georg Lausen]] at the [[University of Mannheim]]. F-logic was originally developed for deductive databases, but is now most frequently used for semantic technologies, especially the [[Semantic Web]]. F-logic is considered as one of the formalisms for [[Ontology (information science)|ontologies]], but [[description logic]] (DL) is more popular and accepted, as is the DL-based [[Web Ontology Language|OWL]].

A development environment for F-logic was developed in the NeOn project and is also used in a range of applications for information integration, [[question answering]] and [[semantic search]]. Prior to the version 4 of Prot&#233;g&#233; ontology editor, F-Logic is supported as one of the two kinds of ontology.

The frame syntax of the [[Rule Interchange Format]] Basic Logic Dialect (RIF BLD) standardized by the [[World Wide Web Consortium]] is based on F-logic; RIF BLD however does not include [[non-monotonic reasoning]] features of F-logic.&lt;ref name="Kr&#246;tzsch2010"&gt;{{cite book|author=M. Kr&#246;tzsch|title=Description Logic Rules|url=https://books.google.com/books?id=Z8h7AgAAQBAJ&amp;pg=PA10|year=October 2010|publisher=IOS Press|isbn=978-1-61499-342-1|page=10}}&lt;/ref&gt;

In contrast to [[description logic]] based ontology formalism the semantics of F-logic are normally that of a [[closed world assumption]] as opposed to DL's [[open world assumption]]. Also, F-logic is generally [[Undecidable problem|undecidable]]{{Citation needed|date=May 2014}}, whereas 
the [[SHOIN|SHOIN description logic]] that [[Web Ontology Language|OWL DL]] is based on is decidable. However it is possible to represent more expressive statements in F-logic than are possible with description logics.

The most comprehensive description of F-logic appears in.&lt;ref&gt;M. Kifer, G. Lausen, J. Wu (1995). ''Foundations of Object-Oriented and Frame-Based Languages]'', Journal of ACM, May 1995. [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.115.3586 PDF]&lt;/ref&gt; The preliminary paper &lt;ref&gt;M. Kifer and G. Lausen (1989). ''F-logic: a higher-order language for reasoning about objects, inheritance, and scheme'', ACM SIGMOD Conference, 1989. [http://dl.acm.org/citation.cfm?id=66939 PDF]&lt;/ref&gt; &lt;ref&gt;M. Kifer and G. Lausen (1997). ''F-logic: a higher-order language for reasoning about objects, inheritance, and scheme'', re-issued 1997. [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.48.7149 PDF]&lt;/ref&gt; has won the 1999 [http://www.sigmod.org/sigmod-awards/sigmod-awards#time Test of Time Award] from [[SIGMOD|ACM SIGMOD]]. A follow-up paper &lt;ref&gt;M. Kifer, W. Kim, Y. Sagiv (1992). ''Querying object-oriented databases'', ACM SIGMOD Conference, 1992. [http://dl.acm.org/citation.cfm?doid=130283.130342 PDF]&lt;/ref&gt; has won the 2002 [http://www.sigmod.org/sigmod-awards/sigmod-awards#time Test of Time Award] from [[SIGMOD|ACM SIGMOD]].

== F-logic syntax ==

Classes and individuals may be defined in F-logic as follows
 man::person.
 woman::person.
 brad:man.
 angelina:woman.
This states, that "men and women are persons" and that "Brad is a man", and "Angelina is a woman".

Statements about classes and individuals may be made as follows
 person[hasSon=&amp;gt;man].
 brad[hasSon-&amp;gt;&amp;gt;{maddox,pax}].
 married(brad,angelina).
This defines that "the son of a person is a man", "Maddox and Pax are the sons of Brad" and "Brad and Angelina are married". Note that &lt;code&gt;-&amp;gt;&amp;gt;&lt;/code&gt; is used for sets of values.

In addition it is possible to represent axioms in F-logic in the following manner
 man(X) &amp;lt;- person(X) AND NOT woman(X).
 FORALL X, Y &amp;lt;- X:person[hasFather-&gt;Y] &amp;lt;- Y:man[hasSon -&gt; X].
These mean "X is a man if X is a person but not a woman" and "if X is the son of Y then Y is the father of X".

The [[Flora-2]] system introduced a number of changes to the syntax of F-logic, making it more suitable for a knowledge representation and reasoning system as opposed to just a theoretical logic. In particular, variables became prefixed with a ?-mark, the distinction between functional and multi-valued properties was dropped and replaced by cardinality constraints, plus other important changes.

==F-logic based Languages==
* [[Flora-2]] is an extension of F-logic with [[HiLog]], [[Transaction logic]], and [[defeasible reasoning]].
* [http://pathlp.sourceforge.net/ PathLP] is a full logic programming language based on F-logic.
* [http://dbis.informatik.uni-freiburg.de/index.php?project=Florid FLORID] is a C++ &#8212; based implementation
* [http://www.wsmo.org/wsml/ Web Services Modeling Language (WSML)]
* [http://www.daml.org/services/swsl/ Semantic Web Services Language (SWSL)]
* [[ObjectLogic]] language is based on F-logic; [[OntoStudio]] is an ObjectLogic implementation by [[semafora systems GmbH]] (former [[Ontoprise GmbH]]).

== References ==
{{reflist}}

[[Category:Knowledge representation]]
[[Category:Semantic Web]]
[[Category:Logic programming languages]]
[[Category:Declarative programming languages]]</text>
      <sha1>p58gr8g2ipliwyz5gef3w6pukyrpc75</sha1>
    </revision>
  </page>
  <page>
    <title>Living graph</title>
    <ns>0</ns>
    <id>10308920</id>
    <revision>
      <id>613367185</id>
      <parentid>607865037</parentid>
      <timestamp>2014-06-18T02:05:25Z</timestamp>
      <contributor>
        <username>Monkbot</username>
        <id>20483999</id>
      </contributor>
      <minor />
      <comment>/* References */Task 5: Fix [[Help:CS1_errors#deprecated_params|CS1 deprecated coauthor parameter errors]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1617" xml:space="preserve">In terms of knowledge representation, a '''living graph''' (also referred to as a "lifeline", "living timeline"&lt;ref name="history"/&gt; or "fortune line".&lt;ref name="NatStrat"/&gt;) is a graph similar to a [[chronology]] timeline which places events along a vertical axis to reflect changes over time. The vertical axis can be used to represent many factors, such as relative importance, degrees of success/failure, danger/safety or happiness/sadness. In this sense they have been described as being "timelines with attitude".&lt;ref name="history"/&gt;

==References==
{{Reflist|refs=
&lt;ref name="history"&gt; {{Cite web
  | last = Dawson
  | first = Ian
  | authorlink = 
  |author2=Dawson, Patricia Ann
   | title = Introducing Living Graphs
  | work = 
  | publisher = 
  | url = http://thinkinghistory.co.uk/ActivityModel/ActModTimeline.html#graph
  | format = 
  | doi = 
  | accessdate = 25 March 2010}} from Thinking History website
&lt;/ref&gt;
&lt;ref name="NatStrat"&gt; {{Cite web
  | last = 
  | first = 
  | authorlink = 
  | title = Living Graphs and Fortune Lines
  | work = 
  | publisher = The National Strategies
  | url = http://downloads.nationalstrategies.co.uk/pdf/67dbff4bdcf5e5534122e1d6ead53abc.pdf
  | format = pdf
  | doi = 
  | accessdate = 25 March 2010}}&lt;/ref&gt;}}

==External links==
*[http://classtools.net/samples/sample.php?livingGraph Interactive Living Graph Templates in Flash]
*[http://www.face-online.org.uk/index.php?option=com_content&amp;task=view&amp;id=57&amp;Itemid=172 FACE Living Graph Exercise]

[[Category:Knowledge representation]]
[[Category:Diagrams]]
[[Category:Quality control tools]]
{{comm-design-stub}}</text>
      <sha1>6ahin5esj4kb9cllk309j36r11jl88d</sha1>
    </revision>
  </page>
  <page>
    <title>Knowledge space</title>
    <ns>0</ns>
    <id>11851855</id>
    <revision>
      <id>755006919</id>
      <parentid>754947386</parentid>
      <timestamp>2016-12-15T19:07:19Z</timestamp>
      <contributor>
        <username>David Eppstein</username>
        <id>2051880</id>
      </contributor>
      <minor />
      <comment>COI but very minor edit: Add two authorlinks</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6645" xml:space="preserve">{{about|knowledge spaces in mathematical psychology|the concept studied by philosopher Pierre L&#233;vy|Knowledge space (philosophy)}}

In [[mathematical psychology]], a '''knowledge space''' is a [[antimatroid|combinatorial structure]] describing the possible states of knowledge of a human learner.&lt;ref&gt;{{citation|title=Knowledge Spaces|last1=Doignon|first1=J.-P.|last2=Falmagne|first2=J.-Cl.|author2-link=Jean-Claude Falmagne|publisher=Springer-Verlag|year=1999|isbn = 3-540-64501-2}}.&lt;/ref&gt;
To form a knowledge space, one models a domain of knowledge as a [[set (mathematics)|set]] of concepts, and a feasible state of knowledge as a [[subset]] of that set containing the concepts known or knowable by some individual. Typically, not all subsets are feasible, due to prerequisite relations among the concepts. The knowledge space is the family of all the feasible subsets. Knowledge spaces were introduced in 1985 by [[Jean-Paul Doignon]] and [[Jean-Claude Falmagne]]&lt;ref&gt;{{citation|last1=Doignon|first1=J.-P.|last2=Falmagne|first2=J.-Cl.|author2-link=Jean-Claude Falmagne|year=1985|title=Spaces for the assessment of knowledge|journal=International Journal of Man-Machine Studies|volume=23|issue=2|pages=175&#8211;196|doi=10.1016/S0020-7373(85)80031-6}}.&lt;/ref&gt; and have since been studied by many other researchers.&lt;ref&gt;{{citation|title=Knowledge Spaces. Applications in Education|last1=Falmagne|first1=J.-Cl.|author1-link=Jean-Claude Falmagne|last2=Albert|first2=D.|last3=Doble|first3=C.|last4=Eppstein|first4=D.|author4-link=David Eppstein|last5=Hu|first5=X.|publisher=Springer|year=2013}}.&lt;/ref&gt;&lt;ref&gt;A [http://kst.hockemeyer.at/kst-bib.html bibliography on knowledge spaces] maintained by Cord Hockemeyer contains over 400 publications on the subject.&lt;/ref&gt; They also form the basis for two computerized tutoring systems, [http://wundt.kfunigraz.ac.at/rath/ RATH] and [[ALEKS]].&lt;ref&gt;[http://wundt.uni-graz.at/MathPsych/cda/overview_sokrates.htm Introduction to Knowledge Spaces: Theory and Applications], Christof K&#246;rner, Gudrun Wesiak, and Cord Hockemeyer, 1999 and 2001.&lt;/ref&gt;

It is possible to interpret a knowledge space as a special form of a restricted [[latent class model]].&lt;ref&gt;{{citation|title=About the connection between knowledge structures and latent class models |last1=Schrepp |first1=M. |journal=Methodology|volume=1|issue=3|pages=92&#8211;102|year=2005|doi=10.1027/1614-2241.1.3.92}}.&lt;/ref&gt;

==Definitions==
Some basic definitions used in the knowledge space approach -
*A tuple &lt;math&gt;(Q, K)&lt;/math&gt; consisting of a non-empty set &lt;math&gt;Q&lt;/math&gt; and a set &lt;math&gt;K&lt;/math&gt; of subsets from &lt;math&gt;Q&lt;/math&gt; is called a ''knowledge structure'' if &lt;math&gt;K&lt;/math&gt; contains the empty set and &lt;math&gt;Q&lt;/math&gt;.
*A knowledge structure is called a ''knowledge space'' if it is closed under union, i.e. if &lt;math&gt;S, T \in Q&lt;/math&gt; implies &lt;math&gt;S\cup T \in Q&lt;/math&gt;.
*A knowledge space is called a ''quasi-ordinal knowledge space'' if it is in addition closed under intersection, i.e. if &lt;math&gt;S, T \in Q&lt;/math&gt; implies &lt;math&gt;S\cap T \in Q&lt;/math&gt;. Closure under both unions and intersections gives (''Q'',&#8746;,&#8745;) the structure of a [[distributive lattice]]; [[Birkhoff's representation theorem]] for distributive lattices shows that there is a one-to-one correspondence between the set of all [[preorder|quasiorders]] on Q and the set of all quasi-ordinal knowledge spaces on Q. I.e., each quasi-ordinal knowledge space can be represented by a quasi-order and vice versa.

An important subclass of knowledge spaces, the ''well-graded knowledge spaces'' or ''learning spaces'', can be defined as satisfying two additional mathematical axioms:
# If &lt;math&gt;S&lt;/math&gt; and &lt;math&gt;T&lt;/math&gt; are both feasible subsets of concepts, then &lt;math&gt;S\cup T&lt;/math&gt; is also feasible. In educational terms: if it is possible for someone to know all the concepts in ''S'', and someone else to know all the concepts in ''T'', then we can posit the potential existence of a third person who combines the knowledge of both people.
# If &lt;math&gt;S&lt;/math&gt; is a nonempty feasible subset of concepts, then there is some concept ''x'' in ''S'' such that &lt;math&gt;S\setminus\{x\}&lt;/math&gt; is also feasible. In educational terms: any feasible state of knowledge can be reached by learning one concept at a time, for a finite set of concepts to be learned.
A set family satisfying these two axioms forms a [[mathematical structure]] known as an [[antimatroid]].

==Construction of knowledge spaces==
In practice, there exist several methods to construct knowledge spaces. The most frequently used method is querying experts. There exist several querying algorithms that allow one or several experts to construct a knowledge space by answering a sequence of simple questions.&lt;ref&gt;{{citation|title=Extracting human expertise for constructing knowledge spaces: An algorithm |last1=Koppen |first1=M. |journal=Journal of Mathematical Psychology|volume=37|pages=1&#8211;20 |year=1993 |doi=10.1006/jmps.1993.1001}}.&lt;/ref&gt;&lt;ref&gt;{{citation|title=How to build a knowledge space by querying an expert |last1=Koppen |first1=M. |last2=Doignon |first2=J.-P. |journal=Journal of Mathematical Psychology|volume=34|issue=3|pages=311&#8211;331 |year=1990 |doi=10.1016/0022-2496(90)90035-8}}.&lt;/ref&gt;&lt;ref&gt;{{citation|title=A simulation study concerning the effect of errors on the establishment of knowledge spaces by querying experts |last1=Schrepp |first1=M. |last2=Held |first2=T. |journal=Journal of Mathematical Psychology|volume=39|issue=4|pages=376&#8211;382 |year=1995|doi=10.1006/jmps.1995.1035}}&lt;/ref&gt;

Another method is to construct the knowledge space by explorative data analysis (for example by [[Item tree analysis]]) from data.&lt;ref&gt;{{citation|title=Extracting knowledge structures from observed data |last1=Schrepp |first1=M. |journal=[[British journal of mathematical and statistical psychology]]|volume= 52|issue=2 |pages=213&#8211;224 |year=1999|doi=10.1348/000711099159071}}&lt;/ref&gt;&lt;ref&gt;{{citation|title=A method for the analysis of hierarchical dependencies between items of a questionnaire |last1=Schrepp |first1=M. |journal= Methods of Psychological Research Online|volume=19|pages=43&#8211;79  |year=2003|url=http://www.dgps.de/fachgruppen/methoden/mpr-online/issue19/art3/mpr106_04.pdf }}&lt;/ref&gt;
A third method is to derive the knowledge space from an analysis of the problem solving processes in the corresponding domain.&lt;ref&gt;{{citation|title=Knowledge Spaces: Theories, Empirical Research, Applications|last1=Albert|first1=D.|last2=Lukas|first2=J.|publisher=Lawrence Erlbaum Associates, Mahwah, NJ|year=1999}}&lt;/ref&gt;

==References==

{{reflist}}

[[Category:Cognition]]
[[Category:Knowledge representation]]</text>
      <sha1>nhgbphrb2pnlx5ojv2e0prn61hq7grs</sha1>
    </revision>
  </page>
  <page>
    <title>SERVQUAL</title>
    <ns>0</ns>
    <id>2755912</id>
    <revision>
      <id>761417820</id>
      <parentid>758350469</parentid>
      <timestamp>2017-01-22T21:59:01Z</timestamp>
      <contributor>
        <username>BronHiggs</username>
        <id>29331605</id>
      </contributor>
      <comment>add marketing template</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="28349" xml:space="preserve">{{marketing}}

'''SERVQUAL''' is a multi-dimensional research instrument, designed to capture consumer expectations and perceptions of a service along the five dimensions that are believed to represent service quality. SERVQUAL is built on the expectancy-disconfirmation paradigm, which in simple terms means that service quality is understood as the extent to which consumers' pre-consumption expectations of quality are confirmed or disconfirmed by their actual perceptions of the service experience. When the SERVQUAL questionnaire was first published in 1988 by a team of academic researchers, A. Parasurman, [[Valarie Zeithaml]] and [[Leonard Berry (professor)|Leonard L. Berry]]  to measure quality in the service sector,&lt;ref&gt;Parasuraman, A, Ziethaml, V. and Berry, L.L., "SERVQUAL: A Multiple- Item Scale for Measuring Consumer Perceptions of Service Quality' ''Journal of Retailing,'' Vo. 62,  no. 1, 1988, pp 12-40 &lt;online:https://www.researchgate.net/publication/225083802_SERVQUAL_A_multiple-_Item_Scale_for_measuring_consumer_perceptions_of_service_quality&gt;&lt;/ref&gt; it represented a breakthrough in the measurement methods used for service quality research. The diagnostic value of the instrument is supported by the ''model of service quality'' which forms the conceptual framework for the development of the scale (i.e. instrument or questionnaire). The instrument has been widely applied in a variety of contexts and cultural settings and found to be relatively robust. It has become the dominant measurement scale in the area of service quality. In spite of the long-standing interest in SERVQUAL and its myriad of context-specific applications, it has attracted some criticism from researchers.

==The SERVQUAL instrument==

[[File:Measuring service quality using SERVQUAL model (Kumar et al, 2009).png|thumb|The five dimensions of service quality)]]

SERVQUAL is a multidimensional instrument (i.e. questionnaire or measurement scale) designed to measure service quality by capturing respondents&#8217; expectations and perceptions along the five dimensions of service quality.&lt;ref&gt;Parasuraman, A., Berry, L.L. and Zeithaml, V.A.,  &#8220;Refinement and Reassessment of the SERVQUAL scale,&#8221; ''Journal of Retailing,'' Vol. 67, no. 4, 1991, pp 57-67&lt;/ref&gt;  The questionnaire consists of matched pairs of items; 22 expectation items and 22 perceptions items, organised into five dimensions which are believed to  align with the consumer&#8217;s mental map of service quality dimensions. Both the expectations component and the perceptions component of the questionnaire consist a total of 22 items, comprising 4 items to capture tangibles, 5 items to capture reliabiility, 4 items for responsiveness, 5 items for assurance and 5 items to capture empathy.&lt;ref&gt;Parasuraman, A, Ziethaml, V. and Berry, L.L., "SERVQUAL: A Multiple- Item Scale for Measuring Consumer Perceptions of Service Quality' ''Journal of Retailing,'' Vol. 62,  no. 1, 1988, p. 25&lt;/ref&gt; The questionnaire is designed to be administered in a face-to-face interview and requires a moderate to large size sample for statistical reliability. In practice, it is customary to add additional items such as the respondent's demographics, prior experience with the brand or category and behavioural intentions (intention to revisit/ repurchase, loyalty intentions and propensity to give word-of-mouth referrals). Thus, the final questionnaire may have up to 60 items and typically takes at least one hour, per respondent, to administer. The length of the questionnaire combined with sample size requirements contribute to substantial costs in administration and data analysis.


{| class="wikitable"
|+ Summary of SERVQUAL items &lt;ref&gt;Based on Parasuraman, A, Ziethaml, V. and Berry, L.L., "SERVQUAL: A Multiple- Item Scale for Measuring Consumer Perceptions of Service Quality' ''Journal of Retailing,'' Vol. 62,  no. 1, 1988, p. 22, 25 and 29&lt;/ref&gt;
|-
! Dimension
! No. of Items in Questionnaire
! Definition
|-
| '''Reliability'''
| colspan="1" style="text-align: center;" | 5 
| The ability to perform the promised service dependably and accurately 
|-
| '''Assurance'''
| colspan="1" style="text-align: center;" | 5 
| The knowledge and courtesy of  employees and their ability to convey trust and confidence
|-
| '''Tangibles'''
| colspan="1" style="text-align: center;" | 4 
| The appearance of physical facilities, equipment, personnel and communication materials 
|-
| '''Empathy'''
| colspan="1" style="text-align: center;" | 5 
| The provision of caring, individualized attention to customer
|-
| '''Responsiveness'''
| colspan="1" style="text-align: center;" | 4 
| The willingness to help customers and to provide prompt service
|}

The instrument was developed over a five year period; was tested, pre-tested and refined before appearing in its final form. The instrument's developers, claim that it is a highly reliable and valid instrument.&lt;ref&gt;Zeithaml, V., Parasuraman, A. and Berry, L.L., ''Delivering Service Quality: Balancing Customer Perceptions and Expectations,'' N.Y., The Free Press, 1990&lt;/ref&gt; Certainly, it has been widely used and adapted in service quality research for numerous industries and various geographic regions. In application, many researchers are forced to make minor modifications to the instrument as necessary for context-specific applications. Some researchers label their revised instruments with innovative labels such as EDUQUAL (educational context),&lt;ref&gt;Mahapatra, S.S. and Khan, M.S., "A Methodology for Evalution of Service Quality Using Neural Networks," in ''Proceedings of the International Conference on Global Manufacturing and Innovation,' July 27&#8211;29, 2006&lt;/ref&gt; HEALTHQUAL (hospital context) &lt;ref&gt;Lee, D., "HEALTHQUAL: a multi-item scale for assessing healthcare service quality," Service Business, 2016; pp 1-26, doi:10.1007/s11628-016-0317-2&lt;/ref&gt; and ARTSQUAL (art museum).&lt;ref&gt;Higgs, B., Polonsky M.J. and Hollick, M., &#8220;Measuring Expectations: Pre and Post Consumption: Does It Matter?&#8221; ''Journal of Retailing and Consumer Services'', vol. 12, no. 1, 2005&lt;/ref&gt;


{| class="wikitable"
|+ Examples of matched pairs of items in the SERVQUAL questionnaire &lt;ref&gt;Based on Parasuraman, A, Ziethaml, V. and Berry, L.L., "SERVQUAL: A Multiple- Item Scale for Measuring Consumer Perceptions of Service Quality' ''Journal of Retailing,'' Vol. 62,  no. 1, 1988, [Appendix: SERVQUAL questionnaire, pp 37-40&lt;/ref&gt;
|-
! Dimension
! Sample expectations item
! Sample perceptions item
|-
| '''Reliability'''
| When excellent telephone companies promise to do something by a certain time, they do so
| XYZ company provides it services at the promised time 
|-
| '''Assurance'''
| The behaviour of employees in excellent banks will instill confidence in customers
| The behaviour of employees in the XYZ bank instils confidence in you.
|-
| '''Tangibles'''
| Excellent telephone companies will have modern looking equipment
| XYZ company has modern looking equipment
|-
| '''Empathy'''
| Excellent banks will have operating hours convenient to customers
| XYZ bank has convenient operating hours
|-
| '''Responsiveness'''
| Employees of excellent telephone companies will never be too busy to help a customer
| XYZ employees are never too busy to help you
|}

The SERVQUAL questionnaire has been described as "the most popular standardized questionnaire to measure service quality." &lt;ref&gt;Caruanaa,A., Ewing, M.T and Ramaseshanc, B., "Assessment of the Three-Column Format SERVQUAL: An Experimental Approach," ''Journal of Business Research,'' Vol. 49, no. 1, July 2000, pp 57&#8211;65&lt;/ref&gt; It is widely used by service firms, most often in conjunction with other measures of service quality and customer satisfaction. The SERVQUAL instrument was developed as part of a broader conceptualisation of how customers understand service quality. This conceptualisation is known as the ''model of service quality'' or more popularly as the ''gaps model.''

==The model of service quality==

The model of service quality, popularly known as the ''gaps model'' was developed by a group of American authors, A. Parasuraman, [[Valarie Zeithaml|Valarie A. Zeithaml]] and [[Leonard Berry (professor)|Len Berry]], in a systematic research program carrie out between 1983 and 1988. The model identifies the principal dimensions (or components) of service quality; proposes a scale for measuring service quality (SERVQUAL) and suggests possible causes of service quality problems. The model's developers originally identified [[SERVQUAL#Determinants|ten dimensions of service quality]], but after testing and retesting, some of the dimensions were found to be autocorrelated and the total number of dimensions was reduced to five, namely - reliability, assurance, tangibles, empathy and responsiveness.  These five dimensions are thought to represent the dimensions of service quality across a range of industries and settings. &lt;ref&gt;Zeithaml, V.A., Berry, L.L. and Parasuraman, A., "Communication and Control Processes in the Delivery of Service Quality," ''Journal of Marketing,'' Vol. 52, No. 2, 1988, pp. 35-48 &lt;/ref&gt; Among students of marketing, the memnonic,  '''RATER''', an acronym formed from the first letter of each of the five dimensions is often used as an aid to recall.

[[File:Servqual.jpg|thumb|A simplified model of service quality]]

Businesses use the SERVQUAL instrument (i.e. questionnaire) to measure potential service quality problems and the model of service quality to help diagnose possible causes of the problem. The model of service quality is built on the ''expectancy-confirmation paradigm'' which suggests that consumers perceive quality in terms of their perceptions of how well a given service delivery meets their expectations of that delivery.&lt;ref&gt;Oliver, R.L., Balakrishnan, P.V. S. and Barry, B., "Outcome Satisfaction in Negotiation: A Test of Expectancy Disconfirmation," ''Organizational Behavior and Human Decision Processes,'' Vol. 60, no. 2, 1994, Pages 252-275&lt;/ref&gt; Thus, service quality can be conceptualised as a simple equation:

'''SQ = P- E'''
: where;

: '''SQ''' is service quality
: '''P''' is the individual's perceptions of given service delivery
: '''E''' is the individual's expectations of a given service delivery

When customer expectations are greater than their perceptions of received delivery, service quality is deemed low. When perceptions exceed expectations then service quality is high. The model of service quality identifies five gaps that may cause customers to experience poor service quality. In this model, gap 5 is the service quality gap and is the ''only'' gap that can be directly measured. In other words, the SERVQUAL instrument was specifically designed to capture gap 5. In contrast, Gaps 1-4 cannot be measured, but have diagnostic value.


{| class="wikitable"
|+ Summary of Gaps with Diagnostic Indications &lt;ref&gt;Based on Zeithaml, V.A., Berry, L.L. and Parasuraman, A., "Communication and Control Processes in the Delivery of Service Quality," ''Journal of Marketing,'' Vol. 52, No. 2, 1988, pp. 35-48 &lt;/ref&gt;
|-
! '''Gap'''
! '''Brief description'''
! '''Probable Causes
|-
| '''Gap 1'''
The Knowledge Gap
| Difference between the target market&#8217;s expected service and management&#8217;s perceptions of the target market&#8217;s expected service
| 
* Insufficient marketing research
* Inadequate upward communications
* Too many layers of management
|-
| '''Gap 2'''
The standards Gap 
| Difference between management&#8217;s perceptions of customer expectations and the translation into service procedures and specifications
| 
* Lack of management commitment to service quality
* Employee perceptions of infeasibility
* Inadequate goal setting
* Inadequate task standardisation
|-
|'''Gap 3''' 
The Delivery Gap
| Difference between service quality specifications and the service actually delivered
| 
* Technical breakdowns or malfunctions
*  Role conflict/ ambiguity
*  Lack of perceived control
* Poor employee-job fit
* Poor technology- fit
* Poor supervision or training
|-
| '''Gap 4''' 
The Communications Gap
| Difference between service delivery intentions and what is communicated to the customer
| 
*Lack of horizontal communications
* Poor communication with advertising agency
* Inadequate communications between sales and operations
* Differences in policies and procedures across branches or divisions of an entity
* Propensity to overpromise
|}

== Development of the model ==

The development of the model of service quality involved a systematic research undertaking which began in 1983 and after various refinements resulted in the publication of the SERVQUAL instrument in 1988.&lt;ref&gt;Parasuraman, A., Berry, L.L.,  Zeithaml, V. A., "Understanding Customer Expectations of Service," ''Sloan Management Review,'' Vol. 32, no. 3, 1991, p. 39&lt;/ref&gt; The model's developers began with an exhaustive literature search in order to identify items that were believed to impact on perceived service quality. This initial search identified some 100 items which were used in the first rounds of consumer testing. Prelimiary data analysis, using a data reduction technique known as [[factor analysis]] (also known as [[principal components analysis]]) revealed that these items loaded onto ten dimensions (or components) of service quality. The initial ten dimensions that were believed to represent service quality were:

# '''[[competence (human resources)|Competence]]''' is the possession of the required skills and knowledge to perform the service. For example, there may be competence in the knowledge and skill of contact personnel, knowledge and skill of operational support personnel and research capabilities of the organization.
# '''Courtesy''' is the consideration for the customer's property and a clean and neat appearance of contact personnel, manifesting as politeness, respect, and friendliness.
# '''Credibility''' includes factors such as trustworthiness, belief and honesty. It involves having the customer's best interests at prime position. It may be influenced by company name, company reputation and the personal characteristics of the contact personnel.
# '''Security''' enables the customer to feel free from danger, risk or doubt including physical safety, financial security and confidentiality.
# '''Access''' is approachability and ease of contact. For example, convenient office operation hours and locations.
# '''Communication''' means both informing customers in a language they are able to understand and also listening to customers. A company may need to adjust its language for the varying needs of its customers. Information might include for example, explanation of the service and its cost, the relationship between services and costs and assurances as to the way any problems are effectively managed.
# '''Knowing the customer''' means making an effort to understand the customer's individual needs, providing individualized attention, recognizing the customer when they arrive and so on. This in turn helps to delight the customers by rising above their expectations. 
# '''Tangibles''' are the physical evidence of the service, for instance, the  appearance of the physical facilities, tools and equipment used to provide the service; the appearance of personnel and communication materials and the presence of other customers in the service facility.
# '''Reliability'''  is the ability to perform the promised service in a dependable and accurate manner. The service is performed correctly on the first occasion, the accounting is correct, records are up to date and schedules are kept.
# '''Responsiveness''' is the readiness and willingness of employees to help customers by providing prompt timely services, for example, mailing a transaction slip immediately or setting up appointments quickly.

Further testing suggested that some of the ten preliminary dimensions of service quality were closely related or autocorrelated. Thus the ten initial dimensions were reduced and the labels amended to accurately reflect the revised dimensions. By the early 1990s, the authors had refined the model to five factors which in testing, appear to be relatively stable and robust.

# '''Reliability:''' the ability to perform the promised service dependably and accurately
# '''Assurance:''' the knowledge and courtesy of employees and their ability to convey trust and confidence
# '''Tangibles:''' the appearance of physical facilities, equipment, personnel and communication materials
# '''Empathy:''' the provision of caring, individualized attention to customers
# '''Responsiveness:''' the willingness to help customers and to provide prompt service

These are the five dimensions of service quality that form the basis of the individual items in the SERVQUAL research instrument (questionnaire). The acronym RATER, is often used to help students of marketing remember the five dimensions of quality explicitly mentioned in the research instrument.

Nyeck, Morales, Ladhari, and Pons (2002) stated the SERVQUAL measuring tool &#8220;appears to remain the most complete attempt to conceptualize and measure service quality&#8221; (p.&amp;nbsp;101). The SERVQUAL measuring tool has been used by many researchers across a wide range of service industries and contexts, such as healthcare, banking, financial services, and education (Nyeck, Morales, Ladhari, &amp; Pons, 2002).

== Criticisms of SERVQUAL and the model of service quality ==

Although the SERVQUAL instrument has been widely applied in a variety of industry and cross-cultural contexts, there are many criticisms of the approach. Francis Buttle published one of the most comprehensive criticisms of the model of service quality and the associated SERVQUAL instrument in 1996 in which both operational and theoretical concerns were identified.&lt;ref&gt;Buttle, F., &#8220;SERVQUAL: Review, Critique, Research Agenda,"  ''European Journal of Marketing,'' Vol. 30, no.  1, pp. 8-32 1996&lt;/ref&gt; Some of the more important criticisms include:

: ''Face validity'': The model of service quality has its roots in the expectancy-disconfimation paradigm that informs customer satisfaction.&lt;ref&gt;Oliver, R.L., ''Satisfaction: A Behavioural Perspective on the Consumer,'' Boston, MA, Irwin McGraw-Hill, 1996&lt;/ref&gt; A number of researchers have argued that the research instrument actually captures ''satisfaction'' rather than ''service quality''.&lt;ref&gt;Souca, Ma. L., "SERVQUAL - Thirty years of research on service quality with implications for customer satisfaction," in ''Marketing - from Information to Decision,'' [Proceedings of the International Conference], Cluj-Napoca: Babes Bolyai University, 2011, pp 420 -429&lt;/ref&gt; Other researchers have questioned the validity of conceptualising service quality as a gap.&lt;ref&gt;van Dyke, T.P.,   Kappelman, L.A. and Prybutok, V.R.,"Measuring Information Systems Service Quality: Concerns on the Use of the SERVQUAL Questionnaire," ''MIS Quarterly,'' Vol. 21, No. 2, 1997, pp. 195-208, &lt;Online:  http://www.jstor.org/stable/249419&gt;&lt;/ref&gt;

: ''Construct validity'': The model's developers tested and retested the SERVQUAL scale for reliability and validity. However, at the same time the model's developers recommended that applied use of the instrument should modify or adapt the for specific contexts. Any attempt to adapt or modify the scale will have implications for the validity of items with implications for the validity of the dimensions of reliability, assurance, tangibles, empathy and repsonsiveness.&lt;ref&gt;Smith, A.M., "Measuring Service Quality: Is SERVQUAL now redundant?  ''Journal of Marketing Management,'' [Special Issue: Marketing in the Services Sector],  Vol 11, no. 1, 1995, pp 257-276&lt;/ref&gt;
 
: ''Ambiguity of expectations construct'': SERVQUAL is designed to be administered after respondents have experienced a service. They are therefore asked to ''recall'' their pre-experience expectations. However,  recall is not always accurate, raising concerns about whether the research design accurately captures true pre-consumption expectations. In addition, studies show that expectations actually change over time. Consumers are continually modifying their expectations as they gain experience with a product category or brand.&lt;ref&gt;Parasuraman, A.; Berry, Leonard L.; Zeithaml, Valarie A., "Understanding Customer Expectations of Service," ''Sloan Management Review,'' Vol. 32, no. 3, 1991, pp 39 - 48&lt;/ref&gt; In light of these insights, concerns have been raised about whether the act of experiencing the service might colour respondents' expectations.

: The way that expectations has been operationalised also represents a concern for theorists investigating the validity of the gaps model. The literature identifies different types of expectations.&lt;ref&gt;Parasuraman, A., Zeithaml, V. A., Berry, L. L., "Reassessment of Expectations as a Comparison Standard in Measuring Service Quality: Implications for Further Research," ''Journal of Marketing,'' Vol. 58 January 1994, pp 111&#8211;124&lt;/ref&gt; Of these, there is an argument that only ''forecast expectations'' are true expectations. Yet, the SERVQUAL instrument appears to elicit ''ideal expectations''.&lt;ref&gt;Johnson, C. and Mathews, B.P. , "The influence of experience on service expectations", ''International Journal of Service Industry Management,'' Vol. 8 no. 4, pp 290-305&lt;/ref&gt;  Note the wording in the questionnaire in the preceding figure which grounds respondents in their expectations of what ''excellent'' companies will do. Subtle use of words can elicit different types of expectations.  Capturing true expectations is important because it has implications for service quality scores. When researchers elicit ideal expectations, overall service quality scores are likely to be lower, making it much more difficult for marketers to deliver on those expectations.&lt;ref&gt;Boulding, W., Kalra, A., Staelin, R. and Zeithaml, V. A., "Dynamic Process Model of Service Quality: From Expectations to Behavioral Intentions," ''Journal of Marketing Research,'' Vol. 30, no 1, 1993, pp 7-27&lt;/ref&gt;

:''Questionnaire length:''  The matched pairs design of the questionnaire (total of 22 expectation items plus 22 perception items= 44 total items) makes for a very long questionnaire. If researchers add demographic and other behavioural items such as prior experience with product or category and the standard battery of demographics including: age, gender, occupation, educational attainment etc. then the average questionnaire will have around 60 items. In practical terms, this means that the questionnaire would take more than one hour per respondent to administer in a face-to-face interview. Lengthy questionnaires are known to induce ''respondent fatigue'' which may have potential implications for data reliability. In addition, lengthy questionnaires add to the time and cost involved in data collection and data analysis. Coding, collation and interpretation of data is very time consuming and in the case of lengthy questionnaires administered across large samples, the findings cannot be used to address urgent quality-related problems. In some cases, it may be necessary to carry out 'quick and dirty' research while waiting for the findings of studies with superior research design.

: Some analysts have pointed out that the SERVPERF instrument, developed by Cronin and Taylor,&lt;ref&gt;Cronin, J. J. and Taylor, S. A., "Measuring Service Quality: A Re-examination and Extension," ''Journal of Marketing,'' Vol. 56, no. 3, 1992, pp 55-68.&lt;/ref&gt;&lt;ref&gt;Cronin J.J., Steven, J. and Taylor, A., "SERVPERF versus SERVQUAL: Reconciling performance based  and  perceptions-minus-expectations  measurement  of  service  quality," ''Journal  of  Marketing,''  Vol.  58, January, 1994, pp. 125-131&lt;/ref&gt; and which reduced the number of questionnaire items by half (22 perceptions items only), achieves results that correlate well with SERVQUAL, with no reduction in diagnostic power, improved data accuracy through reductions in respondent boredom and fatigue and savings in the form of reduced administration costs.

:''Dimensional instability'':  A number of studies have reported that the five dimensions of service quality implicit in the model (reliability, assurance, tangibles, empathy and responsiveness) do not hold up when the research is replicated in different countries, different industries, in different market segments or even at different time periods.&lt;ref&gt;Carman, J.M., "Consumer Perceptions of Service Quality: An assessment of the SERVQUAL dimensions," ''Journal of Retailing,'' Vol. 66, no 1, 1990&lt;/ref&gt;&lt;ref&gt;Lam, S. K and Woo, K. S.,  "Measuring Service Quality: A test-retest reliability investigation of SERVQUAL,"  ''Journal of the Market Research Society, '' Vol. 39, no. 2, 1997, pp 381-396&lt;/ref&gt; Some studies report that the SERVQUAL items do not always load onto the same factors. In some empirical research, the items load onto fewer dimensions, while other studies report tht the items load onto more than five dimensions of quality. In statistical terms, the robustness of the factor loadings is known as a model's ''dimensional stability.'' Across a wide range of empirical studies, the factors implicit in the SERVQUAL instrument have been shown to be unstable.&lt;ref&gt;Niedricha, R.W., Kiryanovab, E. and Black, W.C., "The Dimensional Stability of the Standards used in the Disconfirmation Paradigm," ''Journal of Retailing,'' Vol. 81, no. 1, 2005, pp 49&#8211;57&lt;/ref&gt; Problems associated  with  the stability of the factor loadings may be attributed, at least in part, to the  requirement  that  each  new  SERVQUAL  investigation needed  to  make  context-sensitive  modifications to  the  instrument  in  order  to  accommodate the  unique  aspects  of  the focal service setting or problem. However, it has also been hypothesised that the dimensions of service quality represented by the SERVQUAL research instrument fail to capture the true dimensionality of the service quality construct and that there may not be a universal set of service quality dimensions that are relevant across all service industries.&lt;ref&gt;Miller, R.E., Hardgrave, B.C. and Jones, R.W., "SERVQUAL Dimensionality: An investigation of presentation order effect," ''International Journal of Services and Standards,'' Vol. 7, no. 1 DOI: 10.1504/IJSS.2011.040639&lt;/ref&gt;

In spite of these criticisms, the SERVQUAL instrument, or any one of its variants (i.e. modified forms), dominates current research into service quality.&lt;ref&gt;Ladhari, R., "A review of twenty years of SERVQUAL research", ''International Journal of Quality and Service Sciences,'' Vol. 1 no. 2, pp.172 - 198&lt;/ref&gt;  In a review of more than 40 articles that made use of SERVQUAL. a team of researchers found that  &#8220;few researchers concern themselves with the validation of the measuring tool&#8221;.&lt;ref&gt;Nyeck, S., Morales, M., Ladhari, R., &amp; Pons, F., "10 Years of Service Quality  Measurement: Reviewing the use of the SERVQUAL Instrument," ''Cuadernos de Difusion,'' Vol. 7, no 13, pp 101-107.&lt;/ref&gt;  SERVQUAL is not only the subject of academic papers, but it is also widely used by industry practitioners.&lt;ref&gt;Asubonteng, P., McCleary, K.J. and Swan, J.E.,  "SERVQUAL revisited: a critical review of service quality", ''Journal of Services Marketing,'' Vol. 10, no 6, 1996, pp 62-81&lt;/ref&gt;

== See also==

* [[Customer satisfaction]]
* [[Customer satisfaction research]]
* [[Disconfirmed expectancy]]
* [[Quality management]]
* [[Service quality]]
* [[Services marketing]]

== References ==
{{Reflist}}

== External links ==

*[http://www.farrell-associates.com.au/BSOM/Papers/SERVQUAL.doc/ SERVQUAL questionnaire]  - Annotated copy of SERVQUAL questionnaire
*[http://www.kinesis-cem.com/pdf/ServQual.pdf/ SERVQUAL Instructions] - Detailed instructions for administering the SERVQUAL questionnaire

==Further reading==
* Luis Filipe Lages &amp; Joana Cosme Fernandes, 2005, "The SERPVAL scale: A multi-item instrument for measuring service personal values", ''Journal of Business Research,'' Vol.58, Issue 11, pp 1562&#8211;1572.
* Deborah McCabe, Mark S. Rosenbaum, and Jennifer Yurchisin (2007), &#8220;Perceived Service Quality and Shopping Motivations:  A Dynamic Relationship,&#8221; ''Services Marketing Quarterly,'' 29 (1), pp 1&#8211;21.

[[Category:Knowledge representation]]
[[Category:Quality management]]
[[Category:Service industries]]</text>
      <sha1>64fsdixdjbdf30ba43a5nxagp70835z</sha1>
    </revision>
  </page>
  <page>
    <title>Allen's interval algebra</title>
    <ns>0</ns>
    <id>10483232</id>
    <revision>
      <id>741648490</id>
      <parentid>730568889</parentid>
      <timestamp>2016-09-28T20:47:52Z</timestamp>
      <contributor>
        <username>Zeno Gantner</username>
        <id>6435</id>
      </contributor>
      <minor />
      <comment>/* References */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4526" xml:space="preserve">{{Use dmy dates|date=June 2013}}
''For the type of boolean algebra called interval algebra, see [[Boolean algebra (structure)#Examples|Boolean algebra (structure)]]''

'''Allen's interval algebra''' is a [[calculus (disambiguation)|calculus]] for [[spatial-temporal reasoning|temporal reasoning]] that was introduced by [[James F. Allen]] in 1983.

The calculus defines possible relations between time intervals and provides a composition table that can be used as a basis
for reasoning about temporal descriptions of events.

==Formal description==

=== Relations ===

The following 13 base relations capture the possible relations between two intervals.

{| class="wikitable"
 !Relation
 !Illustration
 !Interpretation
 |-
 |&lt;math&gt;X \,\mathrel{\mathbf{&lt;}}\, Y&lt;/math&gt;
&lt;math&gt;Y \,\mathrel{\mathbf{&gt;}}\, X&lt;/math&gt;
 |[[Image:Allen calculus before.png|X takes place before Y]]
 |X takes place before Y
 |-
 |&lt;math&gt;X \,\mathrel{\mathbf{m}}\, Y&lt;/math&gt;
&lt;math&gt;Y \,\mathrel{\mathbf{mi}}\, X&lt;/math&gt;
 |[[Image:Allen calculus meet.png|X meets Y]]
 |X meets Y (''i'' stands for '''''i'''nverse'')
 |-
 |&lt;math&gt;X \,\mathrel{\mathbf{o}}\, Y&lt;/math&gt;
&lt;math&gt;Y \,\mathrel{\mathbf{oi}}\, X&lt;/math&gt;
 |[[Image:Allen calculus overlap.png|X overlaps with Y]]
 |X overlaps with Y
 |-
 |&lt;math&gt;X \,\mathrel{\mathbf{s}}\, Y&lt;/math&gt;
&lt;math&gt;Y \,\mathrel{\mathbf{si}}\, X&lt;/math&gt;
 |[[Image:Allen calculus start.png|X starts with Y]]
 |X starts Y
 |-
 |&lt;math&gt;X \,\mathrel{\mathbf{d}}\, Y&lt;/math&gt;
&lt;math&gt;Y \,\mathrel{\mathbf{di}}\, X&lt;/math&gt;
 |[[Image:Allen calculus during.png|X during Y]]
 |X during Y
 |-
 |&lt;math&gt;X \,\mathrel{\mathbf{f}}\, Y&lt;/math&gt;
&lt;math&gt;Y \,\mathrel{\mathbf{fi}}\, X&lt;/math&gt;
 |[[Image:Allen calculus finish.png|X finishes with Y]]
 |X finishes Y
 |-
 |&lt;math&gt;X \,\mathrel{\mathbf{=}}\, Y&lt;/math&gt;
 |[[Image:Allen calculus equal.png|X is equal to Y]]
 |X is equal to Y

 |}Using this calculus, given facts can be formalized and then used for automatic reasoning. Relations between intervals are formalized as sets of base relations.

The sentence
: ''During dinner, Peter reads the newspaper. Afterwards, he goes to bed.''
is formalized in Allen's Interval Algebra as follows:

&lt;math&gt;\mbox{newspaper } \mathbf{\{ \operatorname{d}, \operatorname{s}, \operatorname{f} \}} \mbox{ dinner}&lt;/math&gt;

&lt;math&gt;\mbox{dinner } \mathbf{\{ \operatorname{&lt;}, \operatorname{m} \}} \mbox{ bed}&lt;/math&gt;

In general, the number of different relations between n intervals is 1, 1, 13, 409, 23917, 2244361... [http://oeis.org/A055203 OEIS A055203]. The special case shown above is for n=2.

===Composition of relations between intervals===
For reasoning about the relations between temporal intervals, Allen's Interval Algebra provides a [[Relation composition|composition]] table. Given the relation between &lt;math&gt;X&lt;/math&gt; and &lt;math&gt;Y&lt;/math&gt; and the relation between &lt;math&gt;Y&lt;/math&gt; and &lt;math&gt;Z&lt;/math&gt;, the composition table allows for concluding about the relation between &lt;math&gt;X&lt;/math&gt; and &lt;math&gt;Z&lt;/math&gt;. Together with a [[Inverse relation|converse]] operation, this turns Allen's Interval Algebra into a [[relation algebra]].

For the example, one can infer &lt;math&gt;\mbox{newspaper } \mathbf{\{ \operatorname{&lt;}, \operatorname{m} \}} \mbox{ bed}&lt;/math&gt;.

==Extensions==
Allen's Interval Algebra can be used for the description of both temporal intervals and spatial configurations. For the latter use, the relations are interpreted as describing the relative position of spatial objects. This also works for three-dimensional objects by listing the relation for each coordinate separately.

==Implementation==
* [https://code.google.com/p/allenintervalrelationships/ A simple java library implementing the concept of Allen's temporal relations and the path consistency algorithm]

==See also==
* [[Temporal logic]]
* [[Logic]]
* [[Region Connection Calculus]].
* [[Spatial relation]] (analog)
* [[Commonsense reasoning]]

==References==
* James F. Allen: ''Maintaining knowledge about temporal intervals''. In: ''Communications of the ACM''. 26 November 1983. ACM Press. pp.&amp;nbsp;832&#8211;843, ISSN 0001-0782
* [[Bernhard Nebel]], Hans-J&#252;rgen B&#252;rckert: ''Reasoning about Temporal Relations: A Maximal Tractable Subclass of Allen's Interval Algebra.'' In: ''Journal of the ACM'' 42, pp.&amp;nbsp;43&#8211;66. 1995.
* Peter van Beek, Dennis W. Manchak: ''The design and experimental analysis of algorithms for temporal reasoning.'' In: ''Journal of Artificial Intelligence Research'' 4, pp.&amp;nbsp;1&#8211;18, 1996.

[[Category:Knowledge representation]]
[[Category:Constraint programming]]</text>
      <sha1>b26vb9qrrvm0g6gdbdoxxfk8jca5pm5</sha1>
    </revision>
  </page>
  <page>
    <title>New Classification Scheme for Chinese Libraries</title>
    <ns>0</ns>
    <id>17618735</id>
    <revision>
      <id>670755085</id>
      <parentid>657231421</parentid>
      <timestamp>2015-07-10T00:33:28Z</timestamp>
      <contributor>
        <username>Niceguyedc</username>
        <id>5288432</id>
      </contributor>
      <minor />
      <comment>[[:en:WP:CLEANER|WPCleaner]] v1.35 - Repaired 1 link to disambiguation page - [[WP:DPL|(You can help)]] - [[Serial publications]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4473" xml:space="preserve">{{Unreferenced|date=May 2009}}
The '''New Classification Scheme for Chinese Libraries''' is a system of [[library classification]] developed by Yung-Hsiang Lai since 1956.  It is modified from "[[:zh:&#20013;&#22269;&#22270;&#20070;&#20998;&#31867;&#27861;|A System of Book Classification for Chinese Libraries]]" of [[Liu Guojun]], which is based on the [[Dewey Decimal Classification|Dewey Decimal System]]. 

The scheme is developed for Chinese books, and commonly used in [[Taiwan]], [[Hong Kong]] and [[Macau]]. 

==Main classes==
*000 Generalities
*100 [[Philosophy]]
*200 [[Religion]]
*300 [[Sciences]]
*400 [[Applied sciences]]
*500 [[Social sciences]]
*600-700 [[History]] and [[Geography]]
*800 [[Linguistics]] and [[Literature]]
*900 [[Arts]]

==Outline of the classification tables==
*'''000 Generalities'''
**000 Special collections
**010 [[Bibliography]]; [[Literacy]] ([[Documentation]])
**020 [[Library science|Library]] and [[information science]]; Archive management
**030 [[Sinology]]
**040 [[General encyclopedia]]
**050 [[Serial (publishing)|Serial publications]]; [[Periodicals]]
**060 General [[organization]]; [[Museology]]
**070 General collected [[essays]]
**080 General [[Book series|series]]
**090 Collected [[Chinese classics]]
*'''100 [[Philosophy]]'''
**100 Philosophy: general
**110 [[Thought]]; [[Learning]]
**120 [[Chinese philosophy]]
**130 [[Oriental philosophy]]
**140 [[Western philosophy]]
**150 [[Logic]]
**160 [[Metaphysics]]
**170 [[Psychology]]
**180 [[Esthetics]]
**190 [[Ethics]]
*'''200 [[Religion]]'''
**200 Religion: general
**210 Science of religion
**220 [[Buddhism]]
**230 [[Taoism]]
**240 [[Christianity]]
**250 [[Islam]] ([[Mohammedanism]])
**260 [[Judaism]]
**270 Other religions
**280 [[Mythology]]
**290 [[Astrology]]; [[Superstition]]
*'''300 [[Sciences]]'''
**300 Sciences: general
**310 [[Mathematics]]
**320 [[Astronomy]]
**330 [[Physics]]
**340 [[Chemistry]]
**350 [[Earth science]]; [[Geology]]
**360 [[Biological science]]
**370 [[Botany]]
**380 [[Zoology]]
**390 [[Anthropology]]
*'''400 [[Applied sciences]]'''
**400 Applied sciences: general
**410 [[Medical sciences]]
**420 [[Home economics]]
**430 [[Agriculture]]
**440 [[Engineering]]
**450 [[Mining]] and [[metallurgy]]
**460 [[Chemical engineering]]
**470 [[Manufacture]]
**480 [[Commerce]]: various business
**490 Commerce: [[Administration (business)|administration]] and [[management]]
*'''500 [[Social sciences]]'''
**500 Social sciences: general
**510 [[Statistics]]
**520 [[Education]]
**530 [[Rite]] and [[Convention (norm)|custom]]
**540 [[Sociology]]
**550 [[Economy]]
**560 [[Finance]]
**570 [[Political science]]
**580 [[Law]]; [[Jurisprudence]]
**590 [[Military science]]
*'''600-700 [[History]] and [[geography]]'''
**600 History and geography: General 
*'''History and geography of [[China]]'''
**610 General [[history of China]]
**620 Chinese history by period
**630 History of Chinese civilization
**640 Diplomatic history of China
**650 Historical sources
**660 [[Geography of China]]
**670 Local history
**680 Topical topography
**690 Chinese travels
*'''[[World history]] and geography'''
**710 World: general history and geography
**720 [[Oceans]] and [[sea]]s
**730 [[Asia]]: history and geography
**740 [[Europe]]: history and geography
**750 [[Americas|America]]: history and geography
**760 [[Africa]]: history and geography
**770 [[Oceania]]: history and geography
**780 [[Biography]]
**790 [[Antiquities]] and [[archaeology]]
*'''800 [[Linguistics]] and [[literature]]'''
**800 Linguistics: general
**810 Literature: general
**820 [[Chinese literature]]
**830 Chinese literature: general collections
**840 Chinese literature: individual works
**850 Various Chinese literature
**860 Oriental literature
**870 [[Western literature]]
**880 Other countries literatures
**890 [[Journalism]]
*'''900 [[Arts]]'''
**900 Arts: general
**910 [[Music]]
**920 [[Architecture]]
**930 [[Sculpture]]
**940 [[Drawing]] and [[painting]]; [[Calligraphy]]
**950 [[Photography]]; [[Computer art]]
**960 [[Decorative arts]]
**970 [[Arts and Crafts movement]]
**980 [[Theatre]]
**990 [[Recreation]] and [[leisure]]

==See also==
===Decimal systems===
*[[Dewey Decimal Classification]]
*[[Nippon Decimal Classification]]
*[[Korean Decimal Classification]] 
===Non-decimal systems===
*[[Library of Congress Classification]]
*[[Chinese Library Classification]]

{{Library classification systems}}

[[Category:Library cataloging and classification]]
[[Category:Knowledge representation]]</text>
      <sha1>1d8bghjtrlskx5tvfw7i0td235ir37q</sha1>
    </revision>
  </page>
  <page>
    <title>Knowledge integration</title>
    <ns>0</ns>
    <id>4144848</id>
    <revision>
      <id>710817190</id>
      <parentid>705762078</parentid>
      <timestamp>2016-03-19T06:00:35Z</timestamp>
      <contributor>
        <username>BG19bot</username>
        <id>14508071</id>
      </contributor>
      <minor />
      <comment>Remove blank line(s) between list items per [[WP:LISTGAP]] to fix an accessibility issue for users of [[screen reader]]s. Do [[WP:GENFIXES]] and cleanup if needed. Discuss this at [[Wikipedia talk:WikiProject Accessibility#LISTGAP]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3782" xml:space="preserve">'''Knowledge integration''' is the process of synthesizing multiple [[knowledge model]]s (or representations) into a common model (representation).

Compared to [[information integration]], which involves merging information having different schemas and representation models, knowledge integration focuses more on synthesizing the understanding of a given subject from different perspectives.

For example, multiple interpretations are possible of a set of student grades, typically each from a certain perspective. An overall, integrated view and understanding of this information can be achieved if these interpretations can be put under a common model, say, a student performance index.

The [http://wise.berkeley.edu Web-based Inquiry Science Environment (WISE)], from the [[University of California at Berkeley]] has been developed along the lines of knowledge integration theory.

'''Knowledge integration''' has also been studied as the process of incorporating new information into a body of existing knowledge with an [[interdisciplinary]] approach.  This process involves determining how the new information and the existing knowledge interact, how existing knowledge should be modified to accommodate the new information, and how the new information should be modified in light of the existing knowledge.

A learning agent that actively investigates the consequences of new information can detect and exploit a variety of learning opportunities; e.g., to resolve knowledge conflicts and to fill knowledge gaps.  By exploiting these learning opportunities the learning agent is able to learn beyond the explicit content of the new information.

The [[machine learning]] program KI, developed by Murray and Porter at the [[University of Texas at Austin]], was created to study the use of automated and semi-automated knowledge integration to assist [[knowledge engineers]] constructing a large [[knowledge base]].

A possible technique which can be used is [[semantic matching]]. More recently, a technique useful to minimize the effort in mapping validation and visualization has been presented which is based on [[Minimal mappings|Minimal Mappings]]. Minimal mappings are high quality mappings such that i) all the other mappings can be computed from them in time linear in the size of the input graphs, and ii) none of them can be dropped without losing property i).

The [[University of Waterloo]] operates a Bachelor of Knowledge Integration [[undergraduate degree]] program as an academic major or minor. The program started in 2008.

==See also==
* [[Knowledge value chain]]

==References==
{{Reflist}}&lt;!--added under references heading by script-assisted edit--&gt;

==Further reading==
* Linn, M. C. (2006) The Knowledge Integration Perspective on Learning and Instruction. R. Sawyer (Ed.). In ''The Cambridge Handbook of the Learning Sciences.'' Cambridge, MA. Cambridge University Press
* Murray, K. S. (1996) KI: A tool for Knowledge Integration. Proceedings of the Thirteenth National Conference on Artificial Intelligence
* Murray, K. S. (1995) [http://www.ai.sri.com/pubs/files/1636.pdf Learning as Knowledge Integration], Technical Report TR-95-41, The University of Texas at Austin
* Murray, K. S. (1990) Improving Explanatory Competence, Proceedings of the Twelfth Annual Conference of the Cognitive Science Society
* Murray, K. S., Porter, B. W. (1990) Developing a Tool for Knowledge Integration: Initial Results. International Journal for Man-Machine Studies, volume 33
* Murray, K. S., Porter, B. W. (1989) Controlling Search for the Consequences of New Information during Knowledge Integration. Proceedings of the Sixth International Machine Learning Conference

[[Category:Knowledge representation]]
[[Category:Learning]]
[[Category:Machine learning]]</text>
      <sha1>9ta9v9xiakskoh5sz5stga0082mvhjv</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Knowledge representation software</title>
    <ns>14</ns>
    <id>20966976</id>
    <revision>
      <id>547524181</id>
      <parentid>438532343</parentid>
      <timestamp>2013-03-28T23:53:35Z</timestamp>
      <contributor>
        <username>KLBot2</username>
        <id>14393296</id>
      </contributor>
      <minor />
      <comment>Bot: Migrating 1 interwiki links, now provided by [[Wikidata]] on [[:d:Q8575278]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="71" xml:space="preserve">[[Category:Knowledge representation]]
[[Category:Application software]]</text>
      <sha1>k88o449d8lbigckqvooogblmot8m1fd</sha1>
    </revision>
  </page>
  <page>
    <title>Paradigm classification</title>
    <ns>0</ns>
    <id>21862082</id>
    <revision>
      <id>687523149</id>
      <parentid>666506511</parentid>
      <timestamp>2015-10-26T02:55:52Z</timestamp>
      <contributor>
        <username>I dream of horses</username>
        <id>9676078</id>
      </contributor>
      <minor />
      <comment>general clean up, added [[CAT:O|orphan]] tag using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="875" xml:space="preserve">{{Multiple issues|
{{Orphan|date=October 2015}}
{{no footnotes|date=April 2013}}
}}

'''Paradigm classification''' in [[ontology]] is a two-dimensional classification scheme, such as a spreadsheet. It is a subset of [[faceted classification]].

== Overview ==
Paradigm classification deals with the large subset of faceted classification where an item may be classified within two dimensions. Examples might include [[genealogy]], where individuals are classified by their gender and relations with other individuals.

== References ==
{{reflist|2}}

== External links ==
{{Commons category|Ontology}}
*[http://www.miskatonic.org/library/facet-web-howto.html How to Make a Faceted Classification and Put It On the Web]
*[http://annotalia.com/philosophy/ontology Ontology]

{{Philosophy topics}}

[[Category:Knowledge representation]]
[[Category:Ontology]]


{{Ontology-stub}}</text>
      <sha1>fvr437ohrpcnd5m2gjrt824rdxm9lcm</sha1>
    </revision>
  </page>
  <page>
    <title>Pretext</title>
    <ns>0</ns>
    <id>8280463</id>
    <revision>
      <id>755349161</id>
      <parentid>742699783</parentid>
      <timestamp>2016-12-17T14:03:21Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* United States */clean up; http&amp;rarr;https for [[The Guardian]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="10862" xml:space="preserve">A '''pretext''' (adj: '''pretextual''') is an excuse to do something or say something that is not accurate. Pretexts may be based on a half-truth or developed in the context of a misleading fabrication. Pretexts have been used to conceal the true purpose or rationale behind actions and words.

In [[Law of the United States|US law]], a pretext usually describes false reasons that hide the true intentions or motivations for a legal action. If a party can establish a [[prima facie]] case for the proffered evidence, the opposing party must prove that these reasons were "pretextual" or false. This can be accomplished by directly demonstrating that the motivations behind the presentation of evidence is false, or indirectly by evidence that the motivations are not "credible".&lt;ref name=uslegal&gt;{{cite web|title=Pretext Law &amp; Legal Definition|url=http://definitions.uslegal.com/p/pretext/|publisher=uslegal.com|accessdate=13 March 2013}}&lt;/ref&gt; In
''Griffith v. Schnitzer'', an employment discrimination case, a jury award was reversed by a [[Court of Appeals]] because the evidence was not sufficient that the defendant's reasons were "pretextual". That is, the defendant's evidence was either undisputed, or the plaintiff's  was "irrelevant subjective assessments and opinions".&lt;ref&gt;[http://www.omwlaw.com/wp-content/uploads/2013/01/Defining-Pretext-In-Discrimination-Cases.pdf Defining "pretext" in discrimination cases] by Karen Sutherland (2013)&lt;/ref&gt;

A "pretextual" arrest by law enforcement officers is one carried out for illegal purposes such as to conduct an unjustified [[search and seizure]].&lt;ref&gt;[http://assets.wne.edu/161/8_note_Criminal.pdf Criminal law - Pretextual arrests and alternatives to the objective tests] by Robert D. Snook&lt;/ref&gt;&lt;ref name=oday&gt;{{cite web|last=O'Day|first=Kathleen M.|title=Pretextual traffic stops: injustice for minority drivers|url=http://academic.udayton.edu/race/03justice/s98oday.htm|publisher=The University of Dayton School of Law|accessdate=13 March 2013}}&lt;/ref&gt;

[[File:Marbleboot.jpg|thumb|right|140px|[[Marble Boat]] on [[Kunming Lake]] near Beijing.]]
As one example of pretext, in the 1880s, the Chinese government raised money on the pretext of modernizing the Chinese navy. Instead, these funds were diverted to repair a ship-shaped, two-story pavilion which had been originally constructed for [[Empress Xiaoshengxian|the mother]] of the [[Qianlong Emperor]]. This pretext and the Marble Barge are famously linked with [[Empress Dowager Cixi]]. This architectural [[folly]], known today as the [[Marble Boat]] (''Shifang''), is "moored" on Lake Kunming in what the empress renamed the "Garden for Cultivating Harmony" (''Yiheyuan'').&lt;ref&gt;Min, Anchee. (2007). [https://books.google.com/books?id=_H8TYkS84E4C&amp;pg=PA155&amp;dq=marble+barge&amp;client=firefox-a#PPA155,M1  ''The Last Empress,'' pp. 155-156;]&lt;/ref&gt;

Another example of pretext was demonstrated in the speeches of the Roman orator [[Cato the Elder]] (234-149 BC). For Cato, every public speech became a pretext for a comment about Carthage. The Roman statesman had come to believe that the prosperity of ancient Carthage represented an eventual and inevitable danger to Rome. In the Senate, Cato famously ended every speech by proclaiming his opinion that [[Carthage]] had to be destroyed (''[[Carthago delenda est]]''). This oft-repeated phrase was the ultimate conclusion of all logical argument in every oration, regardless of the subject of the speech. This pattern persisted until his death in 149, which was the year in which the Third Punic War began. In other words, any subject became a pretext for reminding his fellow senators of the dangers Carthage represented.&lt;ref&gt;Hooper, William Davis ''et al.'' (1934). [http://penelope.uchicago.edu/Thayer/E/Roman/Texts/Cato/De_Agricultura/Introduction*.html   "Introduction,"] in Cato's ''De Agricultura'' (online version of Loeb edition).&lt;/ref&gt;

==Uses in warfare==

[[File:Hokoji-Bell-M1767.jpg|thumb|right|140px|Temple bell at [[H&#333;k&#333;-ji (Kyoto)|H&#333;k&#333;-ji]].]][[File:Hokoji-BellDetail-M1767.jpg|thumb|right|140px|Inscription on bell at Hokoji in Kyoto]]
The early years of Japan's [[Tokugawa shogunate]] were unsettled, with warring factions battling for power. The causes for the fighting were in part pretextural, but the outcome brought diminished armed conflicts after the [[Siege of Osaka]] in 1614-1615.

* '''1614''' (''Keich&#333; 19''): The Shogun vanquished Hideyori and set fire to [[Osaka Castle]], and then he returned for the winter to [[Edo]].&lt;ref name="t410"&gt;Titsingh, [https://books.google.com/books?id=18oNAAAAIAAJ&amp;pg=PP9&amp;dq=nipon+o+dai+itsi+ran#PRA1-PA410,M1  p. 410.]&lt;/ref&gt;
* '''August 24, 1614''' (''Keich&#333; 19, 19th day of the 7th month''): A new bronze bell for the H&#333;k&#333;-ji was cast successfully [http://oldphoto.lb.nagasaki-u.ac.jp/en/target.php?id=4771][http://oldphoto.lb.nagasaki-u.ac.jp/en/target.php?id=3093]; but despite the dedication ceremony planning, Ieyasu forbade any further actions concerning the great bell:
::"[T]he tablet over the Daibutsu-den and the bell bore the inscription ''"Kokka ank&#333;"'' (meaning "the country and the house, peace and tranquility"), and at this [[Tokugawa Ieyasu]] affect to take umbrage, alleging that it was intended as a curse on him for the character &#23433; (''an,'' "peace") was placed between the two characters composing his own name &#23478;&#24247; (''"ka-k&#333;",'' "house tranquility") [suggesting subtly perhaps that peace could only be attained by Ieyasu's dismemberment?] ... This incident of the inscription was, of course, a mere pretext, but Ieyasu realized that he could not enjoy the power he had usurped as long as Hideyori lived, and consequently, although the latter more than once Hideyori dispatched his vassal Katagiri Kastumoto to Ieyasu's residence ([[Sunpu Castle]]) with profuse apologies, Ieyasu refused to be placated."&lt;ref&gt;Ponsonby-Fane, Richard. (1956). ''Kyoto, the Old Capital of Japan,'' p. 292; Titsingh, [https://books.google.com/books?id=18oNAAAAIAAJ&amp;pg=PP9&amp;dq=nipon+o+dai+itsi+ran#PRA1-PA410,M1  p. 410.]&lt;/ref&gt;
* '''October 18, 1614''' (''Keich&#333; 19, 25th day of the 10th month''): A strong earthquake shook Kyoto.&lt;ref name="t410"/&gt;
* '''1615''' (''Keich&#333; 20''): Osaka Summer Battle begins.

The next two-and-a-half centuries of Japanese history were comparatively peaceful under the successors of Tokugawa Ieyasu and the [[bakufu]] government he established.

===United States===
*During the War of 1812, US President [[James Madison]] was often accused of using impressment of American sailors by the [[Royal Navy]] as a pretext to invade [[Canada]].

{{main|Pearl Harbor advance-knowledge debate}}
*Some have argued that United States President [[Franklin D. Roosevelt]] used the [[attack on Pearl Harbor]] by Japanese forces on December 7, 1941 as a pretext to enter [[World War II]].&lt;ref&gt;Bernstein, Richard. [http://www.nytimes.com/1999/12/15/books/books-of-the-times-on-dec-7-did-we-know-we-knew.html "On Dec. 7, Did We Know We Knew?"] ''New York Times.'' December 15, 1999.&lt;/ref&gt; American soldiers and supplies had been assisting British and Soviet operations for almost a year by this point, and the United States had thus "chosen a side", but due to the political climate in the States at the time and some campaign promises made by Roosevelt that he would not send American boys to fight in foreign wars. Roosevelt could not declare war for fear of public backlash. The attack on Pearl Harbor united the American people's resolve against the Axis powers and created the bellicose atmosphere in which to declare war.
* Critics have accused United States President [[George W. Bush]] of using the [[September 11th, 2001 attacks]] and faulty intelligence about the existence of [[weapons of mass destruction]] as a pretext for the [[Iraq war|war in Iraq]].&lt;ref&gt;Borger, Julian. (2006). [https://www.theguardian.com/world/2006/sep/07/usa.books  "Book says CIA tried to provoke Saddam to war,"] ''The Guardian'' (London). 7 September 2006.&lt;/ref&gt;

==Social engineering==
{{main|Social engineering (security)}}
A type of [[Social engineering (security)|social engineering]] called [[Social engineering (security)#Pretexting|pretexting]] uses a pretext to elicit information fraudulently  from a target. The pretext in this case includes research into the identity of a certain authorized person or personality type in order to establish legitimacy in the mind of the target.&lt;ref&gt;[[Federal Trade Commission]] (FTC):  [http://www.ftc.gov/bcp/edu/pubs/consumer/credit/cre10.shtm  "Pretexting: Your Personal Information Revealed."] February 2006.&lt;/ref&gt;

==See also==
{{wiktionary}}

* [[Plausible deniability]]
* [[Proximate cause]]
* [[Causes of the Franco-Prussian War]]

==Notes==
{{reflist|2}}

==References==
* [[James Bamford|Bamford]], James. (2004). [https://books.google.com/books?id=VuOxAAAACAAJ&amp;dq=Pretext+for+War:+9/11,+Iraq,+and+the+Abuse+of+America%27s+Intelligence+Agencies&amp;client=firefox-a  ''Pretext for War: 9/11, Iraq, and the Abuse of America's Intelligence Agencies.'']  New York: [[Doubleday Books]]. ISBN 978-0-385-50672-4; [http://www.worldcat.org/oclc/55068034?referer=di&amp;ht=edition OCLC 55068034]
* [[Cato the Elder|Cato]], Marcus Porcius. [https://books.google.com/books?id=D2mxAAAAIAAJ&amp;q=Hooper+and+De+Agricultura&amp;dq=Hooper+and+De+Agricultura&amp;lr=&amp;client=firefox-a&amp;pgis=1 ''On Agriculture'' (''De agricultura'')] trans,   William Davis Hooper and Harrison Boyd Ash. Cambridge: [[Harvard University Press]]. [http://www.worldcat.org/oclc/230499252 OCLC  230499252]
* [[Michael Isikoff|Isikoff]], Michael and [[David Corn]]. 2006. [https://books.google.com/books?id=Sa14AAAAMAAJ&amp;q=hubris&amp;dq=hubris&amp;pgis=1  ''Hubris: The Inside Story of Spin, Scandal, and the Selling of the Iraq War'']  New York: [[Crown Publishers]]. ISBN 978-0-307-34681-0
* Min, Anchee. (2007). [https://books.google.com/books?id=_H8TYkS84E4C&amp;client=firefox-a  ''The Last Empress.''] New York: [[Houghton Mifflin Harcourt]]. ISBN 978-0-618-53146-2
* [[Richard Ponsonby-Fane|Ponsonby-Fane]], Richard Arthur Brabazon. (1956). ''Kyoto, the Old Capital of Japan,'' Kyoto: Ponsonby Memorial Society.
* [[Robert Stinnett|Stinnett]] Robert B. (2001). [https://books.google.com/books?id=Q2UKN5daNHYC ''Day of Deceit: The Truth about FDR and Pearl Harbor''] New York: [[Simon &amp; Schuster]]. ISBN 978-0-7432-0129-2
* [[Isaac Titsingh|Titsingh]], Isaac. (1834). [Siyun-sai Rin-siyo/[[Hayashi Gah&#333;]], 1652], ''[[Nipon o da&#239; itsi ran]]; ou, [https://books.google.com/books?id=18oNAAAAIAAJ&amp;dq=nipon+o+dai+itsi+ran  Annales des empereurs du Japon.'']  Paris: [[Royal Asiatic Society|Oriental Translation Fund of Great Britain and Ireland]].

[[Category:Propaganda techniques]]
[[Category:Knowledge representation]]
[[Category:Cognition]]
[[Category:Attack on Pearl Harbor]]
[[Category:Social engineering (computer security)]]</text>
      <sha1>1uwz58p6p23abflahcznza64d91ib2z</sha1>
    </revision>
  </page>
  <page>
    <title>Framing (social sciences)</title>
    <ns>0</ns>
    <id>10438439</id>
    <revision>
      <id>762657019</id>
      <parentid>762076333</parentid>
      <timestamp>2017-01-30T01:10:43Z</timestamp>
      <contributor>
        <username>Matthiaspaul</username>
        <id>13467261</id>
      </contributor>
      <comment>/* See also */ +links</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="81728" xml:space="preserve">{{globalize|date=July 2010}}

In the [[social sciences]], '''framing''' comprises a set of concepts and theoretical perspectives on how individuals, groups, and societies, organize, perceive, and communicate about [[reality]]. Framing involves [[social construction]] of a [[social phenomenon]] &#8211; by [[mass media]] sources, political or social movements, political leaders, or other actors and organizations. Participation in a language community necessarily influences an individual's ''[[perception]]'' of the meanings attributed to words or phrases. Politically, the language communities of [[advertising]], [[religion]], and mass media are highly contested, whereas framing in less-sharply defended [[speech community|language communities]] might evolve imperceptibly and organically over [[cultural]] time frames, with fewer overt modes of disputation.  

Framing itself can be framed in one of two ways, depending on whether one chooses to emphasise processes of [[cognition|thought]] or processes of interpersonal [[communication]]. ''Frames in thought'' consist of the mental representations, interpretations, and simplifications of reality. ''Frames in communication'' consist of the communication of frames between different actors.&lt;ref name="Druckman2001"&gt;{{cite journal | last1 = Druckman | first1 = J.N. | year = 2001 | title = The Implications of Framing Effects for Citizen Competence | url = | journal = Political Behavior | volume = 23 | issue = 3| pages = 225&#8211;256 | doi=10.1023/A:1015006907312}}&lt;/ref&gt;

One can view framing in communication as positive or negative &#8211; depending on the audience and what kind of information is being presented. Framing might also be understood as being either ''equivalence frames'', which represent logically equivalent alternatives portrayed in different ways (see [[framing effect (psychology)|framing effect]]) or as ''emphasis frames'', which simplify reality by focusing on a subset of relevant aspects of a situation or issue.&lt;ref name="Druckman2001" /&gt; In the case of  "equivalence frames", the information being presented is based on the same facts, but the "frame" in which it is presented changes, thus creating a reference-dependent perception.

The effects of framing can be seen in many journalism applications. With the same information being used as a base, the "frame" surrounding the issue can change the reader's perception without having to alter the actual facts. In the context of politics or mass-media communication, a frame defines the packaging of an element of [[rhetoric]] in such a way as to encourage certain interpretations and to discourage others.  For political purposes, framing often presents facts in such a way that implicates a problem that is in need of a solution. Members of political parties attempt to frame issues in a way that makes a solution favoring their own political leaning appear as the most appropriate course of action for the situation at hand.&lt;ref name="van der Pas"&gt;{{cite journal|last=van der Pas|first=D.|title=Making Hay While the Sun Shines: Do Parties Only Respond to Media Attention When The Framing is Right?|journal=Journal of Press/Politics|year=2014|volume=19|issue=1|pages=42&#8211;65|doi=10.1177/1940161213508207}}&lt;!--|accessdate=6 March 2014--&gt;&lt;/ref&gt;

In [[social theory]], framing is a [[Schema (psychology)|schema]] of [[interpretation (logic)|interpretation]], a collection of [[Anecdotal evidence|anecdotes]] and [[stereotype]]s, that individuals rely on to understand and respond to events.&lt;ref name="Goffman1974"&gt;
Goffman, E. (1974). Frame analysis: An easy on the organization of experience. Cambridge, MA: Harvard University Press. 
&lt;/ref&gt; In other words, people build a series of mental "filters" through biological and cultural influences. They then use these filters to make sense of the world. The choices they then make are influenced by their creation of a frame.

Framing is also a key component of [[sociology]], the study of social interaction among humans.  Framing is an integral part of conveying and processing data on a daily basis.  Successful framing techniques can be used to reduce the ambiguity of intangible topics by contextualizing the information in such a way that recipients can connect to what they already know.

== Explanation ==
When one seeks to explain an event, the understanding often depends on the frame referred to. If a friend rapidly closes and opens an eye, we will respond very differently depending on whether we attribute this to a purely "physical" frame (they blinked) or to a social frame (they winked).

Though the former might result from a speck of dust (resulting in an involuntary and not particularly meaningful reaction), the latter would imply a voluntary and meaningful action (to convey humor to an accomplice, for example). Observers will read events seen as purely physical or within a frame of "nature" differently from those seen as occurring with social frames. But we do not look at an event and then "apply" a frame to it. Rather, individuals constantly project into the world around them the interpretive frames that allow them to make sense of it; we only shift frames (or realize that we have habitually applied a frame) when incongruity calls for a frame-shift. In other words, we only become aware of the frames that we always already use when something forces us to replace one frame with another.&lt;ref&gt;
This example borrowed from Clifford Geertz: ''Local Knowledge: Further Essays in Interpretive Anthropology'' (1983), Basic Books 2000 paperback: ISBN 0-465-04162-0
&lt;/ref&gt;&lt;ref&gt;
Goffman offers the example of the woman bidding on a mirror at an auction who first examines the frame and surface for imperfections, and then "checks" herself in the mirror and adjusts her hat. See Goffman, Erving. ''Frame Analysis: An essay on the organization of experience''. Boston: Northeastern University Press, 1986. ISBN 0-930350-91-X, page 39. In each case the mirror represents more than simply a physical object.
&lt;/ref&gt;

Framing is so effective because it is a heuristic, or mental shortcut that may not always yield desired results; and is seen as a 'rule of thumb'. According to Susan T. Fiske and Shelley E. Taylor, human beings are by nature "cognitive misers", meaning they prefer to do as little thinking as possible.&lt;ref&gt;Fiske, S. T., &amp; Taylor, S. E. (1991). Social cognition (2nd ed.). New York: McGraw-Hill&lt;/ref&gt; Frames provide people a quick and easy way to process information. Hence, people will use the previously mentioned mental filters (a series of which is called a schema) to make sense of incoming messages. This gives the sender and framer of the information enormous power to use these schemas to influence how the receivers will interpret the message.&lt;ref name="EntmanRobertTree"&gt;Entman,Robert "Tree Beard". Framing: Toward Clarification of a Fractured Paradigm. Journal of Communication; Autumn 1993, 43, 4, p.51&lt;/ref&gt;

Though some consider framing to be synonymous with [[Agenda-setting theory|agenda setting]], other scholars state that there is a distinction. According to an article written by Donald H. Weaver, framing selects certain aspects of an issue and makes them more prominent in order to elicit certain interpretations and evaluations of the issue, whereas agenda setting introduces the issue topic to increase its salience and accessibility.&lt;ref&gt;{{Cite journal|last=Weaver|first=David H.|title=Thoughts on Agenda Setting, Framing, and Priming|journal=Journal of Communication|volume=57}}&lt;/ref&gt;

==Framing effect in communication research==
In the field of communication, framing defines how news media coverage shapes [[mass opinion]]. [[Richard Vatz|Richard E. Vatz's]] discourse on creation of rhetorical meaning relates directly to framing, although he references it little.  To be specific, framing effects refer to behavioral or attitudinal strategies and/or outcomes that are due to how a given piece of information is being framed in [[public discourse]]. Today, many volumes of the major communication journals contain papers on media frames and framing effects.&lt;ref&gt;Scheufele, D. A. &amp; Iyengar, S. (forthcoming). The state of framing research: A call for new directions. In K. kENSKI, &amp; K. H. Jamieson (Eds.), The Oxford Handbook of political communication theories. New York: Oxford University Press.&lt;/ref&gt; Approaches used in such papers can be broadly classified into two groups: studies of framing as the dependent variable and studies of framing as the independent variable.&lt;ref&gt;Tewksbury &amp; Scheufele (2009). News framing theory and research, In J. Bryant, &amp; M. B. Oliver (Eds.) Media effects: Advances in theory and research, New York: Routledge.&lt;/ref&gt; The former usually deals with ''frame building'' (i.e. how frames create societal discourse about an issue and how different frames are adopted by journalists) and latter concerns ''frame setting'' (i.e. how media framing influences an audience).

===Frame building===
Frame building is related to at least three areas: journalist norms, political actors, and cultural situations. It assumes that several media frames compete to set one frame regarding an issue, and one frame finally gains influence because it resonates with [[popular culture]], fits with media practices, or is heavily sponsored by [[elite]]s. 
First, in terms of practices of news production, there are at least five aspects of news work that may influence how journalists frame a certain issue: larger societal norms and values, organizational pressures and constraints, external pressures from [[interest group]]s and other [[policy maker]]s, professional routines, and ideological or political orientations of journalists. The second potential influence on frame building comes from elites, including interest groups, government bureaucracies, and other political or corporate actors. Empirical studies show that these influences of elites seem to be strongest for issues in which journalists and various players in the policy arena can find shared narratives. Finally, cultural contexts of a society are also able to establish frame. Goffman&lt;ref name="Goffman1974"/&gt; assumes that the meaning of a frame has implicit cultural roots. This context dependency of media frame has been described as 'cultural resonance'&lt;ref&gt;Gamson, W. A. &amp; Modigliani, A. (1987) The changing culture of affirmative action. Research in Political Sociology, 3, 137-177&lt;/ref&gt; or 'narrative fidelity'.&lt;ref name="SnowBenford1988"&gt;Snow, D. A., &amp; Benford, R. D. (1988). Ideology, frame resonance, and participant mobilization. In B. Klandermans, H. Kriesi, &amp; S. Tarrow (Eds.), International social movement research. Vol 1, From structure on action: Comparing social movement research across cultures (pp. 197-217). Greenwich, CT: JAI Press.&lt;/ref&gt;

===Frame setting===
When people are exposed to a novel news frame, they will accept the constructs made applicable to an issue, but they are significantly more likely to do so when they have existing schema for those constructs. This is called the applicability effect. That is, when new frames invite people to apply their existing schema to an issue, the implication of that application depends, in part, on what is in that schema. Therefore, generally, the more the audiences know about issues, the more effective are frames.

There are a number of levels and types of framing effects that have been examined. For example, scholars have focused on attitudinal and behavioral changes, the degrees of perceived importance of the issue, voting decisions, and opinion formations. Others are interested in psychological processes other than applicability. For instance, Iyengar&lt;ref&gt;Iyengar, S. (1991). Is anyone responsible? How television frames political issues. Chicago: University of Chicago Press.&lt;/ref&gt; suggested that news about social problems can influence attributions of causal and treatment responsibility, an effect observed in both cognitive responses and evaluations of political leaders, or other scholars looked at the framing effects on receivers' evaluative processing style and the complexity of audience members' thoughts about issues.

==In mass communication research==
News media frame all news items by emphasizing specific values, facts, and other considerations, and endowing them with greater apparent applicability for making related judgments.  News media promotes particular definitions, interpretations, evaluations and recommendations.&lt;ref name=Entman1993&gt;{{cite journal|last=Entman|first=R.M.|title=Framing: Toward clarification of a fractured paradigm|journal=Journal of Communication|year=1993|volume=43|issue=4|pages=51&#8211;58|doi=10.1111/j.1460-2466.1993.tb01304.x}}&lt;/ref&gt;&lt;ref name=NelsonClawsonOxley1997&gt;{{cite journal|last=Nelson|first=T.E.|author2=Clawson, R.A. |author3=Oxley, Z.M. |title=Media framing of a civil liberties conflict and its effect on tolerance|journal=American Political Science Review|year=1997|volume=91|issue=3|pages=567&#8211;583|doi=10.2307/2952075}}&lt;/ref&gt;

===Foundations in mass communication research===

Anthropologist [[Gregory Bateson]] first articulated the concept of framing in his 1972 book ''[[Steps to an Ecology of Mind]]''.  A frame, Bateson wrote, is "a spatial and temporal bounding of a set of interactive messages."&lt;ref name=Bateson1972&gt;{{cite book|last=Bateson|first=G.|title=Steps to an Ecology of Mind|year=1972|publisher=Ballantine Books|location=New York}}&lt;/ref&gt;

====Sociological roots of media framing research====

Media framing research has both sociological and psychological roots.  Sociological framing focuses on "the words, images, phrases, and presentation styles" that communicators use when relaying information to recipients.&lt;ref name="Druckman2001" /&gt; Research on frames in sociologically driven media research generally examines the influence of "social norms and values, organizational pressures and constraints, pressures of interest groups, journalistic routines, and ideological or political orientations of journalists" on the existence of frames in media content.&lt;ref name=Scheufele2000&gt;{{cite journal|last=Scheufele|first=D.A.|title=Agenda-setting, priming, and framing revisited: Another look at cognitive effects of political communication|journal=Mass Communication &amp; Society|year=2000|volume=3|issue=2&amp;3|pages=297&#8211;316|doi=10.1207/S15327825MCS0323_07}}&lt;/ref&gt;

[[Todd Gitlin]], in his analysis of how the news media trivialized the student [[New Left]] movement during the 1960s, was among the first to examine media frames from a sociological perspective.  Frames, Gitlin wrote, are "persistent patterns of cognition, interpretations, and presentation, of selection [and] emphasis ... [that are] largely unspoken and unacknowledged ... [and] organize the world for both journalists [and] for those of us who read their reports."&lt;ref name=Gitlin1980&gt;{{cite book|last=Gitlin|first=T.|title=The Whole World is Watching: Mass Media in the Making and Unmaking of the New Left|year=1980|publisher=University of California Press|location=Berkeley, CA}}&lt;/ref&gt;

====Psychological roots of media framing research====

Research on frames in psychologically driven media research generally examines the effects of media frames on those who receive them.  For example, Iyengar explored the impact of episodic and thematic news frames on viewers' attributions of responsibility for political issues including crime, terrorism, poverty, unemployment, and racial inequality.&lt;ref name=Iyengar1991&gt;{{cite book|last=Iyengar|first=S.|title=Is Anyone Responsible? How Television Frames Political Issues|year=1991|publisher=University of Chicago Press|location=Chicago}}&lt;/ref&gt; According to Iyengar, an episodic news frame "takes the form of a case study or event-oriented report and depicts public issues in terms of concrete instances," while a thematic news frame "places public issues in some more general abstract context ... directed at general outcomes or conditions."&lt;ref name=Entman1993 /&gt;&lt;ref name=Iyengar1991 /&gt; Iyengar found that the majority of television news coverage of poverty, for example, was episodic.&lt;ref name=Iyengar1991 /&gt;  In fact, in a content analysis of six years of television news, Iyengar found that the typical news viewer would have been twice as likely to encounter episodic rather than thematic television news about poverty.&lt;ref name=Iyengar1991 /&gt;  Further, experimental results indicate participants who watched episodic news coverage of poverty were more than twice as likely as those who watched thematic news coverage of poverty to attribute responsibility of poverty to the poor themselves rather than society.&lt;ref name=Iyengar1991 /&gt;  Given the predominance of episodic framing of poverty, Iyengar argues that television news shifts responsibility of poverty from government and society to the poor themselves.&lt;ref name=Iyengar1991 /&gt;  After examining content analysis and experimental data on poverty and other political issues, Iyengar concludes that episodic news frames divert citizens' attributions of political responsibility away from society and political elites, making them less likely to support government efforts to address those issue and obscuring the connections between those issues and their elected officials' actions or lack thereof.&lt;ref name=Iyengar1991 /&gt;

===Clarifying and distinguishing a "fractured paradigm"===

Perhaps because of their use across the social sciences, frames have been defined and used in many disparate ways.  Entman called framing "a scattered conceptualization" and "a fractured paradigm" that "is often defined casually, with much left to an assumed tacit understanding of the reader."&lt;ref name=Entman1993 /&gt; In an effort to provide more conceptual clarity, Entman suggested that frames "select some aspects of a perceived reality and make them more salient in a communicating text, in such a way as to promote a particular problem definition, causal interpretation, moral evaluation, and/or treatment recommendation for the item described."&lt;ref name=Entman1993 /&gt;

Entman's&lt;ref name=Entman1993 /&gt;  conceptualization of framing, which suggests frames work by elevating particular pieces of information in salience, is in line with much early research on the psychological underpinnings of framing effects (see also Iyengar,&lt;ref name=Iyengar1991 /&gt;  who argues that accessibility is the primary psychological explanation for the existence of framing effects).  Wyer and Srull&lt;ref name=WyerSrull1984 /&gt;  explain the construct of accessibility thus:
# People store related pieces of information in "referent bins" in their long-term memory.&lt;ref name=WyerSrull1984&gt;{{cite book|last=Wyer, Jr.|first=R.S.|title=Social Cognition: The Ontario Symposium|year=1984|publisher=Lawrence Erlbaum|location=Hillsdale, NJ|author2=Srull, T.K.|editor=E.T. Higgins |editor2=N.A. Kuiper |editor3=M.P Zanna (Eds.)|chapter=Category Accessibility: Some theoretic and empirical issues concerning the processing of social stimulus information}}&lt;/ref&gt; 
# People organize "referent bins" such that more frequently and recently used pieces of information are stored at the top of the bins and are therefore more accessible.&lt;ref name=WyerSrull1984 /&gt; 
# Because people tend to retrieve only a small portion of information from long-term memory when making judgments, they tend to retrieve the most accessible pieces of information to use for making those judgments.&lt;ref name=WyerSrull1984 /&gt;

The argument supporting accessibility as the psychological process underlying framing can therefore be summarized thus: Because people rely heavily on news media for public affairs information, the most accessible information about public affairs often comes from the public affairs news they consume.  The argument supporting accessibility as the psychological process underlying framing has also been cited as support in the debate over whether framing should be subsumed by [[agenda-setting theory]] as part of the second level of agenda setting.  McCombs and other agenda-setting scholars generally agree that framing should be incorporated, along with [[Priming (media)|priming]], under the umbrella of agenda setting as a complex model of media effects linking media production, content, and audience effects.&lt;ref name=Kosicki1993&gt;{{cite journal|last=Kosicki|first=G.M.|title=Problems and opportunities in Agenda-setting research|journal=Journal of Communication|year=1993|volume=43|issue=2|pages=100&#8211;127|doi=10.1111/j.1460-2466.1993.tb01265.x}}&lt;/ref&gt;&lt;ref name=McCombsShaw1993&gt;{{cite journal|last=McCombs|first=M.E.|author2=Shaw, D.L.|title=The evolution of agenda-setting research: Twenty-five years in the marketplace of ideas|journal=Journal of Communication|year=1993|volume=43|issue=2|pages=58&#8211;67|doi=10.1111/j.1460-2466.1993.tb01262.x}}&lt;/ref&gt;&lt;ref name=McCombsLlamasLopez-EscobarRey1997 /&gt;  Indeed, McCombs, Llamas, Lopez-Escobar, and Rey justified their attempt to combine framing and agenda-setting research on the assumption of parsimony.&lt;ref name=McCombsLlamasLopez-EscobarRey1997&gt;{{cite journal|last=McCombs|first=M.F.|author2=Llamas, J.P. |author3=Lopez-Escobar, E. |author4=Rey, F. |title=Candidate images in Spanish elections: Second-level agenda-setting effects|journal=Journalism &amp; Mass Communication Quarterly|year=1997|volume=74|pages=703&#8211;717|doi=10.1177/107769909707400404|issue=4}}&lt;/ref&gt;

Scheufele, however, argues that, unlike agenda setting and priming, framing does not rely primarily on accessibility, making it inappropriate to combine framing with agenda setting and priming for the sake of parsimony.&lt;ref name=Scheufele2000 /&gt; Empirical evidence seems to vindicate Scheufele's claim.  For example, Nelson, Clawson, and Oxley empirically demonstrated that applicability, rather than their salience, is key.&lt;ref name="NelsonClawsonOxley1997" /&gt; By operationalizing accessibility as the response latency of respondent answers where more accessible information results in faster response times, Nelson, Clawson, and Oxley demonstrated that accessibility accounted for only a minor proportion of the variance in framing effects while applicability accounted for the major proportion of variance.&lt;ref name="NelsonClawsonOxley1997" /&gt; Therefore, according to Nelson and colleagues, "frames influence opinions by stressing specific values, facts, and other considerations, endowing them with greater apparent relevance to the issue than they might appear to have under an alternative frame."&lt;ref name="NelsonClawsonOxley1997" /&gt;

In other words, while early research suggested that by highlighting particular aspects of issues, frames make certain considerations more accessible and therefore more likely to be used in the judgment process,&lt;ref name=Entman1993 /&gt;&lt;ref name=Iyengar1991 /&gt;  more recent research suggests that frames work by making particular considerations more applicable and therefore more relevant to the judgment process.&lt;ref name="NelsonClawsonOxley1997" /&gt;&lt;ref name=Scheufele2000 /&gt;

===Equivalency versus emphasis: two types of frames in media research===

Chong and Druckman suggest framing research has mainly focused on two types of frames: equivalency and emphasis frames.&lt;ref name=ChongDruckman2007&gt;{{cite journal|last=Chong|first=D.|author2=Druckman, J.N. |title=Framing theory|journal=Annual Review of Political Science|year=2007|volume=10|pages=103&#8211;126|doi=10.1146/annurev.polisci.10.072805.103054}}&lt;/ref&gt;  Equivalency frames offer "different, but logically equivalent phrases," which cause individuals to alter their preferences.&lt;ref name="Druckman2001" /&gt; Equivalency frames are often worded in terms of "gains" versus "losses."  For example, Kahneman and Tversky asked participants to choose between two "gain-framed" policy responses to a hypothetical disease outbreak expected to kill 600 people.&lt;ref name=KahnemanTversky1984&gt;{{cite journal |last=Kahneman |first=D.|author2=Tversky, A.|title=Choices, values, and frames|journal=American Psychologist |year=1984|volume=39|issue=4|pages=341&#8211;350 |doi=10.1037/0003-066X.39.4.341}}&lt;/ref&gt;  Response A would save 200 people while Response B had a one-third probability of saving everyone, but a two-thirds probability of saving no one.  Participants overwhelmingly chose Response A, which they perceived as the less risky option. Kahneman and Tversky asked other participants to choose between two equivalent "loss-framed" policy responses to the same disease outbreak.  In this condition, Response A would kill 400 people while Response B had a one-third probability of killing no one but a two-thirds probability of killing everyone.  Although these options are  mathematically identical to those given in the "gain-framed" condition, participants overwhelmingly chose Response B, the risky option.  Kahneman and Tversky, then, demonstrated that when phrased in terms of potential gains, people tend to choose what they perceive as the less risky option (i.e., the sure gain).  Conversely, when faced with a potential loss, people tend to choose the riskier option.&lt;ref name=KahnemanTversky1984 /&gt;

Unlike equivalency frames, emphasis frames offer "qualitatively different yet potentially relevant considerations" which individuals use to make judgments.&lt;ref name=ChongDruckman2007 /&gt; For example, Nelson, Clawson, and Oxley exposed participants to a news story that presented the [[Ku Klux Klan]]'s plan to hold a rally.&lt;ref name="NelsonClawsonOxley1997" /&gt;  Participants in one condition read a news story that framed the issue in terms of public safety concerns while participants in the other condition read a news story that framed the issue in terms of free speech considerations.  Participants exposed to the public safety condition considered public safety applicable for deciding whether the Klan should be allowed to hold a rally and, as expected, expressed lower tolerance of the Klan's right to hold a rally.&lt;ref name="NelsonClawsonOxley1997" /&gt;  Participants exposed to the free speech condition, however, considered free speech applicable for deciding whether the Klan should be allowed to hold a rally and, as expected, expressed greater tolerance of the Klan's right to hold a rally.&lt;ref name="NelsonClawsonOxley1997" /&gt;

==Framing effect in psychology and economics==
[[File:Daniel KAHNEMAN.jpg|thumb|180px|[[Daniel Kahneman]]]]
{{Main|Framing effect (psychology)}}
[[Amos Tversky]] and [[Daniel Kahneman]] have shown that framing can affect the outcome (i.e. the choices one makes) of choice problems, to the extent that several of the classic axioms of [[rational choice]] do not hold.&lt;ref name="TverskyKahneman1981"&gt;{{cite journal | last1 = Tversky | first1 = Amos | last2 = Kahneman | first2 = Daniel | year = 1981 | title = The Framing of Decisions and the Psychology of Choice | url = | journal = Science | volume = 211 | issue = 4481| pages = 453&#8211;458 | doi = 10.1126/science.7455683 | pmid = 7455683 }}&lt;/ref&gt; This led to the development of [[prospect theory]] as an alternative to rational choice theory.&lt;ref&gt;Econport. "Decision-Making Under Uncertainty - Advanced Topics: An Introduction to Prospect Theory". (EconPort is an economics digital library specializing in content that emphasizes the use of experiments in teaching and research.) [http://www.econport.org/econport/request?page=man_ru_advanced_prospect]&lt;/ref&gt;

The context or framing of problems adopted by decision-makers results in part from extrinsic manipulation of the decision-options offered, as well as from forces intrinsic to decision-makers, e.g., their norms, habits, and unique [[temperament]].

===Experimental demonstration===
Tversky and Kahneman (1981) demonstrated systematic [[preference reversal|reversals of preference]] when the same problem is presented in different ways, for example in the Asian disease problem. Participants were asked to "imagine that the U.S. is preparing for the outbreak of an unusual Asian disease, which is expected to kill 600 people. Two alternative programs to combat the disease have been proposed. Assume the exact scientific estimate of the consequences of the programs are as follows."

The first group of participants was presented with a choice between programs:
In a group of 600 people,
* Program A: "200 people will be saved"
* Program B: "there is a 1/3 probability that 600 people will be saved, and a 2/3 probability that no people will be saved"

72 percent of participants preferred program A (the remainder, 28%, opting for program B).

The second group of participants was presented with the choice between the following:
In a group of 600 people,
* Program C: "400 people will die"
* Program D: "there is a 1/3 probability that nobody will die, and a 2/3 probability that 600 people will die"

In this decision frame, 78% preferred program D, with the remaining 22% opting for program C.

Programs A and C are identical, as are programs B and D. The change in the decision frame between the two groups of participants produced a preference reversal: when the programs were presented in terms of lives saved, the participants preferred the secure program, A (= C). When the programs were presented in terms of expected deaths, participants chose the gamble D (= B).&lt;ref&gt;{{Cite journal
| last = Entman
| first = R. M.
| year = 1993
| contribution = Framing: Toward Clarification of a Fractured Paradigm
| periodical = [[Journal of Communication]]
| volume = 43
| issue = 4
| pages = 51&#8211;58 [pp. 53&#8211;54] |doi=10.1111/j.1460-2466.1993.tb01304.x
| postscript = &lt;!--None--&gt;
}}&lt;/ref&gt;

===Absolute and relative influences===
Framing effects arise because one can frequently frame a decision using multiple [[scenario]]s, wherein one may express benefits either as a relative risk reduction (RRR), or as absolute risk reduction (ARR). Extrinsic control over the cognitive distinctions (between [[risk tolerance]] and [[Incentive|reward anticipation]]) adopted by decision makers can occur through altering the presentation of [[relative risk]]s and [[Three degrees of comparison|absolute]] benefits.

People generally prefer the absolute certainty inherent in a positive framing-effect, which offers an assurance of gains. When decision-options appear framed as a ''likely gain'', risk-averse choices predominate.

A shift toward risk-seeking behavior occurs when a decision-maker frames decisions in negative terms, or adopts a negative framing effect.

In [[Decision-making|medical decision making]], [[framing bias]] is best avoided by using absolute measures of efficacy.&lt;ref name="pmid21792695"&gt;{{cite journal|vauthors=Perneger TV, Agoritsas T | title=Doctors and Patients' Susceptibility to Framing Bias: A Randomized Trial | journal=J Gen Intern Med | year= 2011 | volume= 26| issue= 12| pages= 1411&#8211;7| pmid=21792695 | doi=10.1007/s11606-011-1810-x | pmc= 3235613| url= }}&lt;/ref&gt;

===Frame-manipulation research===
Researchers have found&lt;ref name="TverskyKahneman1981" /&gt; that framing decision-problems in a positive light generally results in less-risky choices; with negative framing of problems, riskier choices tend to result. According to [[behavioral economics|behavioral economist]]s{{Citation needed|date=November 2007}}:

*positive framing effects (associated with [[risk aversion]]) result from presentation of options as sure (or absolute) gains
*negative framing effects (associated with a preference shift toward choosing riskier options) result from options presented as the relative likelihood of losses

Researchers have found{{Citation needed|date=October 2007}} that framing-manipulation invariably affects subjects, but to varying degrees. Individuals proved risk averse when presented with value-increasing options; but when faced with value decreasing contingencies, they tended towards increased risk-taking. Researchers {{Who|date=September 2008}} found that variations in decision-framing achieved by manipulating the options to represent either a gain or as a loss altered the risk-aversion preferences of decision-makers.

In one study, 57% of the subjects chose a medication when presented with benefits in relative terms, whereas only 14.7% chose a medication whose benefit appeared in absolute terms. Further questioning of the patients suggested that, because the subjects ignored the underlying risk of disease, they perceived benefits as greater when expressed in relative terms.&lt;ref&gt;[http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=8271086&amp;dopt=Abstract The framing effect of relative and absolute risk. [J Gen Intern Med. 1993&amp;#93; - PubMed Result&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;

===Theoretical models===
Researchers have proposed&lt;ref&gt;Chong, D. and Druckman, J. N. (2007): Framing Theory, Annual Review of Political Science, vol. 10&lt;/ref&gt;&lt;ref&gt;Price, V., Tewksburg, D. and Powers, E. (1997): Switching Trains of Thought: The Impact of News Frames on Readers' Cognitive Responses, Communication Research, Vol. 24 No. 5 s. 481 - 506&lt;/ref&gt; various models explaining the '''framing effect''':

*cognitive theories, such as the [[fuzzy-trace theory]], attempt to explain the framing-effect by determining the amount of cognitive processing effort devoted to determining the value of potential gains and losses.
*[[prospect theory]] explains the framing-effect in functional terms, determined by preferences for differing perceived values, based on the assumption that people give a greater weighting to losses than to equivalent gains.
*[[motivation]]al theories explain the framing-effect in terms of [[hedonic]] forces affecting individuals, such as fears and wishes&#8212;based on the notion that negative emotions evoked by potential losses usually out-weigh the emotions evoked by hypothetical gains.
*cognitive [[Cost-benefit analysis|cost-benefit]] trade-off theory defines choice as a compromise between desires, either as a preference for a correct decision or a preference for minimized cognitive effort. This model, which dovetails elements of cognitive and motivational theories, postulates that calculating the value of a sure gain takes much less cognitive effort than that required to select a risky gain.

===Neuroimaging===
Cognitive [[neuroscientist]]s have linked the framing-effect to neural activity in the [[amygdala]], and have identified another brain-region, the orbital and medial [[prefrontal cortex]] (OMPFC), that appears to moderate the role of [[emotion]] on decisions. Using [[functional magnetic resonance imaging]] (fMRI) to monitor brain-activity during a financial decision-making task, they observed greater activity in the OMPFC of those research subjects less susceptible to the framing-effect.&lt;ref&gt;{{cite journal | last1 = De Martino | first1 = B. | last2 = Kumaran | first2 = D. | last3 = Seymour | first3 = B. | last4 = Dolan | first4 = R. J. | year = 2006 | title = Frames, biases, and rational decision-making in the human brain | url = | journal = Science | volume = 313 | issue = 5787| pages = 684&#8211;687 | doi = 10.1126/science.1128356 | pmid = 16888142 | pmc = 2631940 }}&lt;/ref&gt;

==Framing theory and frame analysis in sociology==
Framing theory and frame analysis provide a broad theoretical approach that analysts have used in [[communication studies]], [[journalism|news]] (Johnson-Cartee, 1995), politics, and [[social movement]]s (among other applications).

According to some sociologists, the "social construction of collective action frames" involves "public discourse, that is, the interface of media discourse and interpersonal interaction; persuasive communication during mobilization campaigns by movement organizations, their opponents and countermovement organizations; and consciousness raising during episodes of collective action."&lt;ref&gt;
Bert Klandermans. 1997. ''The Social Psychology of Protest''. Oxford: Blackwell, page 45
&lt;/ref&gt;

===History===
[[diction|Word-selection]] or diction has been a component of [[rhetoric]] since time immemorial. But most commentators attribute the concept of framing to the work of [[Erving Goffman]] on [[frame analysis]] and point especially to his 1974 book, ''Frame analysis: An essay on the organization of experience''. Goffman used the idea of frames to label "schemata of interpretation" that allow individuals or groups "to locate, perceive, identify, and label" events and occurrences, thus rendering meaning, organizing experiences, and guiding actions.&lt;ref&gt;
Erving Goffman (1974). ''Frame Analysis: An essay on the organization of experience''. Cambridge: Harvard University Press, 1974, page 21.
&lt;/ref&gt;
Goffman's framing concept evolved out of his 1959 work, ''[[The Presentation of Self in Everyday Life]]'', a commentary on the [[management]] of [[Impression management|impression]]s. These works arguably depend on [[Kenneth Boulding]]'s concept of image.&lt;ref&gt;
Kenneth Boulding: ''The Image: Knowledge in Life and Society'', University of Michigan Press, 1956)
&lt;/ref&gt;

===Social movements===
Sociologists have utilized framing to explain the process of [[social movement]]s.&lt;ref name="SnowBenford1988" /&gt;
Movements act as carriers of beliefs and ideologies (compare [[meme]]s). In addition, they operate as part of the process of constructing meaning for participants and opposers (Snow &amp; Benford, 1988). Sociologists deem the mobilization of mass-movements "successful" when the frames projected align with the frames of participants to produce resonance between the two parties. Researchers of framing speak of this process as ''frame re-alignment''.

===Frame-alignment===
Snow and Benford (1988) regard frame-alignment as an important element in social mobilization or movement. They argue that when individual frames become linked in congruency and complementariness, "frame alignment" occurs,&lt;ref name="Snowetal1986"&gt;
Snow, D. A., Rochford, E. B., Worden, S. K., &amp; Benford, R. D. (1986). Frame alignment processes, micromobilization, and movement participation. American Sociological Review, 51, page 464
&lt;/ref&gt;
producing "frame resonance", a catalyst in the process of a group making the transition from one frame to another (although not all framing efforts prove successful). The conditions that affect or constrain framing efforts include the following:

*"The robustness, completeness, and thoroughness of the framing effort". Snow and Benford (1988) identify three core framing-tasks, and state that the degree to which framers attend to these tasks will determine participant mobilization. They characterize the three tasks as the following:
*#diagnostic framing for the identification of a problem and assignment of blame
*#prognostic framing to suggest solutions, strategies, and tactics to a problem
*#motivational framing that serves as a call to arms or rationale for action
*The relationship between the proposed frame and the larger [[belief system|belief-system]]; centrality: the frame cannot be of low hierarchical significance and salience within the larger belief system. Its range and interrelatedness, if the framer links the frame to only one core belief or value that, in itself, has a limited range within the larger belief system, the frame has a high degree of being discounted.
*Relevance of the frame to the realities of the participants; a frame must seem relevant to participants and must also inform them. Empirical credibility or testability can constrain relevancy: it relates to participant experience, and has narrative fidelity, meaning that it fits in with existing cultural myths and narrations.
*[[Protest cycle|Cycles of protest]] (Tarrow 1983a; 1983b); the point at which the frame emerges on the timeline of the current era and existing preoccupations with social change. Previous frames may affect efforts to impose a new frame.

Snow and Benford (1988) propose that once someone has constructed proper frames as described above, large-scale changes in society such as those necessary for social movement can be achieved through frame-alignment.

====Types====
Frame-alignment comes in four forms: frame bridging, frame amplification, frame extension and frame transformation.

#''Frame bridging'' involves the "linkage of two or more ideologically congruent but structurally unconnected frames regarding a particular issue or problem" (Snow et al., 1986, p.&amp;nbsp;467). It involves the linkage of a movement to "unmobilized [''[[sic]]''] sentiment pools or public opinion preference clusters" (p.&amp;nbsp;467) of people who share similar views or grievances but who lack an organizational base.
#''Frame amplification'' refers to "the clarification and invigoration of an interpretive frame that bears on a particular issue, problem, or set of events" (Snow et al., 1986, p.&amp;nbsp;469). This interpretive frame usually involves the invigorating of values or beliefs.
#''Frame extensions'' represent a movement's effort to incorporate participants by extending the boundaries of the proposed frame to include or encompass the views, interests, or sentiments of targeted groups (Snow et al., 1986, p.&amp;nbsp;472).
#''Frame transformation'' becomes necessary when the proposed frames "may not resonate with, and on occasion may even appear antithetical to, conventional lifestyles or rituals and extant interpretive frames" (Snow et al., 1986, p.&amp;nbsp;473).

When this happens, the securing of participants and support requires new values, new meanings and understandings. Goffman (1974, p.&amp;nbsp;43&#8211;44) calls this "keying", where "activities, events, and biographies that are already meaningful from the standpoint of some primary framework, in terms of another framework" (Snow et al., 1986, p.&amp;nbsp;474) such that they are seen differently. Two types of frame transformation exist:

#Domain-specific transformations, such as the attempt to alter the status of groups of people, and
#Global interpretive frame-transformation, where the scope of change seems quite radical&#8212;as in a change of [[world view|world-views]], total conversions of thought, or uprooting of everything familiar (for example: moving from [[communism]] to [[market capitalism]], or vice versa; [[religious conversion]], etc.).

==Frame analysis as rhetorical criticism==
Although the idea of language-framing had been explored earlier by [[Kenneth Burke]] (terministic screens), political communication researcher [[Jim A. Kuypers]] first published work advancing [[frame analysis]] (framing analysis) as a rhetorical perspective in 1997. His approach begins inductively by looking for themes that persist across time in a text (for Kuypers, primarily news narratives on an issue or event) and then determining how those themes are framed. Kuypers's work begins with the assumption that frames are powerful rhetorical entities that "induce us to filter our perceptions of the world in particular ways, essentially making some aspects of our multi-dimensional reality more noticeable than other aspects. They operate by making some information more salient than other information...."&lt;ref&gt;
Jim A. Kuypers, "Framing Analysis" in ''Rhetorical Criticism: Perspectives in Action'', edited by J.A. Kuypers,  Lexington Press, 2009. p. 181.&lt;/ref&gt;

In his 2009 essay "Framing Analysis" in ''Rhetorical Criticism: Perspectives in Action''&lt;ref&gt;
''Rhetorical Criticism: Perspectives in Action''&lt;/ref&gt; and his 2010 essay "Framing Analysis as a Rhetorical Process",&lt;ref&gt;Kuypers, Jim A. "Framing Analysis as a Rhetorical Process," Doing News Framing Analysis.  Paul D'Angelo and  Jim A. Kuypers, eds. (New York: Routeledge, 2010).&lt;/ref&gt; Kuypers offers a detailed conception for doing framing analysis from a rhetorical perspective. According to Kuypers, "Framing is a process whereby communicators, consciously or unconsciously, act to construct a point of view that encourages the facts of a given situation to be interpreted by others in a particular manner. Frames operate in four key ways: they define problems, diagnose causes, make moral judgments, and suggest remedies. Frames are often found within a narrative account of an issue or event, and are generally the central organizing idea."&lt;ref&gt;Jim A. Kuypers, ''Bush's War: Media Bias and Justifications for War in a Terrorist Age'', Rowman &amp; Littlefield Publishers, Inc., 2009.&lt;/ref&gt; Kuypers's work is based on the premise that framing is a rhetorical process and as such it is best examined from a rhetorical point of view. Curing the problem is not rhetorical and best left to the observer.

==Rhetorical framing in politics==

===Semiotic analysis of 2016 Republican primaries===

Framing is used to construct, refine, and deliver messages. Framing in politics is essential to getting your message across to the masses. Frames are mental structures that shape the way we view the world (Lakoff, Don't Think of an Elephant! Know Your Values and Frame the Debate 2004).&lt;ref name=Lakoff&gt;In Don't Think of an Elephant! Know Your Values and Frame the Debate, by George Lakoff, 144. Chelsea Green Publishing, 2004.&lt;/ref&gt; Reframing is used particularly well by both conservatives and liberals in the political arena, so well that they have news anchors and commentators discussing the ideas, supplied phrases and framing (Lakoff, Don't Think of an Elephant! Know Your Values and Frame the Debate 2004).&lt;ref name=Lakoff/&gt; 

The neoconservatives in the Bush Administration and the Pentagon viewed the 9/11 attack as an opportunity to go to war in the Middle East and finally take out Saddam Hussain. The Bush administration sold the war by convincing the nation that Iraq had WMDs and collected supportive evidence that they had Secretary of State Colin Powell present at the United Nations. The War on Terror was the label assigned by the Bush administration to its national security policy, launched in response to the attacks of 9/11 (Lewis 2009).&lt;ref name="Lewis 2009"&gt;Lewis, Stephen D. Reese and Seth C. "Framing the War on Terror The internalization of policy in the US press." Journalism, 2009: 777&#8211;797.&lt;/ref&gt; The cultural construction and political rationale supporting this slogan represent a powerful organizing principle that has become a widely accepted framing, laying the groundwork for the invasion of Iraq (Lewis 2009).&lt;ref name="Lewis 2009" /&gt; 

The challenge of political violence has grown with new means of global coordination and access to weapons of mass destruction. The Bush administration's response to this threat, following the now iconic policy reference point of 11 September 2001, has had far-ranging implications for national security strategy, relations with the world community, and civil liberties (Lewis 2009).&lt;ref name="Lewis 2009" /&gt; Labeled the 'War on Terror', the policy was framed within a phrase now part of the popular lexicon, becoming a natural and instinctive shorthand. More than phrases though, frames are 'organizing principles that are socially shared and persistent over time, that work symbolically to meaningfully structure the social world' (Lewis 2009).&lt;ref name="Lewis 2009" /&gt; As a particularly powerful organizing principle, the War on Terror created a supportive political climate for what has been called the biggest US foreign policy blunder in modern times: the invasion of Iraq. Thus, in the scope and consequences of its policy-shaping impact, the War on Terror may be the most important frame in recent memory. (Lewis 2009)

In the now well-known evolution of the administration's policy, influential neoconservatives within the administration had advocated regime change in Iraq for some time, but the events of 9/11 gave them a compelling way to fast-track their ideas and justify a new policy of preemptive war, fist in Afghanistan and then in Iraq. The National Strategy for Combating Terrorism defined the attacks of 9/11 as 'acts of war against the United States of America and its allies, and against the very idea of civilized society'. It identified the enemy as terrorism, an 'evil' threatening our 'freedoms and our way of life. The related National Security Strategy of the United States of America clearly divides 'us' from 'them', linking terrorism to rogue states that 'hate the United States and everything for which it stands (Lewis 2009).&lt;ref name="Lewis 2009" /&gt; Presenting himself as God's agent, Bush's Manichean struggle pitted the USA and its leader against the evildoers (Lewis 2009).&lt;ref name="Lewis 2009" /&gt; 

This argument is being played out in the 2016 Republican primaries, especially by Donald Trump. Trump has portrayed the Syrian refugees as foot soldiers for ISIS, coming to America to kill us in our main streets. Trump's rhetoric appears to be working; many middle class Americans are consuming his rhetoric.{{citation needed|date=June 2016}} The Americans that are supporting Trump and the Republicans in general, many of them are working class and the Republican agenda although it appears to be in their favor it is not. Framing their message to say one thing and mean something completely different is what the conservatives have become masters at. The 2016 Republican primary has been a knock down fight since it started in August 2015. Donald Trump has approached this contest as if Vince McMahon were the promoter and the rest of the field are a bunch of jobbers (persons who are paid to lose). Trump was inducted into the World Wrestling Entertainment (WWE) Hall of Fame in 2003. Even his attacks on Megan Kelly from FOX News are straight out of the WWE's playbook. Roland Barthes analyzed wrestling and boxing in his book ''Mythologies''.
&lt;blockquote&gt;''This public knows very well the distinction between wrestling and boxing; it knows that boxing is a Jansenist sport, based on a demonstration of excellence. One can bet on the outcome of a boxing-match: with wrestling, it would make no sense. A boxing- match is a story which is constructed before the eyes of the spectator; in wrestling, on the contrary, it is each moment which is intelligible, not the passage of time... The logical conclusion of the contest does not interest the wrestling-fan, while on the contrary a boxing-match always implies a science of the future. In other words, wrestling is a sum of spectacles, of which no single one is a function: each moment imposes the total knowledge of a passion which rises erect and alone, without ever extending to the crowning moment of a result.'' (Legum 2015)&lt;ref&gt;{{cite web|author=Legum, Judd|title=This French Philosopher Is The Only One Who Can Explain The Donald Trump Phenomenon|work=thinkprogress.org|date=September 14, 2015|url=http://thinkprogress.org/politics/2015/09/14/3701084/donald-trump/|accessdate=April 23, 2016}}&lt;/ref&gt;&lt;/blockquote&gt;

==Applications==

===Finance===
Preference reversals and other associated phenomena are of wider relevance within behavioural economics, as they contradict the predictions of [[rational choice]], the basis of traditional economics. Framing biases affecting investing, lending, borrowing decisions make one of the themes of [[behavioral finance]].

===Law===
[[Edward Zelinsky]] has shown that framing effects can explain some observed behaviors of legislators.&lt;ref&gt;[[Edward Zelinsky|Zelinsky, Edward A.]]. 2005. Do Tax Expenditures Create Framing Effects? Volunteer Firefighters, Property Tax Exemptions, and the Paradox of Tax Expenditure Analysis. ''Virginia Tax Review'' 24. [http://www.allbusiness.com/accounting/3584666-1.html]&lt;/ref&gt;

===Media===
The role framing plays in the effects of media presentation has been widely discussed, with the central notion that associated perceptions of factual information can vary based upon the presentation of the information.

====News media examples====
In ''Bush's War: Media Bias and Justifications for War in a Terrorist Age,''&lt;ref&gt;Jim A. Kuypers, ''Bush's War: Media Bias and Justifications for War in a Terrorist Age'' (Lanham, MD: Rowman and Littlefield, 2006),&lt;/ref&gt;[[Jim A. Kuypers]] examined the differences in framing of the war on terror between the Bush administration and the U.S. Mainstream News between 2001 and 2005.  Kuypers looked for common themes between presidential speeches and press reporting of those speeches, and then determined how the president and the press had framed those themes.  By using a rhetorical version of framing analysis, Kuypers determined that the U.S. news media advanced frames counter to those used by the Bush administration:

{{quote| the press actively contested the framing of the War on Terror as early as eight weeks following 9/11. This finding stands apart from a collection of communication literature suggesting the press supported the President or was insufficiently critical of the President's efforts after 9/11. To the contrary, when taking into consideration how themes are framed, [Kuypers] found that the news media framed its response in such a way that it could be viewed as supporting the idea of some action against terrorism, while concommitantly opposing the initiatives of the President.  The news media may well relay what the president says, but it does not necessarily follow that it is framed in the same manner; thus, an echo of the theme, but not of the frame.  The present study demonstrates, as seen in Table One [below], that shortly after 9/11 the news media was beginning to actively counter the Bush administration and beginning to leave out information important to understanding the Bush Administration's conception of the War on Terror.  In sum, eight weeks after 9/11, the news media was moving beyond reporting political opposition to the President&#8212;a very necessary and invaluable press function&#8212;and was instead actively choosing themes, and framing those themes, in such a way that the President's focus was opposed, misrepresented, or ignored.&lt;ref&gt;Jim A. Kuypers, Stephen D. Cooper, Matthew T. Althouse, "George W. Bush, The American Press, and the Initial Framing of the War on Terror after 9/11," ''The George W. Bush Presidency: A Rhetorical Perspective,'' Robert E. Denton, ed. (Lanham, MD: Lexington Books, 2012), 89-112.&lt;/ref&gt;}}

Table One: Comparison of President and News Media Themes and Frames 8 Weeks after 9/11&lt;ref&gt;Jim A. Kuypers, Stephen D. Cooper, Matthew T. Althouse, "George W. Bush, "The American Press, and the Initial Framing of the War on Terror after 9/11," ''The George W. Bush Presidency: A Rhetorical Perspective,'' Robert E. Denton, ed. (Lanham, MD: Lexington Books, 2012), 105.&lt;/ref&gt;

{| class="wikitable"
|-
! Themes !! President's Frame !! Press Frame
|-
| Good v. Evil || Struggle of good and evil || Not mentioned
|-
| Civilization v. Barbarism || Struggle of civilization v. barbarism || Not mentioned
|-
| Nature of Enemy ||Evil, implacable, murderers || Deadly, indiscriminant

Bush Administration
|-
| Nature of War || Domestic/global/enduring

War
 || Domestic/global/longstanding

War or police action
|-
| Similarity to Prior Wars || Different Kind of War || WWII or Vietnam?
|-
| Patience || Not mentioned || Some, but running out
|-
| International Effort || Stated || Minimally reported
|-

|}

In 1991 Robert M. Entman published findings&lt;ref&gt;Entman, R. M. (1991), Symposium Framing U.S. Coverage of International News: Contrasts in Narratives of the KAL and Iran Air Incidents. Journal of Communication, 41: 6&#8211;27. {{DOI|10.1111/j.1460-2466.1991.tb02328.x}}&lt;/ref&gt; surrounding the differences in media coverage between [[Korean Air Lines Flight 007]] and [[Iran Air Flight 655]]. After evaluating various levels of media coverage, based on both amount of airtime and pages devoted to similar events, Entman concluded that the frames the events were presented in by the media were drastically different:

{{quote| By de-emphasizing the agency and the victims and by the choice of graphics and adjectives, the news stories about the U.S. downing of an Iranian plane called it a technical problem, while the Soviet downing of a Korean jet was portrayed as a moral outrage&#8230; [T]he contrasting news frames employed by several important U.S. media outlets in covering these two tragic misapplications of military force. For the first, the frame emphasized the moral bankruptcy and guilt of the perpetrating nation, for the second, the frame de-emphasized the guilt and focused on the complex problems of operating military high technology. }}

Differences in coverage amongst various media outlets:

{| class="wikitable"
|-
! Amounts of Media coverage dedicated to each event !! Korean Air !! Iran Air
|-
| Time Magazine and Newsweek || 51 pages || 20 pages
|-
| CBS || 303 minutes || 204 minutes
|-
| New York Times || 286 stories || 102 stories
|}

In 1988 Irwin Levin and Gary Gaeth did a study on the effects of framing attribute information on consumers before and after consuming a product (1988). In this study they found that in a study on beef. People who ate beef labeled as 75% lean rated it more favorably than people whose beef was labelled 25% fat.

===Politics===

Linguist and rhetoric scholar [[George Lakoff]] argues that, in order to persuade a political audience of one side of and argument or another, the facts must be presented through a rhetorical frame.  It is argued that, without the frame, the facts of an argument become lost on an audience, making the argument less effective.  The rhetoric of politics uses framing to present the facts surrounding an issue in a way that creates the appearance of a problem at hand that requires a solution.  Politicians using framing to make their own solution to an exigence appear to be the most appropriate compared to that of the opposition.&lt;ref name="van der Pas" /&gt;  Counter-arguments become less effective in persuading an audience once one side has framed an argument, because it is argued that the opposition then has the additional burden of arguing the frame of the issue in addition to the issue itself.

Framing a political issue, a political party or a political opponent is a [[strategy|strategic]] goal in [[politics]], particularly in the [[United States of America]]. Both the [[Democratic Party (United States)|Democratic]] and [[Republican Party (United States)|Republican]] political parties compete to successfully harness its power of persuasion. According to the ''[[New York Times]]'':

{{quote|Even before the [[United States presidential election, 2004|election]], a new political word had begun to take hold of the party, beginning on the [[West Coast of the United States|West Coast]] and spreading like a virus all the way to the inner offices of the [[United States Capitol|Capitol]]. That word was 'framing.' Exactly what it means to 'frame' issues seems to depend on which Democrat you are talking to, but everyone agrees that it has to do with choosing the language to define a debate and, more important, with fitting individual issues into the contexts of broader story lines.|&lt;ref name="framingwars"&gt;
[http://www.nytimes.com/2005/07/17/magazine/17DEMOCRATS.html?pagewanted=1&amp;ei=5070&amp;en=e3e686efd4fa97c5&amp;ex=1183608000 The Framing Wars. ''[[New York Times]]'' 17 July 2005]&lt;/ref&gt;}}

Because framing has the ability to alter the public's perception, politicians engage in battles to determine how issues are framed. Hence, the way the issues are framed in the media reflects who is winning the battle. For instance, according to Robert Entman, professor of Communication at George Washington University, in the build-up to the Gulf War the conservatives were successful in making the debate whether to attack sooner or later, with no mention of the possibility of not attacking. Since the media picked up on this and also framed the debate in this fashion, the conservatives won.&lt;ref name="EntmanRobertTree" /&gt;

One particular example of [[George Lakoff|Lakoff's]] work that attained some degree of fame was his advice to rename&lt;ref&gt;[[Walter Olson]], [http://www.overlawyered.com/2005/07/some_framing_advice.html Overlawyered weblog], 2005-07-18&lt;/ref&gt; [[trial lawyer]]s (unpopular in the United States) as "public protection attorneys". Though Americans have not generally adopted this suggestion, the [[Association of Trial Lawyers of America]] did rename themselves the "American Association of Justice", in what the [[Chamber of Commerce]] called an effort to hide their identity.&lt;ref&gt;[[Al Kamen]], [http://www.washingtonpost.com/wp-dyn/content/article/2007/01/16/AR2007011601429_pf.html "Forget Cash -- Lobbyists Should Set Support for Lawmakers in Stone"], ''[[Washington Post]]'', 2007-01-17&lt;/ref&gt;

The ''[[New York Times]]'' depicted similar intensity among Republicans:

{{quote|In one recent memo, titled 'The 14 Words Never to Use,' [[Frank Luntz|[Frank] Luntz]] urged conservatives to restrict themselves to phrases from what he calls ... the 'New American Lexicon.' Thus, a smart Republican, in Luntz's view, never advocates '[[oil drilling|drilling for oil]]'; he prefers 'exploring for energy.' He should never criticize the 'government,' which cleans our streets and pays our firemen; he should attack '[[Washington, D.C.|Washington]],' with its ceaseless thirst for taxes and regulations. 'We should never use the word [[outsourcing]],' Luntz wrote, 'because we will then be asked to defend or end the practice of allowing companies to ship American jobs overseas.'|&lt;ref name="framingwars"/&gt;}}

From a political perspective, framing has widespread consequences. For example, the concept of framing links with that of [[agenda setting theory|agenda-setting]]: by consistently invoking a particular frame, the framing party may effectively control discussion and perception of the issue. [[Sheldon Rampton]] and [[John Stauber]] in ''[[Trust Us, We're Experts]]'' illustrate how [[Public Relations|public-relations]] (PR) firms often use language to help frame a given issue, structuring the questions that then subsequently emerge. For example, one firm advises clients to use "bridging language" that uses a strategy of answering questions with specific terms or ideas in order to shift the discourse from an uncomfortable topic to a more comfortable one.&lt;ref&gt;
Rampton, Sheldon and Stauber, John. ''Trust Us, We're Experts!'' Putnam Publishing, New York, NY, 2002. Page 64.&lt;/ref&gt;
Practitioners of this strategy might attempt to draw attention away from one frame in order to focus on another. As Lakoff notes, "On the day that [[George W. Bush]] took office, the words "tax relief" started coming out of the White House."&lt;ref name="Lakoff2004"&gt;{{Cite book|last=Lakoff|first=George|title=Don't think of an elephant!: know your values and frame the debate|year=2004|publisher=Chelsea Green Publishing|isbn=978-1-931498-71-5|page=56}}&lt;/ref&gt;
By refocusing the structure away from one frame ("tax burden" or "tax responsibilities"), individuals can set the agenda of the questions asked in the future.

[[Cognitive linguistics|Cognitive linguists]] point to an example of framing in the phrase "[[tax cut|tax relief]]". In this frame, use of the concept "relief" entails a concept of (without mentioning the benefits resulting from) taxes putting strain on the citizen:

{{quote|The current tax code is full of inequities. Many single moms face higher marginal tax rates than the wealthy. Couples frequently face a higher tax burden after they marry. The majority of Americans cannot deduct their charitable donations. Family farms and businesses are sold to pay the death tax. And the owners of the most successful small businesses share nearly half of their income with the government. President Bush's tax cut will greatly reduce these inequities. It is a fair plan that is designed to provide tax relief to everyone who pays income taxes.|&lt;ref&gt;[http://georgewbush-whitehouse.archives.gov/news/reports/taxplan.html The President's Agenda for Tax Relief] retrieved 3 July 2007.&lt;/ref&gt;}}

Alternative frames may emphasize the concept of taxes as a source of infrastructural support to businesses:

{{quote|The truth is that the wealthy have received more from America than most Americans&#8212;not just wealth but the infrastructure that has allowed them to amass their wealth: banks, the Federal Reserve, the stock market, the Securities and Exchange Commission, the legal system, federally sponsored research, patents, tax supports, the military protection of foreign investments, and much much more. American taxpayers support the infrastructure of wealth accumulation. It is only fair that those who benefit most should pay their fair share.|&lt;ref&gt;[http://www.cognitivepolicyworks.com/resource-center/planning-tools/framing-tutorials/simple-framing/ Cognitive Policy Works/Rockridge Institute: Simple Framing]&lt;/ref&gt;}}

Frames can limit debate by setting the vocabulary and [[metaphor]]s through which participants can comprehend and discuss an issue. They form a part not just of political discourse, but of [[cognition]]. In addition to generating new frames, politically oriented framing research aims to increase public awareness of the connection between framing and reasoning.

====Examples====
*The initial response of the [[George W. Bush administration|Bush administration]] to the [[September 11, 2001 attacks|assault of September 11, 2001]] was to frame the acts of [[Counterterrorism|terror]] as [[crime]]. This framing was replaced within hours by a war metaphor, yielding the "[[War on Terrorism|War on Terror]]". The difference between these two framings is in the implied response. Crime connotes bringing criminals to justice, putting them on trial and sentencing them, whereas as [[war]] implies enemy territory, military action and war powers for government.&lt;ref name="Lakoff2004" /&gt;&lt;ref&gt;{{Cite journal|last=Zhang|first=Juyan |title=Beyond anti-terrorism: Metaphors as message strategy of post-September-11 U.S. public diplomacy |journal=Public Relations Review|year=2007|volume=33|issue=1|pages=31&#8211;39|doi=10.1016/j.pubrev.2006.11.006}}&lt;/ref&gt;
*The term "escalation" to describe an increase in American troop-levels in [[Iraq]] in 2007 implied that the United States deliberately increased the scope of conflict in a provocative manner and possibly implies that U.S. strategy entails a long-term military presence in Iraq, whereas [[Iraq War troop surge of 2007|"surge"]] framing implies a powerful but brief, transitory increase in intensity.&lt;ref&gt;[http://www.alternet.org/waroniraq/48059/ "It's Escalation, Stupid." ''Alternet''] retrieved 3 July 2007&lt;/ref&gt;
*The "bad apple" frame, as in the proverb "one bad [[apple]] spoils the barrel". This frame implies that removing one underachieving or corrupt official from an [[institution]] will solve a given problem; an opposing frame presents the same problem as systematic or structural to the institution itself&#8212;a source of infectious and spreading rot.&lt;ref&gt;[http://www.huffingtonpost.com/bruce-budner/the-rumsfeld-dilemma-dem_b_29550.html "The Rumsfeld Dilemma: Demand an Exit Strategy, Not a Facelift"] by Bruce Budner, in ''The Huffington Post'' 15 September 2006&lt;/ref&gt;
*The "[[taxpayers]] money" frame, rather than [[government spending|public or government funds]], which implies that individual taxpayers have a claim or right to set [[government policy]] based upon their payment of tax rather than their status as [[citizen]]s or [[voters]] and that taxpayers have a right to control public funds that are the shared property of all citizens and also privileges individual self-interest above group interest.{{Citation needed|date=April 2009}}
*The "collective property" frame, which implies that property owned by individuals is really owned by a collective in which those individuals are members. This collective can be a territorial one, such as a nation, or an abstract one that does not map to a specific territory.
*Program-names that may describe only the intended effects of a program but may also imply their effectiveness. These include the following:
**"[[Foreign aid]]"&lt;ref&gt;[http://hij.sagepub.com/cgi/content/abstract/12/2/120 "Is It All in a Word? The Effect of Issue Framing on Public Support for U.S. Spending on HIV/AIDS in Developing Countries."] by Sara Bleich. Retrieved 2007-07-03
&lt;/ref&gt; (which implies that spending money will aid foreigners, rather than harm them)
**"[[Social security]]" (which implies that the program can be relied on to provide security for a society)
**"[[Stabilisation policy]]" (which implies that a policy will have a stabilizing effect).
* Based on [[opinion polling]] and [[focus group]]s, [[ecoAmerica]], a nonprofit environmental marketing and messaging firm, has advanced the position that [[global warming]] is an ineffective framing due to its identification as a leftist advocacy issue. The organization has suggested to government officials and environmental groups that alternate formulations of the issues would be more effective.&lt;ref&gt;[http://www.nytimes.com/2009/05/02/us/politics/02enviro.html "Seeking to Save the Planet, With a Thesaurus"] article by John M. Broder in ''[[The New York Times]]'' May 1, 2009&lt;/ref&gt;
*In her 2009 book ''Frames of War'', [[Judith Butler]] argues that the justification within liberal-democracies for war, and atrocities committed in the course of war, (referring specifically to the current war in Iraq and to [[Abu Ghraib torture and prisoner abuse|Abu Ghraib]] and [[Guantanamo Bay detention camp|Guantanamo Bay]]) entails a framing of the (especially Muslim) 'other' as pre-modern/primitive and ultimately not human in the same way as citizens within the liberal order.&lt;ref&gt;Butler, J. (2009), ''Frames of War'', London: Verso.&lt;/ref&gt;

==See also==
{{div col|3}}
*[[Anecdotal value]]
*[[Alternative facts]]
*[[Argumentation theory]]
*[[Bias]]
*[[Choice architecture]]
*[[Code word (figure of speech)]]
*[[Communication theory]]
*[[Connotation]]
*[[Cultural bias]]
*[[Decision making]]
*[[Definition of the situation]]
*[[Demagoguery]]
*[[Domain of discourse]]
*[[Echo chamber (media)]]
*[[Fallacy of many questions]]
*[[Figure of speech]]
*[[Filter bubble]]
*[[Freedom of speech]]
*[[Freedom of the press|Free press]]
*[[Idea networking]]
*[[Language and thought]]
*[[Meme]]
*[[Newspeak]]
&lt;!--* [[Political frame]] - self ref after merge/redirect--&gt;
*[[Power word]]
*[[Overton window]]
*[[Political correctness]]
*[[Rhetorical device]]
*[[Semantics]]
*[[Semantic domain]]
*[[Social heuristics]]
*[[Sophism]]
*[[Spin doctor]]
*[[Stovepiping]]
*''[[Thought Reform (book)]]''
*[[Trope (linguistics)|Trope]]
*[[Unspeak]] (book)
*[[Virtue word]]
{{div col end}}

==References==
{{reflist|colwidth=30em}}
Levin, Irwin P., and Gary J. Gaeth. "How Consumers Are Affected By The Framing Of Attribute Information Before And After Consuming The Product." Journal of Consumer Research 15.3 (1988): 374. Print.

==Further reading==
*[[Bernard Baars|Baars, B]]. ''A cognitive theory of consciousness'', NY: [[Cambridge University Press]] 1988, ISBN 0-521-30133-5.
*[[Kenneth E. Boulding|Boulding, Kenneth E.]] (1956). The Image: Knowledge in Life and Society. Michigan University Press.
* {{cite journal | last1 = Carruthers | first1 = P. | authorlink = Peter Carruthers (philosopher) | year = 2003 | title = On Fodor's Problem | url = | journal = Mind and Language | volume = 18 | issue = 5| pages = 502&#8211;523 | doi = 10.1111/1468-0017.00240 }}
*Clark, A. (1997), Being There: Putting Brain, Body, and World Together Again, Cambridge, MA: MIT Press.
*Cutting, Hunter and Makani Themba Nixon (2006). Talking the Walk: A Communications Guide for Racial Justice: AK Press
*[[Daniel Dennett|Dennett, D.]] (1978), Brainstorms, Cambridge, MA: MIT Press.
*Fairhurst, Gail T. and Sarr, Robert A. 1996. ''The Art of Framing: Managing the Language of Leadership.'' USA: Jossey-Bass, Inc.
*Feldman, Jeffrey. (2007), ''Framing the Debate: Famous Presidential Speeches and How Progressives Can Use Them to Control the Conversation (and Win Elections)''. Brooklyn, NY: Ig Publishing.
*[[Jerry Fodor|Fodor, J.A.]] (1983), The Modularity of Mind, Cambridge, MA: MIT Press.
*Fodor, J.A. (1987), "Modules, Frames, Fridgeons, Sleeping Dogs, and the Music of the Spheres", in Pylyshyn (1987).
*Fodor, J.A. (2000), The Mind Doesn't Work That Way, Cambridge, MA: MIT Press.
*Ford, K.M. &amp; Hayes, P.J. (eds.) (1991), Reasoning Agents in a Dynamic World: The Frame Problem, New York: JAI Press.
*[[Erving Goffman|Goffman, Erving]]. 1974. ''Frame Analysis: An Essay on the Organization of Experience.'' London: Harper and Row.
*Goffman, E. (1974). Frame Analysis. Cambridge: Harvard University Press.
*Goffman, E. (1959). Presentation of Self in Everyday Life. New York: Doubleday.
*Goodman, N. (1954), Fact, Fiction, and Forecast, Cambridge, MA: Harvard University Press.
*{{cite journal | last1 = Hanks | first1 = S. | last2 = McDermott | first2 = D. | year = 1987 | title = Nonmonotonic Logic and Temporal Projection | url = | journal = Artificial Intelligence | volume = 33 | issue = 3| pages = 379&#8211;412 | doi = 10.1016/0004-3702(87)90043-9 }}
*Haselager, W.F.G. (1997). Cognitive science and folk psychology: the right frame of mind. London: Sage
*{{cite journal | last1 = Haselager | first1 = W.F.G. | last2 = Van Rappard | first2 = J.F.H. | year = 1998 | title = Connectionism, Systematicity, and the Frame Problem | url = | journal = Minds and Machines | volume = 8 | issue = 2| pages = 161&#8211;179 | doi = 10.1023/A:1008281603611 }}
*Hayes, P.J. (1991), "Artificial Intelligence Meets David Hume: A Reply to Fetzer", in Ford &amp; Hayes (1991).
*Heal, J. (1996), "Simulation, Theory, and Content", in Theories of Theories of Mind, eds. P. Carruthers &amp; P. Smith, Cambridge: Cambridge University Press, pp.&amp;nbsp;75&#8211;89.
*Johnson-Cartee, K. (2005). News narrative and news framing: Constructing political reality. Lanham, MD: Rowman &amp; Littlefield.
*[[Diana Kendall|Kendall, Diana]], ''Sociology In Our Times'', Thomson Wadsworth, 2005, ISBN 0-534-64629-8 [https://books.google.com/books?vid=ISBN0534646298&amp;id=kzU-gtx2VfoC&amp;pg=PA531&amp;lpg=PA531&amp;dq=%22Resource+Mobilization%22&amp;sig=NgTePMtdl2stO7V2FofPqeZuP5I&amp;hl=en Google Print, p.531]
*Klandermans, Bert. 1997. ''The Social Psychology of Protest.'' Oxford: Blackwell.
*[[George Lakoff|Lakoff, G.]] &amp; Johnson, M. (1980), Metaphors We Live By, Chicago: University of Chicago Press.
*Leites, N. &amp; Wolf, C., Jr. (1970). Rebellion and authority. Chicago: Markham Publishing Company.
*{{cite journal | last1 = Martino | first1 = De | year = 2006 | title = Frames, Biases, and Rational Decision-Making in the Human Brain | url = | journal = Science | volume = 313 | issue = 5787| pages = 684&#8211;687 | doi = 10.1126/science.1128356 | pmid = 16888142 | last2 = Kumaran | first2 = D | last3 = Seymour | first3 = B | last4 = Dolan | first4 = RJ | pmc = 2631940 }}
*McAdam, D., McCarthy, J., &amp; Zald, M. (1996). Introduction: Opportunities, Mobilizing Structures, and Framing Processes&#8212;Toward a Synthetic, Comparative Perspective on Social Movements. In D. McAdam, J. McCarthy &amp; M. Zald (Eds.), Comparative Perspectives on Social Movements; Political Opportunities, Mobilizing Structures, and Cultural Framings (pp.&amp;nbsp;1&#8211;20). New York: Cambridge University Press.
*McCarthy, J. (1986), "Applications of Circumscription to Formalizing Common Sense Knowledge", [[Artificial Intelligence (journal)|Artificial Intelligence]], vol. 26(3), pp.&amp;nbsp;89&#8211;116.
*McCarthy, J. &amp; Hayes, P.J. (1969), "Some Philosophical Problems from the Standpoint of Artificial Intelligence", in Machine Intelligence 4, ed. D.Michie and B.Meltzer, Edinburgh: Edinburgh University Press, pp.&amp;nbsp;463&#8211;502.
*McDermott, D. (1987), "We've Been Framed: Or Why AI Is Innocent of the Frame Problem", in Pylyshyn (1987).
*Mithen, S. (1987), ''The Prehistory of the Mind'', London: Thames &amp; Hudson.
*{{cite journal | last1 = Nelson | first1 = T. E. | last2 = Oxley | first2 = Z. M. | last3 = Clawson | first3 = R. A. | year = 1997 | title = Toward a psychology of framing effects | url = | journal = Political Behavior | volume = 19 | issue = 3| pages = 221&#8211;246 | doi = 10.1023/A:1024834831093 }}
*{{cite journal | last1 = Pan | first1 = Z. | last2 = Kosicki | first2 = G. M. | year = 1993 | title = Framing analysis: An approach to news discourse | url = | journal = Political Communication | volume = 10 | issue = 1| pages = 55&#8211;75 | doi = 10.1080/10584609.1993.9962963 }}
*Pan. Z. &amp; Kosicki, G. M. (2001). Framing as a strategic action in public deliberation. In S. D. Reese, O. H. Gandy, Jr., &amp; A. E. Grant (Eds.), Framing public life: Perspectives on media and our understanding of the social world, (pp.&amp;nbsp;35&#8211;66). Mahwah, NJ: Lawrence Erlbaum Associates.
*Pan, Z. &amp; Kosicki, G. M. (2005). Framing and the understanding of citizenship. In S. Dunwoody, L. B. Becker, D. McLeod, &amp; G. M. Kosicki (Eds.), Evolution of key mass communication concepts, (pp.&amp;nbsp;165&#8211;204). New York: Hampton Press.
*[[Zenon Pylyshyn|Pylyshyn, Zenon W.]] (ed.) (1987), The Robot's Dilemma: The Frame Problem in Artificial Intelligence, Norwood, NJ: Ablex.
*Stephen D. Reese, Oscar H. Gandy and August E. Grant. (2001). [https://books.google.com/books?id=I0BlAAAAMAAJ&amp;q=journalist+subject:%22Reporters+and+reporting%22&amp;dq=journalist+subject:%22Reporters+and+reporting%22&amp;lr=&amp;client=firefox-a&amp;pgis=1 ''Framing Public Life: Perspectives on Media and Our Understanding of the Social World.''] Maywah, New Jersey: Lawrence Erlbaum. ISBN 978-0-8058-3653-0
*Russell, S. &amp; Wefald, E. (1991), Do the Right Thing: Studies in Limited Rationality, Cambridge, MA: MIT Press.
*{{cite journal | last1 = Scheufele | first1 =  DA| authorlink = Scheufele | last2 = Dietram | first2 = A.  | year = 1999 | title = Framing as a theory of media effects | url = | journal = [[Journal of Communication]] | volume = 49 | issue = 1| pages = 103&#8211;122 | doi = 10.1111/j.1460-2466.1999.tb02784.x }}
*Shanahan, Murray P. (1997), ''Solving the Frame Problem: A Mathematical Investigation of the Common Sense Law of Inertia'', Cambridge, MA: MIT Press. ISBN 0-262-19384-1
*Shanahan, Murray P. (2003), "The Frame Problem", in ''The Macmillan Encyclopedia of Cognitive Science'', ed. L.Nadel, Macmillan, pp.&amp;nbsp;144&#8211;150.
*[[Herbert A. Simon|Simon, Herbert]] (1957), ''Models of Man, Social and Rational: Mathematical Essays on Rational Human Behavior in a Social Setting'', New York: John Wiley. {{OCLC|165735}}
*{{cite journal | last1 = Snow | first1 = D. A. | last2 = Benford | first2 = R. D. | year = 1988 | title = Ideology, frame resonance, and participant mobilization | url = | journal = International Social Movement Research | volume = 1 | issue = | pages = 197&#8211;217 }}
*{{cite journal | last1 = Snow | first1 = D. A. | last2 = Rochford | first2 = E. B. | last3 = Worden | first3 = S. K. | last4 = Benford | first4 = R. D. | year = 1986 | title = Frame alignment processes, micromobilization, and movement participation | url = | journal = American Sociological Review | volume = 51 | issue = 4| pages = 464&#8211;481 | doi = 10.2307/2095581 }}
*{{cite journal | last1 = Sperber | first1 = D. | last2 = Wilson | first2 = D. | year = 1996 | title = Fodor's Frame Problem and Relevance Theory | url = | journal = Behavioral and Brain Sciences | volume = 19 | issue = 3| pages = 530&#8211;532 | doi = 10.1017/S0140525X00082030 }}
*[[Sidney Tarrow|Tarrow, S.]] (1983a). "Struggling to Reform: social Movements and policy change during cycles of protest". Western Societies Paper No. 15. Ithaca, NY: Cornell University.
*Tarrow, S. (1983b). "Resource mobilization and cycles of protest: Theoretical reflections and comparative illustrations". Paper presented at the Annual Meeting of the [[American Sociological Association]], Detroit, August 31&#8211;September 4.
*Triandafyllidou, A. and Fotiou, A. (1998), [http://www.socresonline.org.uk/3/1/2.html "Sustainability and Modernity in the European Union: A Frame Theory Approach to Policy-Making"], Sociological Research Online, vol. 3, no. 1.
*[[Charles Tilly|Tilly, C.]], Tilly, L., &amp; Tilly, R. (1975). ''The rebellious century, 1830&#8211;1930''. Cambridge, MA: Cambridge University Press.
*Turner, R. H., &amp; Killian, L. M. (1972). ''Collective Behavior''. Englewood Cliffs, NJ: Prentice-Hall.
* "Rational Choice and the Framing of Decisions", A.Tversky, D.Kahneman, ''Journal of Business'', 1986, vol.59, no.4, pt.2.
*{{cite journal | last1 = Wilkerson | first1 = W.S. | year = 2001 | title = Simulation, Theory, and the Frame Problem | url = | journal = [[Philosophical Psychology (journal)|Philosophical Psychology]] | volume = 14 | issue = 2| pages = 141&#8211;153 | doi = 10.1080/09515080120051535 }}
*[[Charles Arthur Willard|Willard, Charles Arthur]]. ''Liberalism and the Social Grounds of Knowledge'' Chicago: University of Chicago Press, 199

==External links==
*[http://www.nytimes.com/2005/07/17/magazine/17DEMOCRATS.html The Framing Wars. ''New York Times'' 17 July 2005]
*Curry, Tom. 2005. [http://www.msnbc.msn.com/id/7640262/page/2/ "Frist chills talk of judges deal (Page 2)."] "The question in the poll was not '''framed''' as a matter of whether nominee ought to get an up-or-down vote. And that '''framing''' of the issue, Republican strategists believe, is the most advantageous one..."; [[MSNBC]]
*[http://www.ccbi.cmu.edu/reprints/Gonzalez_JOEP2005-decision-making.pdf CMU.edu (pdf)] - 'The Framing effect and risky decision: Examining cognitive functions with fMRI', C. Gonzalez, et al., ''[[Journal of Economic Psychology]]'' (2005)
*[http://hbswk.hbs.edu/item/5488.html HBS.edu] - 'Fixing Price Tag Confusion'(interview), Sean Silverthorne (December 11, 2006)
*[http://www.msnbc.msn.com/id/14170927/ "'Framing effect' influences decisions: Emotions play a role in decision-making when information is too complex"], Charles Q. Choi, [[MSNBC]] (August 3, 2006)

{{Media culture}}
{{Media manipulation}}
{{Propaganda}}
{{World view}}

{{DEFAULTSORT:Framing (Social Sciences)}}
[[Category:Cognitive biases]]
[[Category:Framing (social sciences)| ]]
[[Category:Knowledge representation]]
[[Category:Propaganda techniques]]
[[Category:Prospect theory]]
[[Category:Social constructionism]]</text>
      <sha1>1pptdn7t6g9wo5p7u6sui3iy7xmia0g</sha1>
    </revision>
  </page>
  <page>
    <title>Reification (computer science)</title>
    <ns>0</ns>
    <id>232423</id>
    <revision>
      <id>713271172</id>
      <parentid>711384159</parentid>
      <timestamp>2016-04-03T01:04:13Z</timestamp>
      <contributor>
        <username>MindlessXD</username>
        <id>481404</id>
      </contributor>
      <minor />
      <comment>Place images on the right as per [[WP:IMGLOC]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="17153" xml:space="preserve">{{Other uses|Reification (disambiguation)}}

'''Reification''' is the process by which an abstract idea about a [[computer program]] is turned into an explicit [[data model]] or other object created in a [[programming language]]. A computable/addressable object &#8212; a resource &#8212; is created in a system as a proxy for a non computable/addressable object. By means of reification, something that was previously implicit, unexpressed, and possibly inexpressible is explicitly formulated and made available to conceptual (logical or computational) manipulation. Informally, reification is often referred to as "making something a [[first-class citizen]]" within the scope of a particular system. Some aspect of a system can be reified at ''language design time'', which is related to [[Reflection (computer science)|reflection]] in programming languages. It can be applied as a stepwise refinement at ''system design time''. Reification is one of the most frequently used techniques of [[conceptual analysis]] and [[knowledge representation]].

== Reification and reflective programming languages ==
In the context of [[programming language]]s, reification is the process by which a user program or any aspect of a programming language that was implicit in the translated program and the run-time system, are  expressed in the language itself. This process makes it available to the program, which can inspect all these aspects as ordinary [[data]]. In [[Reflection (computer science)|reflective languages]], reification data is causally connected to the related reified aspect such that a modification to one of them affects the other. Therefore, the reification data is always a faithful representation of the related reified aspect. Reification data is often said to be made a [[first class object]]. Reification, at least partially, has been experienced in many languages to date: in early [[Lisp (programming language)|Lisp dialects]] and in current [[Prolog| Prolog dialects]], programs have been treated as data, although the causal connection has often been left to the responsibility of the programmer. In [[Smalltalk]]-80, the compiler from the source text to bytecode has been part of the run-time system since the very first implementations of the language.&lt;ref&gt;J. Malenfant, M. Jacques and F.-N. Demers, [http://www2.parc.com/csl/groups/sda/projects/reflection96/docs/malenfant/ref96/ref96.html A Tutorial on Behavioral Reflection and its Implementation]&lt;/ref&gt;

* The [[C (programming language)|C programming language]] reifies the low-level detail of [[memory address]]es.{{paragraph break}}Many programming language designs encapsulate the details of memory allocation in the compiler and the run-time system. In the design of the C programming language, the memory address is reified and is available for direct manipulation by other language constructs. For example, the following code may be used when implementing a memory-mapped device driver. The buffer pointer is a proxy for the memory address 0xB800000.{{paragraph break}}&lt;source lang="c"&gt;
 char* buffer = (char*) 0xB800000;
 buffer[0] = 10; 
&lt;/source&gt;
* [[Functional programming languages]] based on [[lambda-calculus]] reify the concept of a procedure abstraction and procedure application in the form of the [[Lambda calculus#Lambda calculus and programming languages|Lambda expression]].
* The [[Scheme (programming language)|Scheme]] programming language reifies [[continuations]] (approximately, the call stack).
* In [[C Sharp (programming language)|C#]], reification is used to make [[parametric polymorphism]] implemented as generics as a first-class feature of the language.
* In the [[Java (programming language)|Java]] programming language, there exist "reifiable types" that are "completely available at run time" (i.e. their information is not erased during compilation).&lt;ref&gt;[http://docs.oracle.com/javase/specs/jls/se7/html/jls-4.html#jls-4.7 The Java Language Specification, section 4.7], Java SE 7 Edition&lt;/ref&gt;
* [[REBOL]] reifies code as data and vice versa.
* Many languages, such as [[Lisp (programming language)|Lisp]], [[JavaScript]], and [[Curl (programming language)|Curl]], provide an [[eval|&lt;code&gt;eval&lt;/code&gt; or &lt;code&gt;evaluate&lt;/code&gt; procedure]] that effectively reifies the language interpreter.
* The [[Logtalk]] framework for [[Prolog]] offers a means to explore reification in the context of [[logic programming]].
* [[Smalltalk]] and [[Actor model|Actor languages]] permit the reification of blocks and [[message passing|messages]],&lt;ref&gt;{{cite web|url=http://c2.com/cgi/wiki?SmalltalkBlocksAndClosures |title=Smalltalk Blocks And Closures |publisher=C2.com |date=2009-10-15 |accessdate=2010-10-09}}&lt;/ref&gt; which are equivalent of lambda expressions in Lisp, and [[thisContext]] which is a reification of the current executing block.
* [[Homoiconicity|Homoiconic languages]] reify the syntax of the language itself in the form of an [[abstract syntax tree]], typically together with &lt;code&gt;eval&lt;/code&gt;.

== Data reification vs. data refinement ==
Data reification ([[stepwise refinement]]) involves finding a more concrete representation of the [[abstract data type]]s used in a [[formal specification]].

Data reification is the terminology of the [[Vienna Development Method]] (VDM) that most other people would call data refinement. An example is taking a step towards an implementation by replacing a data representation without a counterpart in the intended implementation language, such as sets, by one that does have a counterpart (such as maps with fixed domains that can be implemented by arrays), or at least one that is closer to having a counterpart, such as sequences. The VDM community prefers the word "reification" over "refinement", as the process has more to do with concretising an idea than with refining it.&lt;ref&gt;[https://www.cs.tcd.ie/FME/original/FAQ/vdm/part13.html Formal Methods Europe, Frequently Asked Questions, part 13].&lt;/ref&gt;

For similar usages, see [[Reification (linguistics)]].

== Reification in conceptual modeling ==
Reification is widely used in [[Conceptual model (computer science)|conceptual modeling]].&lt;ref&gt;Antoni Oliv&#233;, Conceptual Modeling of Information Systems, Springer Verlag, 2007.&lt;/ref&gt; Reifying a relationship means viewing it as an entity. The purpose of reifying a relationship is to make it explicit, when additional information needs to be added to it. Consider the relationship type ''&lt;code&gt;IsMemberOf(member:Person, Committee)&lt;/code&gt;''. An instance of ''&lt;code&gt;IsMemberOf&lt;/code&gt;'' is a relationship that represents the fact that a person is a member of a committee. The figure below shows an example population of ''&lt;code&gt;IsMemberOf&lt;/code&gt;'' relationship in tabular form. Person ''P1'' is a member of committees ''C1'' and ''C2''. Person ''P2'' is a member of committee ''C1'' only. [[File:reification example1.png|500px|thumb|Example population of &lt;code&gt;IsMemberOf&lt;/code&gt; relationship in tabular form. Person P1 is a member of committees C1 and C2. Person P2 is a member of committee C1 only.]]

The same fact, however, could also be viewed as an entity. Viewing a relationship as an entity, one can say that the entity reifies the relationship. This is called reification of a relationship. Like any other entity, it must be an instance of an entity type. In the present example, the entity type has been named &lt;code&gt;Membership&lt;/code&gt;. For each instance of ''&lt;code&gt;IsMemberOf&lt;/code&gt;'', there is one and only one instance of ''&lt;code&gt;Membership&lt;/code&gt;'', and vice versa. Now, it becomes possible to add more information to the original relationship. As an example, we can express the fact that "person p1 was nominated to be the member of committee c1 by person p2". Reified relationship ''&lt;code&gt;Membership&lt;/code&gt;'' can be used as the source of a new relationship ''&lt;code&gt;IsNominatedBy(Membership, Person)&lt;/code&gt;''.

For related usages see [[Reification (knowledge representation)]].

== Reification in Unified Modeling Language (UML) ==
[[File:reification example2.png|400px|thumb|The UML [[class diagram]] for the Membership example.]] [[Unified Modeling Language|UML]] provides an ''association class'' construct for defining reified relationship types. The association class is a single model element that is both a kind of association and a kind of a class.&lt;ref&gt;''Unified Modeling Language, UML superstructure'', Object Management Group, 2007-11-02.&lt;/ref&gt; The association and the entity type that reifies are both the same model element. Note that attributes cannot be reified.

== Reification on Semantic Web ==

=== RDF and OWL ===
In [[Semantic Web]] languages, such as [[Resource Description Framework]] (RDF) and [[Web Ontology Language]] (OWL), a statement is a binary relation. It is used to link two individuals or an individual and a value. Applications sometimes need to describe other RDF statements, for instance, to record information like when statements were made, or who made them, which is sometimes called "[[provenance]]" information. As an example, we may want to represent properties of a relation, such as our certainty about it, severity or strength of a relation, relevance of a relation, and so on.

The example from the conceptual modeling section describes a particular person with &lt;code&gt;URIref person:p1&lt;/code&gt;, who is a member of the &lt;code&gt;committee:c1&lt;/code&gt;. The RDF triple from that description is
&lt;source lang="sparql"&gt;
  person:p1   committee:isMemberOf   committee:c1 .
&lt;/source&gt;
Consider to store two further facts: (i) to record who nominated this particular person to this committee (a statement about the membership itself), and (ii) to record who added the fact to the database (a statement about the statement).

The first case is a case of classical reification like above in UML: reify the membership and store its attributes and roles etc.:

&lt;source lang="sparql"&gt;
 committee:Membership        rdf:type              owl:Class .
 committee:membership12345   rdf:type              committee:Membership .
 committee:membership12345   committee:ofPerson    person:p1 .
 committee:membership12345   committee:inCommittee committee:c1 .
 person:p2                   committee:nominated   committee:membership12345 .  
&lt;/source&gt;

Additionally, RDF provides a built-in vocabulary intended for describing RDF statements. A description of a statement using this vocabulary is called a reification of the statement. The RDF reification vocabulary consists of the type &lt;code&gt;rdf:Statement&lt;/code&gt;, and the properties &lt;code&gt;rdf:subject&lt;/code&gt;, &lt;code&gt;rdf:predicate&lt;/code&gt;, and &lt;code&gt;rdf:object&lt;/code&gt;.&lt;ref name="rdf"&gt;{{cite web|url=http://www.w3.org/TR/2004/REC-rdf-primer-20040210/#reification |title=RDF Primer |publisher=W3.org |date= |accessdate=2010-10-09}}&lt;/ref&gt;

Using the reification vocabulary, a reification of the statement about the person's membership would be given by assigning the statement a URIref such as &lt;code&gt;committee:membership12345&lt;/code&gt; so that describing statements can be written as follows:
&lt;source lang="sparql"&gt;
 committee:membership12345Stat   rdf:type        rdf:Statement .
 committee:membership12345Stat   rdf:subject     person:p1 .
 committee:membership12345Stat   rdf:predicate   committee:isMemberOf . 
 committee:membership12345Stat   rdf:object      committee:c1 .
&lt;/source&gt;
These statements say that the resource identified by the &lt;code&gt;URIref committee:membership12345Stat&lt;/code&gt; is an RDF statement, that the subject of the statement refers to the resource identified by &lt;code&gt;person:p1&lt;/code&gt;, the predicate of the statement refers to the resource identified by &lt;code&gt;committee:isMemberOf&lt;/code&gt;, and the object of the statement refers to the resource &lt;code&gt;committee:c1&lt;/code&gt;. Assuming that the original statement is actually identified by &lt;code&gt;committee:membership12345&lt;/code&gt;, it should be clear by comparing the original statement with the reification that the reification actually does describe it. The conventional use of the RDF reification vocabulary always involves describing a statement using four statements in this pattern. Therefore, they are sometimes referred to as the "reification quad".&lt;ref name="rdf"/&gt;

Using reification according to this convention, we could record the fact that &lt;code&gt;person:p3&lt;/code&gt; added the statement to the
database by
&lt;source lang="sparql"&gt;
  person:p3    committee:addedToDatabase    committee:membership12345Stat .
&lt;/source&gt;
It is important to note that in the conventional use of reification, the subject of the reification triples is assumed to identify a particular instance  of a triple in a particular RDF document, rather than some arbitrary triple having the same subject, predicate, and object. This particular convention is used because reification is intended for expressing properties such as dates of composition and source information, as in the examples given already, and these properties need to be applied to specific instances of triples. 
Note that the described triple &lt;code&gt;(subject predicate object)&lt;/code&gt; itself is not implied by such a reification quad (and it is not necessary that it actually exists in the database). This allows also to use this mechanism to express which triples do ''not'' hold.

The power of the reification vocabulary in RDF is restricted by the lack of a built-in means for assigning URIrefs to statements, so in order to express "provenance" information of this kind in RDF, one has to use some mechanism (outside of RDF) to assign URIs to individual RDF statements, then make further statements about those individual statements, using their URIs to identify them.&lt;ref name="rdf"/&gt;

=== Reification in Topic Maps ===
In an [[Topic Maps|XML Topic Map]] (XTM), only a topic can have a name or play a role in an association. One may use an association to make an assertion about a topic, but one cannot directly make assertions about that assertion. However, it is possible to create a topic that reifies a non-topic construct in a map, thus enabling the association to be named and treated as a topic itself.&lt;ref&gt;[http://www.techquila.com/practical_intro.html Practical Introduction into Topic Maps].&lt;/ref&gt;

=== Reification and n-ary relations ===
In Semantic Web languages, such as RDF and OWL, a property is a binary relation used to link two individuals or an individual and a value. However, in some cases, the natural and convenient way to represent certain concepts is to use relations to link an individual to more than just one individual or value. These relations are called [[n-ary relations]]. Examples are representing relations among multiple individuals, such as a committee, a person who is a committee member and another person who has nominated the first person to become the committee member, or a buyer, a seller, and an object that was bought when describing a purchase of a book.

A more general approach to reification is to create an explicit new class and n new properties to represent an n-ary relation, making an instance of the relation linking the n individuals an instance of this class. This approach can also be used to represent provenance information and other properties for an individual relation instance.&lt;ref&gt;{{cite web|url=http://www.w3.org/TR/swbp-n-aryRelations/ |title=W3C Defining N-ary relations on Semantic Web |publisher=W3.org |date= |accessdate=2010-10-09}}&lt;/ref&gt;
&lt;source lang="turtle"&gt;
 :p1
      a       :Person ;
      :has_membership _:membership_12345 .
 _:membership_12345
      a       :Membership ;
      :committee :c1;
      :nominated_by :p2 .
&lt;/source&gt;

=== Reification vs. quotation ===
It is also important to note that the reification described here is not the same as "quotation" found in other languages. Instead, the reification describes the relationship between a particular instance of a triple and the resources the triple refers to. The reification can be read intuitively as saying "this RDF triple talks about these things", rather than (as in quotation) "this RDF triple has this form." For instance, in the reification example used in this section, the triple:
&lt;source lang="sparql"&gt;
  committee:membership12345   rdf:subject   person:p1 .
&lt;/source&gt;
describing the &lt;code&gt;rdf:subject&lt;/code&gt; of the original statement says that the subject of the statement is the resource (the person) identified by the URIref &lt;code&gt;person:p1&lt;/code&gt;. It does not state that the subject of the statement is the URIref itself (i.e., a string beginning with certain characters), as quotation would.

==See also==
{{Wiktionary|reification}}
* [[Denotational semantics]]
* [[Formal semantics of programming languages]]
* [[Meta-circular evaluator]]
* [[Metamodeling]]
* [[Metaobject]]
* [[Metaprogramming]]
* [[Normalization by evaluation]]
* [[Operational semantics]]
* [[Reflection (computer science)]]
* [[Resource Description Framework]]
* [[Self-interpreter]]
* [[Topic Maps]]

==References==
{{reflist}}

&lt;!--Interwikies--&gt;

{{DEFAULTSORT:Reification (Computer Science)}}
&lt;!--Categories--&gt;
[[Category:Object-oriented programming]]
[[Category:Formal methods terminology]]
[[Category:Knowledge representation]]

[[de:Reifikation#Informatik]]
[[fr:R&#233;ification]]</text>
      <sha1>murjlqzlmq73rpkf0yh6c0lszavmzed</sha1>
    </revision>
  </page>
  <page>
    <title>Ontology (information science)</title>
    <ns>0</ns>
    <id>49681</id>
    <revision>
      <id>758611062</id>
      <parentid>756733648</parentid>
      <timestamp>2017-01-06T14:00:59Z</timestamp>
      <contributor>
        <username>JamesBWatson</username>
        <id>1909773</id>
      </contributor>
      <comment>Removing link(s) to "CA ERwin Data Modeler": Removing links to deleted page CA ERwin Data Modeler. ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="53862" xml:space="preserve">{{redirect|Knowledge graph|the Google knowledge base|Knowledge Graph||Knowledge engine (disambiguation)}}
{{About|ontology in information science|the study of the nature of being|Ontology}}
{{Information science}}

In [[computer science]] and [[information science]], an '''ontology''' is a formal naming and definition of the types, properties, and interrelationships of the [[entities]] that really or fundamentally exist for a particular [[domain of discourse]]. It is thus a practical application of philosophical [[ontology]], with a [[taxonomy (general)|taxonomy]].

An ontology compartmentalizes the variables needed for some set of computations and establishes the relationships between them.&lt;ref name="TRG93"&gt;{{cite journal |first=Thomas R. |last=Gruber |authorlink=Tom Gruber |date=June 1993 |url=http://tomgruber.org/writing/ontolingua-kaj-1993.pdf |format=PDF |title=A translation approach to portable ontology specifications |journal=[[Knowledge Acquisition]] |volume=5 |issue=2 |pages=199&#8211;220 |doi=10.1006/knac.1993.1008}}&lt;/ref&gt;&lt;ref&gt;{{cite web |first1=F. |last1=Arvidsson |first2=A. |last2=Flycht-Eriksson |url=http://www.ida.liu.se/~janma/SemWeb/Slides/ontologies1.pdf |title=Ontologies I |format=PDF |accessdate=26 November 2008}}&lt;/ref&gt;

The fields of [[artificial intelligence]], the [[Semantic Web]], [[systems engineering]], [[software engineering]], [[biomedical informatics]], [[library science]], [[enterprise bookmarking]], and [[information architecture]] all create ontologies to limit complexity and to organize information. The ontology can then be applied to [[problem solving]].

==Etymology and definition==
The term ''[[ontology]]'' has its origin in [[philosophy]] and has been applied in many different ways. The word element ''[[wiktionary:onto-|onto-]]'' comes from the [[Greek language|Greek]] ''[[wiktionary:&#8036;&#957;|&#8036;&#957;]], &#8004;&#957;&#964;&#959;&#962;'', ("being", "that which is"), present participle of the verb ''[[wiktionary:&#949;&#7984;&#956;&#943;|&#949;&#7984;&#956;&#943;]]'' ("be"). The core meaning within [[computer science]] is a model for describing the world that consists of a set of types, properties, and relationship types. There is also generally an expectation that the features of the model in an ontology should closely resemble the real world (related to the object).&lt;ref&gt;{{cite web |first=L. M. |last=Garshol |year=2004 |url=http://www.ontopia.net/topicmaps/materials/tm-vs-thesauri.html#N773 |title=Metadata? Thesauri? Taxonomies? Topic Maps! Making sense of it all'' |accessdate=13 October 2008 }}&lt;/ref&gt;

==Overview==
What many ontologies have in common in both computer science and in philosophy is the representation of entities, ideas, and events, along with their properties and relations, according to a system of categories. In both fields, there is considerable work on problems of [[Confirmation holism|ontological relativity]] (e.g., [[Willard Van Orman Quine|Quine]] and [[Saul Kripke|Kripke]] in philosophy, [[John F. Sowa|Sowa]] and [[Nicola Guarino|Guarino]] in computer science),&lt;ref&gt;{{cite journal |first=J. F. |last=Sowa |title=Top-level ontological categories
|journal=[[International Journal of Human-Computer Studies]] |volume=43 |issue=5-6 (November/December)
|year=1995 |pages=669&#8211;85 |doi=10.1006/ijhc.1995.1068 }}&lt;/ref&gt; and debates concerning whether a [[normative]] ontology is viable (e.g., debates over [[foundationalism]] in philosophy, and over the [[Cyc]] project in AI). Differences between the two are largely matters of focus. Computer scientists are more concerned with establishing fixed, controlled vocabularies, while philosophers are more concerned with first principles, such as whether there are such things as [[Essence|fixed essences]] or whether enduring objects must be ontologically more primary than processes.

Other fields make ontological assumptions that are sometimes explicitly elaborated and explored.  For instance, the [[philosophy and economics#Definition and ontology of economics|definition and ontology of economics]] (also sometimes called the [[political economy]]) is hotly debated especially in [[Marxist economics]]&lt;ref&gt;{{cite journal|first=Giulio |last=Palermo|url=http://cje.oxfordjournals.org/content/31/4/539.short |title=The ontology of economic power in capitalism: mainstream economics and Marx |journal=Cambridge Journal of Economics |volume=31 |issue=4 |pages=539&#8211;561 |date=10 January 2007 |doi=10.1093/cje/bel036 |accessdate=16 June 2013 |via=Oxford Journals }}&lt;/ref&gt; where it is a primary concern, but also in other subfields.&lt;ref&gt;{{cite web|author=Zuniga, Gloria L. |url=https://ideas.repec.org/p/pra/mprapa/5566.html |title=An Ontology Of Economic Objects |website=Ideas |publisher=Research Division of the Federal Reserve Bank of St. Louis |date=1999-02-02 |accessdate=2013-06-16}}&lt;/ref&gt;  Such concerns intersect with those of information science when a simulation or model is intended to enable decisions in the economic realm; for example, to determine what [[capital asset]]s are at risk and if so by how much (see [[risk management]]).  Some claim all social sciences have explicit ontology issues because they do not have hard [[falsifiability]] criteria like most models in physical sciences and that indeed the lack of such widely accepted hard falsification criteria is what defines a social or soft science.{{Citation needed|date=March 2012}}

==History==
Historically, ontologies arise out of the branch of [[philosophy]] known as [[metaphysics]], which deals with the nature of reality&amp;nbsp;&#8211; of what exists. This fundamental branch is concerned with analyzing various types or modes of existence, often with special attention to the relations between [[particular]]s and [[Universal (metaphysics)|universals]], between [[Intrinsic and extrinsic properties (philosophy)|intrinsic and extrinsic properties]], and between [[essence]] and [[existence]]. The traditional goal of ontological inquiry in particular is to divide the world "at its joints" to discover those fundamental categories or kinds into which the world&#8217;s objects naturally fall.&lt;ref name="PCB94"&gt;{{cite web |first1=Perakath C. |last1=Benjamin |first2=Christopher P. |last2=Menzel |first3=Richard J. |last3=Mayer |first4=Florence |last4=Fillion |first5=Michael T. |last5=Futrell |first6=Paula S. |last6=deWitte |first7=Madhavi |last7=Lingineni |date=September 21, 1994 |url=http://www.idef.com/pdf/Idef5.pdf |format=PDF |title=IDEF5 Method Report |publisher=Knowledge Based Systems, Inc.}}&lt;/ref&gt;

During the second half of the 20th century, philosophers extensively debated the possible methods or approaches to building ontologies without actually ''building'' any very elaborate ontologies themselves. By contrast, computer scientists were building some large and robust ontologies, such as [[WordNet]] and [[Cyc]], with comparatively little debate over ''how'' they were built.

Since the mid-1970s, researchers in the field of [[artificial intelligence]] (AI) have recognized that capturing knowledge is the key to building large and powerful AI systems.  AI researchers argued that they could create new ontologies as [[computational model]]s that enable certain kinds of [[automated reasoning]]. In the 1980s, the AI community began to use the term ''ontology'' to refer to both a theory of a modeled world and a component of knowledge systems. Some researchers, drawing inspiration from philosophical ontologies, viewed computational ontology as a kind of applied philosophy.&lt;ref name="TG08"&gt;{{cite book |first=T. |last=Gruber |authorlink=Tom Gruber |year=2008 |url=http://tomgruber.org/writing/ontology-definition-2007.htm  |title=Ontology |work=Encyclopedia of Database Systems |editor1-first=Ling |editor1-last=Liu |editor2-first=M. Tamer |editor2-last=&#214;zsu |publisher=Springer-Verlag|isbn=978-0-387-49616-0}}&lt;/ref&gt;

In the early 1990s, the widely cited Web page and paper "Toward Principles for the Design of Ontologies Used for Knowledge Sharing" by [[Tom Gruber]]&lt;ref name="TRG95"&gt;{{cite journal |first=T. |last=Gruber |authorlink=Tom Gruber |title=Toward Principles for the Design of Ontologies Used for Knowledge Sharing |journal=[[International Journal of Human-Computer Studies]] |volume=43 |issue=5-6 |pages=907&#8211;928 |year=1995 |doi=10.1006/ijhc.1995.1081}}&lt;/ref&gt; is credited with a deliberate definition of ''ontology'' as a technical term in [[computer science]]. Gruber introduced the term to mean a specification of a conceptualization: &lt;blockquote&gt;An ontology is a description (like a formal specification of a program) of the concepts and relationships that can formally exist for an agent or a community of agents. This definition is consistent with the usage of ontology as set of concept definitions, but more general. And it is a different sense of the word than its use in philosophy.&lt;ref name="TRG01"&gt;{{cite web |first=T. |last=Gruber |authorlink=Tom Gruber |year=2001 |url=http://www-ksl.stanford.edu/kst/what-is-an-ontology.html |title=What is an Ontology? |publisher=[[Stanford University]] |accessdate=2009-11-09}}&lt;/ref&gt;&lt;/blockquote&gt;

According to Gruber (1993): &lt;blockquote&gt;Ontologies are often equated with taxonomic hierarchies of classes, class definitions, and the subsumption relation, but ontologies need not be limited to these forms. Ontologies are also not limited to [[Conservative extension|conservative definitions]]&amp;nbsp;&amp;mdash; that is, definitions in the traditional logic sense that only introduce terminology and do not add any knowledge about the world.&lt;ref&gt;{{cite book
|first=H. B. |last=Enderton |authorlink=Herbert Enderton |date=1972-05-12 |title=A Mathematical Introduction to Logic |location=San Diego, CA |publisher=Academic Press |edition=1 |page=295 |isbn=978-0-12-238450-9 |postscript=&amp;nbsp;2nd edition; January 5, 2001, ISBN 978-0-12-238452-3}}&lt;/ref&gt; To specify a conceptualization, one needs to state axioms that do constrain the possible interpretations for the defined terms.&lt;ref name="TRG93"/&gt;&lt;/blockquote&gt;

== Components ==
{{Main article|Ontology components}}
Contemporary ontologies share many structural similarities, regardless of the language in which they are expressed.  As mentioned above, most ontologies describe individuals (instances), classes (concepts), attributes, and relations.  In this section each of these components is discussed in turn.

Common components of ontologies include:
; Individuals
: Instances or objects (the basic or "ground level" objects)
; [[Class (set theory)|Class]]es&lt;!-- This deliberately links to the disambiguation page --&gt;
: Sets, collections, concepts, [[Class (computer science)|classes in programming]], [[Class (philosophy)|types of objects]], or kinds of things
; [[Attribute (computing)|Attribute]]s
: Aspects, properties, features, characteristics, or parameters that objects (and classes) can have
; [[Relation (mathematics)|Relations]]
: Ways in which classes and individuals can be related to one another
; Function terms
: Complex structures formed from certain relations that can be used in place of an individual term in a statement
; Restrictions
: Formally stated descriptions of what must be true in order for some assertion to be accepted as input
; Rules
: Statements in the form of an if-then (antecedent-consequent) sentence that describe the logical inferences that can be drawn from an assertion in a particular form
; Axioms
: Assertions (including rules) in a [[logical form]] that together comprise the overall theory that the ontology describes in its domain of application.  This definition differs from that of "axioms" in [[generative grammar]] and [[formal logic]].  In those disciplines, axioms include only statements asserted as ''a priori'' knowledge.  As used here, "axioms" also include the theory derived from axiomatic statements
; [[Event (philosophy)|Events]]&lt;!-- this links  to the philosophy sense of 'Events' as that is currently the only article describing the issues around defining events in the ontology community--&gt;
: The changing of attributes or relations

Ontologies are commonly encoded using [[ontology language]]s.

== Types ==

=== Domain ontology&lt;!--linked from 'Domain ontology'--&gt; ===
A domain ontology (or domain-specific ontology) represents concepts which belong to part of the world. Particular meanings of terms applied to that domain are provided by domain ontology. For example, the word ''[[:wikt:card|card]]'' has many different meanings. An ontology about the domain of [[poker]] would model the "[[playing card]]" meaning of the word, while an ontology about the domain of [[computer hardware]] would model the "[[punched card]]" and "[[video card]]" meanings.

Since domain ontologies represent concepts in very specific and often eclectic ways, they are often incompatible. As systems that rely on domain ontologies expand, they often need to merge domain ontologies into a more general representation.  This presents a challenge to the ontology designer. Different ontologies in the same domain arise due to different languages, different intended usage of the ontologies, and different perceptions of the domain (based on cultural background, education, ideology, etc.).

At present, merging ontologies that are not developed from a common foundation ontology is a largely manual process and therefore time-consuming and expensive. Domain ontologies that use the same foundation ontology to provide a set of basic elements with which to specify the meanings of the domain ontology elements can be merged automatically. There are studies on generalized techniques for merging ontologies,&lt;ref name="Dynamic Ontology Repair"&gt;{{cite web |url=http://dream.inf.ed.ac.uk/projects/dor/ |title=Project: Dynamic Ontology Repair |publisher= University of Edinburgh Department of Informatics|accessdate=2 January 2012}}&lt;/ref&gt; but this area of research is still largely theoretical.

=== Upper ontology ===
{{Main article|Upper ontology}}

An [[Upper ontology (computer science)|upper ontology]] (or foundation ontology) is a model of the common objects that are generally applicable across a wide range of domain ontologies. It usually employs a [[core glossary]] that contains the terms and associated object descriptions as they are used in various relevant domain sets.

There are several standardized upper ontologies available for use, including [[Basic Formal Ontology|BFO]], [[BORO method]], [[Dublin Core]], [[General Formal Ontology|GFO]], [[OpenCyc]]/[[ResearchCyc]], [[Suggested Upper Merged Ontology|SUMO]], the Unified Foundational Ontology (UFO),&lt;ref&gt;{{cite web|last=Giancarlo Guizzardi &amp; Gerd Wagner|url=http://ceur-ws.org/Vol-125/paper2.pdf|accessdate=31 March 2014|title=A Unified Foundational Ontology and some Applications of it in Business Modeling}}&lt;/ref&gt; and [[Upper ontology (computer science)#DOLCE and DnS|DOLCE]].&lt;ref name="DOLCE"&gt;{{cite web |url=http://www.loa-cnr.it/DOLCE.html |title=Laboratory for Applied Ontology - DOLCE |publisher=Laboratory for Applied Ontology (LOA)|accessdate=10 February 2011}}&lt;/ref&gt;&lt;ref name="DOLCE-OWL"&gt;{{cite web |url=http://www.ontologydesignpatterns.org/ont/dul/DUL.owl |title=OWL version of DOLCE+DnS  |publisher=Semantic Technology Lab|accessdate=21 February 2013}}&lt;/ref&gt; [[WordNet]], while considered an upper ontology by some, is not strictly an ontology. However, it has been employed as a linguistic tool for learning domain ontologies.&lt;ref&gt;{{cite journal |first1=Roberto |last1=Navigli |authorlink1=Roberto Navigli |first2=Paola |last2=Velardi |authorlink2=Paola Velardi |year=2004 |url=http://www.mitpressjournals.org/doi/pdf/10.1162/089120104323093276 |format=PDF |title=Learning Domain Ontologies from Document Warehouses and Dedicated Web Sites |journal=[[Computational Linguistics (journal)|Computational Linguistics]] |volume=30 |issue=2 |publisher=MIT Press |pages=151&#8211;179 |doi=10.1162/089120104323093276}}&lt;/ref&gt;

=== Hybrid ontology ===

The [[Gellish]] ontology is an example of a combination of an upper and a domain ontology.

== Visualization ==
A survey of ontology visualization techniques is presented by Katifori et al.&lt;ref&gt;{{cite journal |last1=Katifori |first1=A. |last2=Halatsis |first2=C. |last3=Lepouras |first3=G. |last4=Vassilakis |first4=C. |last5=Giannopoulou |first5=E. |title=Ontology Visualization Methods - A Survey |journal=ACM Computing Surveys |volume=39 |issue=4 |page=10 |date=2007 |url=http://entrezneuron.googlecode.com/svn-history/r2/trunk/references/12-onto-vis-survey-final.pdf |archive-url=http://web.archive.org/web/20160304203317/http://entrezneuron.googlecode.com/svn-history/r2/trunk/references/12-onto-vis-survey-final.pdf |archive-date=4 March 2016 |format=PDF }}&lt;/ref&gt; An evaluation of two most established ontology visualization techniques: indented tree and graph is discussed in.&lt;ref&gt;{{cite conference |first1=Bo |last1=Fu |first2=Natalya F. |last2=Noy |first3=Margaret-Anne |last3=Storey |title=Indented Tree or Graph? A Usability Study of Ontology Visualization Techniques in the Context of Class Mapping Evaluation |book-title=The Semantic Web &#8211; ISWC 2013: 12th International Semantic Web Conference, Sydney, NSW, Australia, October 21&#8211;25, 2013, Proceedings, Part I |series=Lecture Notes in Computer Science |volume=8218 |pages=117&#8211;134 |url=http://link.springer.com/chapter/10.1007/978-3-642-41335-3_8 |doi=10.1007/978-3-642-41335-3_8 |isbn=978-3-642-41335-3 |publisher=Springer |location=Berlin |date=2013 |via=SpringerLink }}&lt;/ref&gt; A visual language for ontologies represented in [[Web Ontology Language|OWL]] is specified by the ''Visual Notation for OWL Ontologies (VOWL)''.&lt;ref&gt;{{cite web |last1=Negru |first1=Stefan |last2=Lohmann |first2=Steffen |last3=Haag |first3=Florian |date=7 April 2014 |title=VOWL: Visual Notation for OWL Ontologies: Specification of Version 2.0 |website=Visual Data Web |url=http://vowl.visualdataweb.org/v2/ }}&lt;/ref&gt;

== Engineering ==
{{Main article|Ontology engineering}}
[[Ontology engineering]] (or ontology building) is a subfield of [[knowledge engineering]]. It studies the ontology development process, the ontology life cycle, the methods and methodologies for building ontologies, and the tool suites and languages that support them.&lt;ref name="PFC04"&gt;{{cite book |first1=Ascunion |last1=G&#243;mez-P&#233;rez |authorlink1=Ascunion G&#243;mez-P&#233;rez |first2=Mariano |last2=Fern&#225;ndez-L&#243;pez |authorlink2=Mariano Fern&#225;ndez-L&#243;pez |first3=Oscar |last3=Corcho |authorlink3=Oscar Corcho |year=2004 |title=Ontological Engineering: With Examples from the Areas of Knowledge Management, E-commerce and the Semantic Web |publisher=Springer |isbn=978-1-85233-551-9 |page=403 |edition=1 }}&lt;/ref&gt;&lt;ref name="DMN"&gt;{{cite journal |first1=Antonio |last1=De Nicola |authorlink1=Antonio De Nicola |first2=Michele |last2=Missikoff |authorlink2=Michele Missikoff |first3=Roberto |last3=Navigli |authorlink3=Roberto Navigli |year=2009 |url=http://www.dsi.uniroma1.it/~navigli/pubs/De_Nicola_Missikoff_Navigli_2009.pdf |format=PDF |title=A Software Engineering Approach to Ontology Building |journal=[[Information Systems (journal)|Information Systems]] |volume=34 |issue=2 |publisher=Elsevier |pages=258&#8211;275 | doi = 10.1016/j.is.2008.07.002 }}&lt;/ref&gt;

Ontology engineering aims to make explicit the knowledge contained within software applications, and within enterprises and business procedures for a particular domain. Ontology engineering offers a direction towards solving the interoperability problems brought about by semantic obstacles, such as the obstacles related to the definitions of business terms and software classes. Ontology engineering is a set of tasks related to the development of ontologies for a particular domain.&lt;ref name="PIS00"&gt;{{cite journal |first1=Line  |last1=Pouchard |authorlink1=Line Pouchard |first2=Nenad |last2=Ivezic |authorlink2=Nenad Ivezic |first3=Craig |last3=Schlenoff |authorlink3=Craig Schlenoff |date=March 2000 |url=http://www.mel.nist.gov/msidlibrary/doc/AISfinal2.pdf |format=PDF |title=Ontology Engineering for Distributed Collaboration in Manufacturing |work=Proceedings of the AIS2000 conference }}&lt;/ref&gt;

Known challenges with ontology engineering include:
# Ensuring the ontology is ''current'' with domain knowledge and term use
# Providing ''sufficient specificity and concept coverage'' for the domain of interest, thus minimizing the [[content completeness problem]]
# Ensuring the ontology can support its use cases

=== Editor ===
'''Ontology editors''' are applications designed to assist in the creation or  manipulation of [[ontology (computer science)|ontologies]]. They often express ontologies in one of many [[ontology language (computer science)|ontology languages]]. Some provide export to other ontology languages however.

Among the most relevant criteria for choosing an ontology editor are the degree to which the editor abstracts from the actual [[Ontology language (computer science)|ontology representation language]] used for [[persistence (computer science)|persistence]] and the visual navigation possibilities within the [[knowledge model]]. Next come built-in [[inference engine]]s and [[information extraction]] facilities, and the support of meta-ontologies such as [[OWL-S]], [[Dublin Core]], etc. Another important feature is the ability to import &amp; export foreign [[knowledge representation]] languages for [[ontology matching]]. Ontologies are developed for a specific purpose and application.

*a.k.a. software (Ontology, taxonomy and thesaurus management software available from The Synercon Group)
*Anzo for Excel (Includes an RDFS and OWL ontology editor within Excel; generates ontologies from Excel spreadsheets)
*Chimaera (Other web service by Stanford)
*CmapTools Ontology Editor (COE) (Java based ontology editor from the Florida Institute for Human and Machine Cognition. Supports numerous formats)
*[[dot15926]] Editor (Open source ontology editor for data compliant to engineering ontology standard [[ISO 15926]]. Allows [[Python (programming language)|Python]] scripting and pattern-based data analysis. Supports extensions.)
*EMFText OWL2 Manchester Editor, Eclipse-based, open-source, Pellet integration
* Enterprise Architect, along with [[Unified Modeling Language|UML]] modeling, supports [[Object Management Group|OMG's]] [[Ontology Definition MetaModel]] which includes [[Web Ontology Language|OWL]] and [[Resource Description Framework|RDF]].
*[[Fluent Editor]], a comprehensive ontology editor for OWL and SWRL with Controlled Natural Language (Controlled English). Supports [[Web Ontology Language|OWL]], [[Resource Description Framework|RDF]], [[Description Logic|DL]] and Functional rendering, unlimited imports and built-in reasoning services.
*[[HOZO]] (Java-based graphical editor especially created to produce heavy-weight and well thought out ontologies, from [[Osaka University]] and Enegate Co, ltd.)
*Java Ontology Editor (JOE)  (1998)
*[[KAON]] (single user and server based solutions possible, open source, from FZI/AIFB Karlsruhe)
*KMgen (Ontology editor for the KM language.&amp;nbsp;km: The Knowledge Machine)
*Knoodl (Free web application/service that is an ontology editor, [[wiki]], and [[Digital repository|ontology registry]].  Supports creation of communities where members can collaboratively import, create, discuss, document and publish ontologies.  Supports [[Web Ontology Language|OWL]], [[Resource Description Framework|RDF]], [[RDF Schema|RDFS]], and [[SPARQL]] queries.  Available since early Nov 2006 from Revelytix, Inc..)
*Model Futures IDEAS AddIn (free) A plug-in for Sparx Systems [[Enterprise Architect (software)|Enterprise Architect]] that allows [[IDEAS Group]] [[4D ontology|4D ontologies]] to be developed using a [[Unified Modeling Language|UML]] profile
*Model Futures OWL Editor (Free) Able to work with very large OWL files (e.g. [[Cyc]]) and has extensive import and export capabilities (inc. [[Unified Modeling Language|UML]], Thesaurus Descriptor, [[MS Word]], CA ERwin Data Modeler, CSV, etc.)
*myWeb (Java-based, mySQL connection, bundled with applet that allows online browsing of ontologies (including OBO))
*Neologism (Web-based, open source, supports RDFS and a subset of OWL, built on [[Drupal]])
*[[NeOn Toolkit (software)|NeOn Toolkit]]  (Eclipse-based, open source, OWL support, several import mechanisms, support for reuse and management of networked ontologies, visualization, etc.&#8230;from NeOn Project)
*OBO-Edit (Java-based, downloadable, open source, developed by the [[Gene Ontology Consortium]] for editing biological ontologies)
*OntoStudio (Eclipse-based, downloadable, support for RDF(S), OWL and F-Logic, graphical rule editor, visualizations, from ontoprise)
*Ontolingua (Web service offered by Stanford University)
* [[Open Semantic Framework]] (OSF), an integrated software stack using semantic technologies for knowledge management, which includes an ontology editor
*OWLGrEd (A graphical ontology editor, easy-to-use)
*PoolParty Thesaurus Server (Commercial ontology, taxonomy and thesaurus management software available from Semantic Web Company, fully based on standards like RDFS, SKOS and SPARQL, integrated with [[Virtuoso Universal Server]])
*[[Prot&#233;g&#233; (software)|Prot&#233;g&#233;]] (Java-based, downloadable, Supports OWL, open source, many sample ontologies, from Stanford University)
*ScholOnto (net-centric representations of research)
*Semantic Turkey (Firefox extension - also based on Java - for managing ontologies and acquiring new knowledge from the Web; developed at University of Rome, Tor Vergata )
*[[Sigma knowledge engineering environment]] is a system primarily for development of the [[Suggested Upper Merged Ontology]]
*Swoop (Java-based, downloadable, open source, OWL Ontology browser and editor from the University of Maryland)
*Semaphore Ontology Manager  (Commercial ontology, taxonomy and thesaurus management software available from [[Smartlogic Semaphore Limited]]. Intuitive tool to manage the entire "build - enhance - review - maintain" ontology lifecycle.)
*Synaptica  (Ontology, taxonomy and thesaurus management software available from Synaptica, LLC. Web based, supports [[Web Ontology Language|OWL]] and [[SKOS]].)
*TopBraid Composer  (Eclipse-based, downloadable, full support for RDFS and OWL, built-in inference engine, SWRL editor and SPARQL queries, visualization, import of XML and UML, from TopQuadrant)
*Transinsight (The editor is especially designed for creating text mining ontologies and part of GoPubMed.org)
*WebODE (Web service offered by the Technical University of Madrid)
*TwoUse Toolkit (Eclipse-based, open source, model-driven ontology editing environment especially designed for software engineers)
*Be Informed Suite (Commercial tool for building large ontology based applications. Includes visual editors, inference engines, export to standard formats)
*Thesaurus Master (Manages creation and use of ontologies for use in data management and semantic enrichment by enterprise, government, and scholarly publishers.)
*[[Tool for Ontology Development and Editing (TODE)|TODE]] (A Dot Net-based Tool for Ontology Development and Editing)
*VocBench (Collaborative Web Application for SKOS/SKOS-XL Thesauri Management - developed on a joint effort between University of Rome, Tor Vergata and the Food and Agriculture Organization of the United Nations: FAO )
*OBIS (Web based user interface that allows to input ontology instances in a user friendly way that can be accessed via SPARQL endpoint)
*[[Menthor Editor]] (An ontology engineering tool for dealing with [[OntoUML]]. It also includes OntoUML syntax validation, [[Alloy Analyzer|Alloy]] simulation, [[Anti-pattern|Anti-Pattern]] verification, and transformations from [[OntoUML]] to [[Web Ontology Language|OWL]], [[Semantics of Business Vocabulary and Business Rules|SBVR]] and [[Brazilian Portuguese|Natural Language (Brazilian Portuguese)]])

=== Learning ===
{{Main article|Ontology learning}}

Ontology learning is the automatic or semi-automatic creation of ontologies, including extracting a domain's terms from natural language text. As building ontologies manually is extremely labor-intensive and time consuming, there is great motivation to automate the process. 
Information extraction and text mining methods have been explored to automatically link ontologies to documents, e.g. in the context of the BioCreative challenges.&lt;ref&gt;{{Cite journal
 | pmid = 22438567
| year = 2012
| author1 = Krallinger
| first1 = M
| title = How to link ontologies and protein-protein interactions to literature: Text-mining approaches and the Bio ''Creative'' experience
| journal = Database
| volume = 2012
| pages = bas017
| last2 = Leitner
| first2 = F
| last3 = Vazquez
| first3 = M
| last4 = Salgado
| first4 = D
| last5 = Marcelle
| first5 = C
| last6 = Tyers
| first6 = M
| last7 = Valencia
| first7 = A
| last8 = Chatr-Aryamontri
| first8 = A
| doi = 10.1093/database/bas017
| pmc = 3309177
}}&lt;/ref&gt;

== Languages ==
{{Main article|Ontology language}}
An [[ontology language]] is a [[formal language]] used to encode the ontology. There are a number of such languages for ontologies, both proprietary and standards-based:
* [[Common Algebraic Specification Language]] is a general logic-based specification language developed within the IFIP working group 1.3 "Foundations of System Specifications" and functions as a de facto standard in the area of software specifications. It is now being applied to ontology specifications in order to provide modularity and structuring mechanisms.
* [[Common logic]] is ISO standard 24707, a specification for a family of ontology languages that can be accurately translated into each other.
* The [[Cyc]] project has its own ontology language called [[CycL]], based on [[first-order predicate calculus]] with some higher-order extensions.
* [[DOGMA]] (Developing Ontology-Grounded Methods and Applications) adopts the fact-oriented modeling approach to provide a higher level of semantic stability.
* The [[Gellish]] language includes rules for its own extension and thus integrates an ontology with an ontology language.
* [[IDEF5]] is a [[software engineering]] method to develop and maintain usable, accurate, domain ontologies.
* [[Knowledge Interchange Format|KIF]] is a syntax for [[first-order logic]] that is based on [[S-expression]]s.  SUO-KIF is a derivative version supporting the [[Suggested Upper Merged Ontology]].
* [[Meta-Object Facility|MOF]] and [[Unified Modeling Language|UML]] are standards of the [[Object Management Group|OMG]]
* [[Olog]] is a [[Category theory|category theoretic]] approach to ontologies, emphasizing translations between ontologies using [[functor]]s. 
* [[Open Biomedical Ontologies|OBO]], a language used for biological and biomedical ontologies.
* [[OntoUML]] is an ontologically well-founded profile of UML for conceptual modeling of domain ontologies.
* [[Web Ontology Language|OWL]] is a language for making ontological statements, developed as a follow-on from [[Resource Description Framework|RDF]] and [[RDFS]], as well as earlier ontology language projects including [[Ontology Inference Layer|OIL]], [[DARPA Agent Markup Language|DAML]], and [[DAMLplusOIL|DAML+OIL]]. OWL is intended to be used over the [[World Wide Web]], and all its elements (classes, properties and individuals) are defined as RDF [[web resource|resource]]s, and identified by [[Uniform Resource Identifier|URI]]s.
* [[Rule Interchange Format]] (RIF) and [[F-Logic]] combine ontologies and rules.
* [[Semantic Application Design Language]] (SADL)&lt;ref&gt;{{cite web |url=http://sadl.sourceforge.net/sadl.html |title=SADL |work=[[Sourceforge]] |accessdate=10 February 2011}}&lt;/ref&gt; captures a subset of the expressiveness of [[Web Ontology Language|OWL]], using an English-like language entered via an [[Eclipse (software)|Eclipse]] Plug-in.
* [[SBVR]] (Semantics of Business Vocabularies and Rules) is an OMG standard adopted in industry to build ontologies.
* [[TOVE Project]], TOronto Virtual Enterprise project

== Published examples ==
* AURUM - Information Security Ontology,&lt;ref&gt;{{cite web |url=http://www.securityontology.com |title=AURUM - Information Security Ontology |accessdate=29 January 2016}}&lt;/ref&gt; An ontology for information security knowledge sharing, enabling users to collaboratively understand and extend the domain knowledge body. It may serve as a basis for automated information security risk and compliance management.
* [[BabelNet]], a very large multilingual semantic network and ontology, lexicalized in many languages
* Basic Formal Ontology,&lt;ref&gt;{{cite web |url=http://www.ifomis.org/bfo/ |title=Basic Formal Ontology (BFO)
|publisher=[[Institute for Formal Ontology and Medical Information Science]] (IFOMIS) |accessdate=}}&lt;/ref&gt; a formal upper ontology designed to support scientific research
* BioPAX,&lt;ref&gt;{{cite web |url=http://biopax.org |title=BioPAX |accessdate=10 February 2011}}&lt;/ref&gt; an ontology for the exchange and interoperability of biological pathway (cellular processes) data
* BMO,&lt;ref&gt;{{cite journal |first1=Alexander |last1=Osterwalder |first2=Yves |last2=Pigneur | author-link2= Yves Pigneur | url=http://129.3.20.41/eps/io/papers/0202/0202004.pdf |title=An e-Business Model Ontology for Modeling e-Business |location=[[Bled eConference|15th Bled eConference]], [[Slovenia]] |date=June 17&#8211;19, 2002}}&lt;/ref&gt; an e-Business Model Ontology based on a review of enterprise ontologies and business model literature
* SSBMO,&lt;ref&gt;{{cite journal |first1=Antony |last1=Upward |first2=Peter |last2=Jones |url=https://www.academia.edu/14461116 |title=An Ontology for Strongly Sustainable Business Models: Defining an Enterprise Framework Compatible with Natural and Social Science |journal=Organization &amp; Environment |volume=29 |issue=1 |pages=97-123 |doi=10.1177/1086026615592933}}&lt;/ref&gt; a Strongly Sustainable Business Model Ontology based on a review of the systems based natural and social science literature (including business).  Includes critique of and significant extensions to the Business Model Ontology (BMO).
* CCO and GexKB,&lt;ref&gt;{{cite web|title=About CCO and GexKB|url=http://www.semantic-systems-biology.org/apo/|publisher=Semantic Systems Biology}}&lt;/ref&gt; Application Ontologies (APO) that integrate diverse types of knowledge with the Cell Cycle Ontology (CCO) and the Gene Expression Knowledge Base (GexKB)
* CContology (Customer Complaint Ontology),&lt;ref&gt;{{cite web |url=http://www.jarrar.info/CContology/ |title=CContology |accessdate=10 February 2011}}&lt;/ref&gt; an e-business ontology to support online customer complaint management
* [[CIDOC Conceptual Reference Model]], an ontology for [[cultural heritage]]&lt;ref&gt;{{cite web |url=http://www.cidoc-crm.org/ |title=The CIDOC Conceptual Reference Model (CRM) |accessdate=10 February 2011}}&lt;/ref&gt;
* COSMO,&lt;ref&gt;{{cite web |url=http://micra.com/COSMO/ |title=COSMO |publisher=MICRA Inc.|accessdate=10 February 2011}}&lt;/ref&gt; a Foundation Ontology (current version in OWL) that is designed to contain representations of all of the primitive concepts needed to logically specify the meanings of any domain entity.  It is intended to serve as a basic ontology that can be used to translate among the representations in other ontologies or databases.  It started as a merger of the basic elements of the OpenCyc and SUMO ontologies, and has been supplemented with other ontology elements (types, relations) so as to include representations of all of the words in the [[Longman Dictionary of Contemporary English|Longman dictionary]] [[defining vocabulary]].
* [[Cyc]], a large Foundation Ontology for formal representation of the universe of discourse
* [[Disease Ontology]],&lt;ref&gt;{{cite journal |pmid=19594883}}&lt;/ref&gt; designed to facilitate the mapping of diseases and associated conditions to particular medical codes
* [[Upper ontology (computer science)#DOLCE and DnS|DOLCE]], a Descriptive Ontology for Linguistic and Cognitive Engineering&lt;ref name="DOLCE"/&gt;&lt;ref name="DOLCE-OWL"/&gt;
* [[Drammar]], ontology of drama
* [[Dublin Core]], a simple ontology for documents and publishing
* Foundational, Core and Linguistic Ontologies&lt;ref&gt;{{cite web |url=http://www.loa-cnr.it/Ontologies.html
|title=Foundational, Core and Linguistic Ontologies |accessdate=10 February 2011}}&lt;/ref&gt;
* [[Foundational Model of Anatomy]],&lt;ref&gt;{{cite web |url=http://sig.biostr.washington.edu/projects/fm/AboutFM.html |title=Foundational Model of Anatomy |accessdate=10 February 2011}}&lt;/ref&gt; an ontology for human anatomy
* [[FOAF (software)|Friend of a Friend]], an ontology for describing persons, their activities and their relations to other people and objects
* [[Gene Ontology]] for [[genomics]]
* [[Gellish English dictionary]], an ontology that includes a dictionary and taxonomy that includes an upper ontology and a lower ontology that focusses on industrial and business applications in engineering, technology and procurement.
* [[Geopolitical ontology]], an ontology describing geopolitical information created by [[Food and Agriculture Organization]](FAO). The geopolitical ontology includes names in multiple languages (English, French, Spanish, Arabic, Chinese, Russian and Italian); maps standard coding systems (UN, ISO, FAOSTAT, AGROVOC, etc.); provides relations among territories (land borders, group membership, etc.); and tracks historical changes. In addition, FAO provides web services of geopolitical ontology and a module maker to download modules of the geopolitical ontology into different formats (RDF, XML, and EXCEL). See more information at [[FAO Country Profiles]].
* GOLD,&lt;ref&gt;{{cite web |url=http://www.linguistics-ontology.org/gold.html |title=GOLD |accessdate=10 February 2011}}&lt;/ref&gt; General Ontology for [[descriptive linguistics|Linguistic Description]]
* GUM (Generalized Upper Model),&lt;ref&gt;{{cite web |url=http://www.fb10.uni-bremen.de/anglistik/langpro/webspace/jb/gum/index.htm |title=Generalized Upper Model |accessdate=10 February 2011}}&lt;/ref&gt; a linguistically motivated ontology for mediating between clients systems and natural language technology
* [[IDEAS Group]],&lt;ref&gt;{{cite web |url=http://www.ideasgroup.org |title=The IDEAS Group Website |accessdate=10 February 2011}}&lt;/ref&gt; a formal ontology for enterprise architecture being developed by the Australian, Canadian, UK and U.S. Defence Depts.
* Linkbase,&lt;ref&gt;{{cite web |url=http://www.landcglobal.com/pages/linkbase.php |title=Linkbase |accessdate=10 February 2011}}&lt;/ref&gt; a formal representation of the biomedical domain, founded upon [http://www.ifomis.org/bfo/ Basic Formal Ontology].
* LPL, Lawson Pattern Language
* NCBO Bioportal,&lt;ref&gt;{{cite web|title=Bioportal|url=http://www.bioontology.org/tools/portal/bioportal.html|publisher=National Center for Biological Ontology (NCBO)}}&lt;/ref&gt; biological and biomedical ontologies and associated tools to search, browse and visualise
* [[NIFSTD]] Ontologies from the [[Neuroscience Information Framework]]: a modular set of ontologies for the neuroscience domain.
* OBO-Edit,&lt;ref&gt;{{cite web|title=Ontology browser for most of the Open Biological and Biomedical Ontologies|url=http://oboedit.org/?page=index|publisher=Berkeley Bioinformatics Open Source Project (BBOP)}}&lt;/ref&gt; an ontology browser for most of the Open Biological and Biomedical Ontologies
* [[OBO Foundry]],&lt;ref&gt;{{cite web|title=The Open Biological and Biomedical Ontologies|url=http://www.obofoundry.org/|publisher=Berkeley Bioinformatics Open Source Project (BBOP)}}&lt;/ref&gt; a suite of interoperable reference ontologies in biology and biomedicine
* OMNIBUS Ontology,&lt;ref&gt;{{cite web |url=http://edont.qee.jp/omnibus/ |title=OMNIBUS Ontology |accessdate=10 February 2011}}&lt;/ref&gt; an ontology of learning, instruction, and instructional design
* [[Ontology for Biomedical Investigations]], an open access, integrated ontology for the description of biological and clinical investigations
* ONSTR,&lt;ref&gt;{{cite web |url= https://nbsdc.org/onstr.php |title=ONSTR |accessdate=16 April 2014}}&lt;/ref&gt; Ontology for Newborn Screening Follow-up and Translational Research, Newborn Screening Follow-up Data Integration Collaborative, Emory University, Atlanta.
* Plant Ontology&lt;ref name="Plant Ontology"&gt;{{cite web |url=http://www.plantontology.org/ |title=Plant Ontology |accessdate=10 February 2011}}&lt;/ref&gt; for plant structures and growth/development stages, etc.
* POPE, Purdue Ontology for Pharmaceutical Engineering
* PRO,&lt;ref&gt;{{cite web |url=http://pir.georgetown.edu/pro/ |title=PRO |accessdate=10 February 2011}}&lt;/ref&gt; the Protein Ontology of the Protein Information Resource, Georgetown University
* Program abstraction taxonomy
* Protein Ontology&lt;ref&gt;{{cite web |url=http://pir.georgetown.edu/pro/ |title=Protein Ontology |accessdate=10 February 2011}}&lt;/ref&gt; for [[proteomics]]
* [[RXNO Ontology]], for [[name reaction]]s in chemistry
* [[Sequence Ontology]],&lt;ref&gt;{{cite journal |vauthors= Eilbeck K, Lewis SE, Mungall CJ, Yandell M, Stein L, Durbin R, Ashburner M |title= The Sequence Ontology: a tool for the unification of genome annotations |journal= Genome Biology |volume= 6 |issue= 5 |pages= R44 |year= 2005 |pmid= 15892872 |pmc= 1175956 |doi= 10.1186/gb-2005-6-5-r44 |authorlink5= Lincoln Stein |authorlink6= Richard M. Durbin |authorlink7= Michael Ashburner |authorlink2= Suzanna Lewis}}&lt;/ref&gt; for representing genomic feature types found on [[Sequence (biology)|biological sequences]]
* [[SNOMED CT]] (Systematized Nomenclature of Medicine&#8212;Clinical Terms)
* [[Suggested Upper Merged Ontology]], a formal upper ontology
* [[Systems Biology Ontology]] (SBO), for computational models in biology
* SWEET,&lt;ref&gt;{{cite web |url=http://sweet.jpl.nasa.gov/ |title=SWEET |accessdate=10 February 2011}}&lt;/ref&gt; Semantic Web for Earth and Environmental Terminology
* [[ThoughtTreasure]] ontology
* [[TIME-ITEM]], Topics for Indexing Medical Education
* [[Uberon]],&lt;ref&gt;{{cite journal|pmid=22293552}}&lt;/ref&gt; representing [[metazoa|animal]] anatomical structures
* [[UMBEL]], a lightweight reference structure of 20,000 subject concept classes and their relationships derived from [[Opencyc|OpenCyc]]
* [[WordNet]], a lexical reference system
* YAMATO,&lt;ref&gt;{{cite web |url=http://www.ei.sanken.osaka-u.ac.jp/hozo/onto_library/upperOnto.htm |title=YAMATO |accessdate=10 February 2011}}&lt;/ref&gt; Yet Another More Advanced Top-level Ontology

The W3C [[Linked data#Linking Open Data community project|Linking Open Data community project]] coordinates attempts to converge different ontologies into worldwide [[Semantic Web]].

== Libraries ==
The development of ontologies for the Web has led to the emergence of services providing lists or directories of ontologies with search facility. Such directories have been called ontology libraries.

The following are libraries of human-selected ontologies.
* COLORE&lt;ref&gt;{{cite web |url=http://stl.mie.utoronto.ca/colore/ |title=COLORE |accessdate=4 May 2011}}&lt;/ref&gt; is an open repository of first-order ontologies in [[Common Logic]] with formal links between ontologies in the repository.
* DAML Ontology Library&lt;ref&gt;{{cite web |url=http://www.daml.org/ontologies/ |title=DAML Ontology Library |accessdate=10 February 2011}}&lt;/ref&gt; maintains a legacy of ontologies in DAML.
* Ontology Design Patterns portal&lt;ref&gt;{{cite web |url=http://www.ontologydesignpatterns.org |title=ODP Library |accessdate=21 February 2013}}&lt;/ref&gt; is a wiki repository of reusable components and practices for ontology design, and also maintains a list of ''exemplary ontologies''. 
* Prot&#233;g&#233; Ontology Library&lt;ref&gt;{{cite web
|url=http://protegewiki.stanford.edu/index.php/Protege_Ontology_Library |title=Protege Ontology Library |accessdate=10 February 2011}}&lt;/ref&gt; contains a set of OWL, Frame-based and other format ontologies.
* SchemaWeb&lt;ref&gt;{{cite web |url=http://www.schemaweb.info/ |title=SchemaWeb |accessdate=10 February 2011}}&lt;/ref&gt; is a directory of RDF schemata expressed in RDFS, OWL and DAML+OIL.

The following are both directories and search engines. They include crawlers searching the Web for well-formed ontologies.
* [[OBO Foundry]] is a suite of interoperable reference ontologies in biology and biomedicine.&lt;ref&gt;{{cite web |url=http://www.obofoundry.org/ |title=OBO Foundry|accessdate=10 February 2011}}&lt;/ref&gt;&lt;ref name="pmid17989687"&gt;{{Cite journal 
| last1 = Smith | first1 = B. 
| authorlink1 = Barry Smith (ontologist)
| last2 = Ashburner | first2 = M. 
| authorlink2 = Michael Ashburner
| last3 = Rosse | first3 = C. 
| last4 = Bard | first4 = J. 
| last5 = Bug | first5 = W. 
| last6 = Ceusters | first6 = W. 
| last7 = Goldberg | first7 = L. J. 
| last8 = Eilbeck | first8 = K. 
| last9 = Ireland | first9 = A. 
| last10 = Mungall 
| doi = 10.1038/nbt1346 | first10 = C. J. 
| last11 = Leontis | first11 = N. 
| last12 = Rocca-Serra | first12 = P. 
| last13 = Ruttenberg | first13 = A. 
| last14 = Sansone | first14 = S. A. 
| last15 = Scheuermann | first15 = R. H. 
| last16 = Shah | first16 = N. 
| last17 = Whetzel | first17 = P. L. 
| last18 = Lewis | first18 = S. | authorlink18 = Suzanna Lewis
| title = The OBO Foundry: Coordinated evolution of ontologies to support biomedical data integration 
| journal = [[Nature Biotechnology]] 
| volume = 25 
| issue = 11 
| pages = 1251&#8211;1255 
| year = 2007 
| pmid = 17989687 
| pmc =2814061 
}} {{open access}}&lt;/ref&gt;
* Bioportal (ontology repository of NCBO)
* OntoSelect&lt;ref&gt;{{cite web |url=http://olp.dfki.de/OntoSelect/ |title=OntoSelect |accessdate=10 February 2011}}&lt;/ref&gt; Ontology Library offers similar services for RDF/S, DAML and OWL ontologies.
* Ontaria&lt;ref&gt;{{cite web |url=http://www.w3.org/2004/ontaria/ |title=Ontaria |accessdate=10 February 2011}}&lt;/ref&gt; is a "searchable and browsable directory of semantic web data" with a focus on RDF vocabularies with OWL ontologies. (NB Project "on hold" since 2004).
* [[Swoogle]] is a directory and search engine for all RDF resources available on the Web, including ontologies.
* Open Ontology Repository initiative
* ROMULUS is a foundational ontology repository aimed at improving semantic interoperability. Currently there are three foundational ontologies in the repository: DOLCE, BFO and GFO.

== Examples of applications ==
In general, ontologies can be used beneficially in 
* enterprise applications.&lt;ref&gt;{{cite journal |first=Daniel |last=Oberle |title=How ontologies benefit enterprise applications |journal=Semantic Web Journal |volume=5 |issue=6 |pages=473&#8211;491 |publisher=IOS Press |date=2014 |doi=10.3233/SW-130114 |url=http://www.semantic-web-journal.net/system/files/swj212_2.pdf |format=PDF }}&lt;/ref&gt; A more concrete example is [[SAPPHIRE (Health care)]] or ''Situational Awareness and Preparedness for Public Health Incidences and Reasoning Engines'' which is a [[semantics]]-based [[health information system]] capable of tracking and evaluating situations and occurrences that may affect [[public health]].
* [[geographic information systems]] bring together data from different sources and benefit therefore from ontological metadata which helps to connect the semantics of the data.&lt;ref&gt;{{cite journal|first=Andrew U. |last=Frank|title=Tiers of ontology and consistency constraints in geographical information systems|journal=International Journal of Geographical Information Science|volume=15|issue=7|year=2001|pages=667&#8211;678|doi=10.1080/13658810110061144}}&lt;/ref&gt;

== See also ==
{{div col||25em}}
* [[Commonsense knowledge bases]]
* [[Controlled vocabulary]]
* [[Folksonomy]]
* [[Formal concept analysis]]
* [[Formal ontology]]
* [[Gene Ontology]]
* [[General formal ontology]]
* [[Lattice (order)|Lattice]]
* [[Ontology]]
* [[Ontology alignment]]
* [[Ontology chart]]
* [[Open Biomedical Ontologies]]
* [[Open Semantic Framework]]
* [[Soft ontology]]
* [[Terminology extraction]]
* [[Weak ontology]]
* [[Web Ontology Language]]
{{div col end}}

;Related philosophical concepts
* [[Alphabet of human thought]]
* [[Characteristica universalis]]
* [[Interoperability]]
* [[Metalanguage]]
* [[Natural semantic metalanguage]]

==References==
{{Reflist|2}}

==Further reading==
* Oberle, D., Guarino, N., &amp; Staab, S. (2009) [http://userpages.uni-koblenz.de/~staab/Research/Publications/2009/handbookEdition2/what-is-an-ontology.pdf What is an ontology?]. In: "Handbook on Ontologies". Springer, 2nd edition, 2009.
* Fensel, D., van Harmelen, F., Horrocks, I., McGuinness, D. L., &amp; Patel-Schneider, P. F. (2001). [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=920598 "OIL: an ontology infrastructure for the Semantic Web"]. In: ''Intelligent Systems''. IEEE, 16(2): 38&amp;ndash;45.
* Gangemi A., Presutti V. (2009). [http://hem.hj.se/~blev/HandbookChapter_ODPs.pdf Ontology Design Patterns].{{dead link|date=September 2016}} In Staab S. et al. (eds.): Handbook on Ontologies (2nd edition), Springer, 2009.
* Maria Golemati, Akrivi Katifori, Costas Vassilakis, George Lepouras, Constantin Halatsis (2007). [http://oceanis.mm.di.uoa.gr/pened/papers/11-onto-user-final.pdf "Creating an Ontology for the User Profile: Method and Applications"]. In: ''Proceedings of the First IEEE International Conference on Research Challenges in Information Science (RCIS)'', Morocco 2007.
* Mizoguchi, R. (2004). [http://www.ei.sanken.osaka-u.ac.jp/pub/miz/Part3V3.pdf "Tutorial on ontological engineering: part 3: Advanced course of ontological engineering"]. In: ''New Generation Computing''. Ohmsha &amp; Springer-Verlag, 22(2):198-220.
* [[Tom Gruber|Gruber, T. R.]] 1993. [http://tomgruber.org/writing/ontolingua-kaj-1993.pdf "A translation approach to portable ontology specifications"]. In: ''Knowledge Acquisition''. 5: 199&amp;ndash;199.
* Maedche, A. &amp; Staab, S. (2001). [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=920602 "Ontology learning for the Semantic Web"]. In: ''Intelligent Systems''. IEEE, 16(2): 72&amp;ndash;79.
* Natalya F. Noy and [[Deborah McGuinness|Deborah L. McGuinness]]. [http://www-ksl.stanford.edu/people/dlm/papers/ontology-tutorial-noy-mcguinness-abstract.html Ontology Development 101: A Guide to Creating Your First Ontology]. Stanford Knowledge Systems Laboratory Technical Report KSL-01-05 and Stanford Medical Informatics Technical Report SMI-2001-0880, March 2001.
* Prabath Chaminda Abeysiriwardana, Saluka R Kodituwakku, [http://www.ijorcs.org/manuscript/id/51/prabath-chaminda-abeysiriwardana-saluka-r-kodituwakku/ontology-based-information-extraction-ford-disease-intelligence "Ontology Based Information Extraction for Disease Intelligence"]. International Journal of Research in Computer Science, 2 (6): pp.&amp;nbsp;7&#8211;19, November 2012. doi:10.7815/ijorcs.26.2012.051
* Razmerita, L., Angehrn, A., &amp; Maedche, A. 2003. [http://www.springerlink.com/index/THW9RMVMVKLX9HAC.pdf "Ontology-Based User Modeling for Knowledge Management Systems"]. In: ''Lecture Notes in Computer Science'': 213&amp;ndash;17.
* Soylu, A., De Causmaecker, Patrick. 2009.[http://dx.doi.org/10.1109/ISCIS.2009.5291915 Merging model driven and ontology driven system development approaches pervasive computing perspective]. in Proc 24th Intl Symposium on Computer and Information Sciences. pp 730&#8211;735.
* Smith, B. [http://precedings.nature.com/documents/2027/version/2 Ontology (Science)], in C. Eschenbach and [[Michael Gruninger|M. Gruninger]] (eds.), Formal Ontology in Information Systems. Proceedings of FOIS 2008, Amsterdam/New York: ISO Press, 21&amp;ndash;35.
* [[Uschold, Mike]] &amp; [[Michael Gruninger|Gruninger, M.]] (1996). [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.111.5903&amp;rep=rep1&amp;type=pdf Ontologies: Principles, Methods and Applications]. Knowledge Engineering Review, 11(2).
* W. Pidcock, [http://infogrid.org/wiki/Reference/PidcockArticle ''What are the differences between a vocabulary, a taxonomy, a thesaurus, an ontology, and a meta-model?'']
* Yudelson, M., Gavrilova, T., &amp; Brusilovsky, P. 2005. [http://www.springerlink.com/index/3n0ekp8dgm4v3pr2.pdf Towards User Modeling Meta-ontology]. Lecture Notes in Computer Science, 3538: 448.
* Movshovitz-Attias, Dana and Cohen, William W. (2012) [http://www.cs.cmu.edu/~dmovshov/papers/dma_bioNELL_bioNLP2012.pdf Bootstrapping Biomedical Ontologies for Scientific Text using NELL]. BioNLP in NAACL, Association for Computational Linguistics, 2012.

==External links==
{{Commons category|Ontology}}
* [http://www.dmoz.org/Reference/Knowledge_Management/Knowledge_Representation/ Knowledge Representation] at Open Directory Project
* [http://protegewiki.stanford.edu/wiki/Protege_Ontology_Library Library of ontologies]
* [http://www.GoPubMed.com GoPubMed] using Ontologies for searching
* [http://ontolog.cim3.net/wiki ONTOLOG] (a.k.a. "[http://ontolog.cim3.net/forum/ontolog-forum/ Ontolog Forum]") - an Open, International, Virtual Community of Practice on Ontology, Ontological Engineering and Semantic Technology
* [http://trimc-nlp.blogspot.com/2013/08/nlp-driven-ontology-modeling-for.html Use of Ontologies in Natural Language Processing]
* [http://ontolog.cim3.net/cgi-bin/wiki.pl?OntologySummit Ontology Summit] - an annual series of events (first started in 2006) that involves the ontology community and communities related to each year's theme chosen for the summit.
&lt;!--
***********************************************************************************************

      This section shouldn't contain external links to specific ontologies,
      or to specific new subjects.

***********************************************************************************************
--&gt;
* [http://kore-nordmann.de/talks/09_04_standardization_of_ontologies_paper.pdf Standardization of Ontologies]

{{Semantic Web}}
{{Software engineering}}
{{computable knowledge}}

{{Authority control}}

{{DEFAULTSORT:Ontology (Information Science)}}
[[Category:Knowledge engineering]]
[[Category:Technical communication]]
[[Category:Information science]]
[[Category:Semantic Web]]
[[Category:Ontology (information science)| ]]
[[Category:Knowledge representation]]
[[Category:Knowledge bases]]</text>
      <sha1>83jxg0obhnun9zg07dg81z2iryha2gm</sha1>
    </revision>
  </page>
  <page>
    <title>Semantic parameterization</title>
    <ns>0</ns>
    <id>18922270</id>
    <revision>
      <id>735037147</id>
      <parentid>735036985</parentid>
      <timestamp>2016-08-18T07:16:30Z</timestamp>
      <contributor>
        <username>Cedar101</username>
        <id>374440</id>
      </contributor>
      <minor />
      <comment>/* Introduction with Example */ ''p''</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6435" xml:space="preserve">'''Semantic parameterization''' is a conceptual modeling process for expressing natural language descriptions of a domain in first-order predicate logic.&lt;ref&gt;Travis D. Breaux and Annie I. Ant&#243;n (2004). [http://theprivacyplace.org/blog/wp-content/uploads/2008/07/tr-2004-36.pdf Deriving Semantic Models from Privacy Policies]. North Carolina State University Computer Science Technical Report TR-2004-36.&lt;/ref&gt;&lt;ref&gt;Travis D. Breaux and Annie I. Ant&#243;n (2008). [http://theprivacyplace.org/blog/wp-content/uploads/2008/07/tr-2005-31.pdf "Mining Rule Semantics to Understand Legislative Compliance"]. North Carolina State University Computer Science Technical Report TR-2005-31.&lt;/ref&gt;&lt;ref name="Breaux"&gt;T.D. Breaux, A.I. Anton, J. Doyle, [http://www4.ncsu.edu/~tdbreaux/publications/tdbreaux-tosem09.pdf "Semantic parameterization: a process for modeling domain descriptions"], ''ACM Transactions on Software Engineering Methodology'', vol. 18, no. 2, Article 5, 2008.&lt;/ref&gt; The process yields a formalization of natural language sentences in [[Description Logic]] to answer the ''who,'' ''what'' and ''where'' questions in the Inquiry-Cycle Model (ICM) developed by Colin Potts and his colleagues at the Georgia Institute of Technology.&lt;ref name="Potts"&gt;C. Potts, K. Takahashi, and A.I. Anton, "Inquiry-based requirements analysis", ''IEEE Software'' 11(2): 21&#8211;32, 1994.&lt;/ref&gt; The parameterization process complements the Knowledge Acquisition and autOmated Specification (KAOS) method,&lt;ref&gt;A. Dardenne, A. van Lamsweerde and S. Fickas, "Goal-Directed Requirements Acquisition", ''Science of Computer Programming'' v. 20, North Holland, 1993, pp. 3-50.&lt;/ref&gt; which formalizes answers to the ''when'', ''why'' and ''how'' ICM questions in [[Temporal Logic]], to complete the ICM formalization. The artifacts used in the parameterization process include a dictionary that aligns the domain lexicon with unique concepts, distinguishing between [[synonyms]] and [[polysemes]], and several natural language patterns that aid in mapping common domain descriptions to formal specifications.

== Relationship to other theories ==

Semantic Parameterization defines a meta-model consisting of eight roles that are domain-independent and reusable. Seven of these roles correspond to Jeffrey Gruber's [[thematic relations]]&lt;ref&gt;J. Gruber, ''Lexical Structures in Syntax and Semantics'', North Holland, New York, 1976.&lt;/ref&gt; and [[case role]]s in Charles Fillmore's [[case grammar]]:&lt;ref&gt;C. Fillmore, "The Case for Case", ''Universals in Linguistic Theory'', Holt, Rhinehart and Winston, New York, 1968.&lt;/ref&gt;

{| class="wikitable" border="1" cellpadding="3" align="center"
|+ Meta-model Mapping to Case Frames and Thematic Relations
! Breaux's Meta-model
! Fillmore's Case Roles
! Thematic Relations
|-
| Subject
| Agentive
| Agent
|-
| Action
|
|-
| Object
| Objective/ Factitive
| Theme/ Patient
|-
| Target
| Dative
| Goal
|-
| Source
| Source
| Source
|-
| Instrument
| Instrumental
| Instrument
|-
| Purpose
|
| Purposive
|-
| Location
| Locative
| Location
|-
|
| Comitative
| Accompaniment
|}

The Inquiry-Cycle Model (ICM) was introduced to drive elicitation between engineers and stakeholders in requirements engineering.&lt;ref name="Potts" /&gt; The ICM consists of ''who'', ''what'', ''where'', ''why'', ''how'' and ''when'' questions. All but the ''when'' questions, which require a [[Temporal Logic]] to represent such phenomena, have been aligned with the meta-model in semantic parameterization using [[Description Logic]] (DL).

{| class="wikitable" border="1" cellpadding="3" align="center"
|+ Mapping from DL roles to questions in the Inquiry-Cycle Model
! DL Role in Meta-model
! ICM Question
|-
| isSubjectOf.Activity
| Who performs the action?
|-
| isObjectOf.Activity
| Upon what is the action performed?
|-
| isTargetOf.Activity
| With whom is the transaction performed?
|-
| isPurposeOf.Activity
| Why is the action performed?
|-
| isInstrumentOf.Activity
| How is the action performed?
|-
| isLocationOf.Activity
| Where is the action performed?
|}

== Introduction with Example ==

The semantic parameterization process is based on [[Description Logic]], wherein the TBox is composed of words in a ''dictionary'', including nouns, verbs, and adjectives, and the ABox is partitioned into two sets of assertions: 1) those assertions that come from words in the natural language statement, called the ''grounding'', and 2) those assertions that are inferred by the (human) modeler, called the ''meta-model''. Consider the following unstructured natural language statement (UNLS) (see Breaux et al.&lt;ref name="Breaux" /&gt; for an extended discussion):

;UNLS&lt;sub&gt;1.0&lt;/sub&gt;: The customer&lt;sub&gt;1,1&lt;/sub&gt; must not share&lt;sub&gt;2,2&lt;/sub&gt; the access-code&lt;sub&gt;3,3&lt;/sub&gt; of the customer&lt;sub&gt;1,1&lt;/sub&gt; with someone&lt;sub&gt;4,4&lt;/sub&gt; who is not the provider&lt;sub&gt;5,4&lt;/sub&gt;.

The modeler first identifies intensional and extensional polysemes and synonyms, denoted by the subscripts: the first subscript uniquely refers to the intensional index, i.e., the same first index in two or more words refer to the same concept in the TBox; the second subscript uniquely refers to the extensional index, i.e., two same second index in two or more words refer to the same individual in the ABox. This indexing step aligns words in the statement and concepts in the dictionary. Next, the modeler identifies concepts from the dictionary to compose the meta-model. The following table illustrates the complete DL expression that results from applying semantic paramterization.

{| class="wikitable" border="1" cellpadding="3" align="center"
|+ The grounding {{mvar|G}} and meta-model {{mvar|M}} derived from UNLS&lt;sub&gt;1.0&lt;/sub&gt;
! Grounding ({{mvar|G}})
! Meta-model ({{mvar|M}})
|-valign="top"
| {{math|Customer(''p''&lt;sub&gt;1&lt;/sub&gt;) &lt;br /&gt;
&#10757; Share(''p''&lt;sub&gt;2&lt;/sub&gt;) &lt;br /&gt;
&#10757; isAccessCodeOf(''p''&lt;sub&gt;3&lt;/sub&gt;, ''p''&lt;sub&gt;1&lt;/sub&gt;) &lt;br /&gt;
&#10757; Someone(''p''&lt;sub&gt;4&lt;/sub&gt;) &lt;br /&gt;
&#10757; Provider(''p''&lt;sub&gt;4&lt;/sub&gt;)}}
| {{math|Activity(''p''&lt;sub&gt;5&lt;/sub&gt;) &lt;br /&gt;
&#10757; hasSubject(''p''&lt;sub&gt;5&lt;/sub&gt;, ''p''&lt;sub&gt;1&lt;/sub&gt;) &lt;br /&gt;
&#10757; hasAction(''p''&lt;sub&gt;5&lt;/sub&gt;, ''p''&lt;sub&gt;2&lt;/sub&gt;) &lt;br /&gt;
&#10757; hasObject(''p''&lt;sub&gt;5&lt;/sub&gt;, ''p''&lt;sub&gt;3&lt;/sub&gt;) &lt;br /&gt;
&#10757; hasTarget(''p''&lt;sub&gt;5&lt;/sub&gt;, ''p''&lt;sub&gt;4&lt;/sub&gt;) &lt;br /&gt;
&#10757; isRefrainmentOf(''p''&lt;sub&gt;5&lt;/sub&gt;, ''p''&lt;sub&gt;1&lt;/sub&gt;)}}
|}

== References ==
{{Reflist}}

{{DEFAULTSORT:Semantic Parameterization}}
[[Category:Knowledge representation]]</text>
      <sha1>is7613ml77t7zuh1vi3pkj8giuuplzr</sha1>
    </revision>
  </page>
  <page>
    <title>FAO Country Profiles</title>
    <ns>0</ns>
    <id>24515769</id>
    <revision>
      <id>754597919</id>
      <parentid>713683980</parentid>
      <timestamp>2016-12-13T14:27:39Z</timestamp>
      <contributor>
        <username>Chris the speller</username>
        <id>525927</id>
      </contributor>
      <minor />
      <comment>punct, number fmt using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="21059" xml:space="preserve">{{ infobox software
| name                   = FAO Country Profiles 
| logo                   = [[File:FAO countryprofiles logo.jpg]]
| caption                = 
| developer              = [[FAO]] of the [[United Nations]]
| latest_release_version = 2012
| latest_release_date    = 2001
| operating_system       = 
| genre                  = [[Knowledge Representation]], [[Ontology]] Editor
| website                = [http://www.fao.org/countryprofiles/ FAO Country Profiles]
}}

The [[FAO]] [[Country]] Profiles is a [[multilingual]]&lt;ref&gt;Arabic, Chinese, English, French, Russian and Spanish are the languages of the Organization. See FAO's Basic texts http://www.fao.org/docrep/010/k1713e/k1713e02b.htm#47. The FAO Country Profiles system provides information in Arabic, Chinese, English, French and Spanish. Russian is in preparation.&lt;/ref&gt; [[web portal]] which repackages the [[Food and Agriculture Organization]] of the United Nations (FAO) vast archive of information on its global activities in [[agriculture]] and [[food security]] in a single area and catalogues it exclusively by [[country]] and thematic areas.

The portal's purpose is to offer decision-makers, [[researchers]] and project formulators around the world a fast and reliable way to access country-specific information on national [[food security]] situations without the need to search individual [[databases]] and [[systems]]. It gives added-value to [[FAO]]'s wealth of information by providing an easy-to-use [[User interface|interface]] containing [[interactive]] [[maps]] and [[charts]].&lt;ref&gt;For reviews of the [[FAO]] Country profiles initiatives, please see the [http://news.eoportal.org/didyouknow/080226_did2.html Sharing Earth and Observations Resources portal], [http://www.sciencecentral.com/category/962945 Science Central], [http://www.scinet.cc/dir/Science/Agriculture/ SciNet Science &amp; Technology Search, News, Articles], etc.&lt;/ref&gt;

== Background ==

[[FAO]] has always highlighted [[information]] and [[Knowledge sharing]] as priority areas in fighting [[hunger]] and achieving [[food security]].&lt;ref&gt;See ARTICLE I of FAO Constitution: The Organization shall collect, analyze, interpret, and disseminate information relating to nutrition, food and agriculture. http://www.fao.org/docrep/x5584E/x5584e0i.htm&lt;/ref&gt; In this context, [[FAO]] identified that [[countries]] could improve their national programmes on [[agriculture]] and [[food security]] if they could access [[FAO]]'s information through a cross-sectoral (or [[interdisciplinary]]) country-based approach.&lt;ref&gt;Programme of Work and Budget 2002&#8211;2003:  http://www.fao.org/docrep/meeting/003/y1194e/y1194e06b.htm#P11324_311453&lt;/ref&gt;&lt;ref&gt;Programme of Work and Budget 2004&#8211;2005: http://www.fao.org/DOCREP/MEETING/006/y9859e/Y9859e07a.htm#P10820_371793&lt;/ref&gt; However, despite the existence of a large number of country-based [[information systems]] in FAO, the information managed by the various systems lacked [[System integration|integration]]. Information tended to be generated and used in a circumscribed manner and tailored to a specific system, department or [[sector (economic)|sector]].

The [http://www.fao.org/countryprofiles/ FAO Country Profiles] portal, initially called FAO Country Profiles and Mapping Information System, was launched in 2002 responding to the Organization&#8217;s need to provide [[FAO]] web site&#8217;s users an easy to use mechanism to find FAO country-specific [[information]] without the need to [[Search engine technology|search]] individual [[FAO]] [[web sites]], [[databases]] or [[systems]]. The system was designed to integrate [[Scientific modelling|analytical]] and [[multilingual]] information with thematic databases and [[Digital data|digital]] [[map]] [[Disciplinary repository|repositories]] and to facilitate access to information on multiple factors contributing to national [[food insecurity]].

Since its launch, the system has grown by incorporating more and more [[FAO Country Profiles#Data sources|data sources]]. This was achieved thanks to a [[corporate]] effort to reduce [[information silo]]s and the adoption of [[international standards]] for country-based [[information management]] throughout the Organization.

== Country Profiles ==

The methodology behind the [[FAO]] Country Profiles is rather simple; it links, reuses and repackages data and information from most relevant existing [[FAO]] [[databases]] and [[systems]].

The [[FAO]] Country Profiles covers current FAO Members and Associated Nations.&lt;ref&gt;FAO membership as the 17 November 2007: http://www.fao.org/Legal/member-e.htm&lt;/ref&gt; Once a country is selected, the portal presents to the user [[documents]], news feeds, [[statistical data]], project details and [[maps]] from relevant [[FAO Country Profiles#Data sources|FAO databases and systems]] for the selected [[country]] and categorized according to thematic areas.

The thematic areas are grouped in two categories:

* FAO Core Activities: these correspond to  [[FAO]]'s main areas of expertise, such as, [[natural resources]], [[economics]], [[agriculture]], [[forestry]], [[fisheries]] and technical cooperation. This grouping is based on the work of the corresponding [[FAO]] departments.&lt;ref&gt;For a list of FAO departments and divisions, please see http://www.fao.org/about/depart/en/&lt;/ref&gt;
* Global issues: these are themes that [[FAO]] identified as priority areas for action, and include [[biodiversity]], [[biotechnology]], [[climate change]], [[diseases]] and [[Pest (organism)|pests]], [[emergency]] and aid, [[food security]] and [[food safety|safety]], [[trade]] and [[prices]], [[water management]]. These priority areas correspond to [[FAO]]'s strategic response to a fast-changing world where issues ranging from [[biotechnology]] to [[climate change]] and [[trade]] present new challenges and choices to governments and the general public.

===Data sources ===

Country pages provide access to or integrate the following thematic profiles and systems.&lt;ref name="Inventory of resources"&gt;[http://www.fao.org/countryprofiles/resources.asp Inventory of data sources used in the FAO country profiles]&lt;/ref&gt;

==== FAO data sources ====
* [http://www.fao.org/nr/water/aquastat/countries/index.stm Aquastat Country Profiles]: The AQUASTAT country profiles describe the state of [[water resources]] and [[agricultural]] [[water use]] in the respective country. Special attention is given to [[water resource]], [[irrigation]], and [[drainage]] sub-sectors.
* [http://www.fao.org/biotech/inventory_admin/dep/country_rep_search.asp?lang=en Biotechnology Country Profiles]: The objective of the profiles is to provide a platform on which [[developing country]] [[biotechnology]]-related [[policies]], [[regulations]] and activities can be readily accessed, directing the user to key, updated sources of information.
* [http://www.fao.org/biotech/inventory_admin/dep/default.asp?lang=en BIODEC Biotechnologies in Developing Countries]: FAO-BioDeC is a database meant to gather, store, organize and disseminate, updated baseline information on the state-of-the-art of [[crop]] [[biotechnology]] [[Product (business)|products]] and [[Scientific technique|techniques]], which are in use, or in the pipeline in [[developing countries]]. The database includes about 2000 entries from 70 [[developing countries]], including countries with [[economies in transition]].
* [http://www.fao.org/ag/AGP/AGPC/doc/Counprof/regions/index.htm Country Pasture/Forage Resource Profiles]: The Country [[Pasture]]/[[Forage]] Resource Profile provides a broad overview of relevant general, [[topographical]], [[climatic]] and [[agro-ecological]] information with focus on [[livestock]] production systems and the [[pasture]]/[[forage]] resources.
* [http://www.fao.org/documents FAO Corporate Document Repository]: The FAO Corporate Document Repository houses FAO documents and publications, as well as selected non-FAO publications, in electronic format.
* [http://www.fao.org/tc/tcom/index_en.htm FAO Projects in the country]: From the Field Programme Management Information System.
* [http://www.fao.org/faoterm/ FAO Terminology - Names of Countries]: In order to standardize and harmonize the vast quantity of terms used in FAO documents and publications, the Organization developed the [[terminology]] database [[FAOTERM]]. The Corporate NAMES OF COUNTRIES database also aims at facilitating the consultation and harmonization of country names throughout the Organization.
* [http://www.fao.org/fishery/countryprofiles/search/en Fisheries and Aquaculture Country Profiles]: FAO's [[Fisheries]] and [[Aquaculture]] Department prepares and publishes Fishery and Aquaculture Country Profiles. Each profile summarizes the Department's assessment of activities and trends in fisheries and aquaculture for the country concerned.  Economic and [[demographic data]] are based on [[UN]] or [[World Bank]] sources; data on fisheries are generally those published by the FAO Fisheries and Aquaculture Department.
* [http://www.fao.org/forestry/country/en/ Forestry Country Profiles]: The [[forestry]] country profiles provide detailed information on [[forests]] and the forest sector: [[forest cover]] (types, extent and change), [[forest management]], policies, products and trade, and more - in all some 30 pages for each country in the world.
* [http://www.fao.org/geonetwork/srv/en/main.home FAO-GeoNetwork]: FAO-GeoNetwork is a web-based Geographic Data and Information Management System. It enables easy access to local and distributed [[geospatial information]] catalogues and makes available data, graphics, documents for immediate download. FAO-GeoNetwork holds approximately 5000 standardized [[metadata]] records for digital and paper maps, most of them at the global, continent and national level.
* [http://www.fao.org/giews/english/index.htm Global Information and Early Warning System on Food and Agriculture (GIEWS)]: The System aims to provide policy-makers and policy-analysts with the most up-to-date information available on all aspects of [[food supply]] and demand, warning of imminent [[food crises]], so that timely interventions can be planned.
* [http://www.fao.org/ag/againfo/resources/en/pubs_sap.html Livestock Sector Briefs]: The purpose of the [[Livestock]] Sector Briefs is to provide a concise overview of livestock production in the selected countries through tables, maps and graphs.
* [http://www.fao.org/ag/agn/nutrition/profiles_en.stm Nutrition Country Profiles]: The [[Nutrition]] Country Profiles (NCP) provide concise analytical summaries describing the food and nutrition situation in individual countries.

==== Partnerships data sources ====
* [http://www.agrifeeds.org/ AgriFeeds]: AgriFeeds is a service that allows users to search and filter news and events from several agricultural information sources. It harvests, stores and re-aggregates news and events from feeds published by agricultural organizations and information services.
* [http://www.ipfsaph.org/En/default.jsp International Portal on Food Safety, Animal &amp; Plant Health (IPFSAPH)]: IPFSAPH facilitates trade in food and agriculture by providing a single access point to authorized official international and national information across the sectors of food safety, animal and plant health.  It has been developed by FAO in association with the organizations responsible for international standard setting in sanitary and phytosanitary matters.

==== Non-FAO data sources ====
* [http://earthtrends.wri.org/gsearch.php?kw=country&amp;action=results Earthtrends], [[World Resources Institute]]: EarthTrends is a comprehensive online database, maintained by the World Resources Institute, that focuses on the environmental, social, and economic trends that shape the world. The Earthtrends country profiles present environmental information about key variables for different topic areas.
* International Fund for Agricultural Development ([[IFAD]]): Rural poverty country profiles are produced by IFAD.

== Standards ==

[[File:Geopolitical Ontology in Country Profiles August 12 2009 v 1.png||thumb|200px|right| Geopolitical information section in the FAO Country Profiles.]]

There are various [[international standards]] and [[coding systems]] to manage country information. Historically, systems dealing with different types of data used different coding systems that were tailored to specific data type requirements. For example, [[statistical systems]] in the [[United Nations]] commonly use the M-49 classification and pigmentation&lt;ref&gt;Standard Country or Area Codes for Statistical Use http://unstats.un.org/unsd/methods/m49/m49.htm&lt;/ref&gt; (also known as [[UN]] code) or the [[FAOSTAT]] area classification;&lt;ref&gt;FAOSTAT standardized list of country/territories and groupings: http://faostat.fao.org/site/441/default.aspx&lt;/ref&gt; mapping systems could use [[geographic coordinates]] or [[Global Administrative Unit Layers (GAUL)|GAUL]] codes; textual systems (document repositories or web sites) could use [[ISO 3166-1 alpha-2]], [[ISO 3166-1 alpha-3]] or [[AGROVOC]] keywords; etc.

The FAO Country Profiles provide access to systems managing [[statistics]], [[documents]], [[maps]], [[news feeds]], etc., therefore one of its key aspects to succeed was the mapping of all these [[country codes]].

For this purpose a [[geopolitical ontology]] was developed.&lt;ref&gt;For linking country-based heterogeneous data at [[FAO]], please see:[http://www.semanticuniverse.com/articles-integrating-country-based-heterogeneous-data-united-nations-fao%E2%80%99s-geopolitical-ontology-and Integrating Country-based heterogeneous data at the United Nations: FAO's geopolitical ontology and services.]&lt;/ref&gt; This ontology, among other features, maps [[ISO 3166-1 alpha-2|ISO2]], [[ISO 3166-1 alpha-3|ISO3]], [[AGROVOC]], [[Food and Agriculture Organization Corporate Statistical Database|FAOSTAT]], [http://www.fao.org/faoterm/index.asp?lang=EN FAOTERM], [[Global Administrative Unit Layers (GAUL)|GAUL]], [[UN]], and [[UNDP]] codes for all countries.

== Global Resources ==

Besides the profiles for each country the portal also provides access to other important global resources, such as:

=== Low-Income Food Deficit Countries (LIFDC) ===
The FAO Country Profiles keeps updated for the public the list of [[LIFDC]] countries. This list is revised every year according to the methodology explained below. The new list of the LIFDC,&lt;ref&gt;For an updated list of Low-Income Food Deficit Countries, please check this page: http://www.fao.org/countryprofiles/lifdc/en/&lt;/ref&gt; stands at 62 countries, four less than in the (2012) list. These are: [[Georgia (country)|Georgia]], [[Syrian Arab Republic]], [[Timor-Leste]], [[Republic of Moldova]]. While [[Moldova]] graduated from the list on the basis of net food-exporter criterion, the other graduated based on income criterion.

==== LIFDC methodology====

The classification of a country as low-income food-deficit used for analytical purposes by [[FAO]] is traditionally determined by three criteria:

# A country should have a [[per capita income]] below the "historical" ceiling used by the [[World Bank]]&lt;ref&gt;For operational and analytical purposes, the World Bank&#8217;s main criterion for classifying economies is gross national income (GNI) per capita. Classifications are set each year on 1 July. These official analytical classifications are fixed during the World Bank's fiscal year (ending on 30 June), thus countries remain in the categories in which they are classified irrespective of any revisions to their per capita income data. (Source: [[The World Bank]])&lt;/ref&gt; to determine eligibility for [[International Development Association|IDA]] assistance and for 20-year [[IBRD]] terms, applied to countries included in the World Bank categories I and II.&lt;ref&gt;Several important distinctions among member countries are commonly used at the World Bank Group. Countries choose whether they are part of Part I or Part II primarily on the basis of their economic standing. Part I are almost all industrial countries and donors to IDA and they pay their contributions in freely convertible currency. Part II countries are almost all developing countries, some of which are donors to IDA. Part II countries are entitled to pay most of their contribution to IDA in local currency. Please see: "A Guide to the World Bank Group", The World Bank, 2003&lt;/ref&gt; For instance, the historical ceiling of per capita [[gross national income]] (GNI) for 2006, based on the World Bank Atlas method,&lt;ref&gt;Please see: [http://web.worldbank.org/WBSITE/EXTERNAL/DATASTATISTICS/0,,contentMDK:20452009~menuPK:64133156~pagePK:64133150~piPK:64133175~theSitePK:239419~isCURL:Y~isCURL:Y,00.html The World Bank Atlas Method]&lt;/ref&gt; was US$1,735, i.e. higher than the level established for 2005 ($1,675).
# The net food [[trade]]&lt;ref&gt;Net food trade refers to the gross imports less gross exports of food&lt;/ref&gt; position of a country averaged over the preceding three years for which statistics are available, in this case from 2003 to 2005. Trade volumes for a broad basket of basic foodstuffs ([[cereals]], [[root]]s and [[tubers]], [[pulses]], [[oilseeds]] and oils other than tree crop oils, [[meat]] and [[dairy products]]) are converted and aggregated by the [[calorie]] content of individual [[commodities]].
# A self-exclusion criterion is applied when countries that meet the above two criteria specifically request FAO to be excluded from the LIFDC category.

In order to avoid countries changing their LIFDC status too frequently - typically due to short-term, [[exogenous]] shocks - an additional factor was introduced in 2001. This factor, called "persistence of position", would postpone the "exit" of a LIFDC from the list, despite the country not meeting the LIFDC [[income]] criterion or the [[food-deficit]] criterion, until the change in its status is verified for three consecutive years.&lt;ref&gt;For a list of countries and economies sorted by their gross domestic product (GDP) at purchasing power parity (PPP) per capita, please see [[List of countries by GDP (PPP) per capita]]&lt;/ref&gt;

=== FAO Member Countries and Flags ===

The FAO Country Profiles is FAO's source for dissemination of [[FAO]]'s Member Nations and Associated Nations&lt;ref&gt;The list of FAO member countries and date of entry is available at: http://www.fao.org/Legal/member-e.htm&lt;/ref&gt; official flags.&lt;ref&gt;The list of FAO member countries and flags is available at http://www.fao.org/countryprofiles/flags/&lt;/ref&gt; The update of any [[country flag]] is coordinated with the other [[United Nations]] agencies. All flags are made available in a standardized manner which also aims to help web site owners to ensure that they always display the official country flag.

The standard URL for any given country flag would be composed by: the generic URL: "http://www.fao.org/countryprofiles/flags/" to which the [[ISO 3166-1 alpha-2|ISO 3166-1 Alpha-2]] code for the country is added, plus the image format suffix ".gif". For instance, the URL for the [[Argentine flag|Argentina flag]] would be: http://www.fao.org/countryprofiles/flags/AR.gif, with AR being the [[ISO 3166-1 alpha-2]] code of [[Argentina]].&lt;ref&gt;One of several international coding systems (some of the others being: [[ISO2]], [[ISO3]], [[AGROVOC]], [[FAOSTAT]], [[FAOTERM]], [[GAUL]], [[UN]], and [[UNDP]]) for territories and groups.&lt;/ref&gt;

== Criticism ==

Early criticism of the [[FAO]] Country Profiles was that, in its inception phase, it only contained very few resources. Since 2002, the number of available resources has increased to cover country-based information and data, directly linked from [[FAO]]'s web pages or [[FAO]]'s digital repositories.&lt;ref name="Inventory of resources"/&gt; Over the last years, another identified area for improvement was the simplicity of the system methodology, being the resources only linked from country pages and thus, lacking real integration. This need was addressed by starting to integrate additional data, such as, the fisheries charts or the news and events items taken from [[AgriFeeds]]. In addition, in order to provide more complete country profiles, the system started to link or integrate  non-[[FAO]] resources.

== See also ==
* [[Agricultural Information Management Standards]]
* [[AGROVOC]]
* [[Country codes]]
* [[Food and Agriculture Organization]]
* [[Forestry Information Centre]]
* [[Geopolitical ontology]]

== References ==
{{reflist|33em}}

==External links==
* [http://www.fao.org/countryprofiles/default.asp?lang=en FAO Country Profiles]
* [http://www.fao.org/Legal/member-e.htm FAO membership]
* [http://www.fao.org/countryprofiles/lifdc.asp?lang=en Low-Income Food-Deficit Countries (LIFDC)]
* [http://www.fao.org/sids/index_en.asp Small Island Developing States (SIDS)]

{{DEFAULTSORT:Fao Country Profiles}}
[[Category:Agriculture]]
[[Category:Agriculture by country| FAO]]
[[Category:Knowledge representation]]
[[Category:Information systems]]
[[Category:Food and Agriculture Organization]]
[[Category:Country codes]]</text>
      <sha1>43cb1dfm5b7we3lbfs0nllxpjxthyjs</sha1>
    </revision>
  </page>
  <page>
    <title>Geopolitical ontology</title>
    <ns>0</ns>
    <id>20250365</id>
    <revision>
      <id>739443231</id>
      <parentid>739443177</parentid>
      <timestamp>2016-09-14T18:31:39Z</timestamp>
      <contributor>
        <username>Tango303</username>
        <id>18434560</id>
      </contributor>
      <comment>/* Definitions and examples */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="14743" xml:space="preserve">The '''FAO geopolitical ontology''' is an [[Ontology (information science)|Ontology]] developed by the [[FAO|Food and Agriculture Organization of the United Nations (FAO)]] to describe, manage and exchange data related to geopolitical entities such as countries, territories, regions and other similar areas.

==Definitions and examples==
An [[ontology (information science)|ontology]] is a kind of dictionary that describes information  in a certain domain using concepts and relationships. It is often implemented using [[Web Ontology Language|OWL]] (Web Ontology Language), an [[XML]]-based standard language  that can be interpreted by computers.

* A ''Concept'' is defined as abstract knowledge. For example, in the geopolitical ontology a [[United Nations list of Non-Self-Governing Territories|non-self-governing territory]] or a [[geographical region|geographical group]] are concepts. Concepts are explicitly implemented in the ontology with individuals and classes:
** An ''individual'' is defined as an object perceived from the real world. In the geopolitical domain   [[Ethiopia]] or the [[least developed countries]] group are individuals.
** A ''class'' is defined as a set of individuals sharing common properties. In the geopolitical domain, [[Ethiopia]], [[Republic of Korea]] or [[Italy]] are individuals of the class ''self-governing'' territory; and [[least developed countries]] is an individual of the class ''special group''.
* Relationships between concepts are explicitly implemented by:
** ''[[Object (computer science)|Object]] properties'' between individuals of two classes. For example, ''has member'' and ''is in group'' properties, as shown in Figure 1.
** ''[[Datatype]] properties'' between individuals and literals or [[XML]] datatypes. For example, the individual [[Afghanistan]] has the datatype property ''CodeISO3'' with the value "AFG".
** ''Restrictions'' in classes and/or properties. For example, the property ''official English name'' of the class ''self-governing'' territory has been restricted to have only ''one'' value, this means that a self-governing territory (or country) can only have one internationally recognized official English name.&lt;ref&gt;Official names of countries from [http://www.fao.org/faoterm/nocs/pages/homeNocs.jsp?members=allC&amp;lang=en&amp;lang2=en FAO terminology database]&lt;/ref&gt;

[[Image:Concepts November 19 2008 v 2.png||thumb|600px|center|Figure 1. An example of concepts and relationship in the geopolitical ontology.]]

The advantage of describing information in an ontology is that it enables to acquire domain knowledge by defining hierarchical structures of classes, adding individuals, setting object properties and datatype properties, and assigning restrictions.

==FAO ontology==
The geopolitical ontology provides names in seven languages (Arabic, Chinese, French, English, Spanish, Russian and Italian) and identifiers in various international coding systems ([[ISO 3166-1 alpha-2|ISO2]], [[ISO 3166-1 alpha-3|ISO3]], [[AGROVOC]], [[Food and Agriculture Organization Corporate Statistical Database|FAOSTAT]], [http://www.fao.org/faoterm/index.asp?lang=EN FAOTERM], [[Global Administrative Unit Layers (GAUL)|GAUL]], [[UN]], [[List of UNDP country codes|UNDP]] and [[DBPedia]]ID codes) for territories and groups. Moreover, the [[FAO]] geopolitical ontology tracks historical changes from 1985 up until today;&lt;ref&gt;Country or area code changes since 1982:  [http://unstats.un.org/unsd/methods/m49/m49chang.htm United Nations Statistics Division - country or area codes added or changed]&lt;/ref&gt; provides [[geolocation]] (geographical coordinates); implements relationships among [[countries]] and countries, or countries and groups, including properties such as ''has border with'', ''is predecessor of'', ''is successor of'', ''is administered by'', ''has members'', and ''is in group''; and disseminates country statistics including country area, land area, agricultural area, [[GDP]] or [[population]].

The FAO geopolitical ontology provides a structured description of data sources. This includes: source name, source identifier, source creator and source's update date. Concepts are described using the [[Dublin Core]] vocabulary (http://purl.org/dc/elements/1.1/description).

In summary, the main objectives of the FAO geopolitical ontology are:

* To provide the most updated geopolitical information (names, codes, relationships, statistics)
* To track historical changes in geopolitical information
* To improve information management and facilitate standardized data sharing of geopolitical information
* To demonstrate the benefits of the geopolitical ontology to improve [[interoperability]] of [[corporate]] [[information systems]]

It is possible to '''download''' the FAO geopolitical ontology in [http://aims.fao.org/geopolitical.owl OWL] and [http://www.fao.org/countryprofiles/geoinfo/geopolitical/resource/ RDF] formats. Documentation is available in the [[FAO Country Profiles]] [http://www.fao.org/countryprofiles/geoinfo.asp?lang=en Geopolitical information] web page.

==Features of the FAO ontology==
The geopolitical ontology contains :

*Area types:&lt;ref&gt;When an area (territory or group) changed but kept the same name, the ontology differentiates the two areas by sub-fixing the name of the obsolete one with the year (e.g. &#8220;FAO 2006&#8221;). The year indicates the beginning of validity of that particular area.&lt;/ref&gt;
**Territories: [[List of sovereign states|self-governing]], [[United Nations list of Non-Self-Governing Territories|non-self-governing]], [[Disputed area|disputed]], other.&lt;ref&gt;The area type ''Other'' is used for [[Antarctica]] which has no government and belongs to no country. See also [[Antarctica#Politics|Politics in Antarctica]].&lt;/ref&gt;
**Groups: [[organizations]], [[subregion|geographic]], [[economic union|economic]] and special groups.&lt;ref&gt;Special groups term is used for non-economical or greographical territory groups like the [[Small Island Developing States]], [[Landlocked countries|Land Locked Countries]], Low Income Food Deficit Countries, [[Least Developed Countries]], etc.&lt;/ref&gt;
*Names &lt;ref&gt;UN official names: [http://unstats.un.org/unsd/geoinfo/uncsgnreports.htm Reports of the United Nations Conference on the Standardization of Geographical Names]&lt;/ref&gt; (official, short and names for lists) in Arabic, Chinese, English, French, Spanish, Russian and Italian.
*International codes: UN code &#8211; M49, [[ISO 3166]] Alpha-2 and Alpha-3, [[List of UNDP country codes|UNDP code]], [[Global Administrative Unit Layers (GAUL)|GAUL]] code, FAOSTAT, [[AGROVOC]] FAOTERM and [[DBPedia]]ID.
*Coordinates: maximum [[latitude]], minimum [[latitude]], maximum [[longitude]], minimum [[longitude]].
*Basic country statistics: country area, land area, agricultural area, GDP, population.
*Currency names and codes.
*Adjectives of nationality.
*Relations:
**Groups membership.
**Neighbours (land [[border]]), administration of [[United Nations list of Non-Self-Governing Territories|non-self-governing]].
**Historic changes: predecessor, successor, valid since,&lt;ref&gt;The value of the datatype property "validSince" is the first year of validity  of a territory or group. The geopolitical ontology traces back historic changes only until 1985. Therefore if an area has a validSince = 1985, this indicates that the area is valid at least since 1985.&lt;/ref&gt; valid until.&lt;ref&gt;The value of the datatype property "validUntil" is the last year of validity of the territory or group. In case the area is currently valid, this value is set by default to 9999.&lt;/ref&gt;

==Implementation into  OWL==
The [[FAO]] geopolitical ontology is implemented in [[Web Ontology Language|OWL]]. It consists of classes, properties, individuals and restrictions. Table 1 shows all classes, gives a brief description and lists some individuals that belong to each class. Note that the current version of the geopolitical ontology does not provide individuals of the class "disputed" territories. Table 2 and Table 3 illustrate datatype properties and object properties.

&lt;!-- Deleted image removed: [[Image:Class and instances in the geopolitical ontology v 1.png||thumb|667px|center|Table 1. Classes and instances in the geopolitical ontology.]] --&gt;

&lt;!-- Deleted image removed: [[Image:Datatype properties in the geopolitical ontology v 1.png||thumb|667px|center|Table 2. Datatype properties in the geopolitical ontology.]] --&gt;

[[Image:Object properties in the geopolitical ontology v 1.png||thumb|674px|center|Table 3. Object properties in the geopolitical ontology.]]

== Geopolitical ontology in Linked Open Data ==
&lt;!-- Deleted image removed: [[File:Geopol LOD.png|thumb|200px|left|Figure 2. RDF version of FAO geopolitical ontology]]  --&gt;

The FAO Geopolitical ontology is embracing the [http://linkeddata.org W3C Linked Open Data (LOD) initiative] and released its RDF version of the geopolitical ontology in March 2011. 
The term 'Linked Open Data' refers to a set of best practices for publishing and connecting structured data on the Web. The key technologies that support Linked Data are URIs, HTTP and RDF.

The RDF version of the geopolitical ontology is compliant with all [http://www.w3.org/DesignIssues/LinkedData.html Linked data principles] to be included in the [http://richard.cyganiak.de/2007/10/lod/ Linked Open Data cloud], as explained in the following.

==Resolvable http:// URIs ==
Every resource in the OWL format of the FAO Geopolitical Ontology has a unique URI. Dereferenciation was implemented to allow for three different URIs to be assigned to each resource as follows:  
* URI identifying the non-information  resource
* Information resource with an RDF/XML representation
* Information resource with an HTML representation
In addition the current URIs used for OWL format needed to be kept to allow for backwards compatibility for other systems that are using them. Therefore, the new URIs for the FAO Geopolitical Ontology in LOD were carefully created, using  &#8220;Cool URIs for Semantic Web&#8221;  and considering other good practices for URIs, such as DBpedia URIs.

==New URIs==
The URIs of the geopolitical ontology need to be permanent, consequently all transient information, such as year, version, or format was avoided in the definition of the URIs. 
The new URIs can be accessed at
http://www.fao.org/countryprofiles/geoinfo/geopolitical/resource/ 
For example, for the resource &#8220;Italy&#8221; the URIs are the following: 
;http://www.fao.org/countryprofiles/geoinfo/geopolitical/resource/Italy
: identifies the non-information resource. 
;http://www.fao.org/countryprofiles/geoinfo/geopolitical/data/Italy
: identifies the resource with an RDF/XML representation. 
;http://www.fao.org/countryprofiles/geoinfo/geopolitical/page/Italy
:identifies the information resource with an HTML representation.
In addition, &#8220;owl:sameAs&#8221; is used to map the new URIs to the OWL representation.

==Dereferencing URIs==
When a non-information resource is looked up without any specific representation format, then the server needs to redirect the request to information resource with an HTML representation. 
For example, to retrieve the resource &#8220;Italy&#8221; (http://www.fao.org/countryprofiles/geoinfo/geopolitical/resource/Italy), which is a non-information resource, the server redirects to the html page of &#8220;Italy&#8221; (http://www.fao.org/countryprofiles/geoinfo/geopolitical/page/Italy).

==At least 1000 triples in the datasets==
The total number of triple statements in FAO Geopolitical Ontology is 22,495. 
At least 50 links to a dataset already in the current LOD Cloud:  
FAO Geopolitical Ontology has 195 links to [http://www.dbpedia.org DBpedia], which is already part of the LOD Cloud.

==Access to the entire dataset==
FAO Geopolitical Ontology provides the entire dataset as a RDF dump. It is available at http://www.fao.org/countryprofiles/geoinfo/geopolitical/data

The RDF version of the FAO Geopolitical Ontology has been already registered in CKAN (http://ckan.net/package/fao-geopolitical-ontology) and it was requested to add it into the LOD Cloud.

==Example of use==
[[Image:Geopolitical Ontology in Country Profiles August 12 2009 v 1.png||thumb|200px|right|Figure 3. a website of introducing the geopolitical ontology in FAO Country Profiles.]]

The [[FAO Country Profiles]] is an information retrieval tool which groups the FAO's vast archive of information on its global activities in [[agriculture]] and [[rural development]] in one single area and catalogues it exclusively by country.

The [[FAO Country Profiles]] system provides access to country-based heterogeneous data sources.&lt;ref&gt;[http://www.fao.org/countryprofiles/resources.asp Inventory of data sources used in the FAO country profiles]&lt;/ref&gt; By using the  geopolitical ontology in the system, the following benefits are expected:&lt;ref&gt;[http://semanticweb.com/integrating-country-based-heterogeneous-data-at-the-united-nations-fao-s-geopolitical-ontology-and-services_b10681 Integrating country-based heterogeneous data at the United Nations: FAO's geopolitical ontology and services.]&lt;/ref&gt;

* Enhanced system functionality for content aggregation and synchronization from the multiple source repositories.
* Improved information access and browsing through comparison of data in neighbor countries and groups.

Figure 3 shows a page in the [[FAO Country Profiles]] where the geopolitical ontology is described.

==See also==
*[[Agricultural Information Management Standards]]
*[[AGROVOC]]
*[[Country code]]
*[[FAO Country Profiles]]
*[[Global Administrative Unit Layers (GAUL)|Global Administrative Unit Layers]] (GAUL)
*[[International Organization for Standardization]] (ISO)

==References==
{{reflist|2}}

==External links==
*[http://aims.fao.org/geopolitical.owl Geopolitical ontology in OWL format]
*[http://www.fao.org/countryprofiles/geoinfo/geopolitical/resource/ Geopolitical ontology in RDF format]
*[http://www.fao.org/countryprofiles/geoinfo.asp?lang=en Geopolitical information in the FAO Country Profiles]
*[http://www.slideshare.net/faoaims/faos-geopolitical-ontology-and-services FAO&#8217;s Geopolitical Ontology and Services] (Slides about FAO's geopolitical ontology)
*[http://www.fao.org/countryprofiles/default.asp?lang=en FAO Country Profiles]
*[http://www.fao.org/faoterm FAO Terminology] (FAOTERM)
*[http://faostat.fao.org FAOSTAT]
*[http://unstats.un.org/unsd/methods/m49/m49.htm UN Statistics Division - M49 codes]
*[http://www.iso.org/iso/english_country_names_and_code_elements ISO - Maintenance Agency for ISO 3166 country codes]

{{DEFAULTSORT:Geopolitical Ontology}}
[[Category:Ontology]]
[[Category:Ontology (information science)]]
[[Category:Knowledge representation]]
[[Category:Country codes]]</text>
      <sha1>kcob9xzfjyi0byprqme5i688lmi4fd8</sha1>
    </revision>
  </page>
  <page>
    <title>ISO 15926</title>
    <ns>0</ns>
    <id>4724116</id>
    <revision>
      <id>737691065</id>
      <parentid>720501244</parentid>
      <timestamp>2016-09-04T12:47:25Z</timestamp>
      <contributor>
        <ip>217.123.55.71</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="16610" xml:space="preserve">The '''ISO 15926''' is a standard for [[data integration]], sharing, exchange, and hand-over between [[computer system]]s.

The title, "''Industrial automation systems and integration&amp;mdash;Integration of life-cycle data for process plants including oil and gas production facilities''",  is regarded too narrow by the present ISO 15926 developers. Having developed a [[generic data model]] and Reference Data Library for process plants, it turned out that this subject is already so wide, that actually any state information may be modelled with it.

== History ==
In 1991 a European Union [[European Strategic Program on Research in Information Technology|ESPRIT]]-, named ProcessBase, started. The focus of this research project was to develop a [[data model]] for lifecycle information of a facility that would suit the requirements of the process industries. At the time that the project duration had elapsed, a consortium of companies involved in the process industries had been established: [[EPISTLE]] (European Process Industries STEP Technical Liaison Executive). Initially individual companies were members, but later this changed into a situation where three national consortia were the only members: PISTEP (UK), POSC/Caesar (Norway), and USPI-NL (Netherlands). (later PISTEP merged into POSC/Caesar, and USPI-NL was renamed to USPI).

EPISTLE took over the work of the ProcessBase project. Initially this work involved a standard called ISO 10303-221 (referred to as "[[ISO_10303|STEP]] AP221"). In that AP221 we saw, for the first time, an Annex M with a list of standard instances of the AP221 data model, including types of objects. These standard instances would be for reference and would act as a knowledge base with knowledge about the types of objects.
In the early nineties EPISTLE started an activity to extend Annex M to become a library of such object classes and their relationships: STEPlib. In the STEPlib activities a group of approx. 100 domain experts from all three member consortia, spread over the various expertises (e.g. Electrical, Piping, Rotating equipment, etc.), worked together to define the "core classes".

The development of STEPlib was extended with many additional classes and relationships between classes and published as [[Open Source]] data. Furthermore, the concepts and relation types from the AP221 and ISO 15926-2 data models were also added to the STEPlib dictionary. This resulted in the development of [[Gellish English]], whereas STEPlib became the [[Gellish English dictionary]]. Gellish English is a structured subset of natural English and is a [[modeling language]] suitable for [[knowledge modeling]], [[product modeling]] and [[data exchange]]. It differs from conventional modeling languages ([[meta language]]s) as used in information technology as it not only defines generic concepts, but also includes an English dictionary. The semantic expression capability of Gellish English was significantly increased by extending the number of relation types that can be used to express knowledge and information.

For modelling-technical reasons POSC/Caesar proposed another standard than [[ISO 10303]], called ISO 15926. EPISTLE (and ISO) supported that proposal, and continued the modelling work, thereby writing Part 2 of ISO 15926. This Part 2 has official ISO IS (International Standard) status since 2003.

POSC/Caesar started to put together their own RDL (Reference Data Library). They added many specialized classes, for example for [[American National Standards Institute|ANSI]] (American National Standards Institute) pipe and pipe fittings. Meanwhile STEPlib continued its existence, mainly driven by some members of USPI. Since it was clear that it was not in the interest of the industry to have two libraries for, in essence, the same set of classes, the Management Board of EPISTLE decided that the core classes of the two libraries shall be merged into Part 4 of ISO 15926. This merging process has been finished. Part 4 should act as reference data for part 2 of ISO 15926 as well as for ISO 10303-221 and replaced its Annex M. On June 5, 2007 ISO 15926-4 was signed off as a TS (Technical Specification).

In 1999 the work on an earlier version of Part 7 started. Initially this was based on [[XML Schema (W3C)|XML Schema]] (the only useful W3C Recommendation available then), but when [[Web Ontology Language|Web Ontology Language (OWL)]] became available it was clear that provided a far more suitable environment for Part 7. Part 7 passed the first ISO ballot by the end of 2005, and an implementation project started. A formal ballot for TS (Technical Specification) was planned for December 2007. However, it was decided then to split Part 7 into more than one part, because the scope was too wide.

== The standard ==
{{External links|date=May 2012}}
ISO 15926 has eleven parts (as of June 2009):

* Part 1 [http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=29556] - Introduction, information concerning engineering, construction and operation of production facilities is created, used and modified by many different organizations throughout a facility's lifetime. The purpose of ISO 15926 is to facilitate integration of data to support the lifecycle activities and processes of production facilities.
* Part 2 [http://www.stanford.edu/group/narratives/classes/08-09/CEE215/ReferenceLibrary/FIATECH%20ISO%2015926/ISO/pack/ISO%2015926%20part2%20pack/ECM4.5/lifecycle_integration_schema.html]- Data Model. a generic 4D model that can support all disciplines, supply chain company types and life cycle stages, regarding information about functional requirements, physical solutions, types of objects and individual objects as well as activities.
* Part 3 - Reference data for geometry and topology.
* Parts 4 [http://data.posccaesar.org/rdl/] - Reference Data, the terms used within facilities for the process industry.
* Part 7 [http://www.15926.org] - Integration of life-cycle data for process plants including oil and gas production facilities - Part 7: Implementation methods for the integration of distributed systems: Template methodology.
* Part 8 [http://www.15926.org] - Integration of life-cycle data for process plants including oil and gas production facilities - Part 8: Implementation methods for the integration of distributed systems: [[Web Ontology Language]] (OWL/RDF) implementation.
* Part 9 (in development)- Implementation standards, with the focus on Fa&#231;ades, standard web servers, web services, and security.
* Part 10 (in development)- Test Methods.
* Part 11 (in development)- Industrial Usage Guidelines.
* Part 12 (in development)- Life cycle integration ontology in [[Web Ontology Language]] (OWL2).
* Part 13 (in development)- Integrated lifecycle asset planning.

=== Description ===
The model and the library are suitable for representing lifecycle information about technical installations and their components.

They can also be used for defining the terms used in product catalogs in [[e-commerce]]. Another, more limited, use of the standard is as a reference classification for harmonization purposes between shared databases and product catalogues that are not based on ISO 15926.

The purpose of ISO 15926 is to provide a [[Lingua franca|Lingua Franca]] for computer systems, thereby integrating the information produced by them. Although set up for the process industries with large projects involving many parties, and involving plant operations and maintenance lasting decades, the technology can be used by anyone willing to set up a proper vocabulary of reference data in line with Part 4.

In Part 7 the concept of Templates is introduced. These are semantic constructs, using Part 2 entities, that represent a small piece of information. These constructs then are mapped to more efficient classes of n-ary relations that interlink the Nodes that are involved in the represented information.

In Part 8 the data model of Part 2 is mapped to OWL, and so are, in concept, the Reference Data of Part 4 and the templates of Part 7. For validation and reasoning purposes all are represented in First-Order Logic as well.

In Part 9 these Node and Template instances are stored in Fa&#231;ades. A Fa&#231;ade is an RDF [[quad store]], set up to a standard schema and an API. Any Fa&#231;ade only stores the data for which the Fa&#231;ade owner is responsible.

Each participating computer system maps its data from its internal format to such ISO-standard Node and Template instances. These are stored in a System Fa&#231;ade, each system its own Fa&#231;ade.

Data can be "handed over" from one Fa&#231;ade to another in cases where data custodianship is handed over (e.g. from a contractor to a plant owner, or from a manufacturer to the owners of the manufactured goods). Hand-over can be for a part of all data, whilst maintaining full referential integrity.

Fa&#231;ades can be set up for the consolidation of data by handing over data produced by various participating computer systems and stored in their System Fa&#231;ades. Examples are: a Fa&#231;ade for a project discipline, a project, a plant).

Documents are user-definable. They are defined in [[XML Schema]] and they are, in essence, only a structure containing cells that make reference to instances of Templates. This represents a view on all lifecycle data: since the data model is a 4D (space-time) model, it is possible to present the data that was valid at any given point in time, thus providing a true historical record. It is expected that this will be used for Knowledge Mining.

Data can be queried by means of [[SPARQL]]. In any implementation a restricted number of Fa&#231;ades can be involved, with different access rights. This is done by means of creating a CPF Server (= Confederation of Participating Fa&#231;ades). An [[Ontology (computer science)|Ontology]] Browser allows for access to one or more Fa&#231;ades in a given CPF, depending on the access rights.

== Projects and applications ==
{{External links|date=May 2012}}
There are a number of projects working on the extension of the ISO 15926 standard in different application areas.

=== Capital-intensive projects ===

Within the application of Capital Intensive projects, some cooperating implementation projects are running:

* The EDRC Project of [http://www.fiatech.org FIATECH] [http://www.fiatech.org/images/stories/projects/Project_Resumes/EDRC_Resume_v8_Sept_13_2013.pdf Capturing Equipment Data Requirements Using ISO 15926 and Assessing Conformance]. [http://techinvestlab.ru/EDRCDemo Example data and videos.]
* The ADI Project of [http://www.fiatech.org FIATECH], to build the tools (which will then be made available in the public domain)
** The tools and deliverables can be seen on the ISO 15926 knowledge base: [http://15926.org]
* The IDS Project of [http://www.posccaesar.org POSC Caesar Association], to define product models required for data sheets
* A joint ADI-IDS project is the [[ISO 15926 WIP]]
* The DEXPI project: The objective of DEXPI is to develop and promote a general standard for the process industry covering all phases of the lifecycle of a (petro-)chemical plant, ranging from specification of functional requirements to assets in operation. See more at [http://www.dexpi.org dexpi.org]

=== Upstream Oil and Gas industry ===

The [[Norwegian Oil Industry Association]] (OLF) has decided to use ISO 15926 (also known as the [[Oil and Gas Ontology]]) as the instrument for integrating data across disciplines and business domains for the [[Upstream (oil industry)|Upstream Oil and Gas industry]]. It is seen as one of the enablers of what has been called the next (or second) generation of [[Integrated operations]], where a better integration across companies is the goal.&lt;ref&gt;{{cite web |url=http://www.olf.no/getfile.php/zKonvertert/www.olf.no/Rapporter/Dokumenter/070919%20IO%20and%20Ontology%20-%20Brosjyre.pdf |title=Integrated Operations and the Oil and Gas Ontology |author=The Norwegian Oil Industry Association (OLF) |accessdate=2009-05-06 }}&lt;/ref&gt;

The following projects are currently running (May 2009):

* The [[Integrated Operations in the High North|Integrated Operations in the High North (IOHN)]] project is working on extending ISO 15926 to handle real-time data transmission and (pre-)processing to enable the next generation of [[Integrated operations|Integrated Operations]].
* The [http://trac.posccaesar.org/wiki/EW Environment Web] project to include environmental reporting terms and definitions  as used in [http://www.epim.no EPIM]'s [http://www.epim.no/default.asp?id=945 EnvironmentWeb] in ISO 15926.

Finalised projects include:

* The [http://trac.posccaesar.org/wiki/IIP Integrated Information Platform (IIP)] project working on establishing a real-time information pipeline based on open standards. It worked among others on:
** [http://www.posccaesar.org/wiki/NcsDdr Daily Drilling Report (DDR)] to including all terms and definitions in ISO 15926. This standard became mandatory on February 1, 2008&lt;ref&gt;{{cite web |url=http://www.npd.no/English/Produkter+og+tjenester/Skjemaer/CDRS_reporting_oct_2007.htm |title=Drilling Reporting to the authorities |author=Norwegian Petroleum Directorate |accessdate=2009-05-05 }}&lt;/ref&gt; for reporting on the [[Norwegian Continental Shelf]] by the [[Norwegian Petroleum Directorate|Norwegian Petroleum Directorate (NPD)]] and [http://www.ptil.no/main-page/category9.html Safety Authority Norway (PSA)]. NPD says that the quality of the reports has improved considerably since.
** [http://www.posccaesar.org/wiki/NcsDpr Daily Production Report (DPR)] to including all terms and definitions in ISO 15926. This standard was tested successfully on the [[Valhall oil field|Valhall]] ([[BP]]-operated) and [http://www.statoilhydro.com/en/ouroperations/explorationprod/ncs/aasgard/pages/default.aspx &#197;sgard] ([[StatoilHydro]]-operated) fields offshore Norway. The terminology and XML schemata developed have also been included in [http://www.Energistics.org Energistics]&#8217; [[PRODML]] standard.

== Some technical background ==
One of the main requirements was (and still is) that the scope of the data model covers the entire lifecycle of a facility (e.g. oil refinery) and its components (e.g. pipes, pumps and their parts, etc.). Since such a facility over such a long time entails many different types of activities on a myriad of different objects it became clear that a generic and data-driven data model would be required.

A simple example will illustrate this. There are thousands of different types of physical objects in a facility (pumps, compressors, pipes, instruments, fluids, etc). Each of these has many properties. If all combinations would be modelled in a "hard-coded" fashion, the number of combinations would be staggering, and unmanageable.

The solution is a "template" that represents the semantics of: "This object has a property of  X yyyy" (where yyyy is the unit of measure). Any instance of that template refers to the applicable reference data:
* physical object (e.g. my Induction Motor)
* indirect property type (e.g. the class "cold locked rotor time")
* base property type (here: time)
* scale (here: seconds)

Without being able to make reference to those classes, via the Internet, it will be impossible to express this information.

==References==
{{Reflist}}

== External links ==
* [http://15926.org 15926.org]: A forum for ISO 15926 discussions and team collaboration.
* [http://iringug.org/wiki/index.php?title=Main_Page iringug.org]: -An online community of users, companies, and organizations that have common interest in solutions that implement ISO 15926 reference data and protocols.
* [http://iringtoday.com iringtoday.com]: - An online ISO 15926 thought leadership community geared toward engineering management.
* [http://techinvestlab.ru/ISO15926en .15926 Editor] Open source software to view, edit and verify ISO 15926 data.
* [http://wings.buffalo.edu/philosophy/ontology/bfo/west.pdf Against Idiosyncrasy in Ontology Development]: A critical study of ISO 15926 and of the claims made on its behalf.
* [http://www.matthew-west.org.uk/publications/ResponseToBarrySmithCommentsOnISO15926.pdf A Response to "Against Idiosyncrasy in Ontology Development"]: A rebuttal of "Against Idiosyncracy in Ontology Development".

{{ISO standards}}

{{DEFAULTSORT:Iso 15926}}
[[Category:ISO standards|#15926]]
[[Category:Semantic Web]]
[[Category:Knowledge engineering]]
[[Category:Technical communication]]
[[Category:Information science]]
[[Category:Ontology (information science)]]
[[Category:Knowledge representation]]</text>
      <sha1>1opxwaxt2dsp0qlv5rgual17i8e0aeb</sha1>
    </revision>
  </page>
  <page>
    <title>IMARK</title>
    <ns>0</ns>
    <id>31933815</id>
    <revision>
      <id>724770012</id>
      <parentid>712875797</parentid>
      <timestamp>2016-06-11T10:49:23Z</timestamp>
      <contributor>
        <username>Marcocapelle</username>
        <id>14965160</id>
      </contributor>
      <comment>removed [[Category:Development]]; added [[Category:Rural development]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6144" xml:space="preserve">{{Infobox website
|name=Information Management Resource Kit (IMARK)
|logo = 
|url ={{URL|http://www.imarkgroup.org}}
|type = [[Capacity building]]
|commercial      = No
|registration    = Optional
|language        = English, French, Spanish, Arabic, Chinese
|launch date = 2001
|current status = Online
|screenshot      = }}

The Information Management Resource Kit ('''IMARK''') is a partnership-based [[e-learning]] initiative developed by the [[Food and Agriculture Organization|Food and Agriculture Organization (FAO)]] of the [[United Nations]] and partner organizations to support individuals, institutions and networks world-wide in the effective management of information and agricultural development. IMARK consists of a suite of [[distance learning]] resources and tools on [[information management]].&lt;ref&gt;{{cite web|url=http://www.imarkgroup.org/index_en.asp?m=0 |title=IMARK - Information Management Resource Kit |publisher=Imarkgroup.org |date= |accessdate=2011-06-07}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://usain.org/links2.html |title=Links for Agricultural Librarians |publisher=USAIN |date=2010-05-19 |accessdate=2011-06-07}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://knowledge.cta.int/index.php/en/Dossiers/S-T-Issues-in-Perspective/ICT-for-transforming-research-for-agricultural-and-rural-development/Links/IMARK-FAO |title=IMARK (FAO) / Links / ICT for transforming research for agricultural and rural development / S&amp;T Issues in Perspective / Dossiers / Home - Knowledge for Development |publisher=Knowledge.cta.int |date= |accessdate=2011-06-07}}&lt;/ref&gt;

== About IMARK ==
The [[Food and Agriculture Organization|Food and Agriculture Organization (FAO)]] of the [[United Nations]] initiated a partnership-based [[e-learning]] programme in 2001 to support [[information management]].&lt;ref&gt;http://www.fao.org/rdd/doc/IMARK%20General%20Sheet%20EN%2011-05.pdf&lt;/ref&gt; IMARK is targeted at information professionals in developing countries. Each IMARK curriculum is designed through a consultative process with [[Subject-matter expert|subject matter experts]], field practitioners and representatives from the target audience from around the world. The IMARK initiative is a response to demand for enhanced information and [[knowledge management]] in the effort to achieve the [[Millennium Development Goals|Millennium Development Goals (MDGs)]], especially those related to [[hunger]] and the information society, in the context of bridging the digital divide.&lt;ref&gt;{{cite web|url=http://www.fao.org/rdd/doc/FAO%20Approach%20to%20WSIS2005.pdf |title=FAO's strategies towards the WSIS 2005 |format=PDF |date= |accessdate=2011-06-07}}&lt;/ref&gt;  The development goal of IMARK is to improve the capabilities of people concerned with [[information management]] and [[knowledge sharing]].&lt;ref&gt;{{cite web|url=http://editlib.org/noaccesspresent/26495 |title=Ed/ITLib Digital Library &#8594; No Access |publisher=Editlib.org |date=2007-10-15 |accessdate=2011-06-07}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.infodev.org/en/Publication.183.html |title=Quick Guide: ICT and Rural Livelihood Resources at FAO |publisher=infoDev.org |date=2006-09-28 |accessdate=2011-06-07}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.icimod.org/index.php?page=23 |title=Knowledge Management |publisher=Icimod.org |date=2010-12-16 |accessdate=2011-06-07}}&lt;/ref&gt;

=== Objectives and Scope ===
The development goal of IMARK is to improve the overall effectiveness of programmes in agricultural development and [[food security]] by enhancing access to information by key stakeholders.&lt;ref&gt;http://eprints.rclis.org/bitstream/10760/15682/1/FAO%E2%80%99s%20Capacity-Building%20Initiatives%20in%20Accessing,%20Documenting,%20Communicating%20and%20Managing%20Agricultural%20Information.pdf&lt;/ref&gt;

=== Steering Committee ===
IMARK has over 30 partners and collaborating institutions since its inception in 2001, and its activities are coordinated through a Steering Committee whose members include [[Association for Progressive Communications|The Association for Progressive Communications (APC)]], [[Food and Agriculture Organization|Food and Agriculture Organization (FAO)]] of the [[United Nations]], the [[Agence universitaire de la Francophonie|Agence Universitaire de la Francophonie (AUF)]], [[Commonwealth of Learning|Commonwealth of Learning (COL)]], [[Groupe de Recherches et d'Echanges Technologiques|Groupe de Recherches et d'Echanges Technologiques (GRET)]], [[Bibliotheca Alexandrina]] and [[UNESCO]].&lt;ref&gt;{{cite web|url=http://portal.unesco.org/ci/en/ev.php-URL_ID=21458&amp;URL_DO=DO_TOPIC&amp;URL_SECTION=201.html |title=IMARK launches new e-learning module |publisher=Portal.unesco.org |date=2008-01-24 |accessdate=2011-06-07}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.apc.org/en/news/development/world/e-learning-module-developing-electronic-communitie |title=e-Learning module developing electronic communities &amp;#124; Association for Progressive Communications |publisher=Apc.org |date=2006-05-08 |accessdate=2011-06-07}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://webapp.ciat.cgiar.org/ccc/imark.htm |title=CCC: E-Learning Initiatives - IMARK-FAO |publisher=Webapp.ciat.cgiar.org |date= |accessdate=2011-06-07}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.iaald.org/docs/iisast2_report.pdf |title=2nd Expert Consultation IISAST |format=PDF |date= |accessdate=2011-06-07}}&lt;/ref&gt;

== See also ==
* [[E-Learning|E-learning]]
* [[FAO|Food and Agriculture Organization of the United Nations (FAO)]]
* [[Agricultural Information Management Standards|Agricultural Information Management Standards (AIMS)]]

== References ==
{{Reflist|2}}

== External links ==
* [http://www.imarkgroup.org/ Official IMARK Website ]
* [http://www.fao.org Food and Agriculture Organization of the United Nations]

{{DEFAULTSORT:Imark}}
[[Category:Information technology]]
[[Category:Distance education]]
[[Category:Education]]
[[Category:Virtual learning environments]]
[[Category:Learning methods]]
[[Category:Educational technology projects]]
[[Category:Rural development]]
[[Category:Non-profit technology]]
[[Category:Food and Agriculture Organization]]
[[Category:Information technology management]]
[[Category:Knowledge representation]]</text>
      <sha1>90o1pkoer4nyjidt2po9s6f0h4l8oav</sha1>
    </revision>
  </page>
  <page>
    <title>LEAP (programming language)</title>
    <ns>0</ns>
    <id>911672</id>
    <revision>
      <id>467661561</id>
      <parentid>458626698</parentid>
      <timestamp>2011-12-25T20:14:23Z</timestamp>
      <contributor>
        <username>AHMartin</username>
        <id>3932984</id>
      </contributor>
      <comment>Add: description of triples; Ext link: SAIL User Manual; Cat: Knowledge representation</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1375" xml:space="preserve">{{Unreferenced|date=November 2007}}
'''LEAP''' is an extension to the [[ALGOL 60]] '''[[programming language]]''' which provides an associative memory of triples. The three items in a triple denote the association that an Attribute of an Object has a specific Value.  LEAP was created by Jerome Feldman (University of California Berkeley) and Paul Rovner (MIT Lincoln Lab) in 1967.  LEAP was also implemented in [[SAIL (programming language)|SAIL]].

==References==
* Feldman, Jerome A. and Paul D. Rovner (Jan, 1968).  "The LEAP language and data structure", MIT Lincoln Laboratory, Lexington, MA.  In abbreviated form, ''Proc. 1968 IFIP Congress'', Brandon Systems Press, Princeton, NJ.
* Feldman, Jerome A. and Paul D. Rovner (Aug, 1969).  [http://portal.acm.org/citation.cfm?doid=363196.363204 "An ALGOL-based associative language"], ''Communications of the ACM'', 12:8, pp 439 - 449.
* Rovner, Paul D (Dec, 1968).  "The LEAP users manual", MIT Lincoln Laboratory, Lexington, MA.
* [ftp://reports.stanford.edu/pub/cstr/reports/cs/tr/73/373/CS-TR-73-373.pdf VanLehn, Kurt A. (Jul, 1973).  "SAIL User Manual", Stanford Artificial Intelligence Laboratory, Stanford, CA.]

[[Category:Structured programming languages]]
[[Category:Procedural programming languages]]
[[Category:Programming languages created in 1967]]
[[Category:Knowledge representation]]


{{compu-lang-stub}}</text>
      <sha1>gi0f3jyiez9bla5k7qloa4nzdjdiz6f</sha1>
    </revision>
  </page>
  <page>
    <title>Dublin Core</title>
    <ns>0</ns>
    <id>8742</id>
    <revision>
      <id>741172920</id>
      <parentid>737374976</parentid>
      <timestamp>2016-09-25T21:10:48Z</timestamp>
      <contributor>
        <username>RobbieIanMorrison</username>
        <id>27789191</id>
      </contributor>
      <minor />
      <comment>/* Further reading */ removed honorific for 'Luca Dini' (as per [[MOS:CREDENTIAL]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="17366" xml:space="preserve">{{Use dmy dates|date=July 2012}}
The '''Dublin Core Schema''' is a small set of vocabulary terms that can be used to describe web resources (video, images, web pages, etc.), as well as physical resources such as books or CDs, and objects like artworks.&lt;ref&gt;{{cite web|url=http://dublincore.org/documents/dcmi-type-vocabulary/index.shtml |title=DCMI Metadata Terms |publisher=Dublincore.org |accessdate=5 April 2013}}&lt;/ref&gt; The full set of Dublin Core metadata terms can be found on the Dublin Core Metadata Initiative (DCMI) website.&lt;ref&gt;{{cite web|url=http://dublincore.org/documents/dcmi-terms/ |title=DCMI Metadata Terms |publisher=Dublincore.org |accessdate=5 April 2013}}&lt;/ref&gt; The original set of 15 classic&lt;ref&gt;{{cite web|url=http://dublincore.org/specifications/ |title=DCMI Specifications |publisher=Dublincore.org |date=14 December 2009 |accessdate=5 April 2013}}&lt;/ref&gt; metadata terms, known as the Dublin Core Metadata Element Set&lt;ref name="DCMES"&gt;{{cite web|url=http://dublincore.org/documents/dces/ |title=Dublin Core Metadata Element Set, Version 1.1 |publisher=Dublincore.org |accessdate=5 April 2013}}&lt;/ref&gt; are endorsed in the following standards documents:

* IETF RFC 5013&lt;ref&gt;[http://www.ietf.org/rfc/rfc5013.txt The Dublin Core Metadata Element Set], Dublin Core Metadata Initiative, August 2007&lt;/ref&gt;
* ISO Standard 15836-2009&lt;ref&gt;{{cite web|url=http://www.iso.org/iso/iso_catalogue/catalogue_ics/catalogue_detail_ics.htm?csnumber=52142 |title=ISO 15836:2009 - Information and documentation - The Dublin Core metadata element set |publisher=Iso.org |date=18 February 2009 |accessdate=5 April 2013}}&lt;/ref&gt;
* NISO Standard Z39.85&lt;ref&gt;{{cite web|url=http://www.niso.org/kst/reports/standards?step=2&amp;gid=None&amp;project_key=9b7bffcd2daeca6198b4ee5a848f9beec2f600e5 |title=NISO Standards - National Information Standards Organization |publisher=Niso.org |date=22 May 2007 |accessdate=5 April 2013}}&lt;/ref&gt;

Dublin Core Metadata may be used for multiple purposes, from simple resource description, to combining metadata vocabularies of different [[Metadata#Metadata standards|metadata standards]], to providing interoperability for metadata vocabularies in the [[Linked Data]] cloud and [[Semantic Web]] implementations.

== Background ==
"Dublin" refers to [[Dublin, Ohio]], USA where the schema originated during the 1995 invitational OCLC/NCSA Metadata Workshop,&lt;ref&gt;[http://dublincore.org/workshops/dc1/ OCLC/NCSA Metadata Workshop]&lt;/ref&gt; hosted by the [[Online Computer Library Center]] (OCLC), a library consortium based in Dublin, and the [[National Center for Supercomputing Applications]] (NCSA).  "Core" refers to the metadata terms as "broad and generic being usable for describing a wide range of resources".&lt;ref name="DCMES"/&gt; The semantics of Dublin Core were established and are maintained by an international, cross-disciplinary group of professionals from [[librarianship]], [[computer science]], [[text encoding]], [[museum]]s, and other related fields of scholarship and practice.

Starting in 2000, the Dublin Core community focused on "[[application profile]]s" &amp;ndash; the idea that metadata records would use Dublin Core together with other specialized vocabularies to meet particular implementation requirements. During that time, the World Wide Web Consortium's work on a generic data model for metadata, the [[Resource Description Framework]] (RDF), was maturing. As part of an extended set of DCMI Metadata Terms, Dublin Core became one of the most popular vocabularies for use with RDF, more recently in the context of the Linked Data movement.&lt;ref&gt;{{cite web|title=DCMI Metadata Basics|publisher=dublincore.org/metadata-basics/}}&lt;/ref&gt;

The '''Dublin Core Metadata Initiative''' (DCMI)&lt;ref&gt;{{cite web|url=http://dublincore.org/ |title=DCMI Home: Dublin Core&#174; Metadata Initiative (DCMI) |publisher=Dublincore.org |date= |accessdate=2015-12-04}}&lt;/ref&gt; provides an open forum for the development of interoperable online [[metadata standards]] for a broad range of purposes and of business models. DCMI's activities include consensus-driven working groups, global conferences and workshops, standards liaison, and educational efforts to promote widespread acceptance of metadata standards and practices. In 2008, DCMI separated from OCLC and incorporated as an independent entity.&lt;ref&gt;{{cite web | title=OCLC Research and the Dublin Core Metadata Initiative | url=http://www.oclc.org/research/activities/past/orprojects/dublincore/default.htm | accessdate=21 April 2010}}&lt;/ref&gt;

Currently, any and all changes that are made to the Dublin Core standard, are reviewed by a DCMI Usage Board within the context of a DCMI Namespace Policy (DCMI-NAMESPACE). This policy describes how terms are assigned and also sets limits on the amount of editorial changes allowed to the labels, definitions, and usage comments.&lt;ref&gt;{{cite web|url=http://dublincore.org/documents/dces/ |title=Dublin Core Metadata Element Set, Version 1.1 |publisher=Dublincore.org |date= |accessdate=2015-12-04}}&lt;/ref&gt;

== Levels of the standard ==
The Dublin Core standard originally includes two levels: Simple and Qualified. '''Simple Dublin Core''' comprised 15 elements; '''Qualified Dublin Core''' included three additional elements (Audience, Provenance and RightsHolder), as well as a group of element refinements (also called qualifiers) that could refine the semantics of the elements in ways that may be useful in resource discovery.

Since 2012 the two have been incorporated into the DCMI Metadata Terms as a single set of terms using the [[Resource Description Framework]] (RDF).&lt;ref name="dublincore1"&gt;{{cite web|url=http://dublincore.org/documents/dcmi-terms/ |title=DCMI Metadata Terms |publisher=Dublincore.org |date= |accessdate=2015-12-04}}&lt;/ref&gt; The full set of elements is found under the namespace http://purl.org/dc/terms/. Because the definition of the terms often contains domains and ranges, which may not be compatible with the pre-RDF definitions used for the original 15 Dublin Core elements, there is a separate namespace for the original 15 elements as previously defined: http://purl.org/dc/elements/1.1/.&lt;ref&gt;[http://dublincore.org/documents/dces/ Dublin Core Metadata Element Set, version 1.1]&lt;/ref&gt;

=== Dublin Core Metadata Element Set Version 1.1===
The original '''Dublin Core Metadata Element Set''' consists of 15 metadata elements:&lt;ref name="DCMES"/&gt;
# Title
# Creator
# Subject
# Description
# Publisher
# Contributor
# Date
# Type
# Format
# Identifier
# Source
# Language
# Relation
# Coverage
# Rights

Each Dublin Core element is optional and may be repeated. The DCMI has established standard ways to refine elements and encourage the use of encoding and vocabulary schemes. There is no prescribed order in Dublin Core for presenting or using the elements. The Dublin Core became ISO 15836 standard in 2006 and is used as a base-level data element set for the description of learning resources in the [[ISO/IEC 19788]]-2 Metadata for learning resources (MLR) &amp;ndash; Part 2: Dublin Core elements, prepared by the [[ISO/IEC JTC1/SC36|ISO/IEC JTC1 SC36]].

Full information on element definitions and term relationships can be found in the Dublin Core Metadata Registry.&lt;ref name="registry"&gt;[http://dcmi.kc.tsukuba.ac.jp/dcregistry/ Dublin Core Metadata Registry]&lt;/ref&gt;

==== Example of code ====
: {{code|2=html4strict|1=&lt;meta name="DC.Format" content="video/mpeg; 10 minutes"&gt;}}
: {{code|2=html4strict|1=&lt;meta name="DC.Language" content="en" &gt;}}
: {{code|2=html4strict|1=&lt;meta name="DC.Publisher" content="publisher-name" &gt;}}
: {{code|2=html4strict|1=&lt;meta name="DC.Title" content="HYP" &gt;}}

==== An example of use [and mention] of D.C. (by [[WebCite]]) ====

At the web page which serves as the "archive" form for [[WebCite]],&lt;ref name="WebCite_archive_form_(web_page)"&gt;{{cite web
| url          = http://webcitation.org/archive
| title        = WebCite&#174; archive form
| quote        = Metadata (optional)&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;These are Dublin Core elements. [...]
| publisher    = [[WebCite]]
}}
&lt;/ref&gt; it says, in part: "Metadata (optional)&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;These are Dublin Core elements. [...]".

=== Qualified Dublin Core (deprecated in 2012&lt;ref&gt;{{cite web|url=http://dublincore.org/documents/2000/07/11/dcmes-qualifiers/ |title=Dublin Core Qualifiers |publisher=Dublincore.org |date= |accessdate=2015-12-04}}&lt;/ref&gt;)===
Subsequent to the specification of the original 15 elements, an ongoing process to develop exemplary terms extending or refining the Dublin Core Metadata Element Set (DCMES) was begun. The additional terms were identified, generally in working groups of the Dublin Core Metadata Initiative, and judged by the DCMI Usage Board to be in conformance with principles of good practice for the qualification of Dublin Core metadata elements.

Elements refinements make the meaning of an element narrower or more specific. A refined element shares the meaning of the unqualified element, but with a more restricted scope. The guiding principle for the qualification of Dublin Core elements, colloquially known as the ''Dumb-Down Principle'',&lt;ref&gt;[http://dublincore.org/workshops/dc8/dcgrammar/tsld008.html Dumb-Down Principle for qualifiers]&lt;/ref&gt; states that an application that does not understand a specific element refinement term should be able to ignore the qualifier and treat the metadata value as if it were an unqualified (broader) element. While this may result in some loss of specificity, the remaining element value (without the qualifier) should continue to be generally correct and useful for discovery.

In addition to element refinements, Qualified Dublin Core includes a set of recommended encoding schemes, designed to aid in the interpretation of an element value. These schemes include controlled vocabularies and formal notations or parsing rules. A value expressed using an encoding scheme may thus be a token selected from a controlled vocabulary (for example, a term from a classification system or set of subject headings) or a string formatted in accordance with a formal notation, for example, "2000-12-31" as the ISO standard expression of a date. If an encoding scheme is not understood by an application, the value may still be useful to '''human reader'''.

'''Audience, Provenance''' and '''RightsHolder''' are elements, but not part of the Simple Dublin Core 15 elements. Use Audience, Provenance and RightsHolder only when using Qualified Dublin Core.
DCMI also maintains a small, general vocabulary recommended for use within the element Type. This vocabulary currently consists of 12 terms.&lt;ref name="registry"/&gt;

=== DCMI Metadata Terms ===
The Dublin Core Metadata Initiative (DCMI) Metadata Terms is the current set of the Dublin Core vocabulary.&lt;ref name="dublincore1"/&gt; This set includes the fifteen terms of the Dublin Core Metadata Element Set (in ''italic''), as well as the qualified terms. Each term has a unique URI in the namespace http://purl.org/dc/terms, and all are defined as [[Resource Description Framework|RDF]] properties.

{{columns-list|4|
*abstract
*accessRights
*accrualMethod
*accrualPeriodicity
*accrualPolicy
*alternative
*audience
*available
*bibliographicCitation
*conformsTo
*''contributor''
*''coverage''
*created
*''creator''
*''date''
*dateAccepted
*dateCopyrighted
*dateSubmitted
*''description''
*educationLevel
*extent
*''format''
*hasFormat
*hasPart
*hasVersion
*''identifier''
*instructionalMethod
*isFormatOf
*isPartOf
*isReferencedBy
*isReplacedBy
*isRequiredBy
*issued
*isVersionOf
*''language''
*license
*mediator
*medium
*modified
*provenance
*''publisher''
*references
*''relation''
*replaces
*requires
*''rights''
*rightsHolder
*''source''
*spatial
*''subject''
*tableOfContents
*temporal
*''title''
*''type''
*valid
}}

== Syntax ==
Syntax choices for Dublin Core metadata depends on a number of variables, and "one size fits all" prescriptions rarely apply. When considering an appropriate syntax, it is important to note that Dublin Core concepts and semantics are designed to be syntax independent and are equally applicable in a variety of contexts, as long as the metadata is in a form suitable for interpretation both by machines and by human beings.

The '''Dublin Core Abstract Model'''&lt;ref&gt;[http://dublincore.org/documents/abstract-model/ Dublin Core Abstract Model]&lt;/ref&gt; provides a reference model against which particular Dublin Core encoding guidelines can be compared, independent of any particular encoding syntax. Such a reference model allows implementers to gain a better understanding of the kinds of descriptions they are trying to encode and facilitates the development of better mappings and translations between different syntax.

== Some applications ==
One [[Document Type Definition]] based on Dublin Core is the [http://www.ibiblio.org/osrt/omf/ Open Source Metadata Framework] (OMF) specification. OMF is in turn used by [[Rarian]] (superseding [[ScrollKeeper]]), which is used by the [[GNOME]] desktop and [[KDE]] help browsers and the ScrollServer documentation server. [[PBCore]] is also based on Dublin Core. The [[Zope]] [[Zope Content Management Framework|CMF's]] Metadata products, used by the [[Plone (content management system)|Plone]], [[ERP5]], the Nuxeo CPS [[Content management system]]s, [[SimpleDL]], and [[FedoraCommons]] also implement Dublin Core. The [[EPUB]] [[e-book]] format uses Dublin Core metadata in the [[OPF (file format)|OPF file]].&lt;ref&gt;{{cite web|url=http://www.idpf.org/epub/20/spec/OPF_2.0_latest.htm#Section2.2|title=Open Packaging Format (OPF) 2.0.1 &#8211; 2.2: Publication Metadata|publisher=[[International Digital Publishing Forum]]|accessdate=12 September 2013}}&lt;/ref&gt; [[eXo Platform]] also implements Dublin Core.

DCMI also maintains a list of projects using Dublin Core&lt;ref&gt;{{cite web|url=http://dublincore.org/projects/|title=DCMI Projects - Alphabetical|publisher=DCMI|accessdate=15 March 2013}}&lt;/ref&gt; on its website.

== See also ==
* [[Metadata registry]]
* [[Metadata Object Description Schema]]
* [[Wikiversity:Digital Libraries/Metadata|Metadata from Wikiversity]]
* [[Semantic Web]]
* [[Ontology (information science)]]
* [[Open Archives Initiative]]
* [[Controlled vocabulary]]
* [[Interoperability]]
* [[Asset Description Metadata Schema]] ([http://www.w3.org/TR/vocab-adms/ ADMS]), a metadata standard maintained by the [[World Wide Web Consortium]] for describing semantic standards. Implemented on Joinup.&lt;ref&gt;{{cite web|url=https://joinup.ec.europa.eu/catalogue/all?filters=bs_current_version:true{{!}}Joinup |title=Joinup &amp;#124; Joinup |publisher=Joinup.ec.europa.eu |date=2015-10-22 |accessdate=2015-12-04}}&lt;/ref&gt; 
* [[Metadata Encoding and Transmission Standard]] (METS), maintained by the [[Library of Congress]] for the [[Digital Library Federation]]
* [[Preservation Metadata: Implementation Strategies]] (PREMIS)

=== Related software ===
* [[Dublin Core Meta Toolkit]] (Conversion of Access, MySQL, or CSV data to DublinCore metadata)
* [[Fedora (software)|Fedora]] repository architecture and Project (An open-source software system capable of implementing [[OAI-PMH]] (and thus Dublin Core).
* [[Omeka]], A free, open-source, unqualified Dublin-Core compliant web-publishing system for digital archives.
* The [http://archiviststoolkit.org/ Archivist's Toolkit] is a self-described as an "Archival Data Management system" able to work with the Dublin Core format. It will soon be merged with [[Archon (software)|Archon]], which is ambiguous as to its OAI support.
* [[ICA-AtoM]], a web-based archival description/publication software that can serve as an OAI-PMH repository and uses OAI-PMH as the main language for remote data exchange

== References ==
{{Reflist|2}}

== Further reading ==
* {{cite book |title= Organising Knowledge in a Global Society |last= Harvey  |first= Ross |authorlink= |author2=Philip Hider |year= 2004 |publisher= Charles Sturt University |location= Wagga Wagga NSW |isbn= 1-876938-66-8 |page= |pages= |url= |accessdate=}}
* [https://www.inf.unibz.it/courses/images/stories/2005_2006/Digital_Libraries/dini-less-5-6.ppt "Lecture slides about Dublin Core"], by Luca Dini, lecturer at the [[Free University of Bolzano]]

== External links ==
* [http://dublincore.org/ Dublin Core Metadata Initiative]
* [http://wiki.dublincore.org/index.php/User_Guide Dublin Core usage guide]
* [http://xml.coverpages.org/ni2005-03-21-a.html Dublin Core Metadata Initiative Publishes DCMI Abstract Model] (''Cover Pages'', March 2005)
* [http://www.loc.gov/standards/mods/v3/mods-userguide-3-0.html Metadata Object Description Schema (MODS)]
*[http://www.dublincoregenerator.com/ The Dublin Core Generator: A tool for generating Dublin Core code]
*[http://library.kr.ua/dc/dceditunie.html The Dublin Core Generator-Editor: Free tool for extracting-editing Dublin Core HTML code]

{{Semantic Web}}
{{Authority control}}

[[Category:Archival science]]
[[Category:Bibliography file formats]]
[[Category:Digital libraries]]
[[Category:Information management]]
[[Category:Interoperability]]
[[Category:ISO standards]]
[[Category:Knowledge representation]]
[[Category:Library cataloging and classification]]
[[Category:Metadata standards]]
[[Category:Museology]]
[[Category:Records management]]
[[Category:Reference models]]
[[Category:Semantic Web]]</text>
      <sha1>07wppgdn2zhp046au1rfvvyb4mddp3p</sha1>
    </revision>
  </page>
  <page>
    <title>Library classification</title>
    <ns>0</ns>
    <id>18328</id>
    <revision>
      <id>761468138</id>
      <parentid>761004176</parentid>
      <timestamp>2017-01-23T03:55:38Z</timestamp>
      <contributor>
        <ip>85.53.34.106</ip>
      </contributor>
      <comment>Fix header format.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="12928" xml:space="preserve">{{refimprove|date=March 2012}}
[[Image:HK Wan Chai Library Inside Bookcase a.jpg|thumb|A library book shelf in [[Hong Kong]] arranged using the [[Dewey Decimal Classification|Dewey classification]]]]

A '''library classification''' is a [[system]] by which library resources are arranged according to subject. Library classifications use a notational system that represents the order of topics in the classification and allows items to be stored in that order. Library classification systems group related materials together, typically arranged in a hierarchical tree structure. A different kind of classification system, called a [[faceted classification]] system, is also widely used which allows the assignment of multiple classifications to an object, enabling the classifications to be ordered in multiple ways. The library classification numbers can be considered identifiers for resources but are distinct from the [[International Standard Book Number]] (ISBN) or [[International Standard Serial Number]] (ISSN) system.

== Description ==
Library classification is an aspect of [[library and information science]]. It is distinct from [[taxonomy (general)|scientific classification]] in that it has as its goal to provide a useful ordering of documents rather than a theoretical organization of [[knowledge]].&lt;ref&gt;{{Citation
 | first =Ganesh
 | last =Bhattacharya
 | first2 =S R
 | last2 =Ranganathan
 | author2-link=S R Ranganathan
 | editor-last =Wojciechowski
 | editor-first =Jerzy A.
 | title =From knowledge classification to library classification
 | series =Ottawa Conference on the Conceptual Basis of the Classification of Knowledge, 1971
 | year =1974
 | pages =119&#8211;143
 | place =Munich
 | publisher =Verlag Dokumentation
}}&lt;/ref&gt; Although it has the practical purpose of creating a physical ordering of documents, it does generally attempt to adhere to accepted scientific knowledge.&lt;ref&gt;{{cite book
 | last = Bliss
 | first = Henry Evelyn
 | title = The organization of knowledge in libraries
 | publisher = H. W. Wilson
 | location = New Yorka
 | date = 1933
}}&lt;/ref&gt;

Library classification is distinct from the application of [[Index term|subject headings]] in that classification organizes knowledge into a systematic order, while subject headings provide access to intellectual materials through vocabulary terms that may or may not be organized as a knowledge system.&lt;ref name=chan &gt;{{Citation
 |publisher = The Scarecrow Press, Inc.
 |isbn = 9780810859449
 |title = Cataloging and classification
 |url = http://openlibrary.org/books/OL9558667M/Cataloging_and_Classification
 |author = Lois Mai Chan
 |edition = Cataloging and Classification
 |publication-date = September 28, 2007
 |id = 0810859440
 }}&lt;/ref&gt;

==History==


Library classifications were preceded by classifications used by bibliographers such as [[Conrad Gessner]]. The earliest library classification schemes organized books in broad subject categories. The increase in available printed materials made such broad classification unworkable, and more granular classifications for library materials had to be developed in the nineteenth century.&lt;ref name=shera&gt;{{cite book|last1=Shera|first1=Jesse H|title=Libraries and the organization of knowledge|date=1965|publisher=Archon Books|location=Hamden, Conn.}}&lt;/ref&gt;

Although libraries created order within their collections from as early as the fifth century B.C.,&lt;ref name=shera /&gt; the Paris Bookseller's classification, developed in 1842 by [[Jacques Charles Brunet]], is generally seen as the first of the modern book classifications. Brunet provided five major classes: theology, jurisprudence, sciences and arts, belles-lettres, and history.&lt;ref name=sayers&gt;{{cite book|last1=Sayers|first1=Berwick|title=An introduction to library classification|date=1918|publisher=H. W. Wilson|location=New York}}&lt;/ref&gt;

==Types== 
There are many standard systems of library classification in use, and many more have been proposed over the years. However, in general, classification systems can be divided into three types depending on how they are used:

* '''Universal schemes''' which cover all subjects, for example the [[Dewey Decimal Classification]], [[Universal Decimal Classification]] and [[Library of Congress Classification]]
* '''Specific classification schemes''' which cover particular subjects or types of materials, for example Iconclass, [[British Catalogue of Music Classification]], and [[Dickinson classification]], or the [[NLM Classification]] for medicine. 
* '''National schemes''' which are specially created for certain countries, for example the [[Sweden|Swedish]] library classification system, SAB (Sveriges Allm&#228;nna Biblioteksf&#246;rening).

In terms of functionality, classification systems are often described as:

*'''[[Enumeration|enumerative]]''': subject headings are listed alphabetically, with numbers assigned to each heading in alphabetical order.
*'''[[Hierarchy|hierarchical]]''': subjects are divided hierarchically, from most general to most specific.
*'''[[Faceted classification|faceted]]''' or '''analytico-synthetic''': subjects are divided into mutually exclusive orthogonal facets.

There are few completely enumerative systems or faceted systems; most systems are a blend but favouring one type or the other. The most common classification systems, LCC and DDC, are essentially enumerative, though with some hierarchical and faceted elements (more so for DDC), especially at the broadest and most general level. The first true faceted system was the [[Colon classification]] of [[S. R. Ranganathan]].

==Methods or Systems==

Classification types denote the classification or categorization according the form or characteristics or qualities of a classification scheme or schemes. Method and system has similar meaning. Method or methods or system means the classification schemes like Dewey Decimal Classification or Universal Decimal Classification. The types of classification is for identifying and understanding or education or research purposes while classification method means those classification schemes like DDC, UDC.
 
===English language universal classification systems===
The most common systems in [[English language|English]]-speaking countries are:
* [[Dewey Decimal Classification]] (DDC)
* [[Library of Congress Classification]] (LCC)
* [[Colon classification]] (CC)
* [[Universal Decimal Classification]] (UDC)

Other systems include:
* [[Harvard-Yenching Classification]], an English classification system for Chinese language materials.
* V-LIB 1.2 (2008 Vartavan Library Classification for over 700 fields of knowledge, currently sold under license in the UK by Rosecastle Ltd. (see http://rosecastle.atspace.com/index_files/VartavanLibrary.htm)).

===Non-English universal classification systems===
* A system of book classification for Chinese libraries (Liu's Classification) library classification for user
** [[New Classification Scheme for Chinese Libraries]]
* [[Nippon Decimal Classification]] (NDC)
* [[Chinese Library Classification]] (CLC)
* [[Korean Decimal Classification]] (KDC)
* Russian [[:ru:&#1041;&#1080;&#1073;&#1083;&#1080;&#1086;&#1090;&#1077;&#1095;&#1085;&#1086;-&#1073;&#1080;&#1073;&#1083;&#1080;&#1086;&#1075;&#1088;&#1072;&#1092;&#1080;&#1095;&#1077;&#1089;&#1082;&#1072;&#1103; &#1082;&#1083;&#1072;&#1089;&#1089;&#1080;&#1092;&#1080;&#1082;&#1072;&#1094;&#1080;&#1103;|Library-Bibliographical Classification]] (BBK)

===Universal classification systems that rely on synthesis (faceted systems)===
* [[Bliss bibliographic classification]]
* [[Colon classification]]
* [[Cutter Expansive Classification]]
* [[Universal Decimal Classification]]

Newer classification systems tend to use the principle of synthesis (combining codes from different lists to represent the different attributes of a work) heavily, which is comparatively lacking in LC or DDC.

==The practice of classifying==

Library classification is associated with library (descriptive) cataloging under the rubric of ''cataloging and classification'', sometimes grouped together as ''technical services''. The library professional who engages in the process of cataloging and classifying library materials is called a ''cataloger'' or ''catalog librarian''. Library classification systems are one of the two tools used to facilitate [[subject access]].  The other consists of alphabetical indexing languages such as Thesauri and Subject Headings systems.

Library classification of a piece of work consists of two steps. Firstly, the "aboutness" of the material is ascertained. Next, a call number (essentially a book's address) based on the classification system in use at the particular library will be assigned to the work using the notation of the system.

It is important to note that unlike subject heading or thesauri where multiple terms can be assigned to the same work, in library classification systems, each work can only be placed in one class. This is due to shelving purposes: A book can have only one physical place. However, in classified catalogs one may have main entries as well as added entries. Most classification systems like the [[Dewey Decimal Classification]] (DDC) and [[Library of Congress Classification]] also add a [[cutter number]] to each work which adds a code for the author of the work.

Classification systems in libraries generally play two roles. Firstly, they facilitate [[subject access]] by allowing the user to find out what works or documents the library has on a certain subject.&lt;ref&gt;{{cite web|url=http://www.iva.dk/bh/lifeboat_ko/concepts/subject_access_points.htm|title=Subject access points|work=iva.dk}}&lt;/ref&gt; Secondly, they provide a known location for the information source to be located (e.g. where it is shelved).

Until the 19th century, most libraries had closed stacks, so the library classification only served to organize the subject [[library catalog|catalog]]. In the 20th century, libraries opened their stacks to the public and started to shelve library material itself according to some library classification to simplify subject browsing.

Some classification systems are more suitable for aiding subject access, rather than for shelf location. For example, [[Universal Decimal Classification]], which uses a complicated notation of pluses and colons, is more difficult to use for the purpose of shelf arrangement but is more expressive compared to DDC in terms of showing relationships between subjects. Similarly [[faceted classification]] schemes are more difficult to use for shelf arrangement, unless the user has knowledge of the citation order.

Depending on the size of the library collection, some libraries might use classification systems solely for one purpose or the other. In extreme cases, a public library with a small collection might just use a classification system for location of resources but might not use a complicated subject classification system. Instead all resources might just be put into a couple of wide classes (travel, crime, magazines etc.). This is known as a "mark and park" classification method, more formally called reader interest classification.&lt;ref&gt;Lynch, Sarah N., and Eugene Mulero. [http://www.nytimes.com/2007/07/14/us/14dewey.html "Dewey? At This Library With a Very Different Outlook, They Don't"] ''[[The New York Times]]'', July 14, 2007.&lt;/ref&gt;

== Comparing classification systems ==
As a result of differences in notation, history, use of enumeration, hierarchy, and facets, classification systems can differ in the following ways:
* Type of Notation: Notation can be pure (consisting of only numerals, for example) or mixed (consisting of letters and numerals, or letters, numerals, and other symbols). 
* Expressiveness: This is the degree to which the notation can express relationship between concepts or structure.
* Whether they support mnemonics: For example, the number 44 in DDC notation often means it concerns some aspect of France. For example, in the Dewey classification 598.0944 concerns "Birds in France", the 09 signifies geographical division, and 44 represents France.
* Hospitality: The degree to which the system is able to accommodate new subjects.
* Brevity: The length of the notation to express the same concept.
* Speed of updates and degree of support: The better classification systems are frequently being reviewed.
* Consistency 
* Simplicity
* Usability

== See also ==&lt;!-- PLEASE RESPECT ALPHABETICAL ORDER --&gt;
{{Wikipedia books}}
* [[Attribute-value system]]
* [[Categorization]]
* [[Document classification]]
* [[Knowledge organization]]
* [[Library management]]
* [[Library of Congress Subject Headings]]

==Notes==
{{reflist}}

==References==
{{commons category|Library cataloging and classification}}
* Chan, Lois Mai. (1994)''Cataloging and Classification: An Introduction'', second ed. New York: McGraw-Hill, . ISBN 978-0-07-010506-5, ISBN 978-0-07-113253-4.

{{Library classification systems}}
{{Computable knowledge}}

{{Authority control}}

{{DEFAULTSORT:Library Classification}}
[[Category:Library cataloging and classification| ]]
[[Category:Knowledge representation]]</text>
      <sha1>pjkno57bwutemn6w9p47mo0xkbl802t</sha1>
    </revision>
  </page>
  <page>
    <title>Thesaurus</title>
    <ns>0</ns>
    <id>30334</id>
    <revision>
      <id>760342893</id>
      <parentid>742726311</parentid>
      <timestamp>2017-01-16T11:29:07Z</timestamp>
      <contributor>
        <username>Omnipaedista</username>
        <id>8524693</id>
      </contributor>
      <comment>standardized punct.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4682" xml:space="preserve">{{about|thesauri for general or literary applications|thesauri designed for information retrieval|Thesaurus (information retrieval)|the Clare Fischer album|Thesaurus (album)}}
[[File:Historical Thesaurus.jpg|thumb|''Historical Thesaurus of the Oxford English Dictionary'', two-volume set]]
In general usage, a '''thesaurus''' is a [[reference work]] that lists words grouped together according to similarity of meaning (containing [[synonyms]] and sometimes [[antonyms]]), in contrast to a [[dictionary]], which provides [[definitions]] for words, and generally lists them in alphabetical order. The main purpose of such reference works is to help the user "to find the word, or words, by which [an] idea may be most fitly and aptly expressed"&amp;nbsp;&#8211; to quote [[Peter Mark Roget]], architect of the best known thesaurus in the English language.&lt;ref name="Roget"&gt;Roget, Peter. 1852. ''Thesaurus of English Language Words and Phrases''.&lt;/ref&gt;

Although including synonyms, a thesaurus should not be taken as a complete list of all the synonyms for a particular word. The entries are also designed for drawing distinctions between similar words and assisting in choosing exactly the right word. Unlike a [[dictionary]], a thesaurus entry does not give the definition of words.

In [[library science]] and [[information science]], thesauri have been widely used to specify domain models.  Recently, thesauri have been implemented with [[Simple Knowledge Organization System]] (SKOS).{{citation needed|date=January 2016}}

== Etymology ==
The word "thesaurus" is derived from 16th-century [[New Latin]], in turn from [[Latin]] ''[[wikt:en:thesaurus#Latin|th&#275;saurus]]'', which is the [[Latinisation (literature)|Latinisation]] of the [[Ancient Greek|Greek]] {{lang|grc|[[wikt:en:&#952;&#951;&#963;&#945;&#965;&#961;&#972;&#962;#Ancient Greek|&#952;&#951;&#963;&#945;&#965;&#961;&#972;&#962;]]}} (''th&#275;sauros''), "treasure, treasury, storehouse".&lt;ref name="Harper"&gt;[http://www.etymonline.com/index.php?term=thesaurus "thesaurus"]. ''[[Online Etymology Dictionary]]''.&lt;/ref&gt; The word ''th&#275;sauros'' is of uncertain etymology. [[Douglas Harper]] derives it from the root of the Greek verb &#964;&#953;&#952;&#941;&#957;&#945;&#953; ''tithenai'', "to put, to place."&lt;ref name="Harper" /&gt; [[Robert S. P. Beekes|Robert Beekes]] rejected an [[Proto-Indo-European language|Indo-European]] derivation and suggested a [[Pre-Greek]] suffix {{nowrap|''*-ar&lt;sup&gt;w&lt;/sup&gt;o-''}}.&lt;ref&gt;[[Robert S. P. Beekes|R. S. P. Beekes]], ''Etymological Dictionary of Greek'', Brill, 2009, p. 548.&lt;/ref&gt;

From the 16th to the 19th centuries, the term "thesaurus" was applied to any [[dictionary]] or [[encyclopedia]], as in the ''[[Thesaurus linguae latinae]]'' (1532), and the ''[[Thesaurus linguae graecae]]'' (1572). The meaning "collection of words arranged according to sense" is first attested in 1852 in Roget's title and ''thesaurer'' is attested in [[Middle English]] for "[[treasurer]]".&lt;ref name="Harper" /&gt;

== History ==
[[File:Roget P M.jpg|150px|right|thumb|[[Peter Mark Roget]], author of the first thesaurus.]]
In antiquity, [[Philo of Byblos]] authored the first text that could now be called a thesaurus. In [[Sanskrit]], the [[Amarakosha]] is a thesaurus in verse form, written in the 4th century.

The first modern thesaurus was ''[[Roget's Thesaurus]]'', first compiled in 1805 by [[Peter Mark Roget]], and published in 1852. Since its publication it has never been out of print and is still a widely used work across the English-speaking world.&lt;ref&gt;http://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780199254729.001.0001/acprof-9780199254729-chapter-1&lt;/ref&gt; Entries in ''Roget's Thesaurus'' are listed conceptually rather than alphabetically.
Roget described his thesaurus in the foreword to the first edition:

&lt;blockquote&gt;It is now nearly fifty years since I first projected a system of verbal classification similar to that on which the present work is founded. Conceiving that such a compilation might help to supply my own deficiencies, I had, in the year 1805, completed a classed catalogue of words on a small scale, but on the same principle, and nearly in the same form, as the Thesaurus now published.&lt;ref&gt;Lloyd 1982, p. xix{{Full citation needed|date=August 2014}}&lt;/ref&gt;
&lt;/blockquote&gt;

== See also ==
* [[AGRIS]]
* [[Controlled vocabulary]]
* [[Knowledge Organization Systems]]
* [[Ontology (computer science)]]
* [[Simple Knowledge Organisation System]]
* [[ISO 25964]]

== References ==
{{Reflist}}

== External links ==
* {{Wiktionary-inline|thesaurus}}

{{Lexicography}}

[[Category:Thesauri| ]]
[[Category:Information science]]
[[Category:Knowledge representation]]
[[Category:Reference works]]
[[Category:Dictionaries by type]]
[[Category:Lexical semantics]]</text>
      <sha1>arnrb56ay2pva6hlfw0bnlj01byqazw</sha1>
    </revision>
  </page>
  <page>
    <title>SMW+</title>
    <ns>0</ns>
    <id>21890258</id>
    <revision>
      <id>736878305</id>
      <parentid>736878299</parentid>
      <timestamp>2016-08-30T12:33:07Z</timestamp>
      <contributor>
        <username>David.moreno72</username>
        <id>16075528</id>
      </contributor>
      <minor />
      <comment>Reverted edits by [[Special:Contributions/208.131.186.116|208.131.186.116]] ([[User talk:208.131.186.116|talk]]) ([[WP:HG|HG]]) (3.1.19)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4317" xml:space="preserve">{{Infobox software
|name= SMW+
|screenshot=[[File:Fr Smw plus screenshot.png|300px|SMW+]]
|caption=
|developer=[http://www.diqa-pm.com/en/Main_Page DIQA-Projektmanagement GmbH]
|latest release version = 1.7.0
|latest release date = {{release date|2012|04|25}}
|discontinued = yes
|operating_system= [[Cross-platform]]
|programming language=[[PHP]]
|database=[[MySQL]]
|genre=[[MediaWiki]] extension
|license=[[GNU General Public License|GPL]]
|website=[http://semanticweb.org/wiki/SMW%2B SMW+ homepage]
}}

'''SMW+''' is an [[open source]] [[Software suite|software bundle]] composed of the [[wiki software|wiki application]] [[MediaWiki]] along with a number of its extensions, that was developed by the [[Germany|German]] software company [[Ontoprise GmbH]] from 2007 to 2012. In 2012, Ontoprise GmbH filed for bankruptcy&lt;ref&gt;{{cite web |url=http://www.econo.de/no_cache/nachrichten/einzelansicht/article/ontoprise-stellt-insolvenzantrag.html|title=Ontoprise stellt Insolvenzantrag |trans_title=Ontoprise starts insolvency proceedings |language=German |date=3 May 2012 |publisher=econo |accessdate=30 July 2012}}&lt;/ref&gt; and went out of business. DIQA-Projektmanagement GmbH, a start-up founded by former Ontoprise employees,&lt;ref&gt;{{cite web |author=Michael Erdmann |title=SMW+ is dead, long live SMW+ |url=http://sourceforge.net/mailarchive/message.php?msg_id=29589354 |work=[[MediaWiki]] users mailing list |date=25 July 2012 |accessdate=30 July 2012}}&lt;/ref&gt; now offers support for the software in SMW+, though under the name "[http://diqa-pm.com/en/DataWiki DataWiki]".

== Details ==

SMW+'s extensions include, most notably, [[Semantic MediaWiki]] and the [http://semanticweb.org/wiki/Halo_Extension Halo Extension]. Cumulatively, SMW+ functions as a [[semantic wiki]], and is also meant to serve as an [[enterprise wiki]] for use within companies, for applications such as [[knowledge management]] and [[project management]].

The SMW+ platform was available in a number of formats including a Windows installer, Linux installer and [[VMware]] image.

SMW+ emerged from [[Project Halo]], a research project meant to provide a platform for collaborative knowledge engineering for [[Subject matter expert|domain experts]] in the [[biology]], [[chemistry]] and [[physics]] at the first stage.

SMW+ is used by the [[Intergovernmental Oceanographic Commission]] of [[UNESCO]] to power an online encyclopedia for [[oceanography]].{{citation needed|date=April 2013}}

== References ==
{{reflist}}
* [http://smwplus.net/index.php/Business_applications_with_SMW%2B ''Business applications with SMW+, a Semantic Enterprise Wiki'']. Michael Erdmann, Daniel Hansch.
* [http://smwplus.net/index.php/Practical_applications_of_Semantic_MediaWiki_in_commercial_environments ''Practical applications of Semantic MediaWiki in commercial environments - Case Study: semantic-based project management'']. Daniel Hansch, Hans-Peter Schnurr. Presented at the ESTC 2009.
* [http://swui.webscience.org/SWUI2008CHI/Pfisterer.pdf ''User-Centered Design and Evaluation of Interface Enhancements to the Semantic MediaWiki'']. Frederik Pfisterer, Markus Nitsche, Anthony Jameson and Catalin Barbu. Presented at the CHI2008 (Computer Human Interaction Conference).
* [http://www.academypublisher.com/jetwi/vol1/no1/jetwi01019496.pdf ''Semantic Wikis: A Comprehensible Introduction with Examples from the Health Sciences'']. [[Maged N. Kamel Boulos]]. Journal of Emerging Technologies in Web Intelligence, Vol. 1, No. 1, August 2009
* [http://smwplus.net/index.php/Towards_a_Collaborative_Semantic_Wiki-based_Approach_to_IT_Service_Management ''Towards a Collaborative Semantic Wiki-based Approach to IT Service Management'']. Frank Kleiner, Andreas Abecker. Proceedings of I-SEMANTICS &#8217;09.

==External links==
* [http://semanticweb.org/wiki/SMW%2B SMW+] on semanticweb.org
* [http://www.semanticweb.com/main/semantic_mediawiki_development_picks_up_steam_138918.asp Article at semanticweb.com about SMW+]
* [http://videolectures.net/iswc08_greaves_swfttsotsw/ "Semantic Wikis: Fusing the two strands of the Semantic Web"] - Talk given by Mark Greaves at the ISWC 2008

{{DEFAULTSORT:Smw}}
[[Category:Semantic wiki software]]
[[Category:Knowledge representation]]
[[Category:Free software programmed in PHP]]
[[Category:MediaWiki extensions]]</text>
      <sha1>a745bqbvaxaj4kc171ic8bjbrick2v2</sha1>
    </revision>
  </page>
  <page>
    <title>East Pole&#8211;West Pole divide</title>
    <ns>0</ns>
    <id>21766677</id>
    <revision>
      <id>759259404</id>
      <parentid>759042875</parentid>
      <timestamp>2017-01-10T03:50:09Z</timestamp>
      <contributor>
        <username>AnomieBOT</username>
        <id>7611264</id>
      </contributor>
      <minor />
      <comment>[[User:AnomieBOT/docs/TemplateSubster|Substing templates]]: {{Multicol-end}}. See [[User:AnomieBOT/docs/TemplateSubster]] for info.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4635" xml:space="preserve">{{redirect|West Pole|the album by The Gathering|The West Pole|the location in Texas|The West Pole, Texas}}

The '''East Pole&#8211;West Pole divide''' in the fields of [[cognitive psychology]] and [[cognitive neuroscience]] is an intellectual schism between researchers subscribing to the [[psychological nativism|nativist]] and [[empiricism|empiricist]] schools of thought.  The term arose from the fact that much of the theory and research supporting [[psychological nativism|nativism]], [[modularity of mind]], and [[computational theory of mind]] originated at several universities located on the East Coast, including [[Harvard University]], the [[University of Michigan]], [[Massachusetts Institute of Technology]], and [[Tufts University]]. Conversely, much of the research and theory supporting [[empiricism]], [[emergentism]], and [[embodied cognition]] originated at several universities located on the West Coast, including the [[University of California, Berkeley]], the [[Salk Institute]], and, most notably, the [[University of California, San Diego]].  In reality, the divide is not so clear, with many universities and scholars on both coasts (as well as the Midwest and around the world) supporting each position, as well as more moderate positions in between the two extremes.  The phrase was coined by [[Jerry Fodor]] at an [[MIT]] conference on [[cognition]], at which he referred to another researcher as a "West Coast theorist," apparently unaware that the researcher worked at [[Yale University]].&lt;ref&gt;{{cite book |title=The Blank Slate:The Modern Denial of Human Nature |last=Pinker |first=Steven |authorlink=Steven Pinker |year=2003 |publisher=Penguin |location=New York |isbn=978-0-14-200334-3 }}&lt;/ref&gt;

Very few researchers adhere strictly to the extreme positions highlighted by the East Pole&#8211;West Pole debate.  That is, there are very few empiricists who believe in the [[John Locke|Lockean]] ideal of the ''[[tabula rasa]]'' (namely, that children are born with no innate knowledge or constraints), and there are very few nativists who agree with [[Jerry Fodor|Fodor's]] assertion that all concepts that are learned over the course of life are present in the mind prior to birth.  Nevertheless, most scholars within the fields of [[cognitive science]] and [[developmental psychology]] affiliate themselves with one of the two positions through the means of their research.

The two books best known for espousing the empiricist and nativist positions within the context of cognitive psychology are ''[[Rethinking Innateness]]'' by [[Jeffrey Elman]] et al. and ''[[The Modularity of Mind]]'' by [[Jerry Fodor]], respectively.  Incidentally, the authors are affiliated with the two institutions on which the East Pole&#8211;West Pole metaphor is based, [[UCSD]] and [[MIT]], affirming the relevance and pervasiveness of this moniker for the intellectual divide.

==Notable scholars with affiliations==

{{Col-begin}}
{{Col-break}}
'''Nativists'''
*[[Jerry Fodor]], [[Massachusetts Institute of Technology]]
*[[Steven Pinker]], [[Harvard University]]
*[[Lila R. Gleitman]], [[University of Pennsylvania]]
*[[Leda Cosmides]], [[University of California, Santa Barbara]]
*[[Elizabeth Spelke]], [[Harvard University]]
*[[Thomas Bever]], [[University of Arizona]]
*[[Daniel Dennett]], [[Tufts University]]
*[[Nancy Kanwisher]], [[Massachusetts Institute of Technology]]

{{Col-break}}
'''Empiricists'''
*[[Elizabeth Bates]], [[University of California, San Diego]]
*[[George Lakoff]], [[University of California, Berkeley]]
*[[Brian MacWhinney]], [[Carnegie Mellon University]]
*[[Jeffrey Elman]], [[University of California, San Diego]]
*[[Ronald Langacker]], [[University of California, San Diego]]
*[[Dan Slobin]], [[University of California, Berkeley]]
*[[David Rumelhart]], [[Stanford University]]
*[[James McClelland (psychologist)|James McClelland]], [[Stanford University]]

{{col-end}}

==See also==
*[[Nature and nurture]]
*[[Empiricism]]
*[[Psychological nativism]]
*[[Computational theory of mind]]
*[[Embodied cognition]]
*[[Reductionism]]
*[[Emergentism]]

==References==
{{reflist}}

==External links==
*[http://query.nytimes.com/gst/fullpage.html?res=9B05E4DA1230F937A35752C1A961958260&amp;sec=&amp;spon=&amp;pagewanted=all Recipe for a Brain: Cups of genes or a dash of experience? NY Times article]
*[http://www.edge.org/3rd_culture/lakoff/lakoff_p4.html George Lakoff's discussion of the philosophical roots of embodied cognition]

{{DEFAULTSORT:East Pole-West Pole divide}}
[[Category:Cognition]]
[[Category:Cognitive science]]
[[Category:Knowledge representation]]
[[Category:Arguments in philosophy of mind]]</text>
      <sha1>h064qwb0lgg4orcsor9jijmdnk0gsjq</sha1>
    </revision>
  </page>
  <page>
    <title>Universal Decimal Classification</title>
    <ns>0</ns>
    <id>32129</id>
    <revision>
      <id>761823506</id>
      <parentid>761823485</parentid>
      <timestamp>2017-01-25T00:21:34Z</timestamp>
      <contributor>
        <username>Oshwah</username>
        <id>3174456</id>
      </contributor>
      <minor />
      <comment>Reverted edits by [[Special:Contributions/160.171.136.221|160.171.136.221]] ([[User talk:160.171.136.221|talk]]): Editing tests ([[WP:HG|HG]]) (3.1.22)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="47429" xml:space="preserve">The '''Universal Decimal Classification''' ('''UDC''') is a bibliographic and [[library classification]] developed by the [[Belgium|Belgian]] bibliographers [[Paul Otlet]] and [[Henri La Fontaine]] at the end of the 19th century. They worked with numerous subject specialists, for example, [[Herbert Haviland Field]] at the [[Concilium Bibliographicum]] for Zoology. UDC provides a systematic arrangement of all branches of human knowledge organized as a coherent system in which knowledge fields are related and inter-linked.&lt;ref name="UDC Fact Sheet"&gt;[http://www.udcc.org/index.php/site/page?view=factsheet UDC Fact Sheet], UDC Consortium website&lt;/ref&gt;&lt;ref&gt;[McIlwaine, I. C. "Universal Decimal Classification: a guide to its use. Revised ed. The Hague: UDC Consortium, 2007]&lt;/ref&gt;&lt;ref&gt;[http://www.tandfonline.com/doi/abs/10.1081/E-ELIS3-120043532 McIlwaine, I. C. (2010) Universal Decimal Classification (UDC). In: Encyclopedia of Library and Information Sciences. 3rd ed. New York: Taylor &amp; Francis, 2010. Vol. 1:1, pp. 5432-5439. DOI: 10.1081/E-ELIS3-120043532]&lt;/ref&gt;&lt;ref&gt;Broughton, V: Universal Decimal Classification - chapters 18 and 19. IN: Essential Classification. London: Facet Publishing, 2004, pp. 207-256&lt;/ref&gt;

Originally based on the [[Dewey Decimal Classification]], the UDC was developed as a new analytico-synthetic classification system with a significantly larger vocabulary and syntax that enables very detailed content indexing and information retrieval in large collections.&lt;ref name="UDC History"&gt;[http://www.udcc.org/index.php/site/page?view=about_history UDC History], "About UDC" - UDC Consortium website&lt;/ref&gt;&lt;ref&gt;[http://onlinelibrary.wiley.com/doi/10.1002/%28SICI%291097-4571%28199704%2948:4%3C331::AID-ASI6%3E3.0.CO;2-X/abstract McIlwaine, I. C. (1997) The Universal Decimal Classification: Some factors concerning its origins, development, and influence. Journal of the American Society for Information Science, 48 (4), pp. 331&#8211;339]&lt;/ref&gt; In its first edition in 1905, the UDC already included many features that were revolutionary in the context of knowledge classifications: tables of generally applicable (aspect-free) concepts&#8212;called common auxiliary tables; a series of special auxiliary tables with specific but re-usable attributes in a particular field of knowledge; an expressive notational system with connecting symbols and syntax rules to enable coordination of subjects and the creation of a documentation language proper. Albeit originally designed as an indexing and retrieval system, due to its logical structure and scalability, UDC has become one of the most widely used knowledge organization systems in libraries, where it is used for either shelf arrangement, content indexing or both.&lt;ref&gt;[http://hdl.handle.net/10150/105685 Slavic, A. (2004) UDC implementation: from library shelves to a structured indexing language. International Cataloguing and Bibliographic Control , 33 3(2004), 60-65.]&lt;/ref&gt; UDC codes can describe any type of document or object to any desired level of detail. These can include textual documents and other media such as [[film]]s, [[video]] and [[sound]] recordings, [[illustration]]s, [[map]]s as well as [[realia (library science)|realia]] such as [[museum]] objects.

Since the first edition in French "Manuel du R&#233;pertoire bibliographique universel" (1905), UDC has been translated and published in various editions in 40 languages.&lt;ref&gt;[http://www.udcc.org/index.php/site/page?view=editions UDC Editions], UDC Consortium website&lt;/ref&gt;&lt;ref&gt;[http://hdl.handle.net/10150/106363 Slavic, A. (2004) UDC Translations: a 2004 Survey Report and Bibliography. Extensions &amp; Corrections to the UDC, 26 (2004): 58-80. ]&lt;/ref&gt; UDC Summary, an abridged Web version of the scheme is available in over 50 languages.&lt;ref name="UDCS" /&gt; The classification has been modified and extended over the years to cope with increasing output in all areas of human knowledge, and is still under continuous review to take account of new developments.&lt;ref&gt;[http://www.udcc.org/index.php/site/page?view=major_revisions Major Revisions of the UDC 1993-2013], UDC Consortium website&lt;/ref&gt;&lt;ref&gt;[http://hdl.handle.net/10150/105220 Slavic, A., Cordeiro, M. I. &amp; Riesthuis, G. (2008) Maintenance of the Universal Decimal Classification: overview of the past and preparations for the future. International Cataloguing and Bibliographic Control, 37 (2), 23-29.]&lt;/ref&gt;

== The application of UDC ==

UDC is used in around 150,000 libraries in 130 countries and in many bibliographical services which require detailed content indexing. In a number of countries it is the main classification system for information exchange and is used in all type of libraries: public, school, academic and special libraries.&lt;ref&gt;[http://hdl.handle.net/10150/105579 Slavic, A. (2008) Use of the Universal Decimal Classification: a worldwide survey. Journal of Documentation, 64 (2), 2008: 211-228. ]&lt;/ref&gt;&lt;ref name="UDC Users Worldwide"&gt;[http://www.udcc.org/index.php/site/page?view=users_worldwide  UDC Users Worldwide], UDC Consortium website&lt;/ref&gt;&lt;ref name="UDC Countries"&gt;[http://www.udcc.org/countries.htm UDC Countries], UDC Consortium website&lt;/ref&gt;

UDC is also used in national bibliographies of around 30 countries. Examples of large databases indexed by UDC include:&lt;ref name="Large collections"&gt;[http://www.udcc.org/index.php/site/page?view=collections Collections indexed by UDC], UDC Consortium website&lt;/ref&gt; 
: NEBIS (The Network of Libraries and Information Centers in Switzerland) &#8211; 2.6 million records
: COBIB.SI (Slovenian National Union Catalogue) &#8211; 3.5 million records
: Hungarian National Union Catalogue (MOKKA) &#8211; 2.9 million records
: [[VINITI RAS]] database (All-Russian Scientific and Technical Information Institute of Russian Academy of Science) with 28 million records
: Meteorological &amp; Geoastrophysical Abstracts (MGA) with 600 journal titles
: PORBASE (Portuguese National Bibliography) with 1.5 million records

UDC has traditionally been used for the indexing of scientific articles which was an important source of information of scientific output in the period predating electronic publishing. Collections of research articles in many countries covering decades of scientific output contain UDC codes.  Examples of journal articles indexed by UDC:
:UDC code '''663.12:57.06''' in the article "Yeast Systematics: from Phenotype to Genotype" in the  journal ''Food Technology and Biotechnology'' ({{ISSN|1330-9862}})&lt;ref&gt;[http://www.ftb.pbf.hr/index.php/ftb/article/viewFile/243/241 Example: Journal article indexed by UDC] ({{ISSN|1330-9862}})&lt;/ref&gt;
:UDC code '''37.037:796.56''', provided in the article "The game method as means of interface of technical-tactical and psychological preparation in sports orienteering" in the Russian journal "''Pedagogico-psychological and medico-biological problems of the physical culture and sport''"  ({{ISSN|2070-4798}}).&lt;ref&gt;[http://www.kamgifk.ru/magazin/20_%283%29_2011/20_%283%29_2011_16.pdf Example: Journal article indexed by UDC] ({{ISSN|2070-4798}})&lt;/ref&gt;
:UDC code '''621.715:621.924:539.3''' in the article Residual Stress in Shot-Peened Sheets of AIMg4.5Mn Alloy - in the journal ''Materials and technology'' ({{ISSN|1580-2949}}).&lt;ref&gt;[http://www.docstoc.com/docs/5320753/UDK-Pregledni-znanstveni-lanek-ISSN-MTAEC-M-MI-OVI Example: Journal article indexed by UDC] ({{ISSN|1580-2949}})&lt;/ref&gt;
:
The design of UDC lends itself to machine readability, and the system has been used both with early automatic mechanical sorting devices, and modern library [[OPAC]]s.&lt;ref&gt;[http://hdl.handle.net/10150/105346 Slavic, A. (2006) The level of exploitation of Universal Decimal Classification in library OPACs: a pilot study. Vjesnik bibliotekara Hrvatske, 49(3-4):155-182]&lt;/ref&gt;&lt;ref&gt;[http://hdl.handle.net/10150/105276 Slavic, A. (2006) UDC in subject gateways: experiment or opportunity? Knowledge Organization, 33 2, 67-85.]&lt;/ref&gt; From 1993, a standard version of UDC is maintained and is distributed in a [[database]] format: UDC Master Reference File (UDC MRF) which is updated and released annually.&lt;ref name="UDC MRF"&gt;[http://www.udcc.org/index.php/site/page?view=mrf UDC Master Reference File], UDC Consortium website&lt;/ref&gt; The 2011 version of the MRF (released in 2012) contains over 70,000 classes.&lt;ref name="UDC Fact Sheet"/&gt; In the past full printed editions used to have around 220,000 subdivisions.&lt;ref name="UDCS"&gt;{{cite web
| url         = http://www.udcc.org/udcsummary/php/index.php
| title       = Multilingual Universal Decimal Classification Summary (UDCC Publication No. 088)
| year        = 2012
| work        = Multilingual Universal Decimal Classification Summary
| publisher   = UDC Consortium
| accessdate  = 2012-03-25
| quote = ''Multilingual UDC Summary (2012). Multilingual Universal Decimal Classification Summary. Web resource, v. 1.1. The Hague: UDC Consortium (UDCC Publication No. 088). Available at: http://www.udcc.org/udcsummary/php/index.php ''
}}&lt;/ref&gt;

== UDC structure ==

=== Notation ===
A notation is a code commonly used in classification schemes to represent a class, i.e. a subject and its position in the hierarchy, to enable mechanical sorting and filing of subjects. UDC uses [[Arabic numerals]] arranged decimally. Every number is thought of as a decimal fraction with the initial decimal point omitted, which determines the filing order. An advantage of decimal notational systems is that they are infinitely extensible, and when new subdivisions are introduced, they need not disturb the existing allocation of numbers. For ease of reading, a UDC notation is usually punctuated after every third digit:
{| 
|- 
| style="width:18%; font-weight:bold;" | Notation || style="font-weight:bold;" | Caption (Class description)
|-
| 539.120 ||Theoretical problems of elementary particles physics. Theories and models of fundamental interactions
|-
| 539.120.2 || Symmetries of quantum physics
|-
|539.120.22 ||  Conservation laws
|-
| 539.120.222 ||   Translations. Rotations
|-
| 539.120.224 ||   Reflection in time and space
|-
|539.120.226 ||   Space-time symmetries
|-
| 539.120.23 ||  Internal symmetries
|-
| 539.120.3 || Currents
|-
| 539.120.4 || Unified field theories
|-
|539.120.5 || Strings
|}

In UDC the notation has two features that make the scheme easier to browse and work with: 
* '''hierarchically expressive''' &#8211; the longer the notation, the more specific the class: removing the final digit automatically produces a broader class code.
* '''syntactically expressive''' &#8211; when UDC codes are combined, the sequence of digits is interrupted by a precise type of punctuation sign which indicates that the expression is a combination of classes rather than a simple class e.g. the colon in 34:32 indicates that there are two distinct notational elements: 34 Law. Jurisprudence and 32 Politics; the closing and opening parentheses and double quotes in the following code 913(574.22)"19"(084.3) indicate four separate notational elements: 913 Regional geography, (574.22) North Kazakhstan (Solt&#252;stik Qazaqstan); "19" 20th century and (084.3) Maps (document form)

=== Basic features and syntax ===
UDC is an analytico-synthetic and [[faceted classification]]. It allows an unlimited combination of attributes of a subject and relationships between subjects to be expressed. UDC codes from different tables can be combined to present various aspects of document content and form, e.g. 94(410)"19"(075) History ''(main subject)'' of United Kingdom ''(place)'' in 20th century ''(time)'', a textbook ''(document form)''. Or: 37:2 Relationship between Education and Religion. Complex UDC expressions can be accurately parsed into constituent elements.

UDC is also a disciplinary classification covering the entire universe of knowledge.&lt;ref name="UDC Subject Coverage"&gt;[http://www.udcc.org/index.php/site/page?view=subject_coverage UDC Subject Coverage], UDC Consortium website&lt;/ref&gt; This type of classification can also be described as ''aspect'' or ''perspective'', which means that concepts are subsumed and placed under the field in which they are studied. Thus, the same concept can appear in different fields of knowledge. This particular feature is usually implemented in UDC by re-using the same concept in various combinations with the main subject, e.g. a code for language in common auxiliaries of language is used to derive numbers for ethnic grouping, individual languages in linguistics and individual literatures. Or, a code from the auxiliaries of place, e.g. ''(410) United Kingdom'', uniquely representing the concept of United Kingdom can be used to express ''911(410) Regional geography of United Kingdom'' and ''94(410) History of United Kingdom''.

=== Organization of classes ===

Concepts are organized in two kinds of tables in UDC:&lt;ref name="UDC Structure"&gt;[http://www.udcc.org/index.php/site/page?view=about_structure UDC Structure and Tables], UDC Consortium website&lt;/ref&gt;

*'''Common auxiliary tables''' (including certain auxiliary signs). These tables contain facets of concepts representing, general recurrent characteristics, applicable over a range of subjects throughout the main tables, including notions such as place, language of the text and physical form of the document, which may occur in almost any subject. UDC numbers from these tables, called common auxiliaries are simply added at the end of the number for the subject taken from the main tables. There are over 15,000 of common auxiliaries in UDC.
*'''The main tables or main schedules''' containing the various disciplines and branches of knowledge, arranged in 9 main classes,  numbered from 0 to 9 (with class 4 being vacant). At the beginning of each class there are also series of special auxiliaries, which express aspects that are recurrent within this specific class. Main tables in UDC contain more than 60,000 subdivisions.

==== Main classes ====
*0 [[Science]] and [[Knowledge]]. [[Organization]]. [[Computer Science]]. [[Information Science]]. [[Documentation]]. [[Librarianship]]. [[Institutions]]. [[Publications]]
*1 [[Outline of philosophy|Philosophy]]. [[Outline of psychology|Psychology]]
*2 [[Outline of religion|Religion]]. [[Outline of theology|Theology]]
*3 [[Outline of social science|Social Sciences]]
*4 ''vacant''
*5 [[Outline of mathematics|Mathematics]]. [[Outline of natural science|Natural Sciences]]
*6 [[Outline of applied science|Applied Sciences]]. [[Outline of medicine|Medicine]], [[Outline of technology|Technology]]
*7 [[The arts|The Arts]]. [[Outline of entertainment|Entertainment]]. [[Outline of sports|Sport]]
*8 [[Outline of linguistics|Linguistics]]. [[Outline of literature|Literature]]
*9 [[Outline of geography|Geography]]. [[Outline of history|History]]

The vacant class 4 is the result of a planned schedule expansion. This class was freed by moving linguistics into class 8 in the 1960s to make space for future developments in the rapidly expanding fields of knowledge; primarily natural sciences and technology.

==== Common auxiliary tables ====
''Common auxiliaries'' are aspect-free concepts that can be used in combination with any other UDC code from the main classes or with other common auxiliaries. They have unique notational representations that makes them stand out in complex expressions. Common auxiliary numbers always begin with a certain symbol known as a facet indicator, e.g. &lt;nowiki&gt;=&lt;/nowiki&gt; (equal sign) always introduces concepts representing the language of a document; (0...) numbers enclosed in parentheses starting with zero always represent a concept designating document form. Thus (075) Textbook and =111 English can be combined to express, e.g.(075)=111 Textbooks in English, and when combined with numbers from the main UDC tables they can be used as follows: 2(075)=111 Religion textbooks in English, 51(075)=111 Mathematics textbooks in English etc.

*=...	Common auxiliaries of language. Table 1c
*(0...)	Common auxiliaries of form. Table 1d
*(1/9)	Common auxiliaries of place. Table 1e
*(=...)	Common auxiliaries of human ancestry, ethnic grouping and nationality. Table 1f
*"..."	Common auxiliaries of time. Table 1g helps to make minute division of time e.g.:  "1993-1996''
*-0...	Common auxiliaries of general characteristics: Properties, Materials, Relations/Processes and Persons. Table 1k.
*-02	Common auxiliaries of properties. Table 1k
*-03	Common auxiliaries of materials. Table 1k
*-04	Common auxiliaries of relations, processes and operations. Table 1k
*-05	Common auxiliaries of persons and personal characteristics. Table 1k this table is repeated

==== Connecting signs ====
In order to preserve the precise meaning and enable accurate parsing of complex UDC expressions, a number of connecting symbols are made available to relate and extend UDC numbers. These are:
{| class="wikitable"
!Symbol !! Symbol name !! Meaning !! Example
|-
|&lt;nowiki&gt;+&lt;/nowiki&gt; || [[Plus and minus signs|plus]] || coordination, addition || e.g. 59+636 [[zoology]] and [[animal breeding]]
|-
|&lt;nowiki&gt;/&lt;/nowiki&gt; || [[Slash (punctuation)|stroke]] || consecutive extension || e.g. 592/599 Systematic zoology (everything from 592 to 599 inclusive)
|-
|&lt;nowiki&gt;:&lt;/nowiki&gt; || [[Colon (punctuation)|colon]] || relation || e.g. 17:7  Relation of [[ethics]] to [[art]]
|-
|&lt;nowiki&gt;[ ]&lt;/nowiki&gt; || square [[bracket]]s || subgrouping || e.g. 311:[622+669](485) [[statistics]] of [[mining]] and [[metallurgy]] in [[Sweden]] (the auxiliary qualifiers 622+669 considered as a unit)
|-
|&lt;nowiki&gt;*&lt;/nowiki&gt; || asterisk || Introduces non-UDC notation  || e.g. 523.4*433 Planetology, minor planet Eros (IAU authorized number after the asterisk)
|-
|&lt;nowiki&gt;A/Z&lt;/nowiki&gt; || alphabetical extension || Direct alphabetical specification  || e.g. 821.133.1MOL French literature, works of Moli&#232;re
|}

== UDC outline ==

&lt;small&gt;UDC classes in this outline are taken from the Multilingual Universal Decimal Classification Summary (UDCC Publication No. 088) released by the UDC Consortium under the Creative Commons Attribution Share Alike 3.0 license (first release 2009, subsequent update 2012).&lt;ref name="UDCS" /&gt;&lt;/small&gt;

=== Main tables ===

====0 [[Outline of science|Science]] and [[Outline of knowledge|knowledge]]. Organization. [[Outline of computer science|Computer science]]. Information. Documentation. Librarianship. Institution. Publications====

  00          Prolegomena. Fundamentals of knowledge and culture. Propaedeutics
  001         [[Outline of science|Science]] and [[Outline of knowledge|knowledge]] in general. Organization of intellectual work
  002         Documentation. Books. Writings. Authorship
  003         Writing systems and scripts
  004         [[Outline of computer science|Computer science]] and technology. Computing
  004.2       Computer architecture
  004.3       Computer hardware
  004.4       [[Software]]
  004.5       Human-computer interaction
  004.6       Data
  004.7       Computer communication
  004.8       [[Outline of artificial intelligence|Artificial intelligence]]
  004.9       Application-oriented computer-based techniques
  005         [[Outline of business management|Management]]
  005.1       Management Theory
  005.2       Management agents. Mechanisms. Measures
  005.3       Management activities
  005.5       Management operations. Direction
  005.6       Quality management. Total quality management (TQM)
  005.7       Organizational management (OM)
  005.9       Fields of management
  005.92      Records management
  005.93      Plant management. Physical resources management
  005.94      Knowledge management
  005.95/.96  Personnel management. Human Resources management
  006         Standardization of products, operations, weights, measures and time
  007         Activity and organizing. Information. Communication and control theory generally (cybernetics)
  008         Civilization. [[Outline of culture|Culture]]. Progress   
  01          Bibliography and bibliographies. Catalogues
  02          Librarianship
  030         General reference works (as subject)
  050         Serial publications, periodicals (as subject)
  06          Organizations of a general nature
  069         Museums
  070         Newspapers (as subject). The Press. Outline of [[journalism]]
  08          Polygraphies. Collective works (as subject)
  09          Manuscripts. Rare and remarkable works (as subject)

====1 [[Outline of philosophy|Philosophy]]. [[Outline of psychology|Psychology]]====
  101        Nature and role of philosophy
  11         Metaphysics
  111        General metaphysics. Ontology
  122/129    Special Metaphysics
  13         Philosophy of mind and spirit. Metaphysics of spiritual life
  14         Philosophical systems and points of view
  159.9      [[Outline of psychology|Psychology]]
  159.91     Psychophysiology (physiological psychology). Mental physiology
  159.92     Mental development and capacity. Comparative psychology
  159.93     Sensation. Sensory perception
  159.94     Executive functions
  159.95     Higher mental processes
  159.96     Special mental states and processes
  159.97     Abnormal psychology
  159.98     Applied psychology (psychotechnology) in general
  16         [[Outline of logic|Logic]]. [[Outline of epistemology|Epistemology]]. Theory of knowledge. Methodology of logic
  17         Moral philosophy. [[Outline of ethics|Ethics]]. Practical philosophy

====2 [[Outline of religion|Religion]]. [[Outline of theology|Theology]]====

&lt;small&gt;The UDC tables for religion are fully faceted. Indicated in italics below, are special auxiliary numbers that can be used to express attributes (facets) of any specific faith. Any special number can be combined with any religion e.g.  ''-5 Worship'' can be used to express e.g. ''26-5 Worship in Judaism'', ''27-5 Worship in Christianity'', ''24-5 Worship in Buddhism''. The complete special auxiliary tables contain around 2000 subdivisions of various attributes that can be attached to express various aspects of individual faiths to a great level of specificity allowing equal level of detail for every religion.&lt;/small&gt;
  ''2-1/-9	Special auxiliary subdivision for religion''
  ''2-1	Theory and philosophy of religion. Nature of religion. Phenomenon of religion''
  ''2-2	Evidences of religion''
  ''2-3	Persons in religion''
  ''2-4	Religious activities. Religious practice''
  ''2-5	Worship broadly. Cult. Rites and ceremonies''
  ''2-6	Processes in religion''
  ''2-7	Religious organization and administration''
  ''2-8	Religions characterised by various properties''
  ''2-9	History of the faith, religion, denomination or church''
  21/29	Religious systems. Religions and faiths
  21	Prehistoric and primitive religions
  22	Religions originating in the Far East
  23	Religions originating in Indian sub-continent. Hindu religion in the broad sense
  24	[[Outline of Buddhism|Buddhism]]
  25	Religions of antiquity. Minor cults and religions
  26	[[Outline of Judaism|Judaism]]
  27	[[Outline of Christianity|Christianity]]
  28	[[Outline of Islam|Islam]]
  29	Modern spiritual movements

====3 [[Outline of social science|Social sciences]]====
  303   Methods of the social sciences
  304	Social questions. Social practice. Cultural practice. Way of life (Lebensweise)
  305	Gender studies
  308	Sociography. Descriptive studies of society (both qualitative and quantitative)
  311	[[Outline of statistics|Statistics]] as a science. Statistical theory
  314/316 [[Outline of society|Society]]
  314	Demography. Population studies
  316	[[Outline of sociology|Sociology]]
  32	[[Outline of politics|Politics]]
  33	[[Outline of economics|Economics]]. Economic science
  34	[[Outline of law|Law]]. Jurisprudence
  35	Public administration. Government. Military affairs
  36	Safeguarding the mental and material necessities of life
  37	[[Outline of education|Education]]
  39	Cultural anthropology. Ethnography. Customs. Manners. Traditions. Way of life

====4 Vacant====

This section is currently vacant.

====5 [[Outline of mathematics|Mathematics]]. [[Outline of natural science|Natural sciences]]====
  502/504  Environmental science. Conservation of natural resources. Threats to the environment and protection against them
  502	The environment and its protection
  504	Threats to the environment
  51	[[Outline of mathematics|Mathematics]]
  510	Fundamental and general considerations of mathematics
  511	Number theory
  512	[[Outline of algebra|Algebra]]
  514	[[Outline of geometry|Geometry]]
  517	Analysis
  519.1	Combinatorial analysis. Graph theory
  519.2	[[Outline of probability|Probability]]. Mathematical statistics
  519.6	Computational mathematics. [[Outline of numerical analysis|Numerical analysis]]
  519.7	Mathematical cybernetics
  519.8	Operational research (OR): mathematical theories and methods
  52	[[Outline of astronomy|Astronomy]]. Astrophysics. [[Outline of space exploration|Space research]]. Geodesy
  53	[[Outline of physics|Physics]]
  531/534  Mechanics
  535	Optics
  536	Heat. Thermodynamics. Statistical physics
  537	Electricity. Magnetism. Electromagnetism
  538.9	Condensed matter physics. Solid state physics
  539	Physical nature of matter
  54	[[Outline of chemistry|Chemistry]]. Crystallography. Mineralogy
  542	Practical laboratory chemistry. Preparative and experimental chemistry
  543	Analytical chemistry
  544	Physical chemistry
  546	Inorganic chemistry
  547	[[Outline of organic chemistry|Organic chemistry]]
  548/549 Mineralogical sciences. Crystallography. Mineralogy
  55	[[Outline of earth science|Earth sciences]]. Geological sciences
  56	Paleontology
  57	Biological sciences in general
  58	[[Outline of botany|Botany]]
  59	[[Outline of zoology|Zoology]]

====6 [[Outline of applied science|Applied sciences]]. [[Outline of medicine|Medicine]]. [[Outline of technology|Technology]]====

&lt;small&gt;Class 6 occupies the largest proportion of UDC schedules. It contains over 44,000 subdivisions. Each specific field of technology or industry usually contains more than one special auxiliary table with concepts needed to express operations, processes, materials and products. As a result, UDC codes are often created through the combination of various attributes. Equally, some parts of this class enumerate concepts to a great level of detail e.g.  ''621.882.212 Hexagon screws with additional shapes. Including: Flank screws. Collar screws. Cap screws''
&lt;/small&gt;

  60    [[Outline of biotechnology|Biotechnology]]
  61	Medical sciences
  611/612 Human biology
  613	Hygiene generally. Personal health and hygiene
  614	Public health and hygiene. Accident prevention
  615	Pharmacology. Therapeutics. Toxicology
  616	Pathology. Clinical medicine
  617	Surgery. Orthopaedics. Ophthalmology
  618	Gynaecology. Obstetrics
  62	[[Outline of engineering|Engineering]]. [[Outline of technology|Technology]] in general
  620	Materials testing. Commercial materials. Power stations. Economics of energy
  621	Mechanical engineering in general. Nuclear technology. Electrical engineering. Machinery
  622	[[Outline of mining|Mining]]
  623	Military engineering
  624	Civil and structural engineering in general
  625	Civil engineering of land transport. Railway engineering. Highway engineering
  626/627  Hydraulic engineering and construction. Water (aquatic) structures
  629	Transport vehicle engineering
  63	[[Outline of agriculture|Agriculture]] and related sciences and techniques. Forestry. Farming. Wildlife exploitation
  630	Forestry
  631/635	Farm management. Agronomy. Horticulture
  633/635	Horticulture in general. Specific crops
  636	Animal husbandry and breeding in general. Livestock rearing. Breeding of domestic animals
  64	Home economics. Domestic science. Housekeeping
  65	Communication and transport industries. Accountancy. Business management. Public relations
  654	Telecommunication and telecontrol (organization, services)
  655	Graphic industries. Printing. Publishing. Book trade
  656	Transport and postal services. Traffic organization and control
  657	Accountancy
  658	[[Outline of business management|Business management]], administration. Commercial organization
  659	Publicity. Information work. [[Outline of public relations|Public relations]]
  66	Chemical technology. Chemical and related industries
  67	Various industries, trades and crafts
  68	Industries, crafts and trades for finished or assembled articles
  69	Building ([[Outline of construction|construction]]) trade. Building materials. Building practice and procedure

====7 The arts. Recreation. [[Outline of entertainment|Entertainment]]. [[Outline of sports|Sport]]====
  ''7.01/.09	Special auxiliary subdivision for the arts''
  ''7.01	Theory and philosophy of art. Principles of design, proportion, optical effect''
  ''7.02	Art technique. Craftsmanship''
  ''7.03	Artistic periods and phases. Schools, styles, influences''
  ''7.04	Subjects for artistic representation. Iconography. Iconology''
  ''7.05	Applications of art (in industry, trade, the home, everyday life)''
  ''7.06	Various questions concerning art''
  ''7.07	Occupations and activities associated with the arts and entertainment''
  ''7.08	Characteristic features, forms, combinations etc. (in art, entertainment and sport)''
  ''7.091	Performance, presentation (in original medium)''
  71	Physical planning. Regional, town and country planning. Landscapes, parks, gardens
  72	[[Outline of architecture|Architecture]]
  73	Plastic arts
  74	[[Outline of drawing and drawings|Drawing]]. [[Outline of design|Design]]. [[Outline of crafts|Applied arts and crafts]]
  745/749	Industrial and domestic arts and crafts. Applied arts
  75	[[Outline of painting|Painting]]
  76	Graphic art, printmaking. Graphics
  77	[[Outline of photography|Photography]] and similar processes
  78	[[Outline of music|Music]]
  79	Recreation. [[Outline of entertainment|Entertainment]]. [[Outline of games|Games]]. [[Outline of sports|Sport]]
  791	Cinema. [[Outline of film|Films]] (motion pictures)
  792	[[Outline of theatre|Theatre]]. [[Outline of stagecraft|Stagecraft]]. Dramatic performances
  793	Social entertainments and recreations. Art of movement. [[Outline of dance|Dance]]
  794	Board and table games (of thought, skill and chance)
  796	[[Outline of sports|Sport]]. [[Outline of games|Games]]. [[Outline of exercise|Physical exercises]]
  797	Water sports. Aerial sports
  798	Riding and driving. Horse and other animal sports
  799	Sport fishing. Sport hunting. Shooting and target sports

====8 Language. [[Outline of linguistics|Linguistics]]. [[Outline of literature|Literature]]====

&lt;small&gt;Tables for class 8 are fully faceted and details are expressed through combination with common auxiliaries of language (Table 1c) and a series of special auxiliary tables to indicate other facets or attributes in Linguistics or Literature. As a result, this class allows for great specificity in indexing although the schedules themselves occupy very little space in UDC. The subdivisions of e.g. ''811 Languages'' or ''821 Literature'' are derived from common auxiliaries of language =1/=9 (Table 1c) by substituting a point for the equals sign, e.g. 811.111 English language (as a subject of a linguistic study) and ''821.111 English literature'' derives from ''=111 English language''. Common auxiliaries of place and time are also frequently used in this class to express place and time facets of Linguistics or Literature, e.g. ''821.111(71)"18" English literature of Canada in 19th century''
&lt;/small&gt;
  80	General questions relating to both linguistics and literature. Philology
  801	Prosody. Auxiliary sciences and sources of philology
  808	Rhetoric. The effective use of language  
  '''81	[[Outline of linguistics|Linguistics]] and languages'''
  ''81`1/`4	Special auxiliary subdivision for subject fields and facets of linguistics and languages''
 '' 81`1	General linguistics''
  ''81`2	[[Outline of semiotics|Theory of signs]]. Theory of translation. Standardization. Usage. Geographical linguistics''
  ''81`3	Mathematical and applied linguistics. Phonetics. Graphemics. Grammar. Semantics. Stylistics''
  ''81`4	Text linguistics, Discourse analysis. Typological linguistics''
  ''81`42	Text linguistics. Discourse analysis''
  ''81`44	Typological linguistics''
  811	Languages
        &lt;small&gt;Derived from the common auxiliaries of language =1/=9 (Table 1c) by replacing the equal sign = with prefix ''811.'' e.g. ''=111 English'' becomes ''811.111 Linguistics of English language''&lt;/small&gt;
  811.1/.9	All languages natural or artificial
  811.1/.8	Individual natural languages
  811.1/.2	Indo-European languages
  811.21/.22	Indo-Iranian languages
  811.3	Dead languages of unknown affiliation. Caucasian languages
  811.4	Afro-Asiatic, Nilo-Saharan, Congo-Kordofanian, Khoisan languages
  811.5	Ural-Altaic, Palaeo-Siberian, Eskimo-Aleut, Dravidian and Sino-Tibetan languages. Japanese. Korean. Ainu
  811.6	Austro-Asiatic languages. Austronesian languages
  811.7	Indo-Pacific (non-Austronesian) languages. Australian languages
  811.8	American indigenous languages
  811.9	Artificial languages
  '''82	[[Outline of literature|Literature]]'''
  ''82-1/-9	Special auxiliary subdivision for literary forms, genres''
  ''82-1	[[Outline of poetry|Poetry]]. Poems. Verse''
  ''82-2	Drama. Plays''
  ''82-3	[[Outline of fiction|Fiction]]. Prose narrative''
  ''82-31	Novels. Full-length stories''
  ''82-32	Short stories. Novellas''
  ''82-4	Essays''
  ''82-5	Oratory. Speeches''
  ''82-6	Letters. Art of letter-writing. Correspondence. Genuine letters''
  ''82-7	Prose satire. Humour, epigram, parody''
  ''82-8	Miscellanea. Polygraphies. Selections''
  ''82-9	Various other literary forms''
  ''82-92	Periodical literature. Writings in serials, journals, reviews''
  ''82-94	History as literary genre. Historical writing. Historiography. Chronicles. Annals. Memoirs''
  ''82.02/.09	Special auxiliary subdivision for theory, study and technique of literature''
  ''82.02	Literary schools, trends and movements''
  ''82.09	Literary criticism. Literary studies''
  ''82.091	Comparative literary studies. Comparative literature''
  821	Literatures of individual languages and language families
        &lt;small&gt;Derived from the common auxiliaries of language =1/=9 (Table 1c) by replacing the equal sign = with prefix ''821.'' e.g. ''=111 English'' becomes ''821.111 English literature''&lt;/small&gt;

====9 [[Outline of geography|Geography]]. Biography. [[Outline of history|History]]====

&lt;small&gt;Tables for Geography and History in UDC are fully faceted and place, time and ethnic grouping facets are expressed through combination with common auxiliaries of place (Table 1d), ethnic grouping (Table 1f) and time (Table 1g)
&lt;/small&gt;

  902/908	Archaeology. Prehistory. Cultural remains. Area studies
  902	[[Outline of archaeology|Archaeology]]
  903	Prehistory. Prehistoric remains, artefacts, antiquities
  904	Cultural remains of historical times
  908	Area studies. Study of a locality
  91	[[Outline of geography|Geography]]. Exploration of the Earth and of individual countries. Travel. [[Outline of geography#Regional geography|Regional geography]]
  910	General questions. Geography as a science. Exploration. Travel
  911	General geography. Science of geographical factors (systematic geography). Theoretical geography
  911.2	[[Outline of geography#Physical geography|Physical geography]]
  911.3	[[Outline of geography#Human geography|Human geography]] (cultural geography). Geography of cultural factors
  911.5/.9	Theoretical geography
  912	Nonliterary, nontextual representations of a region
  913	[[Outline of geography#Regional geography|Regional geography]]
  92	Biographical studies. Genealogy. Heraldry. Flags
  929	Biographical studies
  929.5	Genealogy
  929.6	Heraldry
  929.7	Nobility. Titles. Peerage
  929.9	Flags. Standards. Banners
  93/94	[[Outline of history|History]]
  930	Science of history. Historiography
  930.1	History as a science
  930.2	Methodology of history. Ancillary historical sciences
  930.25	Archivistics. Archives (including public and other records)
  930.85	History of civilization. Cultural history
  94	General

=== Common auxiliary tables ===

====Common auxiliaries of language. Table 1c====
  =1/=9	Languages (natural and artificial)
  =1/=8	Natural languages
  =1/=2	Indo-European languages
  =1	Indo-European languages of Europe
  =11	Germanic languages
  =12	Italic languages
  =13	Romance languages
  =14	Greek (Hellenic)
  =15	Celtic languages
  =16	Slavic languages
  =17	Baltic languages
  =18	Albanian
  =19	Armenian
  =2	Indo-Iranian, Nuristani (Kafiri) and dead Indo-European languages
  =21/=22	Indo-Iranian languages
  =21	Indic languages
  =22	Iranian languages
  =29	Dead Indo-European languages (not listed elsewhere)
  =3	Dead languages of unknown affiliation. Caucasian languages
  =34	Dead languages of unknown affiliation, spoken in the Mediterranean and Near East (except Semitic)
  =35	Caucasian languages
  =4	Afro-Asiatic, Nilo-Saharan, Congo-Kordofanian, Khoisan languages
  =41	Afro-Asiatic (Hamito-Semitic) languages
  =42	Nilo-Saharan languages
  =43	Congo-Kordofanian (Niger-Kordofanian) languages
  =45	Khoisan languages
  =5	Ural-Altaic, Palaeo-Siberian, Eskimo-Aleut, Dravidian and Sino-Tibetan languages. Japanese. Korean. Ainu
  =51	Ural-Altaic languages
  =521	Japanese
  =531	Korean
  =541	Ainu
  =55	Palaeo-Siberian languages
  =56	Eskimo-Aleut languages
  =58	Sino-Tibetan languages
  =6	Austro-Asiatic languages. Austronesian languages
  =61	Austro-Asiatic languages
  =62	Austronesian languages
  =7	Indo-Pacific (non-Austronesian) languages. Australian languages
  =71	Indo-Pacific (non-Austronesian) languages
  =72	Australian languages
  =8	American indigenous languages
  =81	Indigenous languages of Canada, USA and Northern-Central Mexico
  =82	Indigenous languages of western North American Coast, Mexico and Yucat&#225;n
  =84/=88	Central and South American indigenous languages
  =84	Ge-Pano-Carib languages. Macro-Chibchan languages
  =85	Andean languages. Equatorial languages
  =86	Chaco languages. Patagonian and Fuegian languages
  =88	Isolated, unclassified Central and South American indigenous languages
  =9	Artificial languages
  =92	Artificial languages for use among human beings. International auxiliary languages (interlanguages)
  =93	Artificial languages used to instruct machines. Programming languages. Computer languages

====(0...) Common auxiliaries of form. Table 1d====
  ''(0.02/.08)	Special auxiliary subdivision for document form''
  ''(0.02)	Documents according to physical, external form''
  ''(0.03)	Documents according to method of production''
  ''(0.032)	Handwritten documents (autograph, holograph copies). Manuscripts. Pictorial documents (drawings, paintings)''
  ''(0.034)	Machine-readable documents''
  ''(0.04)	Documents according to stage of production''
  ''(0.05)	Documents for particular kinds of user''
  ''(0.06)	Documents according to level of presentation and availability''
  ''(0.07)	Supplementary matter issued with a document''
  ''(0.08)	Separately issued supplements or parts of documents''
  (01)	Bibliographies
  (02)	Books in general
  (03)	Reference works
  (04)	Non-serial separates. Separata
  (041)	Pamphlets. Brochures
  (042)	Addresses. Lectures. Speeches
  (043)	Theses. Dissertations
  (044)	Personal documents. Correspondence. Letters. Circulars
  (045)	Articles in serials, collections etc. Contributions
  (046)	Newspaper articles
  (047)	Reports. Notices. Bulletins
  (048)	Bibliographic descriptions. Abstracts. Summaries. Surveys
  (049)	Other non-serial separates
  (05)	Serial publications. Periodicals
  (06)	Documents relating to societies, associations, organizations
  (07)	Documents for instruction, teaching, study, training
  (08)	Collected and polygraphic works. Forms. Lists. Illustrations. Business publications
  (09)	Presentation in historical form. Legal and historical sources
  (091)	Presentation in chronological, historical form. Historical presentation in the strict sense
  (092)	Biographical presentation
  (093)	Historical sources
  (094)	Legal sources. Legal documents

====(1/9) Common auxiliaries of place. Table 1e====
  (1)	Place and space in general. Localization. Orientation
  ''(1-0/-9)	Special auxiliary subdivision for boundaries and spatial forms of various kinds''
  ''(1-0)	Zones''
  ''(1-1)	Orientation. Points of the compass. Relative position''
  ''(1-11)	East. Eastern''
  ''(1-13)	South. Southern''
  ''(1-14)	South-west. South-western''
  ''(1-15)	West. Western''
  ''(1-17)	North. Northern''
  ''(1-19)	Relative location, direction and orientation''
  ''(1-2)	Lowest administrative units. Localities''
  ''(1-5)	Dependent or semi-dependent territories''
  ''(1-6)	States or groupings of states from various points of view''
  ''(1-7)	Places and areas according to privacy, publicness and other special features''
  ''(1-8)	Location. Source. Transit. Destination''
  ''(1-9)	Regionalization according to specialized points of view''
  (100)	Universal as to place. International. All countries in general
  (2)	Physiographic designation
  (20)	Ecosphere
  (21)	Surface of the Earth in general. Land areas in particular. Natural zones and regions
  (23)	Above sea level. Surface relief. Above ground generally. Mountains
  (24)	Below sea level. Underground. Subterranean
  (25)	Natural flat ground (at, above or below sea level). The ground in its natural condition, cultivated or inhabited
  (26)	Oceans, seas and interconnections
  (28)	Inland waters
  (29)	The world according to physiographic features
  (3)	Places of the ancient and mediaeval world
  (31)	Ancient China and Japan
  (32)	[[Outline of ancient Egypt|Ancient Egypt]]
  (33)	Ancient Roman Province of Judaea. The Holy Land. Region of the Israelites
  (34)	[[Outline of ancient India|Ancient India]]
  (35)	Medo-Persia
  (36)	Regions of the so-called barbarians
  (37)	Italia. [[Outline of ancient Rome|Ancient Rome]] and Italy
  (38)	[[Outline of ancient Greece|Ancient Greece]]
  (399)	Other regions. Ancient geographical divisions other than those of classical antiquity
  (4/9)	Countries and places of the modern world
  (4)	[[Outline of Europe|Europe]]
  (5)	[[Outline of Asia|Asia]]
  (6)	[[Outline of Africa|Africa]]
  (7)	[[Outline of North America|North]] and Central America
  (8)	[[Outline of South America|South America]]
  (9)	States and regions of the South Pacific and [[Outline of Australia|Australia]]. Arctic. Antarctic

====(=...) Common auxiliaries of human ancestry, ethnic grouping and nationality. Table 1f====

&lt;small&gt;''They are derived mainly from the common auxiliaries of language =... (Table 1c) and so may also usefully distinguish linguistic-cultural groups, e.g. =111 English is used to represent (=111) English speaking peoples''&lt;/small&gt;

  (=01)	Human ancestry groups
  (=011)	European Continental Ancestry Group
  (=012)	Asian Continental Ancestry Group
  (=013)	African Continental Ancestry Group
  (=014)	Oceanic Ancestry Group
  (=017)	American Native Continental Ancestry Group
  (=1/=8)	Linguistic-cultural groups, ethnic groups, peoples ['''derived from Table 1c''']
  (=1:1/9)	Peoples associated with particular places
                ''e.g. (=111:71) Anglophone population of Canada''

===="..." Common auxiliaries of time. Table 1g====
  "0/2"	Dates and ranges of time (CE or AD) in conventional Christian (Gregorian) reckoning
  "0"	First millennium CE
  "1"	Second millennium CE
  "2"	Third millennium CE
  "3/7"	Time divisions other than dates in Christian (Gregorian) reckoning
  "3"	Conventional time divisions and subdivisions: numbered, named, etc.
  "4"	Duration. Time-span. Period. Term. Ages and age-groups
  "5"	Periodicity. Frequency. Recurrence at specified intervals.
  "6"	Geological, archaeological and cultural time divisions
  "61/62" Geological time division
  "63"	Archaeological, prehistoric, protohistoric periods and ages
  "67/69" Time reckonings: universal, secular, non-Christian religious
  "67"	Universal time reckoning. Before Present
  "68"	Secular time reckonings other than universal and the Christian (Gregorian) calendar
  "69"	Dates and time units in non-Christian (non-Gregorian) religious time reckonings
  "7"	Phenomena in time. Phenomenology of time

====-0 Common auxiliaries of general characteristics. Table 1k====
  '''-02	Common auxiliaries of properties'''
  -021	Properties of existence
  -022	Properties of magnitude, degree, quantity, number, temporal values, dimension, size
  -023	Properties of shape
  -024	Properties of structure. Properties of position
  -025	Properties of arrangement
  -026	Properties of action and movement
  -027	Operational properties
  -028	Properties of style and presentation
  -029	Properties derived from other main classes
  '''-03 Common auxiliaries of materials'''
  -032	Naturally occurring mineral materials
  -033	Manufactured mineral-based materials
  -034	Metals
  -035	Materials of mainly organic origin
  -036	Macromolecular materials. Rubbers and plastics
  -037	Textiles. Fibres. Yarns. Fabrics. Cloth
  -039	Other materials
  '''-04 Common auxiliaries of relations, processes and operations'''
  -042	Phase relations
  -043	General processes
  -043.8/.9 Processes of existence
  -045	Processes related to position, arrangement, movement, physical properties, states of matter
  -047/-049	General operations and activities
  '''-05 Common auxiliaries of persons and personal characteristics'''
  -051	Persons as agents, doers, practitioners (studying, making, serving etc.)
  -052	Persons as targets, clients, users (studied, served etc.)
  -053	Persons according to age or age-groups
  -054	Persons according to ethnic characteristics, nationality, citizenship etc.
  -055	Persons according to gender and kinship
  -056	Persons according to constitution, health, disposition, hereditary or other traits
  -057	Persons according to occupation, work, livelihood, education
  -058	Persons according to social class, civil status

==See also==
'''Special classifications based on or used in combination with UDC'''
*[http://www.spri.cam.ac.uk/library/overview.html#classification Universal Decimal Classification for Use in Polar Libraries - Scott Polar Research Institute, Cambridge] 
*[[Lonclass|BBC LonClass]]
*[http://iufro.forintek.ca/GFDCDefault.aspx Global Forest Decimal Classification]

'''Other faceted classifications:'''
*[[Bliss bibliographic classification]]
*[[Colon classification]]
*[http://www.ucl.ac.uk/fatks/bso/ Broad System of Ordering]

'''Other general bibliographic classifications'''
*[[Dewey Decimal Classification]]
*[[Library of Congress Classification]]
* Russian Library-Bibliographical Classification (BBK) 
*[[Chinese Library Classification]]
*[[Harvard-Yenching Classification]]

==References==
{{reflist}}

==External links==
{{wikidata property|P1190}}
*[http://www.udcc.org/ Universal Decimal Classification Consortium]
**[http://www.udcc.org/about.htm About Universal Decimal Classification]
**[http://www.udcc.org/udcsummary/php/index.php Multilingual UDC Summary]
**[http://udcdata.info/ UDC Linked Data]

{{Library classification systems}}

{{Authority control}}

[[Category:Classification systems]]
[[Category:Library cataloging and classification]]
[[Category:Controlled vocabularies]]
[[Category:Knowledge representation]]</text>
      <sha1>1etgs0x13semlmevf7qkdu553rpc3om</sha1>
    </revision>
  </page>
  <page>
    <title>Frame (artificial intelligence)</title>
    <ns>0</ns>
    <id>9924067</id>
    <revision>
      <id>738214685</id>
      <parentid>714198912</parentid>
      <timestamp>2016-09-07T16:24:20Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <minor />
      <comment>capitalization</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5462" xml:space="preserve">'''Frames''' were proposed by [[Marvin Minsky]] in his 1974 article "A Framework for Representing Knowledge." A frame is an [[artificial intelligence]] [[data structure]] used to divide [[knowledge]] into substructures by representing "[[stereotype]]d situations." Frames are the primary data structure used in artificial intelligence [[frame language]]s. 

Frames are also an extensive part of [[knowledge representation and reasoning]] schemes. Frames were originally derived from semantic networks and are therefore part of structure based knowledge representations. According to Russell and Norvig's "Artificial Intelligence, A Modern Approach," structural representations assemble "...facts about particular object and even types and arrange the types into a large taxonomic hierarchy analogous to a biological taxonomy."

== Frame structure ==

The frame contains information on how to use the frame, what to expect next, and what to do when these expectations are not met. Some information in the frame is generally unchanged while other information, stored in "terminals,"{{clarify|date=January 2010|reason=Which type of 'terminal' is meant?}} usually change. Different frames may share the same terminals.

Each piece of information about a particular frame is held in a slot. The information can contain:

* Facts or Data
** Values (called facets)
* Procedures (also called procedural attachments)
** IF-NEEDED : deferred evaluation
** IF-ADDED : updates linked information
* Default Values
** For Data
** For Procedures
* Other Frames or Subframes

== Features and advantages ==

A frame's terminals are already filled with default values, which is based on how the human mind works. For example, when a person is told "a boy kicks a ball," most people will visualize a particular ball (such as a familiar [[soccer ball]]) rather than imagining some abstract ball with no attributes.

One particular strength of frame based knowledge representations is that, unlike semantic networks, they allow for exceptions in particular
instances. This gives frames an amount of flexibility that allow representations of real world phenomena to be reflected more accurately.

Like [[semantic networks]], frames can be queried using spreading activation. Following the rules of inheritance, any value given to a slot that is inherited by subframes will be updated (IF-ADDED) to the corresponding slots in the subframes and any new instances of a particular frame will feature that new value as the default.

Because frames are structurally based, it is possible to generate a semantic network given a set of frames even though it lacks explicit arcs. The reference to Minsky's teacher [[Noam Chomsky]] and his [[generative grammar]] of 1950 is generally missing in Minsky's publications. However, the semantic strength is originated by that concept. 

The simplified structures of frames allow for easy analogical reasoning, a much prized feature in any intelligent agent. The procedural attachments provided by frames also allow a degree of flexibility that makes for a more realistic representation and gives a natural affordance for programming applications.

== Example ==

Worth noticing here is the easy analogical reasoning (comparison) that can be done between a boy and a monkey just by having similarly named slots.

Also notice that Alex, an instance of a boy, inherits default values like "Sex" from the more general parent object Boy,
but the boy may also have different instance values in the form of exceptions such as the number of legs.

{| class="wikitable"
|-
! Slot !! Value !! Type
|-
| ALEX  || _ || (This Frame)
|-
| NAME || Alex || (key value)
|-
| ISA || Boy || (parent frame)
|-
| SEX || Male || (inheritance value)
|-
| AGE || IF-NEEDED: Subtract(current,BIRTHDATE); || (procedural attachment)
|-
| HOME || 100 Main St. || (instance value)
|-
| BIRTHDATE || 8/4/2000 || (instance value)
|-
| FAVORITE_FOOD || Spaghetti || (instance value)
|-
| CLIMBS || Trees || (instance value)
|-
| BODY_TYPE || Wiry || (instance value)
|-
| NUM_LEGS || 1 || (exception)
|}

{| class="wikitable"
|-
! Slot !! Value !! Type
|-
| BOY  || _ || (This Frame)
|-
| ISA || Person || (parent frame)
|-
| SEX || Male || (instance value)
|-
| AGE || Under 12 yrs. || (procedural attachment - sets constraint)
|-
| HOME || A Place || (frame)
|-
| NUM_LEGS || Default = 2 || (default, inherited from Person frame)
|}

{| class="wikitable"
|-
! Slot !! Value !! Type
|-
| MONKEY  || _ || (This Frame)
|-
| ISA || Primate || (parent frame)
|-
| SEX || OneOf(Male,Female) || (procedural attachment)
|-
| AGE || an integer || (procedural attachment - sets constraint)
|-
| HABITAT || Default = Jungle || (default)
|-
| FAVORITE_FOOD || Default = Bananas || (default)
|-
| CLIMBS || Trees || _ 
|-
| BODY_TYPE || Default = Wiry || (default)
|-
| NUM_LEGS || Default = 2 || (default)
|}

== See also ==
* [[Frame language]]
* [[Frame problem]]

== References ==
Russell, Stuart J.; Norvig, Peter (2010), Artificial Intelligence: A Modern Approach (2nd ed.), Upper Saddle River, New Jersey: Prentice Hall, ISBN 0-13-604259-7, http://aima.cs.berkeley.edu/ , chpt. 1

== External links ==
* [http://web.media.mit.edu/~minsky/papers/Frames/frames.html Minsky's "A Framework for Representing Knowledge"]
* [http://aima.cs.berkeley.edu/ Artificial Intelligence: A Modern Approach Website]

[[Category:Knowledge representation]]
[[Category:History of artificial intelligence]]</text>
      <sha1>s06rzlmj9rqnrmrfk27y48rw8cpwnt4</sha1>
    </revision>
  </page>
  <page>
    <title>Flex expert system</title>
    <ns>0</ns>
    <id>43757720</id>
    <revision>
      <id>720949065</id>
      <parentid>684958194</parentid>
      <timestamp>2016-05-18T22:22:51Z</timestamp>
      <contributor>
        <username>BG19bot</username>
        <id>14508071</id>
      </contributor>
      <minor />
      <comment>Remove blank line(s) between list items per [[WP:LISTGAP]] to fix an accessibility issue for users of [[screen reader]]s. Do [[WP:GENFIXES]] and cleanup if needed. Discuss this at [[Wikipedia talk:WikiProject Accessibility#LISTGAP]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5039" xml:space="preserve">{{multiple issues|
{{COI|date=September 2014}}
{{notability|Products|date=September 2014}}
{{third-party|date=September 2014}}
}}
Flex is a hybrid expert system toolkit developed by [[Logic Programming Associates|LPA]] which incorporates [[Frame language|frame-based]] reasoning with inheritance, [[rule-based programming]] and data-driven procedures.

Flex supports both forwards and backward chaining, and they can be interleaved.

Flex provides its own English-like Knowledge Specification Language, KSL, which helps ensure that knowledge-bases are readable by domain experts. Flex KSL can now be generated automatically for certain classes of problems from [[VisiRule]].

Flex is implemented in, and has access to, [[Prolog]]. As opposed to most expert system shells, which tend to be constrained, Flex is an open toolkit.

Flex has proved very popular in education and was licensed to the Open University as part of T396: 'Artificial intelligence for technology'.

Much of the material for this course is described by Prof Adrian Hopgood in his book: Intelligent Systems for Engineers and Scientists, Third Edition, and on his web-site.&lt;ref name = "AI Toolkit"&gt;{{citation |url=http://www.adrianhopgood.com/aitoolkit/aitoolkit.shtml | title=AI toolkit: support site for Intelligent Systems for Engineers and Scientists, Third Edition by Adrian Hopgood}}&lt;/ref&gt;

There is also a Flex tutorial on the LPA web-site.&lt;ref name = "Flex Tutorial"&gt;{{citation |url=http://www.lpa.co.uk/ftp/5000/flx_tut.pdf | title=Flex Tutorial by Clive Spenser on LPA web-site}}&lt;/ref&gt;

Flex has been used to power AllerGenius, an expert system specifically developed by leading allergologists to help interpret the results of modern in vitro allergy tests such as the ImmunoCAP ISAC. These tests can typically measure specific antibodies to more than 100 allergen components from more than 50 pre-selected allergen sources and require a lot of expert interpretation.&lt;ref name = "Allergenius"&gt;{{citation |url=http://www.allergenius.it/new/index.php/en/general-conceps/2-non-categorizzato/139-the-structure-of-the-exper-system | title=Allergenius web site}}&lt;/ref&gt;

==External links==
*[http://www.generation5.org/content/2001/prg04.asp "Introduction to Flex/KSL (Part I)", James Mathhews, Generation5]
*[http://www.lpa.co.uk/flx_det.htm Flex Technical Details], LPA
*[http://www.lpa.co.uk/wfs_dem.htm WebFlex demos], LPA
*[http://www.intbis.com/intbis_pages/train.php Flex Training], IbIS
*[http://dl.acm.org/citation.cfm?id=297981 "A flex-based expert system for sewage treatment works support", Dixon et al, PCAI Magazine]
*[http://www.lamsade.dauphine.fr/~tsoukias/papers/esse.pdf ESSE: An Expert System for Software Evaluation]
*[http://4c.ucc.ie/web/upload/publications/inProc/KCCP-2007%20Dokas-Nordlander-Wallace.pdf "Fuzzy Fault Tree Representation and Maintenance based on Frames and Constraint Technologies: A Case Study", Dokas, Nordlander and Wallace]
*[https://repositorium.sdum.uminho.pt/bitstream/1822/8868/1/A%20Knowledge-Based%20System%20for%20Spinning%20Management.pdf &#8220;A Knowledge-Based System for Spinning Management&#8221;]
*[http://iraj.in/up_proc/pdf/86-140412387293-96.pdf A NOVEL APPROACH FOR EXPERT SYSTEM AIDED DATACENTER DESIGN]
*[http://www.cscjournals.org/csc/manuscript/Journals/IJAE/volume1/Issue2/IJAE-10.pdf &#8220;An Expert System using A Decision Logic Charting Approach for Indian Legal Domain With specific reference to Transfer of Property Act&#8221;], N B Bilgi, Dr. R V Kulkarni &amp; C. Spenser
*[http://www.ijser.org/researchpaper%5CAN-EXPERT-SYSTEM-FOR-SEISMIC-DATA-INTERPRETATION.pdf &#8220;An expert system for Seismic data interpretation using visual and analytical tools&#8221;], Neelu Jyothi Ahuja and Parag Diwan
*[http://pubcouncil.kuniv.edu.kw/jer/files/19Nov2012102247An%20expert%20system%20machinability%20data%20bank%20%28ESMDB%29%20approach..pdf &#8220;An expert system machinability data bank&#8221;]
*[http://www.iis.sinica.edu.tw/APEC02/Program/chingyeh.pdf &#8220;Development of an Ontology-Based Portal for Digital Archive Services&#8221;], Ching-Long Yeh
*[http://orbit.dtu.dk/fedora/objects/orbit:88354/datastreams/file_7703263/content &#8220;A development process meta-model for Web based expert systems: the Web engineering point of view&#8221;], Ioannis M. Dokas and Alexandre Alapetite
*[http://www.pacis-net.org/file/1997/75.pdf &#8220;Behavioural issues in Information Systems Design&#8221;], Mike McGrath
*[http://www.slaai.lk/proc/2006/chatura.pdf &#8220;Artificial Intelligence Approach to Effective Career Guidance&#8221;], Chathra Hendahewa et al
*[http://www.icsd.aegean.gr/kkemalis/pubs/SETN_CAMES.pdf &#8220;DYNAMIC ACCESS CONTROL MANAGEMENT USING EXPERT SYSTEM TECHNOLOGY&#8221;], Prof. G. Pangalos et al
*[http://www.waojournal.org/content/7/1/15 &#8220;Allergenius, an expert system for the interpretation of allergen microarray results&#8221;], Giovanni Melioli, Clive Spenser et al

== References ==
{{Reflist}}

[[Category:Expert systems]]
[[Category:Rule engines]]
[[Category:Knowledge engineering]]
[[Category:Knowledge representation]]


{{compu-ai-stub}}</text>
      <sha1>plgb3cmsh68u251v9wdauju9ifs1mu7</sha1>
    </revision>
  </page>
  <page>
    <title>WYSIWYM (interaction technique)</title>
    <ns>0</ns>
    <id>15998635</id>
    <revision>
      <id>725284704</id>
      <parentid>724260923</parentid>
      <timestamp>2016-06-14T18:26:04Z</timestamp>
      <contributor>
        <username>Kvng</username>
        <id>910180</id>
      </contributor>
      <comment>[[WP:DEPROD]] see talk for details</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2901" xml:space="preserve">{{for|the editing paradigm|WYSIWYM}}

'''What you see is what you meant''' ('''WYSIWYM''') is a text editing [[interaction technique]] that emerged from two projects at [[University of Brighton]]. It allows users to create abstract [[knowledge representation]]s such as those required by the [[Semantic Web]] using a natural language interface. [[Natural language understanding]] (NLU) technology is not employed. Instead,  [[natural language generation]] (NLG) is used in a highly interactive manner.

The text editor accepts repeated refinement of a selected span of text as it becomes increasingly less vacuous of authored semantics. Using a mouse, a text property held in the evolving text can be further refined by a set of options derived by NLG from a built-in [[ontology]]. An invisible representation of the semantic knowledge is created which can be used for multilingual document generation, formal knowledge formation, or any other task that requires formally specified information.&lt;ref&gt;{{Cite web|url = http://oro.open.ac.uk/39116/1/Thesis_Final.pdf|title = Generating Natural Language Explanations For Entailments In Ontologies|date = 2013|accessdate = 10 November 2014|website = Open Research Online|publisher = The Open University|last = Nguyen|first = Tu}}&lt;/ref&gt;

The two projects at Brighton worked in the field of Conceptual Authoring to lay a foundation for further research and development of a Semantic Web Authoring Tool (SWAT). This tool has been further explored as a means for developing a knowledge base by those without prior experience with Controlled Natural Language tools.&lt;ref&gt;{{Cite journal|url = http://oro.open.ac.uk/40385/|title = How easy is it to learn a controlled natural language for building a knowledge base?|last = Williams|first = Sandra|date = 13 June 2014|journal = Fourth Workshop on Controlled Natural Language, 20&#8211;22 August 2014, Galway, Ireland (forthcoming), Springer International Publishing AG.|accessdate = 10 November 2014|doi = |pmid = }}&lt;/ref&gt;

==See also==
*[[Semantic markup]]
* [[Web Ontology Language|OWL [Web Ontology Language]]]
* [[WYSIWYM]]
* [[Prot&#233;g&#233; (software)]]

== References ==
&lt;references /&gt;

==External links==
* Nguyen, Tu (2013).  ''[http://oro.open.ac.uk/39116/ Generating Natural Language Explanations For Entailments In Ontologies.]  ''PhD thesis The Open University.
* [http://mcs.open.ac.uk/nlg/research/Conceptual_Authoring.html Conceptual Authoring] at Natural Language Generation group of the Open University
* [http://mcs.open.ac.uk/nlg/SWAT/ SWAT: Semantic Web Authoring Tool] research project
* [http://mcs.open.ac.uk/nlg/old_projects/wysiwym/ WYSIWYM home page]

{{DEFAULTSORT:WYSIWYM (interaction technique)}}
[[Category:Knowledge representation]]
[[Category:Natural language generation]]
[[Category:Ontology (information science)]]
[[Category:Ontology editors]]
[[Category:Semantic Web]]


{{software-stub}}</text>
      <sha1>r5b8okhr3z8mlxshjk9brtdb2x4b8z8</sha1>
    </revision>
  </page>
  <page>
    <title>Issue-Based Information System</title>
    <ns>0</ns>
    <id>17477796</id>
    <revision>
      <id>749431662</id>
      <parentid>727398955</parentid>
      <timestamp>2016-11-14T08:20:57Z</timestamp>
      <contributor>
        <username>Pintoch</username>
        <id>16990030</id>
      </contributor>
      <minor />
      <comment>/* External links */change |id={{citeseerx}} to |citeseerx= using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8029" xml:space="preserve">'''Issue-Based Information System''' (IBIS)  was invented by Werner Kunz and  [[Horst Rittel]] as an argumentation-based approach to tackling [[wicked problem]]s &amp;ndash; complex, ill-defined problems that involve multiple [[stakeholder (corporate)|stakeholders]].&lt;ref&gt;Werner, Kunz and Rittel, Horst, Issues as Elements of Information Systems, Working paper No. 131, Studiengruppe f&#252;r Systemforschung, Heidelberg, Germany, July 1970 (Reprinted May 1979)&lt;/ref&gt; 

To quote from their original paper, ''"Issue-Based Information Systems (IBIS) are meant to support coordination and [[planning]] of [[political]] decision processes. IBIS guides the identification, structuring, and settling of issues raised by problem-solving groups, and provides information pertinent to the [[discourse]]..."''. 

Subsequently, the understanding of [[planning]] and [[design]] as a process of [[argumentation]] (of the designer with himself or with others) has led to the use of IBIS as a [[design rationale]].&lt;ref&gt;Noble, Douglas and Rittel, Horst W.J.  1988, Issue-Based Information Systems for Design, Proceedings of the ACADIA `88 Conference, Association for Computer Aided Design in Architecture, University of Michigan, October 1988. Also published as Working Paper #492, Institute of Urban and Regional Development, College of Environmental Design, University of California, Berkeley. November 1988.&lt;/ref&gt;

The basic structure of IBIS is a [[Graph (discrete mathematics)|graph]]. It is therefore quite suitable to be manipulated by [[computer]].

==Overview==
The elements of IBIS are issues (or questions that need to be answered), each of which are associated with alternative positions (or possible answers).  These in turn are associated with arguments which support or object to a given position (or another argument).  In the course of the treatment of issues, new issues come up which are treated likewise.

Issue-Based Information Systems are used as a means of widening the coverage of a problem.  By encouraging a greater degree of participation, particularly in the earlier phases of the process, the designer is increasing the opportunity that difficulties of his proposed solution, unseen by him, will be discovered by others.  Since the problem observed by a designer can always be treated as merely a symptom of another higher-level problem, the argumentative approach also increases the likelihood that someone will attempt to attack the problem from this point of view.  Another desirable characteristic of the Issue-Based Information System is that it helps to make the design process &#8220;transparent.&#8221;  Transparency here refers to the ability of observers as well as participants to trace back the process of decision-making. 

IBIS is used in issue mapping,&lt;ref&gt;Okada, A., Shum, S.J.B. and Sherborne, T. (Eds.),  "Knowledge Cartography: software tools and mapping techniques,"  Springer;  2008, ISBN 978-1-84800-148-0&lt;/ref&gt;  an argument visualization technique related to [[argument mapping]]. It is also the basis of a facilitation technique called dialogue mapping.&lt;ref&gt;Conklin, J., "Dialog Mapping: Reflections on an Industrial Strength Case Study", in Visualizing Argumentation &#8211; Tools for Collaborative and Educational Sense-Making, P. Kirschner, S.J.B Shum,C.S. Carr (Eds), Springer-Verlag, London (2003)&lt;/ref&gt;

==History==
Rittel&#8217;s interest lay in the area of public policy and planning, which is also the context in which he defined [[wicked problem]]s.&lt;ref&gt;Rittel, Horst, and Melvin Webber; "Dilemmas in a General Theory of Planning," pp. 155-169, Policy Sciences, Vol. 4, Elsevier Scientific Publishing Company, Inc., Amsterdam, 1973. Reprinted in N. Cross (ed.), Developments in Design Methodology, J. Wiley &amp; Sons, Chichester, 1984, pp. 135-144&lt;/ref&gt; So it is no surprise that Rittel and Kunz envisaged IBIS as the: 

''"...type of information system meant to support the work of cooperatives like governmental or administrative agencies or committees, planning groups, etc., that are confronted with a problem complex in order to arrive at a plan for decision..."''.&lt;ref&gt;Werner, Kunz and Rittel, Horst, Issues as Elements of Information Systems, Working paper No. 131, Studiengruppe f&#252;r Systemforschung, Heidelberg, Germany, July 1970 (Reprinted May 1979)&lt;/ref&gt; 

When the paper was written, there were three manual, paper-based IBIS-type systems in use&#8212;two in government agencies and one in a university. 

A renewed interest in IBIS-type systems came about in the following decade, when advances in technology made it possible to design relatively inexpensive, computer-based IBIS-type systems. Jeff Conklin and co-workers adapted the IBIS structure for use in software engineering, creating the [[gIBIS]] (graphical IBIS) hypertext system in the late 1980s.&lt;ref&gt;Conklin, J. and Begeman, M.L., gIBIS: A hypertext tool for team design deliberation, Proceedings of the ACM conference on Hypertext, 1987&lt;/ref&gt;   Several other graphical IBIS-type systems were developed once it was realised that such systems facilitated collaborative design and problem solving.&lt;ref&gt;Shum, S.J.B.,Selvin, Albert, M.,  Sierhuis, M., Conklin, J., Haley, C. B. and Nuseibeh, B.,  Hypermedia support for argumentation-based rationale: 15 years on from gIBIS and QOC, Rationale Management in Software Engineering, Springer, 2006&lt;/ref&gt; These efforts culminated in the creation of the open source [[Compendium (software)]]  tool which supports&#8212;among other things&#8212;a graphical IBIS notation. Similar tools which do not rely on a database for storage include DRed &lt;ref&gt;http://www3.imperial.ac.uk/designengineering/tools/dred&lt;/ref&gt; and designVUE.&lt;ref&gt;http://www3.imperial.ac.uk/portal/page/portallive/designengineering/tools/designvue&lt;/ref&gt;

In recent years, there has been a renewed interest in IBIS-type systems, particularly in the context of [[sensemaking]] and collaborative problem solving in a variety of social and technical contexts. Of particular note is facilitation method called dialogue mapping which uses the IBIS notation to map out a design (or any other) dialogue as it evolves.&lt;ref&gt;Conklin, Jeff; "Dialogue Mapping: Building Shared Understanding of Wicked Problems," Wiley; 1st edition, 18 November 2005, ISBN 978-0-470-01768-5&lt;/ref&gt;

Lately, online versions of dialogue- and issue-mapping tools have appeared, for example, Glyma and bCisive (see the links below).

==See also==
{{colbegin||30em}}
*[[Argument mapping]]
*[[Collaborative software]]
*[[Compendium (software)]]
*[[Computational sociology]]
*[[Creative problem solving]]
*[[Critical thinking]]
*[[Decision making]]
*[[Design]]
*[[Design rationale]]
*[[Graph database]]
*[[Knowledge base]]
*[[Planning]]
*[[Problem solving]]
*[[Wicked problem]]
{{colend}}{{clear right}}

==References==
&lt;references /&gt;

== External links ==
* {{cite web | title = Issues as elements of information systems | citeseerx = 10.1.1.134.1741 }}
* [http://cognexus.org/issue_mapping.htm Cognexus Institute &#8211; Issue Mapping]
* [http://www.cleverworkarounds.com/2009/03/04/the-one-best-practice-to-rule-them-all-part-4/ Cleverworkarounds &#8211; the one best practice to rule them all &#8211; part 4]
* [http://eight2late.wordpress.com/2009/07/08/the-what-and-whence-of-issue-based-information-systems/ Eight to Late &#8211; The what and whence of issue-based information systems]
* [http://eight2late.wordpress.com/2009/06/25/visualising-arguments-using-issue-maps-an-example-and-some-general-comments/ Eight to Late &#8211; Visualising arguments using issue maps &#8211; an example and some general comments]
* [http://eight2late.wordpress.com/2009/04/07/issues-ideas-and-arguments-a-communication-centric-approach-to-tackling-project-complexity/ Eight to Late &#8211; Issues, Ideas and Arguments &#8211; a communication-centric approach to tackling project complexity]
* [http://bcisiveonline.com bCisive Online]
* [http://glyma.co/gettingstarted Glyma]

[[Category:Argument mapping]]
[[Category:Information systems]]
[[Category:Knowledge representation]]
[[Category:Problem structuring methods]]</text>
      <sha1>lv9f84ly0v548vi2dm2okfm23zohnrp</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Library of Congress Classification</title>
    <ns>14</ns>
    <id>1138791</id>
    <revision>
      <id>671894608</id>
      <parentid>547541476</parentid>
      <timestamp>2015-07-17T19:14:13Z</timestamp>
      <contributor>
        <username>Fgnievinski</username>
        <id>6727347</id>
      </contributor>
      <comment>added [[Category:Knowledge representation]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="169" xml:space="preserve">{{Cat main|Library of Congress Classification}}
[[Category:Library of Congress]]
[[Category:Library cataloging and classification]]
[[Category:Knowledge representation]]</text>
      <sha1>7pn5rd3bi0rt3jmz591amovw8gfwbpw</sha1>
    </revision>
  </page>
  <page>
    <title>Mind map</title>
    <ns>0</ns>
    <id>19688</id>
    <revision>
      <id>762676897</id>
      <parentid>762674991</parentid>
      <timestamp>2017-01-30T03:35:09Z</timestamp>
      <contributor>
        <username>Roxy the dog</username>
        <id>6200168</id>
      </contributor>
      <comment>Reverted to revision 759635400 by [[Special:Contributions/CAPTAIN RAJU|CAPTAIN RAJU]] ([[User talk:CAPTAIN RAJU|talk]]). ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="16116" xml:space="preserve">{{about|the visual diagram|the geographical concept|Mental mapping}}
[[File:Tennis-mindmap.png|thumb|upright=1.8|A mind map about the sport of [[tennis]]]]

A '''mind map''' is a  [[diagram]] used to visually organize information. A mind map is hierarchical and shows relationships among pieces of the whole.&lt;ref&gt;Carolyn H. Hopper, Practicing College Learning Strategies, 7th Edition, ISBN 9781305109599, Ch. 7&lt;/ref&gt; It is often created around a single concept, drawn as an image in the center of a blank page, to which associated representations of ideas such as images, words and parts of words are added. Major ideas are connected directly to the central concept, and other ideas branch out from those.

Mind maps can be drawn by hand, either as "rough notes" during a lecture, meeting or planning session, for example, or as higher quality pictures when more time is available. Mind maps are considered to be a type of [[spider diagram]].&lt;ref&gt;{{cite web|url=http://dictionary.cambridge.org/dictionary/british/mind-map?q=mind+map |title=Mind Map noun - definition in the British English Dictionary &amp; Thesaurus - Cambridge Dictionaries Online |publisher=Dictionary.cambridge.org |accessdate=2013-07-10}}&lt;/ref&gt; A similar concept in the 1970s was "idea [[sunburst chart|sun bursting]]".&lt;ref&gt;{{cite web|url=http://www.mind-mapping.org/mindmapping-learning-study-memory/who-invented-mind-mapping.html |title=Who invented mind mapping |publisher=Mind-mapping.org |accessdate=2013-07-10}}&lt;/ref&gt;

== Origins ==

Although the term "mind map" was first popularized by British [[popular psychology]] author and television personality [[Tony Buzan]], the use of diagrams that visually "map" information using branching and [[Radial tree|radial maps]] traces back centuries. These pictorial methods record knowledge and model systems, and have a long history in learning, [[brainstorming]], [[memory]], [[visual thinking]], and [[problem solving]] by educators, engineers, psychologists, and others. Some of the earliest examples of such graphical records were developed by [[Porphyry of Tyros]], a noted thinker of the 3rd century, as he graphically visualized the concept [[Categories (Aristotle)|categories of Aristotle]]. Philosopher [[Ramon Llull]] (1235&#8211;1315) also used such techniques.

The [[semantic network]] was developed in the late 1950s as a theory to understand human learning and developed further by [[Allan M. Collins]] and [[M. Ross Quillian]] during the early 1960s. Mind maps are similar in radial structure to [[concept map]]s, developed by learning experts in the 1970s, but differ in that the former are simplified by focusing around a single central key concept.

== Popularisation of the term "mind map" ==

Buzan's specific approach, and the introduction of the term "mind map" arose during a 1974 BBC TV series he hosted, called ''Use Your Head''.&lt;ref&gt;{{cite web|url=http://www.mind-mapping.org/blog/mapping-history/roots-of-visual-mapping/ |title=Roots of visual mapping - The mind-mapping.org Blog |publisher=Mind-mapping.org |date=2004-05-23 |accessdate=2013-07-10}}&lt;/ref&gt;&lt;ref&gt;Buzan, Tony 1974. Use your head. London: BBC Books.&lt;/ref&gt; In this show, and companion book series, Buzan promoted his conception of radial tree, diagramming key words in a colorful, radiant, tree-like structure.&lt;ref&gt;[http://www.knowledgeboard.com/item/2980 Buzan claims mind mapping his invention in interview.] ''KnowledgeBoard'' retrieved Jan. 2010.&lt;/ref&gt;

Buzan says the idea was inspired by [[Alfred Korzybski]]'s [[general semantics]] as popularized in science fiction novels, such as those of [[Robert A. Heinlein]] and [[A. E. van Vogt]]. He argues that while "traditional" outlines force readers to scan left to right and top to bottom, readers actually tend to scan the entire page in a non-linear fashion. Buzan's treatment also uses then-popular assumptions about the functions of [[cerebral hemispheres]] in order to explain the claimed increased effectiveness of mind mapping over other forms of note making.

==Mind map guidelines==
Buzan suggests the following guidelines for creating mind maps:

# Start in the center with an image of the topic, using at least 3 colors.
# Use images, symbols, codes, and dimensions throughout your mind map.
# Select key words and print using upper or lower case letters.
# Each word/image is best alone and sitting on its own line.
# The lines should be connected, starting from the central image. The lines become thinner as they radiate out from the center.
# Make the lines the same length as the word/image they support.
# Use multiple colors throughout the mind map, for visual stimulation and also for encoding or grouping.
# Develop your own personal style of mind mapping.
# Use emphasis and show associations in your mind map.
# Keep the mind map clear by using radial hierarchy or outlines to embrace your branches.

== Uses ==

[[File:Mindmap.gif|thumb|Rough mindmap notes taken during a course session]]
As with other diagramming tools, mind maps can be used to [[generation|generate]], [[creative visualization|visualize]], [[structure]], and [[taxonomic classification|classify]] ideas, and as an aid to [[study skills|studying]]&lt;ref&gt;'Mind maps as active learning tools', by Willis, CL. Journal of computing sciences in colleges. {{ISSN|1937-4771}}. 2006. Volume:
21 Issue: 4&lt;/ref&gt; and [[organization|organizing]] information, [[problem solving|solving problems]], [[decision making|making decisions]], and writing.

Mind maps have many applications in personal, family, [[education]]al, and [[business]] situations, including [[notetaking]], brainstorming (wherein ideas are inserted into the map radially around the center node, without the implicit prioritization that comes from hierarchy or sequential arrangements, and wherein grouping and organizing is reserved for later stages), summarizing, as a [[mnemonic technique]], or to sort out a complicated idea. Mind maps are also promoted as a way to collaborate in color pen creativity sessions.

In addition to these direct use cases, data retrieved from mind maps can be used to enhance several other applications; for instance [[expert system|expert search systems]], [[search engine]]s and search and tag query recommender.&lt;ref name=Beel2009&gt;{{Cite journal| first=J&#246;ran | last=Beel | first2=Bela| last2=Gipp | first3=Jan-Olaf |last3= Stiller | contribution=Information Retrieval On Mind Maps - What Could It Be Good For? | contribution-url=http://www.sciplore.org/publications_en.php | title=Proceedings of the 5th International Conference on Collaborative Computing: Networking, Applications and Worksharing (CollaborateCom'09) | year=2009 | publisher=IEEE | place=Washington | postscript=. --&gt;}}&lt;/ref&gt; To do so, mind maps can be analysed with classic methods of [[information retrieval]] to classify a mind map's author or documents that are linked from within the mind map.&lt;ref name=Beel2009 /&gt;

==Differences from other visualizations==

* '''Concept maps''' - Mind maps differ from [[concept maps]] in that mind maps focus on ''only'' one word or idea, whereas concept maps connect multiple words or ideas. Also, concept maps typically have text labels on their connecting lines/arms. Mind maps are based on radial hierarchies and [[tree structure]]s denoting relationships with a central governing concept, whereas concept maps are based on connections between concepts in more diverse patterns.  However, either can be part of a larger [[personal knowledge base]] system.
* '''Modelling graphs''' - There is no rigorous right or wrong with mind maps, relying on the arbitrariness of [[mnemonic]] systems. A [[UML diagram]] or a [[semantic network]] has structured elements modelling relationships, with lines connecting objects to indicate relationship. This is generally done in black and white with a clear and agreed iconography. Mind maps serve a different purpose: they help with memory and organization. Mind maps are collections of words structured by the mental context of the author with visual mnemonics, and, through the use of colour, icons and visual links, are informal and necessary to the proper functioning of the mind map.

==Research==

'''Effectiveness''' - Cunningham (2005) conducted a user study in which 80% of the students thought "mindmapping helped them understand concepts and ideas in science".&lt;ref name="Cunningham05"&gt;{{cite thesis| type=Ph.D.| author={G}lennis {E}dge {C}unningham| title=Mindmapping: Its Effects on Student Achievement in High School Biology| year=2005| publisher=The University of Texas at Austin| accessdate=1 November 2013}}&lt;/ref&gt; Other studies also report positive effects through the use of mind maps.&lt;ref name="Holland2004"&gt;{{cite journal| author={B}rian {H}olland, {L}ynda {H}olland, {J}enny {D}avies| title=An investigation into the concept of mind mapping and the use of mind mapping software to support and improve student academic performance| year=2004| accessdate=1 November 2013}}&lt;/ref&gt;&lt;ref name="Antoni2006"&gt;{{cite journal| author=D'Antoni, A.V., Zipp, G.P.| title=Applications of the Mind Map Learning Technique in Chiropractic Education: A Pilot Study and Literature| year=2006| accessdate=1 November 2013}}&lt;/ref&gt; Farrand, Hussain, and Hennessy (2002) found that [[spider diagram]]s (similar to concept maps) had limited, but significant, impact on memory recall in undergraduate students (a 10% increase over baseline for a 600-word text only) as compared to preferred study methods (a 6% increase over baseline).&lt;ref name= Farrand2002&gt;{{cite journal |author=Farrand, P. |author2=Hussain, F. |author3=Hennessy, E.  |year=2002 |title=The efficacy of the mind map study technique |journal=Medical Education |volume=36 |issue=5 |pages=426&#8211;431 |url=http://www3.interscience.wiley.com/journal/118952400/abstract |accessdate=2009-02-16 |doi=10.1046/j.1365-2923.2002.01205.x |pmid=12028392}}&lt;/ref&gt; This improvement was only robust after a week for those in the diagram group and there was a significant decrease in motivation compared to the subjects' preferred methods of note taking. A meta study about [[concept map]]ping concluded that concept mapping is more effective than "reading text passages, attending lectures, and participating in class discussions".&lt;ref name="Nesbit06"&gt;{{cite journal| author={N}esbit, {J}.{C}., {A}desope, {O}.{O}.| title=Learning with concept and knowledge maps: A meta-analysis| journal=Review of Educational Research| year=2006| volume=76| number=3| pages=413| publisher=Sage Publications| doi=10.3102/00346543076003413}}&lt;/ref&gt; The same study also concluded that concept mapping is slightly more effective "than other constructive activities such as writing summaries and outlines". In addition, they concluded that low-ability students may benefit more from mind mapping than high-ability students.

'''Features of Mind Maps''' - Beel &amp; Langer (2011) conducted a comprehensive analysis of the content of mind maps.&lt;ref name="Beel2011d"&gt;{{cite book| author={J}oeran {B}eel, {S}tefan {L}anger| chapter=An Exploratory Analysis of Mind Maps| title=Proceedings of the 11th ACM Symposium on Document Engineering (DocEng'11)| year=2011| publisher=ACM| url=http://docear.org/papers/An%20Exploratory%20Analysis%20of%20Mind%20Maps%20--%20preprint.pdf | accessdate=1 November 2013}}&lt;/ref&gt; They analysed 19,379 mind maps from 11,179 users of the mind mapping applications [[SciPlore MindMapping]] (now [[Docear]]) and [[MindMeister]]. Results include that average users create only a few mind maps (mean=2.7), average mind maps are rather small (31 nodes) with each node containing about 3 words (median). However, there were exceptions. One user created more than 200 mind maps, the largest mind map consisted of more than 50,000 nodes and the largest node contained ~7500 words. The study also showed that between different mind mapping applications ([[Docear]] vs [[MindMeister]]) significant differences exist related to how users create mind maps.

'''Automatic Creating of Mind Maps''' - There have been some attempts to create mind maps automatically. Brucks &amp; Schommer created mind maps automatically from full-text streams.&lt;ref name="Brucks2008"&gt;{{cite journal| author={C}laudine {B}rucks, {C}hristoph {S}chommer| title=Assembling Actor-based Mind-Maps from Text Stream| journal=CoRR| year=2008| volume=abs/0810.4616| accessdate=1 November 2013}}&lt;/ref&gt; Rothenberger et al. extracted the main story of a text and presented it as mind map.&lt;ref name="Rothenberger2008"&gt;{{cite journal| author=Rothenberger, T, Oez, S, Tahirovic, E, Schommer, Christoph| title=Figuring out Actors in Text Streams: Using Collocations to establish Incremental Mind-maps| journal= | arxiv=0803.2856| year=2008}}&lt;/ref&gt; And there is a patent about automatically creating sub-topics in mind maps.&lt;ref name="Plotkin09"&gt;{{cite journal|year=2009|title=Software tool for creating outlines and mind maps that generates subtopics automatically|journal=USPTO Application: 20090119584|volume=|author={R}obert {P}lotkin|accessdate=1 November 2013}}&lt;/ref&gt;

'''Pen and Paper vs Computer''' - There are two studies that analyze whether electronic mind mapping or pen based mind mapping is more effective.&lt;ref name="Mahler09"&gt;{{cite book| author={M}ahler, {T}., {W}eber, {M}.| chapter=Dimian-Direct Manipulation and Interaction in Pen Based Mind Mapping| title=Proceedings of the 17th World Congress on Ergonomics, IEA 2009| year=2009| accessdate=1 November 2013}}&lt;/ref&gt;&lt;ref name="Shih09"&gt;{{cite journal| author={S}hih, {P}.{C}., {N}guyen, {D}.{H}., {H}irano, {S}.{H}. and {R}edmiles, {D}.{F}., {H}ayes, {G}.{R}.| title=Groupmind: supporting idea generation through a collaborative mind-mapping tool| year=2009| pages=139&#8211;148| accessdate=1 November 2013}}&lt;/ref&gt;

==Tools==
[[List of concept- and mind-mapping software|Mind-mapping software]] can be used to organize large amounts of information, combining spatial organization, dynamic hierarchical structuring and node folding. Software packages can extend the concept of mind-mapping by allowing individuals to map more than thoughts and ideas with information on their computers and the Internet, like spreadsheets, documents, Internet sites and images.&lt;ref&gt;{{cite web|url=http://www.imdevin.com/top-10-totally-free-mind-mapping-software-tools/|title=Top 10 Totally Free Mind Mapping Software Tools|last=Santos|first=Devin|date=15 February 2013|publisher=IMDevin|accessdate=10 July 2013}}&lt;/ref&gt; It has been suggested that mind-mapping can improve learning/study efficiency up to 15% over conventional [[note-taking]].&lt;ref&gt;
{{cite journal
  | last = Farrand
  | first = Paul |author2=Hussain, Fearzana |author3=Hennessy, Enid
  | title = The efficacy of the 'mind map' study technique
  | journal = Medical Education
  | volume = 36
  | issue = 5
  | pages = 426&#8211;431
  | date = May 2002
  | doi = 10.1046/j.1365-2923.2002.01205.x
  | pmid = 12028392
}}&lt;/ref&gt;

==See also==
{{Portal|Education}}
* [[Brainstorming]]
* [[Graph (discrete mathematics)]]
* [[Idea]]
* [[List of concept mapping and mind mapping software]]
* [[Mental literacy]]
* [[Personal wiki]]

; Related diagrams
* [[Argument map]]
* [[Cognitive map]]
* [[Concept map]]
* [[Nodal organizational structure]]
* [[Radial tree]]
* [[Rhizome (philosophy)]]
* [[Semantic network]]
* [[Social map]]
* [[Spider mapping]]
* [[Tree structure]]

==References==
{{reflist|30em}}

==Further reading==
* {{cite journal |last= Novak |first= J.D. |date= 1993 |title= How do we learn our lesson?: Taking students through the process |journal= [[The Science Teacher]] |volume= 60 |number= 3 |pages= 50&#8211;55 |issn= 0036-8555 }}

==External links==
*{{Commons category-inline|Mind maps}}

{{Mindmaps}}

{{Authority control}}

{{DEFAULTSORT:Mind Map}}
[[Category:Articles with inconsistent citation formats]]
[[Category:Knowledge representation]]
[[Category:Games of mental skill]]
[[Category:Creativity]]
[[Category:Design]]
[[Category:Educational technology]]
[[Category:Diagrams]]
[[Category:Thought]]
[[Category:Note-taking]]
[[Category:Reading (process)]]
[[Category:Zoomable user interfaces]]
[[Category:Educational devices]]</text>
      <sha1>dvtizr68kj2j8s8ygnq0ih5v6uat3uh</sha1>
    </revision>
  </page>
  <page>
    <title>UMBEL</title>
    <ns>0</ns>
    <id>48794339</id>
    <revision>
      <id>749240358</id>
      <parentid>749239198</parentid>
      <timestamp>2016-11-13T06:55:41Z</timestamp>
      <contributor>
        <username>Materialscientist</username>
        <id>7852030</id>
      </contributor>
      <minor />
      <comment>Reverted edits by [[Special:Contribs/208.54.32.253|208.54.32.253]] ([[User talk:208.54.32.253|talk]]) to last version by Mkbergman</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8169" xml:space="preserve">{{for|the part of a plant|Umbel}}
{{Paid contributions|date=December 2015}}
{{Infobox software
 |name = UMBEL
 |logo =
 |screenshot =
 |caption = Upper Mapping and Binding Exchange Layer
 |developer =  Structured Dynamics
 |released = 16 July 2008
 |latest_release_version =  UMBEL 1.50
 |latest_release_date = 10 May 2016&lt;ref&gt;{{cite web |url=http://umbel.org/resources/news/new-major-upgrade-of-umbel-released-version-1.50/
 |title=New, Major Upgrade of UMBEL Released: version 1.50 |publisher=UMBEL Web site |date=11 May 2016}}&lt;/ref&gt;
 |operating_system =
 |genre = {{Flatlist|
* [[Ontology (information science)|Ontology]]
* [[Semantic Web]]
* [[Linked Data]]
* [[Artificial intelligence]]
}}
 |programming language = {{Flatlist|
* [[Web Ontology Language|OWL 2]]
* [[SKOS]]
}}
 |license = [[Creative Commons licenses|Creative Commons Attribution 3.0]]
 |website = {{URL|umbel.org}}
}}
'''UMBEL''' ('''U'''pper '''M'''apping and '''B'''inding '''E'''xchange '''L'''ayer) is a logically organized knowledge [[Graph (discrete mathematics)|graph]] of 34,000 concepts and entity types that can be used in [[information science]] for relating information from disparate sources to one another. The [[Symbol grounding problem|grounding]] of this information occurs by common reference to the permanent [[Uniform Resource Identifier|URIs]] for the UMBEL concepts; the connections within the UMBEL [[Upper ontology (information science)|upper ontology]] enable concepts from sources at different levels of abstraction or specificity to be logically related. Since UMBEL is an [[open source]] extract of the [[Cyc#OpenCyc|OpenCyc]] [[knowledge base]], it can also take advantage of the [[Inference engine|reasoning capabilities]] within [[Cyc]].

UMBEL has two means to promote the [[semantic interoperability]] of information:.&lt;ref&gt;{{cite web
 | url = http://techwiki.umbel.org/index.php/UMBEL_Specification
 | title = UMBEL (Upper Mapping and Binding Exchange Layer)
 | website = UMBEL Web Site
 | access-date = 8 February 2016
}}&lt;/ref&gt; It is:

* An [[ontology (information science)|ontology]] of about 35,000 reference concepts, designed to provide common [[Data mapping|mapping]] points for relating different ontologies or [[Conceptual schema|schema]] to one another, and
* A [[Controlled vocabulary|vocabulary]] for aiding that ontology mapping, including expressions of likelihood relationships distinct from exact identity or equivalence. This vocabulary is also designed for interoperable [[Ontology (information science)#Domain ontologies and upper ontologies|domain ontologies]].

[[File:LOD Cloud 2014.svg|thumb|400px|Diagram showing [[Linking Open Data]] datasets.  UMBEL is near the hub, below and to the right of the central DBpedia.]]

UMBEL is written in the [[Semantic Web]] languages of [[SKOS]] and [[Web Ontology Language|OWL 2]]. It is a [[Class (set theory)|class]] structure used in [[Linked Data]], along with OpenCyc, [[Yago (database)|YAGO]], and the [[DBpedia]] ontology. Besides data integration, UMBEL has been used to aid concept search,&lt;ref&gt;{{cite book
|last1= Sah
|first1= M
|last2= Wade
|first2= V
|year= 2012
|chapter= A novel concept-based search for the web of data using UMBEL and a fuzzy retrieval model
|chapter-url= https://www.researchgate.net/profile/Melike_Sah/publication/262218017_A_novel_concept-based_search_for_the_web_of_data_using_UMBEL_and_a_fuzzy_retrieval_model/links/555d9c5808ae8c0cab2ad795.pdf
|title= The Semantic Web: Research and Applications
|publisher= Springer
|location= Berlin Heidelberg
|publication-date= 27 May 2012
|pages= 103&#8211;118
}}&lt;/ref&gt;&lt;ref&gt;{{cite book
|last1= Sah
|first1= M
|last2= Wade
|first2= V
|year= 2013
|chapter= Personalized concept-based search and exploration on the web of data using results categorization
|chapter-url= http://eswc-conferences.org/sites/default/files/papers2013/sah.pdf
|title= The Semantic Web: Semantics and Big Data
|publisher= Springer
|location= Berlin Heidelberg
|publication-date= 26 May 2013
|pages= 532&#8211;547
}}&lt;/ref&gt; concept definitions,&lt;ref&gt;{{cite book
|last1= Ballatore
|first1= Andrea
|editor1-first= Harlan
|editor1-last= Onsrud
|editor2-first= Werner
|editor2-last= Kuhn
|year= 2016
|chapter= Prolegomena for an Ontology of Place
|chapter-url= http://eprints.cdlib.org/uc/item/0rw1n045.pdf
|title= Advancing Geographic Information Systems
|publisher= GSDI Association Press
|location= Needham, MA
|pages= 91&#8211;103
}}&lt;/ref&gt; [[Ranking (information retrieval)|query ranking]],&lt;ref&gt;{{cite journal
| last       = Stecher
| first      = R
| last2      = Costache
| first2     = S
| last3      = Nieder&#233;e
| first3     = C
| last4      = Nejdl
| first4     = W
| date       = 7 Jun 2010
| title      = Query ranking in information integration
| url        = https://www.researchgate.net/profile/Stefania_Costache2/publication/220921101_Query_Ranking_in_Information_Integration/links/00b7d516d17814201b000000.pdf
| journal    = Advanced Information Systems Engineering
| publisher  = Springer Berlin Heidelberg
| pages      = 230&#8211;235
}}&lt;/ref&gt; ontology integration,&lt;ref&gt;{{cite book
|last1= Damova
|first1= M
|last2= et
|first2= al.
|year= 2012
|editor1-first= Maria Teresa
|editor1-last= Pazienza
|chapter= Creation and Integration of Reference Ontologies for Ef&#64257;cient LOD Management
|url= http://www.igi-global.com/book/semi-automatic-ontology-development/58294
|title= Semi-Automatic Ontology Development: Processes and Resources
|publisher= IGI Global
|pages= 162&#8211;199
}}&lt;/ref&gt; and ontology consistency checking.&lt;ref&gt;{{cite journal
| last       = Sheng
| first      = Z
| last2      = Wang
| first2     = X
| last3      = Shi
| first3     = H
| last4      = Feng
| first4     = Z
| date       = 26 Oct 2012
| title      = Checking and handling inconsistency of DBpedia
| url        = http://link.springer.com/chapter/10.1007/978-3-642-33469-6_60
| journal    = Web Information Systems and Mining
| publisher  = Springer Berlin Heidelberg
| pages      = 480&#8211;488
}}&lt;/ref&gt; It has also been used to build large ontologies &lt;ref&gt;{{cite journal
| last       = Yablonsky
| first      = S
| date       = Jun 2009
| title      = Semantic Web framework for development of very large ontologies
| url        = http://www.scielo.org.mx/pdf/poli/n39/n39a4.pdf
| journal    = Polibits
| issue      = 39
| pages      = 19&#8211;26
}}&lt;/ref&gt; and for online [[question answering]] systems.&lt;ref&gt;{{cite journal
| last       = Bishop
| first      = B
| last2      = et
| first2     = al.
| date       = Jan 2011
| title      = Factforge: A fast track to the web of data
| url        = http://www.semantic-web-journal.net/sites/default/files/swj77_2.pdf
| journal    = Semantic Web
| volume     = 2
| issue      = 2
| pages      = 157&#8211;166
}}&lt;/ref&gt;

Including OpenCyc, UMBEL has about 65,000 formal mappings to [[DBpedia]], PROTON, [[GeoNames]], and [[schema.org]], and provides linkages to more than 2 million [[Wikipedia]] pages (English version). All of its reference concepts and mappings are organized under a hierarchy of 31 different "super types",&lt;ref&gt;See [http://techwiki.umbel.org/index.php/UMBEL_-_Annex_G Annex G] in the UMBEL specifications.&lt;/ref&gt; which are mostly disjoint from one another. Each of these "super types" has its own typology of entity classes to provide flexible tie-ins for external content. 90% of UMBEL is contained in these entity classes.

UMBEL was first released in July 2008. Version 1.00 was released in February 2011.&lt;ref&gt;See the [http://umbel.org/resources/news/finally-umbel-v-100] release announcement.&lt;/ref&gt; Its current release is version 1.50.&lt;ref&gt;See http://umbel.org/resources/news/new-major-upgrade-of-umbel-released-version-1.50/&lt;/ref&gt;

==See also==
* [[Cyc]]
* [[DBpedia]]

==Notes==
{{Reflist}}

==External links==
* [http://umbel.org/ Main page for UMBEL]
* [http://techwiki.umbel.org/index.php/UMBEL_Specification UMBEL specification], and its accompanying [http://umbel.org/specifications/annexes Annexes A - L, Z]

{{Semantic Web}}
{{Use dmy dates|date=October 2012}}

{{DEFAULTSORT:Umbel}}
[[Category:Knowledge representation]]
[[Category:Ontology (information science)]]
[[Category:Semantic Web]]
[[Category:Knowledge bases]]</text>
      <sha1>2xjq8tmjglxzpkeh7nzn5veqsxityyz</sha1>
    </revision>
  </page>
  <page>
    <title>Consistency (knowledge bases)</title>
    <ns>0</ns>
    <id>25154802</id>
    <revision>
      <id>731053736</id>
      <parentid>347480606</parentid>
      <timestamp>2016-07-22T16:50:53Z</timestamp>
      <contributor>
        <username>Yaron K.</username>
        <id>2276977</id>
      </contributor>
      <comment>Changed category back to "Knowledge representation" - nothing web-related here</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="508" xml:space="preserve">A [[knowledge base]] KB is '''consistent''' ''[[iff]]'' its negation is not a [[Tautology (logic)|tautology]].

I.e., a knowledge base KB is inconsistent (not consistent) [[iff]] there is no [[Interpretation (logic)|interpretation]] which [[entailment|entails]] KB.

Example of an inconsistent knowledge base:

KB := { a, &#172;a }

Consistency in terms of knowledge bases is mostly the same as the natural understanding of [[consistency]].

[[Category:Knowledge representation]]
{{logic-stub}}
{{database-stub}}</text>
      <sha1>dtbhlzru95r0t4mdp4ysewf5in0k6gq</sha1>
    </revision>
  </page>
  <page>
    <title>Library branch</title>
    <ns>0</ns>
    <id>52141053</id>
    <revision>
      <id>758403356</id>
      <parentid>747981181</parentid>
      <timestamp>2017-01-05T05:55:16Z</timestamp>
      <contributor>
        <username>Level C</username>
        <id>18054835</id>
      </contributor>
      <comment>/* See also */  links already occur in the article text - is against the Manual of Style to have them in See also section</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1758" xml:space="preserve">[[File:New York Public Library May 2011.JPG|200 px|thumb|right|The [[New York Public Library Main Branch]] in [[Bryant Park]], [[Manhattan]]]]
'''Library branches''' are libraries that form part of a [[library system]] but are not located in the same area, building or city, but use the same [[Library classification]] for their catalogs and are interconnected with all the branches of the system that form part of the systems and to library patrons through a [[integrated library system]].&lt;ref&gt;{{cite web|url=http://www.merriam-webster.com/dictionary/branch |title=Branch &amp;#124; Definition of Branch by Merriam-Webster |website=Merriam-webster.com |date= |accessdate=2016-11-05}}&lt;/ref&gt;

Most of [[County|counties]] of every country have their own [[library system]] that usually have between to 20 libraries on every city of their counties, some of them are; London Public Library (on Canada) with 16 library branches, [[Helsinki Metropolitan Area Libraries]] with 63 libraries,&lt;ref&gt;{{cite web| url=http://www.iii.com/news/pr_display.php?id=559 | title=Helsinki Metropolitan Area Libraries (Finland) Upgrades to Sierra Services Platform | publisher=Innovative | type= Press release | date=5 February 2013 | accessdate=1 August 2014 }}&lt;/ref&gt; [[National Library of Venezuela]] with 685 branches.

Some popular library branches includ [[New York Public Library Main Branch]], part of [[New York Public Library|New York Public Library System]], and [[Martin Luther King Jr. Memorial Library]], a branch of [[District of Columbia Public Library|District of Columbia Public Library System]].

==References==
{{Reflist}}

[[Category:Public libraries]]
[[Category:Private libraries]]
[[Category:Libraries]]
[[Category:Culture]]
[[Category:Knowledge representation]]</text>
      <sha1>9ncejnd70o9liod61zzgve0rd8n4xvu</sha1>
    </revision>
  </page>
  </mediawiki>